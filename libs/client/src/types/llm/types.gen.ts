// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: "https://queue.fal.run" | (string & {});
};

/**
 * OutputModel
 */
export type VideoPromptGeneratorOutput = {
  /**
   * Prompt
   *
   * Generated video prompt
   */
  prompt: string;
};

/**
 * InputModel
 */
export type VideoPromptGeneratorInput = {
  /**
   * Custom Elements
   *
   * Custom technical elements (optional)
   */
  custom_elements?: string;
  /**
   * Style
   *
   * Style of the video prompt
   */
  style?:
    | "Minimalist"
    | "Simple"
    | "Detailed"
    | "Descriptive"
    | "Dynamic"
    | "Cinematic"
    | "Documentary"
    | "Animation"
    | "Action"
    | "Experimental";
  /**
   * Camera Direction
   *
   * Camera direction
   */
  camera_direction?:
    | "None"
    | "Zoom in"
    | "Zoom out"
    | "Pan left"
    | "Pan right"
    | "Tilt up"
    | "Tilt down"
    | "Orbital rotation"
    | "Push in"
    | "Pull out"
    | "Track forward"
    | "Track backward"
    | "Spiral in"
    | "Spiral out"
    | "Arc movement"
    | "Diagonal traverse"
    | "Vertical rise"
    | "Vertical descent";
  /**
   * Pacing
   *
   * Pacing rhythm
   */
  pacing?:
    | "None"
    | "Slow burn"
    | "Rhythmic pulse"
    | "Frantic energy"
    | "Ebb and flow"
    | "Hypnotic drift"
    | "Time-lapse rush"
    | "Stop-motion staccato"
    | "Gradual build"
    | "Quick cut rhythm"
    | "Long take meditation"
    | "Jump cut energy"
    | "Match cut flow"
    | "Cross-dissolve dreamscape"
    | "Parallel action"
    | "Slow motion impact"
    | "Ramping dynamics"
    | "Montage tempo"
    | "Continuous flow"
    | "Episodic breaks";
  /**
   * Special Effects
   *
   * Special effects approach
   */
  special_effects?:
    | "None"
    | "Practical effects"
    | "CGI enhancement"
    | "Analog glitches"
    | "Light painting"
    | "Projection mapping"
    | "Nanosecond exposures"
    | "Double exposure"
    | "Smoke diffusion"
    | "Lens flare artistry"
    | "Particle systems"
    | "Holographic overlay"
    | "Chromatic aberration"
    | "Digital distortion"
    | "Wire removal"
    | "Motion capture"
    | "Miniature integration"
    | "Weather simulation"
    | "Color grading"
    | "Mixed media composite"
    | "Neural style transfer";
  /**
   * Image Url
   *
   * URL of an image to analyze and incorporate into the video prompt (optional)
   */
  image_url?: string;
  /**
   * Model
   *
   * Model to use
   */
  model?:
    | "anthropic/claude-3.5-sonnet"
    | "anthropic/claude-3-5-haiku"
    | "anthropic/claude-3-haiku"
    | "google/gemini-2.5-flash-lite"
    | "google/gemini-2.0-flash-001"
    | "meta-llama/llama-3.2-1b-instruct"
    | "meta-llama/llama-3.2-3b-instruct"
    | "meta-llama/llama-3.1-8b-instruct"
    | "meta-llama/llama-3.1-70b-instruct"
    | "openai/gpt-4o-mini"
    | "openai/gpt-4o"
    | "deepseek/deepseek-r1";
  /**
   * Camera Style
   *
   * Camera movement style
   */
  camera_style?:
    | "None"
    | "Steadicam flow"
    | "Drone aerials"
    | "Handheld urgency"
    | "Crane elegance"
    | "Dolly precision"
    | "VR 360"
    | "Multi-angle rig"
    | "Static tripod"
    | "Gimbal smoothness"
    | "Slider motion"
    | "Jib sweep"
    | "POV immersion"
    | "Time-slice array"
    | "Macro extreme"
    | "Tilt-shift miniature"
    | "Snorricam character"
    | "Whip pan dynamics"
    | "Dutch angle tension"
    | "Underwater housing"
    | "Periscope lens";
  /**
   * Input Concept
   *
   * Core concept or thematic input for the video prompt
   */
  input_concept: string;
  /**
   * Prompt Length
   *
   * Length of the prompt
   */
  prompt_length?: "Short" | "Medium" | "Long";
};

/**
 * Qwen3GuardOutput
 */
export type Qwen3GuardOutput = {
  /**
   * Categories
   *
   * The confidence score of the classification
   */
  categories: Array<
    | "Violent"
    | "Non-violent Illegal Acts"
    | "Sexual Content or Sexual Acts"
    | "PII"
    | "Suicide & Self-Harm"
    | "Unethical Acts"
    | "Politically Sensitive Topics"
    | "Copyright Violation"
    | "Jailbreak"
    | "None"
  >;
  /**
   * Label
   *
   * The classification label
   */
  label: "Safe" | "Unsafe" | "Controversial";
};

/**
 * Qwen3GuardInput
 */
export type Qwen3GuardInput = {
  /**
   * Prompt
   *
   * The input text to be classified
   */
  prompt: string;
};

/**
 * Schema referenced but not defined by fal.ai (missing from source OpenAPI spec)
 */
export type RouterOpenaiV1ChatCompletionsInput = {
  [key: string]: unknown;
};

export type RouterOpenaiV1ChatCompletionsOutput = unknown;

/**
 * UsageInfo
 */
export type UsageInfo = {
  /**
   * Completion Tokens
   */
  completion_tokens?: number | unknown;
  /**
   * Total Tokens
   */
  total_tokens?: number;
  /**
   * Prompt Tokens
   */
  prompt_tokens?: number | unknown;
  /**
   * Cost
   */
  cost: number;
};

/**
 * ChatOutput
 */
export type RouterOutput = {
  /**
   * Token usage information
   */
  usage?: UsageInfo | unknown;
  /**
   * Error
   *
   * Error message if an error occurred
   */
  error?: string | unknown;
  /**
   * Partial
   *
   * Whether the output is partial
   */
  partial?: boolean;
  /**
   * Reasoning
   *
   * Generated reasoning for the final answer
   */
  reasoning?: string | unknown;
  /**
   * Output
   *
   * Generated output
   */
  output: string;
};

/**
 * ChatInput
 */
export type RouterInput = {
  /**
   * Model
   *
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Prompt
   *
   * Prompt to be used for the chat completion
   */
  prompt: string;
  /**
   * Max Tokens
   *
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number | unknown;
  /**
   * Temperature
   *
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
   */
  temperature?: number;
  /**
   * Reasoning
   *
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * System Prompt
   *
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string | unknown;
};

/**
 * Schema referenced but not defined by fal.ai (missing from source OpenAPI spec)
 */
export type RouterOpenaiV1EmbeddingsInput = {
  [key: string]: unknown;
};

export type RouterOpenaiV1EmbeddingsOutput = unknown;

/**
 * Schema referenced but not defined by fal.ai (missing from source OpenAPI spec)
 */
export type RouterOpenaiV1ResponsesInput = {
  [key: string]: unknown;
};

export type RouterOpenaiV1ResponsesOutput = unknown;

export type QueueStatus = {
  status: "IN_QUEUE" | "IN_PROGRESS" | "COMPLETED";
  /**
   * The request id.
   */
  request_id: string;
  /**
   * The response url.
   */
  response_url?: string;
  /**
   * The status url.
   */
  status_url?: string;
  /**
   * The cancel url.
   */
  cancel_url?: string;
  /**
   * The logs.
   */
  logs?: {
    [key: string]: unknown;
  };
  /**
   * The metrics.
   */
  metrics?: {
    [key: string]: unknown;
  };
  /**
   * The queue position.
   */
  queue_position?: number;
};

export type GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/openrouter/router/openai/v1/responses/requests/{request_id}/status";
  };

export type GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdStatusResponse =
  GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdStatusResponses[keyof GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdStatusResponses];

export type PutOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/openrouter/router/openai/v1/responses/requests/{request_id}/cancel";
  };

export type PutOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdCancelResponse =
  PutOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdCancelResponses[keyof PutOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdCancelResponses];

export type PostOpenrouterRouterOpenaiV1ResponsesData = {
  body: RouterOpenaiV1ResponsesInput;
  path?: never;
  query?: never;
  url: "/openrouter/router/openai/v1/responses";
};

export type PostOpenrouterRouterOpenaiV1ResponsesResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostOpenrouterRouterOpenaiV1ResponsesResponse =
  PostOpenrouterRouterOpenaiV1ResponsesResponses[keyof PostOpenrouterRouterOpenaiV1ResponsesResponses];

export type GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/openrouter/router/openai/v1/responses/requests/{request_id}";
};

export type GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: RouterOpenaiV1ResponsesOutput;
};

export type GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdResponse =
  GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdResponses[keyof GetOpenrouterRouterOpenaiV1ResponsesRequestsByRequestIdResponses];

export type GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/openrouter/router/openai/v1/embeddings/requests/{request_id}/status";
  };

export type GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdStatusResponse =
  GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdStatusResponses[keyof GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdStatusResponses];

export type PutOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/openrouter/router/openai/v1/embeddings/requests/{request_id}/cancel";
  };

export type PutOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdCancelResponse =
  PutOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdCancelResponses[keyof PutOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdCancelResponses];

export type PostOpenrouterRouterOpenaiV1EmbeddingsData = {
  body: RouterOpenaiV1EmbeddingsInput;
  path?: never;
  query?: never;
  url: "/openrouter/router/openai/v1/embeddings";
};

export type PostOpenrouterRouterOpenaiV1EmbeddingsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostOpenrouterRouterOpenaiV1EmbeddingsResponse =
  PostOpenrouterRouterOpenaiV1EmbeddingsResponses[keyof PostOpenrouterRouterOpenaiV1EmbeddingsResponses];

export type GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/openrouter/router/openai/v1/embeddings/requests/{request_id}";
};

export type GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: RouterOpenaiV1EmbeddingsOutput;
  };

export type GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdResponse =
  GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdResponses[keyof GetOpenrouterRouterOpenaiV1EmbeddingsRequestsByRequestIdResponses];

export type GetOpenrouterRouterRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/openrouter/router/requests/{request_id}/status";
};

export type GetOpenrouterRouterRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetOpenrouterRouterRequestsByRequestIdStatusResponse =
  GetOpenrouterRouterRequestsByRequestIdStatusResponses[keyof GetOpenrouterRouterRequestsByRequestIdStatusResponses];

export type PutOpenrouterRouterRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/openrouter/router/requests/{request_id}/cancel";
};

export type PutOpenrouterRouterRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutOpenrouterRouterRequestsByRequestIdCancelResponse =
  PutOpenrouterRouterRequestsByRequestIdCancelResponses[keyof PutOpenrouterRouterRequestsByRequestIdCancelResponses];

export type PostOpenrouterRouterData = {
  body: RouterInput;
  path?: never;
  query?: never;
  url: "/openrouter/router";
};

export type PostOpenrouterRouterResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostOpenrouterRouterResponse =
  PostOpenrouterRouterResponses[keyof PostOpenrouterRouterResponses];

export type GetOpenrouterRouterRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/openrouter/router/requests/{request_id}";
};

export type GetOpenrouterRouterRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: RouterOutput;
};

export type GetOpenrouterRouterRequestsByRequestIdResponse =
  GetOpenrouterRouterRequestsByRequestIdResponses[keyof GetOpenrouterRouterRequestsByRequestIdResponses];

export type GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/openrouter/router/openai/v1/chat/completions/requests/{request_id}/status";
  };

export type GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdStatusResponse =
  GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdStatusResponses[keyof GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdStatusResponses];

export type PutOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/openrouter/router/openai/v1/chat/completions/requests/{request_id}/cancel";
  };

export type PutOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdCancelResponse =
  PutOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdCancelResponses[keyof PutOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdCancelResponses];

export type PostOpenrouterRouterOpenaiV1ChatCompletionsData = {
  body: RouterOpenaiV1ChatCompletionsInput;
  path?: never;
  query?: never;
  url: "/openrouter/router/openai/v1/chat/completions";
};

export type PostOpenrouterRouterOpenaiV1ChatCompletionsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostOpenrouterRouterOpenaiV1ChatCompletionsResponse =
  PostOpenrouterRouterOpenaiV1ChatCompletionsResponses[keyof PostOpenrouterRouterOpenaiV1ChatCompletionsResponses];

export type GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/openrouter/router/openai/v1/chat/completions/requests/{request_id}";
  };

export type GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: RouterOpenaiV1ChatCompletionsOutput;
  };

export type GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdResponse =
  GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdResponses[keyof GetOpenrouterRouterOpenaiV1ChatCompletionsRequestsByRequestIdResponses];

export type GetFalAiQwen3GuardRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/qwen-3-guard/requests/{request_id}/status";
};

export type GetFalAiQwen3GuardRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiQwen3GuardRequestsByRequestIdStatusResponse =
  GetFalAiQwen3GuardRequestsByRequestIdStatusResponses[keyof GetFalAiQwen3GuardRequestsByRequestIdStatusResponses];

export type PutFalAiQwen3GuardRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-3-guard/requests/{request_id}/cancel";
};

export type PutFalAiQwen3GuardRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiQwen3GuardRequestsByRequestIdCancelResponse =
  PutFalAiQwen3GuardRequestsByRequestIdCancelResponses[keyof PutFalAiQwen3GuardRequestsByRequestIdCancelResponses];

export type PostFalAiQwen3GuardData = {
  body: Qwen3GuardInput;
  path?: never;
  query?: never;
  url: "/fal-ai/qwen-3-guard";
};

export type PostFalAiQwen3GuardResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiQwen3GuardResponse =
  PostFalAiQwen3GuardResponses[keyof PostFalAiQwen3GuardResponses];

export type GetFalAiQwen3GuardRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-3-guard/requests/{request_id}";
};

export type GetFalAiQwen3GuardRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Qwen3GuardOutput;
};

export type GetFalAiQwen3GuardRequestsByRequestIdResponse =
  GetFalAiQwen3GuardRequestsByRequestIdResponses[keyof GetFalAiQwen3GuardRequestsByRequestIdResponses];

export type GetFalAiVideoPromptGeneratorRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/video-prompt-generator/requests/{request_id}/status";
};

export type GetFalAiVideoPromptGeneratorRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVideoPromptGeneratorRequestsByRequestIdStatusResponse =
  GetFalAiVideoPromptGeneratorRequestsByRequestIdStatusResponses[keyof GetFalAiVideoPromptGeneratorRequestsByRequestIdStatusResponses];

export type PutFalAiVideoPromptGeneratorRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/video-prompt-generator/requests/{request_id}/cancel";
};

export type PutFalAiVideoPromptGeneratorRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVideoPromptGeneratorRequestsByRequestIdCancelResponse =
  PutFalAiVideoPromptGeneratorRequestsByRequestIdCancelResponses[keyof PutFalAiVideoPromptGeneratorRequestsByRequestIdCancelResponses];

export type PostFalAiVideoPromptGeneratorData = {
  body: VideoPromptGeneratorInput;
  path?: never;
  query?: never;
  url: "/fal-ai/video-prompt-generator";
};

export type PostFalAiVideoPromptGeneratorResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVideoPromptGeneratorResponse =
  PostFalAiVideoPromptGeneratorResponses[keyof PostFalAiVideoPromptGeneratorResponses];

export type GetFalAiVideoPromptGeneratorRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/video-prompt-generator/requests/{request_id}";
};

export type GetFalAiVideoPromptGeneratorRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoPromptGeneratorOutput;
};

export type GetFalAiVideoPromptGeneratorRequestsByRequestIdResponse =
  GetFalAiVideoPromptGeneratorRequestsByRequestIdResponses[keyof GetFalAiVideoPromptGeneratorRequestsByRequestIdResponses];
