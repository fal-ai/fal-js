export type Aesthetics = {
  /**
   * The composition of the image to be generated.
   */
  composition?: string;
  /**
   * The color scheme of the image to be generated.
   */
  color_scheme?: string;
  /**
   * The mood and atmosphere of the image to be generated.
   */
  mood_atmosphere?: string;
};
export type AspectRatio = {
  /**
   * Aspect ratio for 4K resolution output Default value: `"1:1"`
   */
  ratio?: "1:1" | "16:9" | "9:16" | "4:3" | "3:4";
};
export type Audio = {
  /**
   * Type of media (always 'audio') Default value: `"audio"`
   */
  media_type?: string;
  /**
   * URL where the media file can be accessed
   */
  url: string;
  /**
   * MIME type of the media file
   */
  content_type: string;
  /**
   * Original filename of the media
   */
  file_name: string;
  /**
   * Size of the file in bytes
   */
  file_size: number;
  /**
   * Duration of the media in seconds
   */
  duration: number;
  /**
   * Overall bitrate of the media in bits per second
   */
  bitrate: number;
  /**
   * Codec used to encode the media
   */
  codec: string;
  /**
   * Container format of the media file (e.g., 'mp4', 'mov')
   */
  container: string;
  /**
   * Number of audio channels
   */
  channels: number;
  /**
   * Audio sample rate in Hz
   */
  sample_rate: number;
};
export type AudioFile = {
  /**
   * URL of the audio file
   */
  url: string;
  /**
   * Content type of the audio file
   */
  content_type: string;
  /**
   * Name of the audio file
   */
  file_name: string;
  /**
   * Size of the audio file in bytes
   */
  file_size: number;
};
export type AudioSetting = {
  /**
   * Sample rate of generated audio Default value: `"32000"`
   */
  sample_rate?: "8000" | "16000" | "22050" | "24000" | "32000" | "44100";
  /**
   * Bitrate of generated audio Default value: `"128000"`
   */
  bitrate?: "32000" | "64000" | "128000" | "256000";
  /**
   * Audio format Default value: `"mp3"`
   */
  format?: "mp3" | "pcm" | "flac";
  /**
   * Number of audio channels (1=mono, 2=stereo) Default value: `"1"`
   */
  channel?: "1" | "2";
};
export type AudioTimeSpan = {
  /**
   * Start time of the span in seconds
   */
  start: number;
  /**
   * End time of the span in seconds
   */
  end: number;
  /**
   * Whether to include (True) or exclude (False) sounds in this span Default value: `true`
   */
  include?: boolean;
};
export type AudioTrack = {
  /**
   * Audio codec used (e.g., 'aac', 'mp3')
   */
  codec: string;
  /**
   * Number of audio channels
   */
  channels: number;
  /**
   * Audio sample rate in Hz
   */
  sample_rate: number;
  /**
   * Audio bitrate in bits per second
   */
  bitrate: number;
};
export type BBoxPromptBase = {
  /**
   * X Min Coordinate of the box (0-1)
   */
  x_min?: number;
  /**
   * Y Min Coordinate of the box (0-1)
   */
  y_min?: number;
  /**
   * X Max Coordinate of the prompt (0-1)
   */
  x_max?: number;
  /**
   * Y Max Coordinate of the prompt (0-1)
   */
  y_max?: number;
};
export type BoundingBox = {
  /**
   * X-coordinate of the top-left corner
   */
  x: number;
  /**
   * Y-coordinate of the top-left corner
   */
  y: number;
  /**
   * Width of the bounding box
   */
  w: number;
  /**
   * Height of the bounding box
   */
  h: number;
  /**
   * Label of the bounding box
   */
  label: string;
};
export type BoxPrompt = {
  /**
   * X Min Coordinate of the box
   */
  x_min?: number;
  /**
   * Y Min Coordinate of the box
   */
  y_min?: number;
  /**
   * X Max Coordinate of the prompt
   */
  x_max?: number;
  /**
   * Y Max Coordinate of the prompt
   */
  y_max?: number;
  /**
   * The frame index to interact with.
   */
  frame_index?: number;
};
export type BoxPromptBase = {
  /**
   * X Min Coordinate of the box
   */
  x_min?: number;
  /**
   * Y Min Coordinate of the box
   */
  y_min?: number;
  /**
   * X Max Coordinate of the prompt
   */
  x_max?: number;
  /**
   * Y Max Coordinate of the prompt
   */
  y_max?: number;
};
export type BriaFiboVlmAesthetics = {
  /**
   * The composition of the image to be generated.
   */
  composition?: string;
  /**
   * The color scheme of the image to be generated.
   */
  color_scheme?: string;
  /**
   * The mood and atmosphere of the image to be generated.
   */
  mood_atmosphere?: string;
  /**
   * The aesthetic score of the image.
   */
  aesthetic_score: string;
  /**
   * The preference score of the image.
   */
  preference_score: string;
};
export type BriaFiboVlmStructuredprompt = {
  /**
   * A short description of the image to be generated.
   */
  short_description?: string;
  /**
   * A list of objects in the image to be generated, along with their attributes and relationships to other objects in the image.
   */
  objects?: Array<PromptObject>;
  /**
   * The background setting of the image to be generated.
   */
  background_setting?: string;
  /**
   * The lighting of the image to be generated.
   */
  lighting?: Lighting;
  /**
   * The aesthetics of the image to be generated.
   */
  aesthetics?: BriaFiboVlmAesthetics;
  /**
   * The photographic characteristics of the image to be generated.
   */
  photographic_characteristics?: PhotographicCharacteristics;
  /**
   * The style medium of the image to be generated.
   */
  style_medium?: string;
  /**
   * A list of text to be rendered in the image.
   */
  text_render?: Array<void>;
  /**
   * The context of the image to be generated.
   */
  context?: string;
  /**
   * The artistic style of the image to be generated.
   */
  artistic_style?: string;
};
export type CameraControl = {
  /**
   * The type of camera movement
   */
  movement_type: "horizontal" | "vertical" | "pan" | "tilt" | "roll" | "zoom";
  /**
   * The value of the camera movement
   */
  movement_value: number;
};
export type ChronoLoraWeight = {
  /**
   * URL or path to the LoRA weights (Safetensors).
   */
  path: string;
  /**
   * Scale factor controlling LoRA strength. Default value: `1`
   */
  scale?: number;
};
export type ColorPalette = {
  /**
   * A list of color palette members that define the color palette
   */
  members?: Array<ColorPaletteMember>;
  /**
   * A color palette preset value
   */
  name?:
    | "EMBER"
    | "FRESH"
    | "JUNGLE"
    | "MAGIC"
    | "MELON"
    | "MOSAIC"
    | "PASTEL"
    | "ULTRAMARINE";
};
export type ColorPaletteMember = {
  /**
   * RGB color value for the palette member
   */
  rgb: RGBColor;
  /**
   * The weight of the color in the color palette Default value: `0.5`
   */
  color_weight?: number;
};
export type CompletionUsage = {
  /**
   * Number of tokens in the completion
   */
  completion_tokens: number;
  /**
   * Number of tokens in the prompt
   */
  prompt_tokens: number;
  /**
   * Total tokens used
   */
  total_tokens: number;
};
export type ControlLoraWeight = {
  /**
   * URL or the path to the LoRA weights.
   */
  path: string;
  /**
   * The scale of the LoRA weight. This is used to scale the LoRA weight
   * before merging it with the base model. Providing a dictionary as {"layer_name":layer_scale} allows per-layer lora scale settings. Layers with no scale provided will have scale 1.0. Default value: `1`
   */
  scale?: any | number;
  /**
   * URL of the image to be used as the control image.
   */
  control_image_url: string | Blob | File;
  /**
   * Type of preprocessing to apply to the input image. Default value: `"None"`
   */
  preprocess?: "canny" | "depth" | "None";
};
export type ControlNet = {
  /**
   * URL or the path to the control net weights.
   */
  path: string;
  /**
   * optional URL to the controlnet config.json file.
   */
  config_url?: string | Blob | File;
  /**
   * The optional variant if a Hugging Face repo key is used.
   */
  variant?: string;
  /**
   * URL of the image to be used as the control image.
   */
  control_image_url: string | Blob | File;
  /**
   * URL of the mask for the control image.
   */
  mask_image_url?: string | Blob | File;
  /**
   * Threshold for mask. Default value: `0.5`
   */
  mask_threshold?: number;
  /**
   * The scale of the control net weight. This is used to scale the control net weight
   * before merging it with the base model. Default value: `1`
   */
  conditioning_scale?: number;
  /**
   * The percentage of the image to start applying the controlnet in terms of the total timesteps.
   */
  start_percentage?: number;
  /**
   * The percentage of the image to end applying the controlnet in terms of the total timesteps. Default value: `1`
   */
  end_percentage?: number;
};
export type ControlNetUnion = {
  /**
   * URL or the path to the control net weights.
   */
  path: string;
  /**
   * optional URL to the controlnet config.json file.
   */
  config_url?: string | Blob | File;
  /**
   * The optional variant if a Hugging Face repo key is used.
   */
  variant?: string;
  /**
   * The control images and modes to use for the control net.
   */
  controls: Array<ControlNetUnionInput>;
};
export type DeepFilterNetTimings = {
  /**
   * Preprocessing time.
   */
  preprocess: number;
  /**
   * Inference time.
   */
  inference: number;
  /**
   * Postprocessing time.
   */
  postprocess: number;
};
export type DialogueBlock = {
  /**
   * The dialogue text
   */
  text: string;
  /**
   * The name or the ID of the voice to be used for the generation.
   */
  voice: string;
};
export type DynamicMask = {
  /**
   * URL of the image for Dynamic Brush Application Area (Mask image created by users using the motion brush)
   */
  mask_url: string | Blob | File;
  /**
   * List of trajectories
   */
  trajectories?: Array<Trajectory>;
};
export type EasyControlWeight = {
  /**
   * URL to safetensor weights of control method to be applied. Can also be one of `canny`, `depth`, `hedsketch`, `inpainting`, `pose`, `seg`, `subject`, `ghibli`
   */
  control_method_url: string | Blob | File;
  /**
   * Scale for the control method. Default value: `1`
   */
  scale?: number;
  /**
   * URL of an image to use as a control
   */
  image_url: string | Blob | File;
  /**
   * Control type of the image. Must be one of `spatial` or `subject`.
   */
  image_control_type: "subject" | "spatial";
};
export type EmotionalStrengths = {
  /**
   * Strength of happiness emotion
   */
  happy?: number;
  /**
   * Strength of anger emotion
   */
  angry?: number;
  /**
   * Strength of sadness emotion
   */
  sad?: number;
  /**
   * Strength of fear emotion
   */
  afraid?: number;
  /**
   * Strength of disgust emotion
   */
  disgusted?: number;
  /**
   * Strength of melancholic emotion
   */
  melancholic?: number;
  /**
   * Strength of surprise emotion
   */
  surprised?: number;
  /**
   * Strength of calm emotion
   */
  calm?: number;
};
export type EQBand = {
  /**
   * Center frequency in Hz (20-20000)
   */
  frequency: number;
  /**
   * Bandwidth in Hz Default value: `100`
   */
  width?: number;
  /**
   * Gain in dB (-30 to 30, negative = cut, positive = boost)
   */
  gain: number;
};
export type FaceChoice = {
  /**
   * ID of the face in the video. Returned by the `identify_face` API.
   */
  face_id: string;
  /**
   * Publicly accessible URL to the audio file. Supported formats: .mp3, .wav, .m4a (max 5MB). Duration must be between 2–60 seconds.
   */
  audio_url: string | Blob | File;
  /**
   * Start time (ms) for cropping the source audio. Must be between 0 and the audio duration. The cropped audio must remain at least 2 seconds long.
   */
  sound_start_time: number;
  /**
   * End time (ms) for cropping the source audio. Must be greater than `sound_start_time` and within the original audio duration. The cropped segment must be at least 2 seconds long.
   */
  sound_end_time: number;
  /**
   * Time (ms) at which the cropped audio will be inserted into the video. Must meet both of the following conditions:
   * 1. `sound_insert_time` must be within the duration of the video (it cannot be greater than the total video length).
   * 2. The cropped audio segment must fully fit within the video when inserted — meaning `sound_insert_time + cropped_sound_length` must not exceed the video's total duration.
   *
   * In other words: the insert point must be inside the video, and the inserted audio must not extend past the end of the video.
   */
  sound_insert_time: number;
  /**
   * Volume multiplier for the inserted audio. Range: [0, 2], where 1 = original volume. Default value: `1`
   */
  sound_volume?: number;
  /**
   * Volume multiplier for the video's original audio track. Range: [0, 2]. Has no effect if the source video contains no audio. Default value: `1`
   */
  original_audio_volume?: number;
};
export type FaceData = {
  /**
   * The face id of video. When the same person's face is separated by more than 1 second in the video, it will be considered as different IDs.
   */
  face_id: string;
  /**
   * A schematic diagram of a face captured from a video (URL).
   */
  face_image: string;
  /**
   * This face can be used as the starting time of lip-sync (milliseconds).
   */
  start_time: number;
  /**
   * This face can be used as the ending time of lip-sync (milliseconds). Note: This value has a millisecond level error and will be longer than the actual ending time.
   */
  end_time: number;
};
export type FalAiFlux2KleinLorainput = {
  /**
   * URL, HuggingFace repo ID (owner/repo), or local path to LoRA weights.
   */
  path: string;
  /**
   * Scale factor for LoRA application (0.0 to 4.0). Default value: `1`
   */
  scale?: number;
};
export type FalToolkitImageImageImage = {
  /**
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * The mime type of the file.
   */
  content_type?: string;
  /**
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string;
  /**
   * The size of the file in bytes.
   */
  file_size?: number;
  /**
   * File data
   */
  file_data?: string;
  /**
   * The width of the image in pixels.
   */
  width?: number;
  /**
   * The height of the image in pixels.
   */
  height?: number;
};
export type File = {
  /**
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * The mime type of the file.
   */
  content_type?: string;
  /**
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string;
  /**
   * The size of the file in bytes.
   */
  file_size?: number;
  /**
   * File data
   */
  file_data?: string;
};
export type Image = {
  /**
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * The mime type of the file.
   */
  content_type?: string;
  /**
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string;
  /**
   * The size of the file in bytes.
   */
  file_size?: number;
  /**
   * File data
   */
  file_data?: string;
  /**
   * The width of the image in pixels.
   */
  width?: number;
  /**
   * The height of the image in pixels.
   */
  height?: number;
};
export type ImageCondition = {
  /**
   * The URL of the image to use as input.
   */
  image_url: string | Blob | File;
  /**
   * The frame number to start the condition on.
   */
  start_frame_number?: number;
  /**
   * The strength of the condition. Default value: `1`
   */
  strength?: number;
};
export type ImageFile = {
  /**
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * The mime type of the file.
   */
  content_type?: string;
  /**
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string;
  /**
   * The size of the file in bytes.
   */
  file_size?: number;
  /**
   * The width of the image
   */
  width?: number;
  /**
   * The height of the image
   */
  height?: number;
};
export type ImageSize = {
  /**
   * The width of the generated image. Default value: `512`
   */
  width?: number;
  /**
   * The height of the generated image. Default value: `512`
   */
  height?: number;
};
export type InpaintSection = {
  /**
   * Start time in seconds of the section to inpaint.
   */
  start: number;
  /**
   * End time in seconds of the section to inpaint.
   */
  end: number;
};
export type IPAdapter = {
  /**
   * Hugging Face path to the IP-Adapter
   */
  path: string;
  /**
   * Subfolder in which the ip_adapter weights exist
   */
  subfolder?: string;
  /**
   * Name of the safetensors file containing the ip-adapter weights
   */
  weight_name?: string;
  /**
   * Path to the Image Encoder for the IP-Adapter, for example 'openai/clip-vit-large-patch14'
   */
  image_encoder_path: string;
  /**
   * Subfolder in which the image encoder weights exist.
   */
  image_encoder_subfolder?: string;
  /**
   * Name of the image encoder.
   */
  image_encoder_weight_name?: string;
  /**
   * URL of Image for IP-Adapter conditioning.
   */
  image_url: string | Blob | File;
  /**
   * URL of the mask for the control image.
   */
  mask_image_url?: string | Blob | File;
  /**
   * Threshold for mask. Default value: `0.5`
   */
  mask_threshold?: number;
  /**
   * Scale for ip adapter.
   */
  scale: number;
};
export type Keyframe = {
  /**
   * The timestamp in milliseconds where this keyframe starts
   */
  timestamp: number;
  /**
   * The duration in milliseconds of this keyframe
   */
  duration: number;
  /**
   * The URL where this keyframe's media file can be accessed
   */
  url: string;
};
export type KeyframeTransition = {
  /**
   * Duration of this transition in seconds Default value: `5`
   */
  duration?: number;
  /**
   * Specific prompt for this transition. Overrides the global prompt if provided.
   */
  prompt?: string;
};
export type KlingV3MultiPromptElement = {
  /**
   * The prompt for this shot.
   */
  prompt: string;
  /**
   * The duration of this shot in seconds Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
};
export type Lighting = {
  /**
   * The conditions of the lighting in the image to be generated.
   */
  conditions?: string;
  /**
   * The direction of the lighting in the image to be generated.
   */
  direction?: string;
  /**
   * The shadows in the image to be generated.
   */
  shadows?: string;
};
export type LoraWeight = {
  /**
   * URL or the path to the LoRA weights.
   */
  path: string;
  /**
   * The scale of the LoRA weight. This is used to scale the LoRA weight
   * before merging it with the base model. Default value: `1`
   */
  scale?: number;
};
export type LoRAWeight = {
  /**
   * URL or the path to the LoRA weights.
   */
  path: string;
  /**
   * Name of the LoRA weight. Used only if `path` is a Hugging Face repository, and required only if you have more than 1 safetensors file in the repo.
   */
  weight_name?: string;
  /**
   * The scale of the LoRA weight. This is used to scale the LoRA weight
   * before merging it with the base model. Default value: `1`
   */
  scale?: number;
  /**
   * Specifies the transformer to load the lora weight into. 'high' loads into the high-noise transformer, 'low' loads it into the low-noise transformer, while 'both' loads the LoRA into both transformers. Default value: `"high"`
   */
  transformer?: "high" | "low" | "both";
};
export type LoudnessNormalizationSetting = {
  /**
   * Enable loudness normalization for the audio Default value: `true`
   */
  enabled?: boolean;
  /**
   * Target loudness in LUFS (default -18.0) Default value: `-18`
   */
  target_loudness?: number;
  /**
   * Target loudness range in LU (default 8.0) Default value: `8`
   */
  target_range?: number;
  /**
   * Target peak level in dBTP (default -0.5). Default value: `-0.5`
   */
  target_peak?: number;
};
export type LoudnormSummary = {
  /**
   * Input integrated loudness in LUFS
   */
  input_integrated?: number;
  /**
   * Input true peak in dBTP
   */
  input_true_peak?: number;
  /**
   * Input loudness range in LU
   */
  input_lra?: number;
  /**
   * Input threshold in LUFS
   */
  input_threshold?: number;
  /**
   * Output integrated loudness in LUFS
   */
  output_integrated?: number;
  /**
   * Output true peak in dBTP
   */
  output_true_peak?: number;
  /**
   * Output loudness range in LU
   */
  output_lra?: number;
  /**
   * Output threshold in LUFS
   */
  output_threshold?: number;
  /**
   * Type of normalization applied (Dynamic/Linear)
   */
  normalization_type?: string;
  /**
   * Target offset in LU
   */
  target_offset?: number;
};
export type MaskMetadata = {
  /**
   * Index of the mask inside the model output.
   */
  index: number;
  /**
   * Score for this mask.
   */
  score?: number;
  /**
   * Bounding box for the mask in normalized cxcywh coordinates.
   */
  box?: Array<number>;
};
export type ModelUrls = {
  /**
   * GLB format 3D model
   */
  glb?: File;
  /**
   * FBX format 3D model
   */
  fbx?: File;
  /**
   * OBJ format 3D model
   */
  obj?: File;
  /**
   * USDZ format 3D model
   */
  usdz?: File;
  /**
   * Blender format 3D model
   */
  blend?: File;
  /**
   * STL format 3D model
   */
  stl?: File;
};
export type MusicCompositionPlan = {
  /**
   * The styles that should be present in the entire song.
   */
  positive_global_styles: Array<string>;
  /**
   * The styles that should not be present in the entire song.
   */
  negative_global_styles: Array<string>;
  /**
   * The sections of the song.
   */
  sections: Array<MusicSection>;
};
export type MusicSection = {
  /**
   * The name of the section. Must be between 1 and 100 characters.
   */
  section_name: string;
  /**
   * The styles that should be present in this section.
   */
  positive_local_styles: Array<string>;
  /**
   * The styles that should not be present in this section.
   */
  negative_local_styles: Array<string>;
  /**
   * The duration of the section in milliseconds. Must be between 3000ms and 120000ms.
   */
  duration_ms: number;
  /**
   * The lyrics of the section. Each line must be at most 200 characters long.
   */
  lines: Array<string>;
};
export type NovaSRTimings = {
  /**
   * Time taken to preprocess the audio in seconds.
   */
  preprocess: number;
  /**
   * Time taken to run the inference in seconds.
   */
  inference: number;
  /**
   * Time taken to postprocess the audio in seconds.
   */
  postprocess: number;
};
export type Object = {
  /**
   * Left boundary of detection box in normalized format (0 to 1)
   */
  x_min: number;
  /**
   * Top boundary of detection box in normalized format (0 to 1)
   */
  y_min: number;
  /**
   * Right boundary of detection box in normalized format (0 to 1)
   */
  x_max: number;
  /**
   * Bottom boundary of detection box in normalized format (0 to 1)
   */
  y_max: number;
};
export type OpenAIInputTokenDetails = {
  /**
   * The number of tokens used in the images.
   */
  image_tokens?: number;
  /**
   * The number of tokens used in the prompt.
   */
  text_tokens?: number;
};
export type OpenAIUsage = {
  /**
   * The number of tokens used in the input.
   */
  input_tokens?: number;
  /**
   * The details of the input tokens.
   */
  input_tokens_details?: OpenAIInputTokenDetails;
  /**
   * The number of tokens used in the output.
   */
  output_tokens?: number;
  /**
   * The total number of tokens used.
   */
  total_tokens?: number;
};
export type PhotographicCharacteristics = {
  /**
   * The depth of field in the image to be generated.
   */
  depth_of_field?: string;
  /**
   * The focus in the image to be generated.
   */
  focus?: string;
  /**
   * The angle of the camera in the image to be generated.
   */
  camera_angle?: string;
  /**
   * The focal length of the lens in the image to be generated.
   */
  lens_focal_length?: string;
};
export type PikaImage = {
  /**
   *
   */
  image_url: string | Blob | File;
};
export type Point = {
  /**
   * X coordinate of the point in normalized format (0 to 1)
   */
  x: number;
  /**
   * Y coordinate of the point in normalized format (0 to 1)
   */
  y: number;
};
export type PointPrompt = {
  /**
   * X Coordinate of the prompt Default value: `305`
   */
  x?: number;
  /**
   * Y Coordinate of the prompt Default value: `350`
   */
  y?: number;
  /**
   * Label of the prompt. 1 for foreground, 0 for background Default value: `"1"`
   */
  label?: "0" | "1";
  /**
   * The frame index to interact with.
   */
  frame_index?: number;
};
export type PointPromptBase = {
  /**
   * X Coordinate of the prompt
   */
  x?: number;
  /**
   * Y Coordinate of the prompt
   */
  y?: number;
  /**
   * 1 for foreground, 0 for background
   */
  label?: "0" | "1";
  /**
   * Optional object identifier. Prompts sharing an object id refine the same object.
   */
  object_id?: number;
};
export type PromptObject = {
  /**
   * A description of the object to be generated.
   */
  description?: string;
  /**
   * The location of the object in the image.
   */
  location?: string;
  /**
   * The relationship of the object to other objects in the image.
   */
  relationship: string;
  /**
   * The relative size of the object in the image.
   */
  relative_size?: string;
  /**
   * The shape and color of the object.
   */
  shape_and_color?: string;
  /**
   * The texture of the object.
   */
  texture?: string;
  /**
   * The appearance details of the object.
   */
  appearance_details?: string;
  /**
   * The number of objects in the image.
   */
  number_of_objects?: number;
  /**
   * The pose of the object in the image.
   */
  pose?: string;
  /**
   * The expression of the object in the image.
   */
  expression?: string;
  /**
   * The clothing of the object in the image.
   */
  clothing?: string;
  /**
   * The action of the object in the image.
   */
  action?: string;
  /**
   * The gender of the object in the image.
   */
  gender?: string;
  /**
   * The skin tone and texture of the object in the image.
   */
  skin_tone_and_texture?: string;
  /**
   * The orientation of the object in the image.
   */
  orientation?: string;
};
export type PronunciationDict = {
  /**
   * List of pronunciation replacements in format ['text/(pronunciation)', ...]. For Chinese, tones are 1-5. Example: ['燕少飞/(yan4)(shao3)(fei1)']
   */
  tone_list?: Array<string>;
};
export type PronunciationDictionaryLocator = {
  /**
   * The ID of the pronunciation dictionary.
   */
  pronunciation_dictionary_id: string;
  /**
   * The ID of the version of the pronunciation dictionary. If not provided, the latest version will be used.
   */
  version_id?: string;
};
export type RawImage = {
  /**
   *
   */
  height: number;
  /**
   *
   */
  content: string;
  /**
   *  Default value: `"image/jpeg"`
   */
  content_type?: string;
  /**
   *
   */
  width: number;
};
export type ReferenceImage = {
  /**
   * URL to the reference image file (PNG format recommended)
   */
  image_url: string | Blob | File;
};
export type RegistryImageFastSdxlModelsImage = {
  /**
   *
   */
  url: string;
  /**
   *
   */
  width: number;
  /**
   *
   */
  height: number;
  /**
   *  Default value: `"image/jpeg"`
   */
  content_type?: string;
};
export type RelightParameters = {
  /**
   * Text prompt describing the desired lighting condition.
   */
  relight_prompt: string;
  /**
   * Direction of the light source (used for IC-light). Default value: `"Left"`
   */
  bg_source?: "Left" | "Right" | "Top" | "Bottom";
  /**
   * Classifier-free guidance scale for relighting. Default value: `2`
   */
  cfg?: number;
  /**
   * Whether to use sky masking for outdoor scenes.
   */
  use_sky_mask?: boolean;
};
export type Resolution = {
  /**
   * Display aspect ratio (e.g., '16:9')
   */
  aspect_ratio: string;
  /**
   * Width of the video in pixels
   */
  width: number;
  /**
   * Height of the video in pixels
   */
  height: number;
};
export type RGBColor = {
  /**
   * Red color value
   */
  r?: number;
  /**
   * Green color value
   */
  g?: number;
  /**
   * Blue color value
   */
  b?: number;
};
export type SAM3DBodyAlignmentInfo = {
  /**
   * Index of the person
   */
  person_id: number;
  /**
   * Scale factor applied for alignment
   */
  scale_factor: number;
  /**
   * Translation [tx, ty, tz]
   */
  translation: Array<number>;
  /**
   * Focal length used
   */
  focal_length: number;
  /**
   * Number of target points for alignment
   */
  target_points_count: number;
  /**
   * Number of cropped vertices
   */
  cropped_vertices_count: number;
};
export type SAM3DBodyMetadata = {
  /**
   * Number of people detected
   */
  num_people: number;
  /**
   * Per-person metadata
   */
  people: Array<SAM3DBodyPersonMetadata>;
};
export type SAM3DBodyPersonMetadata = {
  /**
   * Index of the person in the scene
   */
  person_id: number;
  /**
   * Bounding box [x_min, y_min, x_max, y_max]
   */
  bbox: Array<number>;
  /**
   * Estimated focal length
   */
  focal_length: number;
  /**
   * Predicted camera translation [tx, ty, tz]
   */
  pred_cam_t: Array<number>;
  /**
   * 2D keypoints [[x, y], ...] - 70 body keypoints
   */
  keypoints_2d: Array<Array<number>>;
  /**
   * 3D keypoints [[x, y, z], ...] - 70 body keypoints in camera space
   */
  keypoints_3d?: Array<Array<number>>;
};
export type SAM3DObjectMetadata = {
  /**
   * Index of the object in the scene
   */
  object_index: number;
  /**
   * Scale factors [sx, sy, sz]
   */
  scale?: Array<Array<number>>;
  /**
   * Rotation quaternion [x, y, z, w]
   */
  rotation?: Array<Array<number>>;
  /**
   * Translation [tx, ty, tz]
   */
  translation?: Array<Array<number>>;
  /**
   * Camera pose matrix
   */
  camera_pose?: Array<Array<number>>;
};
export type SegmentSamplingSettings = {
  /**
   * Sampling temperature to use. Higher values will make the output more random, while lower values will make it more focused and deterministic. Default value: `1`
   */
  temperature?: number;
  /**
   * Nucleus sampling probability mass to use, between 0 and 1. Default value: `1`
   */
  top_p?: number;
  /**
   * Maximum number of tokens to generate.
   */
  max_tokens?: number;
};
export type Speaker = {
  /**
   *
   */
  prompt: string;
  /**
   *
   */
  speaker_id: number;
  /**
   *
   */
  audio_url: string | Blob | File;
};
export type SpeechTimestamp = {
  /**
   * The start time of the speech in seconds.
   */
  start: number;
  /**
   * The end time of the speech in seconds.
   */
  end: number;
};
export type StructuredInstruction = {
  /**
   * A short description of the image to be generated.
   */
  short_description?: string;
  /**
   * A list of objects in the image to be generated, along with their attributes and relationships to other objects in the image.
   */
  objects?: Array<PromptObject>;
  /**
   * The background setting of the image to be generated.
   */
  background_setting?: string;
  /**
   * The lighting of the image to be generated.
   */
  lighting?: Lighting;
  /**
   * The aesthetics of the image to be generated.
   */
  aesthetics?: Aesthetics;
  /**
   * The photographic characteristics of the image to be generated.
   */
  photographic_characteristics?: PhotographicCharacteristics;
  /**
   * The style medium of the image to be generated.
   */
  style_medium?: string;
  /**
   * A list of text to be rendered in the image.
   */
  text_render?: Array<void>;
  /**
   * The context of the image to be generated.
   */
  context?: string;
  /**
   * The artistic style of the image to be generated.
   */
  artistic_style?: string;
  /**
   * The edit instruction for the image.
   */
  edit_instruction?: string;
};
export type StructuredPrompt = {
  /**
   * A short description of the image to be generated.
   */
  short_description?: string;
  /**
   * A list of objects in the image to be generated, along with their attributes and relationships to other objects in the image.
   */
  objects?: Array<PromptObject>;
  /**
   * The background setting of the image to be generated.
   */
  background_setting?: string;
  /**
   * The lighting of the image to be generated.
   */
  lighting?: Lighting;
  /**
   * The aesthetics of the image to be generated.
   */
  aesthetics?: Aesthetics;
  /**
   * The photographic characteristics of the image to be generated.
   */
  photographic_characteristics?: PhotographicCharacteristics;
  /**
   * The style medium of the image to be generated.
   */
  style_medium?: string;
  /**
   * A list of text to be rendered in the image.
   */
  text_render?: Array<void>;
  /**
   * The context of the image to be generated.
   */
  context?: string;
  /**
   * The artistic style of the image to be generated.
   */
  artistic_style?: string;
};
export type SubtitleSegment = {
  /**
   * Start time in seconds (e.g., 0.0 for beginning, 5.5 for 5.5 seconds)
   */
  start: number;
  /**
   * End time in seconds (must be greater than start time)
   */
  end: number;
  /**
   * Subtitle text to display (supports multiple lines with
   * )
   */
  text: string;
};
export type TextureFiles = {
  /**
   * Base color texture
   */
  base_color: File;
  /**
   * Metallic texture (PBR)
   */
  metallic?: File;
  /**
   * Normal texture (PBR)
   */
  normal?: File;
  /**
   * Roughness texture (PBR)
   */
  roughness?: File;
};
export type Track = {
  /**
   * Unique identifier for the track
   */
  id: string;
  /**
   * Type of track ('video' or 'audio')
   */
  type: string;
  /**
   * List of keyframes that make up this track
   */
  keyframes: Array<Keyframe>;
};
export type TrackPoint = {
  /**
   * X coordinate of the point
   */
  x: number;
  /**
   * Y coordinate of the point
   */
  y: number;
};
export type TrainingStage = {
  /**
   * Number of training steps for this stage. Default value: `50`
   */
  num_steps?: number;
  /**
   * Number of warmup steps for this stage. Default value: `10`
   */
  num_warmup_steps?: number;
  /**
   * Resolution for this training stage. Default value: `"1024"`
   */
  resolution?: "64" | "128" | "256" | "512" | "768" | "1024" | "1280" | "1536";
  /**
   * Aspect ratio for this training stage. Default value: `"3:4"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "9:21";
  /**
   * Batch size for this training stage. Default value: `1`
   */
  batch_size?: number;
  /**
   * Learning rate for this training stage. Default value: `0.0001`
   */
  learning_rate?: number;
  /**
   * Learning rate scheduler. Default value: `"linear"`
   */
  learning_rate_scheduler?:
    | "linear"
    | "cosine"
    | "cosine_with_restarts"
    | "polynomial"
    | "constant"
    | "constant_with_warmup"
    | "piecewise_constant";
  /**
   * The style of the transformer trainer. Either 'both', 'split', 'transformer_1', or 'transformer_2'. Default value: `"in_sequence"`
   */
  transformer_trainer_style?:
    | "both"
    | "split"
    | "transformer_1_only"
    | "transformer_2_only"
    | "in_sequence";
  /**
   * The timestep to split the training into two parts. Only applicable when transformer_trainer_style is 'split'. Default value: `0.875`
   */
  split_train_timestep?: number;
};
export type Trajectory = {
  /**
   * X coordinate of the motion trajectory
   */
  x: number;
  /**
   * Y coordinate of the motion trajectory
   */
  y: number;
};
export type TrajectoryParameters = {
  /**
   * Horizontal rotation angles (degrees) for each keyframe.
   */
  theta: Array<number>;
  /**
   * Vertical rotation angles (degrees) for each keyframe.
   */
  phi: Array<number>;
  /**
   * Camera distance scaling factors for each keyframe.
   */
  radius: Array<number>;
};
export type TrajectoryPoint = {
  /**
   * X coordinate of the trajectory point in pixels.
   */
  x: number;
  /**
   * Y coordinate of the trajectory point in pixels.
   */
  y: number;
  /**
   * Amount of proportional speed that will affect the movement to the next trajectory point. Higher values mean faster movement. Can't be zero. Default value: `1`
   */
  speed?: number;
};
export type TranscriptionWord = {
  /**
   * The transcribed word or audio event
   */
  text: string;
  /**
   * Start time in seconds
   */
  start: number;
  /**
   * End time in seconds
   */
  end: number;
  /**
   * Type of element (word, spacing, or audio_event)
   */
  type: string;
  /**
   * Speaker identifier if diarization was enabled
   */
  speaker_id?: string;
};
export type Turn = {
  /**
   *
   */
  speaker_id: number;
  /**
   *
   */
  text: string;
};
export type UsageInfo = {
  /**
   * Number of input tokens processed
   */
  input_tokens: number;
  /**
   * Number of output tokens generated
   */
  output_tokens: number;
  /**
   * Time taken for prefill in milliseconds
   */
  prefill_time_ms: number;
  /**
   * Time taken for decoding in milliseconds
   */
  decode_time_ms: number;
  /**
   * Time to first token in milliseconds
   */
  ttft_ms: number;
};
export type V2VValidation = {
  /**
   * The prompt to use for validation.
   */
  prompt: string;
  /**
   * URL to reference video for IC-LoRA validation. This is the input video that will be transformed.
   */
  reference_video_url: string | Blob | File;
};
export type Validation = {
  /**
   * The prompt to use for validation.
   */
  prompt: string;
  /**
   * An image to use for image-to-video validation. If provided for one validation, _all_ validation inputs must have an image.
   */
  image_url?: string | Blob | File;
};
export type VibeVoiceSpeaker = {
  /**
   * Default voice preset to use for the speaker. Not used if `audio_url` is provided. Default value: `"Alice [EN]"`
   */
  preset?:
    | "Alice [EN]"
    | "Carter [EN]"
    | "Frank [EN]"
    | "Mary [EN] (Background Music)"
    | "Maya [EN]"
    | "Anchen [ZH] (Background Music)"
    | "Bowen [ZH]"
    | "Xinran [ZH]";
  /**
   * URL to a voice sample audio file. If provided, `preset` will be ignored.
   */
  audio_url?: string | Blob | File;
};
export type Video = {
  /**
   * Type of media (always 'video') Default value: `"video"`
   */
  media_type?: string;
  /**
   * URL where the media file can be accessed
   */
  url: string;
  /**
   * MIME type of the media file
   */
  content_type: string;
  /**
   * Original filename of the media
   */
  file_name: string;
  /**
   * Size of the file in bytes
   */
  file_size: number;
  /**
   * Duration of the media in seconds
   */
  duration: number;
  /**
   * Overall bitrate of the media in bits per second
   */
  bitrate: number;
  /**
   * Codec used to encode the media
   */
  codec: string;
  /**
   * Container format of the media file (e.g., 'mp4', 'mov')
   */
  container: string;
  /**
   * Frames per second
   */
  fps: number;
  /**
   * Total number of frames in the video
   */
  frame_count: number;
  /**
   * Time base used for frame timestamps
   */
  timebase: string;
  /**
   * Video resolution information
   */
  resolution: Resolution;
  /**
   * Detailed video format information
   */
  format: VideoFormat;
  /**
   * Audio track information if video has audio
   */
  audio?: AudioTrack;
  /**
   * URL of the extracted first frame
   */
  start_frame_url?: string | Blob | File;
  /**
   * URL of the extracted last frame
   */
  end_frame_url?: string | Blob | File;
};
export type VideoCondition = {
  /**
   * The URL of the video to use as input.
   */
  video_url: string | Blob | File;
  /**
   * The frame number to start the condition on.
   */
  start_frame_number?: number;
  /**
   * The strength of the condition. Default value: `1`
   */
  strength?: number;
};
export type VideoFile = {
  /**
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * The mime type of the file.
   */
  content_type?: string;
  /**
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string;
  /**
   * The size of the file in bytes.
   */
  file_size?: number;
  /**
   * File data
   */
  file_data?: string;
  /**
   * The width of the video
   */
  width?: number;
  /**
   * The height of the video
   */
  height?: number;
  /**
   * The FPS of the video
   */
  fps?: number;
  /**
   * The duration of the video
   */
  duration?: number;
  /**
   * The number of frames in the video
   */
  num_frames?: number;
};
export type VideoFormat = {
  /**
   * Container format of the video
   */
  container: string;
  /**
   * Video codec used (e.g., 'h264')
   */
  video_codec: string;
  /**
   * Codec profile (e.g., 'main', 'high')
   */
  profile: string;
  /**
   * Codec level (e.g., 4.1)
   */
  level: number;
  /**
   * Pixel format used (e.g., 'yuv420p')
   */
  pixel_format: string;
  /**
   * Video bitrate in bits per second
   */
  bitrate: number;
};
export type VoiceModify = {
  /**
   * Pitch adjustment in semitones. Range: -100 to 100. Positive values raise pitch, negative values lower it.
   */
  pitch?: number;
  /**
   * Intensity/energy of the voice. Range: -100 to 100. Higher values create more energetic speech.
   */
  intensity?: number;
  /**
   * Timbre adjustment. Range: -100 to 100. Affects the tonal quality of the voice.
   */
  timbre?: number;
};
export type VoicePreview = {
  /**
   * The generated voice
   */
  audio: File;
  /**
   * The ID of the generated voice
   */
  generated_voice_id: string;
  /**
   * The media type of the generated audio
   */
  media_type: string;
  /**
   * Duration of the generated audio in seconds
   */
  duration_seconds: number;
  /**
   * The language of the preview
   */
  language: string;
};
export type VoiceSetting = {
  /**
   * Predefined voice ID to use for synthesis Default value: `"Wise_Woman"`
   */
  voice_id?: string;
  /**
   * Speech speed (0.5-2.0) Default value: `1`
   */
  speed?: number;
  /**
   * Volume (0-10) Default value: `1`
   */
  vol?: number;
  /**
   * Voice pitch (-12 to 12)
   */
  pitch?: number;
  /**
   * Emotion of the generated speech
   */
  emotion?:
    | "happy"
    | "sad"
    | "angry"
    | "fearful"
    | "disgusted"
    | "surprised"
    | "neutral";
  /**
   * Enables English text normalization to improve number reading performance, with a slight increase in latency
   */
  english_normalization?: boolean;
};
export type AceStepAudioInpaintInput = {
  /**
   * URL of the audio file to be inpainted.
   */
  audio_url: string | Blob | File;
  /**
   * Whether the start time is relative to the start or end of the audio. Default value: `"start"`
   */
  start_time_relative_to?: "start" | "end";
  /**
   * start time in seconds for the inpainting process.
   */
  start_time?: number;
  /**
   * Whether the end time is relative to the start or end of the audio. Default value: `"start"`
   */
  end_time_relative_to?: "start" | "end";
  /**
   * end time in seconds for the inpainting process. Default value: `30`
   */
  end_time?: number;
  /**
   * Comma-separated list of genre tags to control the style of the generated audio.
   */
  tags: string;
  /**
   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song. Default value: `""`
   */
  lyrics?: string;
  /**
   * Variance for the inpainting process. Higher values can lead to more diverse results. Default value: `0.5`
   */
  variance?: number;
  /**
   * Number of steps to generate the audio. Default value: `27`
   */
  number_of_steps?: number;
  /**
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * Scheduler to use for the generation process. Default value: `"euler"`
   */
  scheduler?: "euler" | "heun";
  /**
   * Type of CFG to use for the generation process. Default value: `"apg"`
   */
  guidance_type?: "cfg" | "apg" | "cfg_star";
  /**
   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`
   */
  granularity_scale?: number;
  /**
   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`
   */
  guidance_interval?: number;
  /**
   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
   */
  guidance_interval_decay?: number;
  /**
   * Guidance scale for the generation. Default value: `15`
   */
  guidance_scale?: number;
  /**
   * Minimum guidance scale for the generation after the decay. Default value: `3`
   */
  minimum_guidance_scale?: number;
  /**
   * Tag guidance scale for the generation. Default value: `5`
   */
  tag_guidance_scale?: number;
  /**
   * Lyric guidance scale for the generation. Default value: `1.5`
   */
  lyric_guidance_scale?: number;
};
export type AceStepAudioInpaintOutput = {
  /**
   * The generated audio file.
   */
  audio: File;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
  /**
   * The genre tags used in the generation process.
   */
  tags: string;
  /**
   * The lyrics used in the generation process.
   */
  lyrics: string;
};
export type AceStepAudioOutpaintInput = {
  /**
   * URL of the audio file to be outpainted.
   */
  audio_url: string | Blob | File;
  /**
   * Duration in seconds to extend the audio from the start.
   */
  extend_before_duration?: number;
  /**
   * Duration in seconds to extend the audio from the end. Default value: `30`
   */
  extend_after_duration?: number;
  /**
   * Comma-separated list of genre tags to control the style of the generated audio.
   */
  tags: string;
  /**
   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song. Default value: `""`
   */
  lyrics?: string;
  /**
   * Number of steps to generate the audio. Default value: `27`
   */
  number_of_steps?: number;
  /**
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * Scheduler to use for the generation process. Default value: `"euler"`
   */
  scheduler?: "euler" | "heun";
  /**
   * Type of CFG to use for the generation process. Default value: `"apg"`
   */
  guidance_type?: "cfg" | "apg" | "cfg_star";
  /**
   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`
   */
  granularity_scale?: number;
  /**
   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`
   */
  guidance_interval?: number;
  /**
   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
   */
  guidance_interval_decay?: number;
  /**
   * Guidance scale for the generation. Default value: `15`
   */
  guidance_scale?: number;
  /**
   * Minimum guidance scale for the generation after the decay. Default value: `3`
   */
  minimum_guidance_scale?: number;
  /**
   * Tag guidance scale for the generation. Default value: `5`
   */
  tag_guidance_scale?: number;
  /**
   * Lyric guidance scale for the generation. Default value: `1.5`
   */
  lyric_guidance_scale?: number;
};
export type AceStepAudioOutpaintOutput = {
  /**
   * The generated audio file.
   */
  audio: File;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
  /**
   * The genre tags used in the generation process.
   */
  tags: string;
  /**
   * The lyrics used in the generation process.
   */
  lyrics: string;
};
export type AceStepAudioToAudioInput = {
  /**
   * URL of the audio file to be outpainted.
   */
  audio_url: string | Blob | File;
  /**
   * Whether to edit the lyrics only or remix the audio. Default value: `"remix"`
   */
  edit_mode?: "lyrics" | "remix";
  /**
   * Original tags of the audio file.
   */
  original_tags: string;
  /**
   * Original lyrics of the audio file. Default value: `""`
   */
  original_lyrics?: string;
  /**
   * Comma-separated list of genre tags to control the style of the generated audio.
   */
  tags: string;
  /**
   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song. Default value: `""`
   */
  lyrics?: string;
  /**
   * Number of steps to generate the audio. Default value: `27`
   */
  number_of_steps?: number;
  /**
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * Scheduler to use for the generation process. Default value: `"euler"`
   */
  scheduler?: "euler" | "heun";
  /**
   * Type of CFG to use for the generation process. Default value: `"apg"`
   */
  guidance_type?: "cfg" | "apg" | "cfg_star";
  /**
   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`
   */
  granularity_scale?: number;
  /**
   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`
   */
  guidance_interval?: number;
  /**
   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
   */
  guidance_interval_decay?: number;
  /**
   * Guidance scale for the generation. Default value: `15`
   */
  guidance_scale?: number;
  /**
   * Minimum guidance scale for the generation after the decay. Default value: `3`
   */
  minimum_guidance_scale?: number;
  /**
   * Tag guidance scale for the generation. Default value: `5`
   */
  tag_guidance_scale?: number;
  /**
   * Lyric guidance scale for the generation. Default value: `1.5`
   */
  lyric_guidance_scale?: number;
  /**
   * Original seed of the audio file.
   */
  original_seed?: number;
};
export type AceStepAudioToAudioOutput = {
  /**
   * The generated audio file.
   */
  audio: File;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
  /**
   * The genre tags used in the generation process.
   */
  tags: string;
  /**
   * The lyrics used in the generation process.
   */
  lyrics: string;
};
export type AceStepInput = {
  /**
   * Comma-separated list of genre tags to control the style of the generated audio.
   */
  tags: string;
  /**
   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song. Default value: `""`
   */
  lyrics?: string;
  /**
   * The duration of the generated audio in seconds. Default value: `60`
   */
  duration?: number;
  /**
   * Number of steps to generate the audio. Default value: `27`
   */
  number_of_steps?: number;
  /**
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * Scheduler to use for the generation process. Default value: `"euler"`
   */
  scheduler?: "euler" | "heun";
  /**
   * Type of CFG to use for the generation process. Default value: `"apg"`
   */
  guidance_type?: "cfg" | "apg" | "cfg_star";
  /**
   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`
   */
  granularity_scale?: number;
  /**
   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`
   */
  guidance_interval?: number;
  /**
   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
   */
  guidance_interval_decay?: number;
  /**
   * Guidance scale for the generation. Default value: `15`
   */
  guidance_scale?: number;
  /**
   * Minimum guidance scale for the generation after the decay. Default value: `3`
   */
  minimum_guidance_scale?: number;
  /**
   * Tag guidance scale for the generation. Default value: `5`
   */
  tag_guidance_scale?: number;
  /**
   * Lyric guidance scale for the generation. Default value: `1.5`
   */
  lyric_guidance_scale?: number;
};
export type AceStepOutput = {
  /**
   * The generated audio file.
   */
  audio: File;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
  /**
   * The genre tags used in the generation process.
   */
  tags: string;
  /**
   * The lyrics used in the generation process.
   */
  lyrics: string;
};
export type AceStepPromptToAudioInput = {
  /**
   * Prompt to control the style of the generated audio. This will be used to generate tags and lyrics.
   */
  prompt: string;
  /**
   * Whether to generate an instrumental version of the audio.
   */
  instrumental?: boolean;
  /**
   * The duration of the generated audio in seconds. Default value: `60`
   */
  duration?: number;
  /**
   * Number of steps to generate the audio. Default value: `27`
   */
  number_of_steps?: number;
  /**
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * Scheduler to use for the generation process. Default value: `"euler"`
   */
  scheduler?: "euler" | "heun";
  /**
   * Type of CFG to use for the generation process. Default value: `"apg"`
   */
  guidance_type?: "cfg" | "apg" | "cfg_star";
  /**
   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`
   */
  granularity_scale?: number;
  /**
   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`
   */
  guidance_interval?: number;
  /**
   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.
   */
  guidance_interval_decay?: number;
  /**
   * Guidance scale for the generation. Default value: `15`
   */
  guidance_scale?: number;
  /**
   * Minimum guidance scale for the generation after the decay. Default value: `3`
   */
  minimum_guidance_scale?: number;
  /**
   * Tag guidance scale for the generation. Default value: `5`
   */
  tag_guidance_scale?: number;
  /**
   * Lyric guidance scale for the generation. Default value: `1.5`
   */
  lyric_guidance_scale?: number;
};
export type AceStepPromptToAudioOutput = {
  /**
   * The generated audio file.
   */
  audio: File;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
  /**
   * The genre tags used in the generation process.
   */
  tags: string;
  /**
   * The lyrics used in the generation process.
   */
  lyrics: string;
};
export type AddBackgroundInput = {
  /**
   * The URLs of the images to edit. Provide an image with a white or clean background.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the background/scene you want to add behind the object. The model will remove the white background and add the specified environment. Default value: `"Remove white background and add a realistic scene behind the object"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type AddBackgroundOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type AddObjectByTextInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The full natural language command describing what to add and where.
   */
  instruction: string;
};
export type AddSubtitlesToVideoInput = {
  /**
   * URL of the video file to add subtitles to
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * List of subtitle segments. Click + to add more subtitle segments.
   */
  subtitles: Array<SubtitleSegment>;
  /**
   * Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty'). TikTok commonly uses bold sans-serif fonts. Default value: `"Montserrat"`
   */
  font_name?: string;
  /**
   * Font size for subtitles (TikTok style uses larger, bold text) Default value: `70`
   */
  font_size?: number;
  /**
   * Font weight (TikTok style typically uses bold or black) Default value: `"bold"`
   */
  font_weight?: "normal" | "bold" | "black";
  /**
   * Subtitle text color Default value: `"white"`
   */
  font_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Text stroke/outline width in pixels (0 for no stroke, TikTok uses 2-4) Default value: `3`
   */
  stroke_width?: number;
  /**
   * Text stroke/outline color Default value: `"black"`
   */
  stroke_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Background box color for subtitle text (use 'none' for no background, TikTok style uses no background) Default value: `"none"`
   */
  background_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta"
    | "transparent"
    | "none";
  /**
   * Opacity of subtitle background box if used (0.0-1.0, 0.0=transparent)
   */
  background_opacity?: number;
  /**
   * Vertical position of subtitles on screen Default value: `"bottom"`
   */
  position?: "bottom" | "top" | "center";
  /**
   * Vertical offset from position in pixels (-500 to 500, positive moves down)
   */
  y_offset?: number;
};
export type AddSubtitlesToVideoOutput = {
  /**
   * The video with subtitles
   */
  video: File;
};
export type AddTextToImageInput = {
  /**
   * The URL of the image to add text to
   *
   * Max file size: 9.5MB, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Text to add to the image
   */
  text: string;
  /**
   * X position as percentage of image width (0-100) Default value: `50`
   */
  x_percent?: number;
  /**
   * Y position as percentage of image height (0-100) Default value: `50`
   */
  y_percent?: number;
  /**
   * Font size in pixels Default value: `40`
   */
  font_size?: number;
  /**
   * Text color Default value: `"white"`
   */
  font_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Optional background color for text box (use 'none' or leave empty for no background)
   */
  background_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta"
    | "transparent"
    | "none";
  /**
   * Padding around text in pixels (only if background_color is set) Default value: `10`
   */
  background_padding?: number;
  /**
   * Text anchor point Default value: `"center"`
   */
  anchor?: "left" | "center" | "right";
  /**
   * Width of text outline/stroke in pixels (0 for no stroke)
   */
  stroke_width?: number;
  /**
   * Color of text stroke/outline Default value: `"black"`
   */
  stroke_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Output format for the result image Default value: `"png"`
   */
  output_format?: "png" | "jpg" | "jpeg" | "webp";
};
export type AddTextToImageOutput = {
  /**
   * Image with text added
   */
  image: Image;
};
export type AdvancedInput = {
  /**
   * URL to the training data.
   */
  training_data_url: string | Blob | File;
  /**
   * Trigger phrase for the model.
   */
  trigger_phrase: string;
  /**
   * Default caption to use if no caption is found for a media file.
   */
  default_caption?: string;
  /**
   * Whether to include synthetic captions.
   */
  include_synthetic_captions?: boolean;
  /**
   * Whether to use face detection for the training data. When enabled, images will use the center of the face as the center of the image when resizing. Default value: `true`
   */
  use_face_detection?: boolean;
  /**
   * Whether to use face cropping for the training data. When enabled, images will be cropped to the face before resizing.
   */
  use_face_cropping?: boolean;
  /**
   * Whether to use masks for the training data. Default value: `true`
   */
  use_masks?: boolean;
  /**
   * The percentage of the image to use for low resolution training. 0.0 means no low resolution training, 1.0 means full low resolution training. Default value: `0.3`
   */
  low_res_percentage?: number;
  /**
   * The multiplier for the learning rate for the high resolution training stage. This is useful when you want to train the high resolution stage with a different learning rate than the low resolution stage. Default value: `2.5`
   */
  hires_lr_multiplier?: number;
  /**
   * List of training stages. Each stage can have different parameters.
   */
  training_stages?: Array<TrainingStage>;
};
export type AdvancedLipsyncCreateTaskInput = {
  /**
   * The session id of the lip-sync task
   */
  session_id: string;
  /**
   * Specified Face for Lip-Sync. Includes Face ID, lip movement reference data, etc. Currently only supports one person lip-sync.
   */
  face_choose: FaceChoice;
};
export type AdvancedLipsyncOutput = {
  /**
   * The generated lip-sync video
   */
  videos: Array<File>;
};
export type AgeModifyInput = {
  /**
   * Portrait image URL for age modification
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `30`
   */
  target_age?: number;
  /**
   *  Default value: `true`
   */
  preserve_identity?: boolean;
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type AgeModifyOutput = {
  /**
   * Portrait with modified age
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type AgeProgressionInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The age change to apply. Default value: `"20 years older"`
   */
  prompt?: string;
};
export type AgeProgressionOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type AiAvatarInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `145`
   */
  num_frames?: number;
  /**
   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `42`
   */
  seed?: number;
  /**
   * The acceleration level to use for generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type AIAvatarInput = {
  /**
   * The URL of the image to use as your avatar
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * The prompt to use for the video generation. Default value: `"."`
   */
  prompt?: string;
};
export type AiAvatarMultiInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the Person 1 audio file.
   */
  first_audio_url: string | Blob | File;
  /**
   * The URL of the Person 2 audio file.
   */
  second_audio_url?: string | Blob | File;
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `181`
   */
  num_frames?: number;
  /**
   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `81`
   */
  seed?: number;
  /**
   * Whether to use only the first audio file.
   */
  use_only_first_audio?: boolean;
  /**
   * The acceleration level to use for generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type AiAvatarMultiOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type AiAvatarMultiTextInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The text input to guide video generation.
   */
  first_text_input: string;
  /**
   * The text input to guide video generation.
   */
  second_text_input: string;
  /**
   * The first person's voice to use for speech generation Default value: `"Sarah"`
   */
  voice1?:
    | "Aria"
    | "Roger"
    | "Sarah"
    | "Laura"
    | "Charlie"
    | "George"
    | "Callum"
    | "River"
    | "Liam"
    | "Charlotte"
    | "Alice"
    | "Matilda"
    | "Will"
    | "Jessica"
    | "Eric"
    | "Chris"
    | "Brian"
    | "Daniel"
    | "Lily"
    | "Bill";
  /**
   * The second person's voice to use for speech generation Default value: `"Roger"`
   */
  voice2?:
    | "Aria"
    | "Roger"
    | "Sarah"
    | "Laura"
    | "Charlie"
    | "George"
    | "Callum"
    | "River"
    | "Liam"
    | "Charlotte"
    | "Alice"
    | "Matilda"
    | "Will"
    | "Jessica"
    | "Eric"
    | "Chris"
    | "Brian"
    | "Daniel"
    | "Lily"
    | "Bill";
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `191`
   */
  num_frames?: number;
  /**
   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `81`
   */
  seed?: number;
  /**
   * The acceleration level to use for generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type AiAvatarMultiTextOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type AiAvatarOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type AIAvatarOutput = {
  /**
   * The generated video
   */
  video: File;
  /**
   * Duration of the output video in seconds.
   */
  duration: number;
};
export type AiAvatarSingleTextInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The text input to guide video generation.
   */
  text_input: string;
  /**
   * The voice to use for speech generation
   */
  voice:
    | "Aria"
    | "Roger"
    | "Sarah"
    | "Laura"
    | "Charlie"
    | "George"
    | "Callum"
    | "River"
    | "Liam"
    | "Charlotte"
    | "Alice"
    | "Matilda"
    | "Will"
    | "Jessica"
    | "Eric"
    | "Chris"
    | "Brian"
    | "Daniel"
    | "Lily"
    | "Bill";
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `136`
   */
  num_frames?: number;
  /**
   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `42`
   */
  seed?: number;
  /**
   * The acceleration level to use for generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type AiAvatarSingleTextOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type AiBabyAndAgingGeneratorMultiInput = {
  /**
   * Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years).
   */
  age_group:
    | "baby"
    | "toddler"
    | "preschool"
    | "gradeschooler"
    | "teen"
    | "adult"
    | "mid"
    | "senior";
  /**
   * Gender for the generated image. Choose from: 'male' or 'female'.
   */
  gender: "male" | "female";
  /**
   * Text prompt to guide the image generation Default value: `"a newborn baby, well dressed"`
   */
  prompt?: string;
  /**
   * The size of the generated image
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducibility. If None, a random seed will be used
   */
  seed?: number;
  /**
   * The format of the generated image. Choose from: 'jpeg' or 'png'. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * List of mother images for multi mode
   */
  mother_image_urls: Array<string>;
  /**
   * List of father images for multi mode
   */
  father_image_urls: Array<string>;
  /**
   * Weight of the father's influence in multi mode generation Default value: `0.5`
   */
  father_weight?: number;
};
export type AiBabyAndAgingGeneratorMultiOutput = {
  /**
   * The generated image files info
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The final prompt used for generation
   */
  prompt: string;
};
export type AiBabyAndAgingGeneratorSingleInput = {
  /**
   * Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years).
   */
  age_group:
    | "baby"
    | "toddler"
    | "preschool"
    | "gradeschooler"
    | "teen"
    | "adult"
    | "mid"
    | "senior";
  /**
   * Gender for the generated image. Choose from: 'male' or 'female'.
   */
  gender: "male" | "female";
  /**
   * Text prompt to guide the image generation Default value: `"a newborn baby, well dressed"`
   */
  prompt?: string;
  /**
   * The size of the generated image
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducibility. If None, a random seed will be used
   */
  seed?: number;
  /**
   * The format of the generated image. Choose from: 'jpeg' or 'png'. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * List of ID images for single mode (or general reference images)
   */
  id_image_urls: Array<string>;
};
export type AiBabyAndAgingGeneratorSingleOutput = {
  /**
   * The generated image files info
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The final prompt used for generation
   */
  prompt: string;
};
export type AiDetectorDetectImageInput = {
  /**
   * URL pointing to an image to analyze for AI generation.(Max: 3000 characters)
   */
  image_url: string | Blob | File;
};
export type AiDetectorDetectImageOutput = {
  /**
   *
   */
  verdict: string;
  /**
   *
   */
  confidence: number;
  /**
   *
   */
  is_ai_generated: boolean;
  /**
   *
   */
  latency: number;
};
export type AiDetectorDetectTextInput = {
  /**
   * Text content to analyze for AI generation.
   */
  text: string;
};
export type AiDetectorDetectTextOutput = {
  /**
   *
   */
  verdict: string;
  /**
   *
   */
  confidence: number;
  /**
   *
   */
  is_ai_generated: boolean;
  /**
   *
   */
  latency: number;
};
export type AiFaceSwapFaceswapimageInput = {
  /**
   * Source face image. Allowed items: bmp, jpeg, png, tiff, webp
   */
  source_face_url: string | Blob | File;
  /**
   * Target image URL. Allowed items: bmp, jpeg, png, tiff, webp
   */
  target_image_url: string | Blob | File;
  /**
   * Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more.
   */
  enable_occlusion_prevention?: boolean;
};
export type AiFaceSwapFaceswapimageOutput = {
  /**
   * Generated image result
   */
  image: Image;
  /**
   * Optional processing duration in milliseconds
   */
  processing_time_ms?: number;
};
export type AiFaceSwapFaceswapvideoInput = {
  /**
   * Source face image. Allowed items: bmp, jpeg, png, tiff, webp
   */
  source_face_url: string | Blob | File;
  /**
   * Target video URL (max 25 minutes, will be truncated if longer; FPS capped at 25). Allowed items: avi, m4v, mkv, mp4, mpeg, mov, mxf, webm, wmv
   */
  target_video_url: string | Blob | File;
  /**
   * Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more.
   */
  enable_occlusion_prevention?: boolean;
};
export type AiFaceSwapFaceswapvideoOutput = {
  /**
   * Generated video result
   */
  video: Video;
  /**
   * Optional processing duration in milliseconds
   */
  processing_time_ms?: number;
  /**
   * Warning message if video was modified (e.g., truncated or FPS reduced)
   */
  warning?: string;
};
export type AiHomeEditInput = {
  /**
   * URL of the image to do architectural editing
   */
  input_image_url: string | Blob | File;
  /**
   * Type of editing. Structural editing only edits structural elements such as windows, walls etc. Virtual staging edits your furniture. Both do full editing including structural and furniture
   */
  editing_type: "structural editing" | "virtual staging" | "both";
  /**
   * Type of architecture for appropriate furniture selection
   */
  architecture_type:
    | "living room-interior"
    | "bedroom-interior"
    | "kitchen-interior"
    | "dining room-interior"
    | "bathroom-interior"
    | "laundry room-interior"
    | "home office-interior"
    | "study room-interior"
    | "dorm room-interior"
    | "coffee shop-interior"
    | "gaming room-interior"
    | "restaurant-interior"
    | "office-interior"
    | "attic-interior"
    | "toilet-interior"
    | "other-interior"
    | "house-exterior"
    | "villa-exterior"
    | "backyard-exterior"
    | "courtyard-exterior"
    | "ranch-exterior"
    | "office-exterior"
    | "retail-exterior"
    | "tower-exterior"
    | "apartment-exterior"
    | "school-exterior"
    | "museum-exterior"
    | "commercial-exterior"
    | "residential-exterior"
    | "other-exterior";
  /**
   * Style for furniture and decor
   */
  style:
    | "minimalistic-interior"
    | "farmhouse-interior"
    | "luxury-interior"
    | "modern-interior"
    | "zen-interior"
    | "mid century-interior"
    | "airbnb-interior"
    | "cozy-interior"
    | "rustic-interior"
    | "christmas-interior"
    | "bohemian-interior"
    | "tropical-interior"
    | "industrial-interior"
    | "japanese-interior"
    | "vintage-interior"
    | "loft-interior"
    | "halloween-interior"
    | "soho-interior"
    | "baroque-interior"
    | "kids room-interior"
    | "girls room-interior"
    | "boys room-interior"
    | "scandinavian-interior"
    | "french country-interior"
    | "mediterranean-interior"
    | "cyberpunk-interior"
    | "hot pink-interior"
    | "biophilic-interior"
    | "ancient egypt-interior"
    | "pixel-interior"
    | "art deco-interior"
    | "modern-exterior"
    | "minimalistic-exterior"
    | "farmhouse-exterior"
    | "cozy-exterior"
    | "luxury-exterior"
    | "colonial-exterior"
    | "zen-exterior"
    | "asian-exterior"
    | "creepy-exterior"
    | "airstone-exterior"
    | "ancient greek-exterior"
    | "art deco-exterior"
    | "brutalist-exterior"
    | "christmas lights-exterior"
    | "contemporary-exterior"
    | "cottage-exterior"
    | "dutch colonial-exterior"
    | "federal colonial-exterior"
    | "fire-exterior"
    | "french provincial-exterior"
    | "full glass-exterior"
    | "georgian colonial-exterior"
    | "gothic-exterior"
    | "greek revival-exterior"
    | "ice-exterior"
    | "italianate-exterior"
    | "mediterranean-exterior"
    | "midcentury-exterior"
    | "middle eastern-exterior"
    | "minecraft-exterior"
    | "morocco-exterior"
    | "neoclassical-exterior"
    | "spanish-exterior"
    | "tudor-exterior"
    | "underwater-exterior"
    | "winter-exterior"
    | "yard lighting-exterior";
  /**
   * Color palette for furniture and decor
   */
  color_palette:
    | "surprise me"
    | "golden beige"
    | "refined blues"
    | "dusky elegance"
    | "emerald charm"
    | "crimson luxury"
    | "golden sapphire"
    | "soft pastures"
    | "candy sky"
    | "peach meadow"
    | "muted sands"
    | "ocean breeze"
    | "frosted pastels"
    | "spring bloom"
    | "gentle horizon"
    | "seaside breeze"
    | "azure coast"
    | "golden shore"
    | "mediterranean gem"
    | "ocean serenity"
    | "serene blush"
    | "muted horizon"
    | "pastel shores"
    | "dusky calm"
    | "woodland retreat"
    | "meadow glow"
    | "forest canopy"
    | "riverbank calm"
    | "earthy tones"
    | "earthy neutrals"
    | "arctic mist"
    | "aqua drift"
    | "blush bloom"
    | "coral haze"
    | "retro rust"
    | "autumn glow"
    | "rustic charm"
    | "vintage sage"
    | "faded plum"
    | "electric lime"
    | "violet pulse"
    | "neon sorbet"
    | "aqua glow"
    | "fluorescent sunset"
    | "lavender bloom"
    | "petal fresh"
    | "meadow light"
    | "sunny pastures"
    | "frosted mauve"
    | "snowy hearth"
    | "icy blues"
    | "winter twilight"
    | "earthy hues"
    | "stone balance"
    | "neutral sands"
    | "slate shades";
  /**
   * Additional elements to include in the options above (e.g., plants, lighting) Default value: `""`
   */
  additional_elements?: string;
  /**
   * Custom prompt for architectural editing, it overrides above options when used Default value: `""`
   */
  custom_prompt?: string;
  /**
   * The format of the generated image. Choose from: 'jpeg' or 'png'. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type AiHomeEditOutput = {
  /**
   * Generated image
   */
  image: Image;
  /**
   * Status message with processing details
   */
  status: string;
};
export type AiHomeStyleInput = {
  /**
   * URL of the image to do architectural styling
   */
  input_image_url: string | Blob | File;
  /**
   * URL of the style image, optional. If given, other parameters are ignored Default value: `""`
   */
  style_image_url?: string | Blob | File;
  /**
   * Type of architecture for appropriate furniture selection
   */
  architecture_type:
    | "living room-interior"
    | "bedroom-interior"
    | "kitchen-interior"
    | "dining room-interior"
    | "bathroom-interior"
    | "laundry room-interior"
    | "home office-interior"
    | "study room-interior"
    | "dorm room-interior"
    | "coffee shop-interior"
    | "gaming room-interior"
    | "restaurant-interior"
    | "office-interior"
    | "attic-interior"
    | "toilet-interior"
    | "other-interior"
    | "house-exterior"
    | "villa-exterior"
    | "backyard-exterior"
    | "courtyard-exterior"
    | "ranch-exterior"
    | "office-exterior"
    | "retail-exterior"
    | "tower-exterior"
    | "apartment-exterior"
    | "school-exterior"
    | "museum-exterior"
    | "commercial-exterior"
    | "residential-exterior"
    | "other-exterior";
  /**
   * Style for furniture and decor
   */
  style:
    | "minimalistic-interior"
    | "farmhouse-interior"
    | "luxury-interior"
    | "modern-interior"
    | "zen-interior"
    | "mid century-interior"
    | "airbnb-interior"
    | "cozy-interior"
    | "rustic-interior"
    | "christmas-interior"
    | "bohemian-interior"
    | "tropical-interior"
    | "industrial-interior"
    | "japanese-interior"
    | "vintage-interior"
    | "loft-interior"
    | "halloween-interior"
    | "soho-interior"
    | "baroque-interior"
    | "kids room-interior"
    | "girls room-interior"
    | "boys room-interior"
    | "scandinavian-interior"
    | "french country-interior"
    | "mediterranean-interior"
    | "cyberpunk-interior"
    | "hot pink-interior"
    | "biophilic-interior"
    | "ancient egypt-interior"
    | "pixel-interior"
    | "art deco-interior"
    | "modern-exterior"
    | "minimalistic-exterior"
    | "farmhouse-exterior"
    | "cozy-exterior"
    | "luxury-exterior"
    | "colonial-exterior"
    | "zen-exterior"
    | "asian-exterior"
    | "creepy-exterior"
    | "airstone-exterior"
    | "ancient greek-exterior"
    | "art deco-exterior"
    | "brutalist-exterior"
    | "christmas lights-exterior"
    | "contemporary-exterior"
    | "cottage-exterior"
    | "dutch colonial-exterior"
    | "federal colonial-exterior"
    | "fire-exterior"
    | "french provincial-exterior"
    | "full glass-exterior"
    | "georgian colonial-exterior"
    | "gothic-exterior"
    | "greek revival-exterior"
    | "ice-exterior"
    | "italianate-exterior"
    | "mediterranean-exterior"
    | "midcentury-exterior"
    | "middle eastern-exterior"
    | "minecraft-exterior"
    | "morocco-exterior"
    | "neoclassical-exterior"
    | "spanish-exterior"
    | "tudor-exterior"
    | "underwater-exterior"
    | "winter-exterior"
    | "yard lighting-exterior";
  /**
   * Color palette for furniture and decor
   */
  color_palette:
    | "surprise me"
    | "golden beige"
    | "refined blues"
    | "dusky elegance"
    | "emerald charm"
    | "crimson luxury"
    | "golden sapphire"
    | "soft pastures"
    | "candy sky"
    | "peach meadow"
    | "muted sands"
    | "ocean breeze"
    | "frosted pastels"
    | "spring bloom"
    | "gentle horizon"
    | "seaside breeze"
    | "azure coast"
    | "golden shore"
    | "mediterranean gem"
    | "ocean serenity"
    | "serene blush"
    | "muted horizon"
    | "pastel shores"
    | "dusky calm"
    | "woodland retreat"
    | "meadow glow"
    | "forest canopy"
    | "riverbank calm"
    | "earthy tones"
    | "earthy neutrals"
    | "arctic mist"
    | "aqua drift"
    | "blush bloom"
    | "coral haze"
    | "retro rust"
    | "autumn glow"
    | "rustic charm"
    | "vintage sage"
    | "faded plum"
    | "electric lime"
    | "violet pulse"
    | "neon sorbet"
    | "aqua glow"
    | "fluorescent sunset"
    | "lavender bloom"
    | "petal fresh"
    | "meadow light"
    | "sunny pastures"
    | "frosted mauve"
    | "snowy hearth"
    | "icy blues"
    | "winter twilight"
    | "earthy hues"
    | "stone balance"
    | "neutral sands"
    | "slate shades";
  /**
   * Additional elements to include in the options above (e.g., plants, lighting) Default value: `""`
   */
  additional_elements?: string;
  /**
   * It gives better rendering quality with more processing time, additional cost is 0.01 USD per image
   */
  enhanced_rendering?: boolean;
  /**
   * Strength of the input image Default value: `0.85`
   */
  input_image_strength?: number;
  /**
   * Custom prompt for architectural editing, it overrides above options when used Default value: `""`
   */
  custom_prompt?: string;
  /**
   * The format of the generated image. Choose from: 'jpeg' or 'png'. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type AiHomeStyleOutput = {
  /**
   * Generated image
   */
  image: Image;
  /**
   * Status message with processing details
   */
  status: string;
};
export type AIImageDetectionOutput = {
  /**
   *
   */
  verdict: string;
  /**
   *
   */
  confidence: number;
  /**
   *
   */
  is_ai_generated: boolean;
  /**
   *
   */
  latency: number;
};
export type AITextDetectionOutput = {
  /**
   *
   */
  verdict: string;
  /**
   *
   */
  confidence: number;
  /**
   *
   */
  is_ai_generated: boolean;
  /**
   *
   */
  latency: number;
};
export type AmEngOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type AmixAudioInput = {
  /**
   * List of audio file URLs to mix together
   */
  audio_urls: Array<string>;
  /**
   * Optional weights for each audio input. If fewer weights than inputs, last weight applies to remaining. Default is 1.0 for all
   */
  weights?: Array<number>;
  /**
   * How to determine output duration Default value: `"longest"`
   */
  duration?: "longest" | "shortest" | "first";
  /**
   * Transition time in seconds for volume renormalization when an input ends Default value: `2`
   */
  dropout_transition?: number;
  /**
   * Always scale inputs instead of only doing summation. Prevents clipping Default value: `true`
   */
  normalize?: boolean;
};
export type AmixAudioOutput = {
  /**
   * The mixed audio file
   */
  audio: AudioFile;
};
export type ApartmentStagingInput = {
  /**
   * The URL of the empty room image to furnish.
   */
  image_urls: Array<string>;
  /**
   * The prompt to generate a furnished room. Use 'furnish this room' for best results.
   */
  prompt: string;
  /**
   * The size of the generated image. If not provided, the size of the input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the apartment staging effect. Default value: `1`
   */
  lora_scale?: number;
};
export type ApartmentStagingOutput = {
  /**
   * The generated furnished room images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type ArbiterImageImageInput = {
  /**
   * The measurements to use for the measurement.
   */
  measurements: Array<"dists" | "mse" | "lpips" | "sdi" | "ssim">;
  /**
   * The inputs to use for the measurement.
   */
  inputs: Array<ReferenceImageInput>;
};
export type ArbiterImageImageOutput = {
  /**
   * The values of the measurements.
   */
  values?: Array<any>;
};
export type ArbiterImageInput = {
  /**
   * The measurements to use for the measurement.
   */
  measurements: Array<"arniqa" | "clip_iqa" | "musiq" | "nima" | "lapvar">;
  /**
   * The inputs to use for the measurement.
   */
  inputs: Array<ImageInput>;
};
export type ArbiterImageOutput = {
  /**
   * The values of the measurements.
   */
  values?: Array<any>;
};
export type ArbiterImageTextInput = {
  /**
   * The measurements to use for the measurement.
   */
  measurements: Array<"clip_score">;
  /**
   * The inputs to use for the measurement.
   */
  inputs: Array<SemanticImageInput>;
};
export type ArbiterImageTextOutput = {
  /**
   * The values of the measurements.
   */
  values?: Array<any>;
};
export type ArchEditInput = {
  /**
   * URL of the image to do architectural editing
   */
  input_image_url: string | Blob | File;
  /**
   * Type of editing. Structural editing only edits structural elements such as windows, walls etc. Virtual staging edits your furniture. Both do full editing including structural and furniture
   */
  editing_type: "structural editing" | "virtual staging" | "both";
  /**
   * Type of architecture for appropriate furniture selection
   */
  architecture_type:
    | "living room-interior"
    | "bedroom-interior"
    | "kitchen-interior"
    | "dining room-interior"
    | "bathroom-interior"
    | "laundry room-interior"
    | "home office-interior"
    | "study room-interior"
    | "dorm room-interior"
    | "coffee shop-interior"
    | "gaming room-interior"
    | "restaurant-interior"
    | "office-interior"
    | "attic-interior"
    | "toilet-interior"
    | "other-interior"
    | "house-exterior"
    | "villa-exterior"
    | "backyard-exterior"
    | "courtyard-exterior"
    | "ranch-exterior"
    | "office-exterior"
    | "retail-exterior"
    | "tower-exterior"
    | "apartment-exterior"
    | "school-exterior"
    | "museum-exterior"
    | "commercial-exterior"
    | "residential-exterior"
    | "other-exterior";
  /**
   * Style for furniture and decor
   */
  style:
    | "minimalistic-interior"
    | "farmhouse-interior"
    | "luxury-interior"
    | "modern-interior"
    | "zen-interior"
    | "mid century-interior"
    | "airbnb-interior"
    | "cozy-interior"
    | "rustic-interior"
    | "christmas-interior"
    | "bohemian-interior"
    | "tropical-interior"
    | "industrial-interior"
    | "japanese-interior"
    | "vintage-interior"
    | "loft-interior"
    | "halloween-interior"
    | "soho-interior"
    | "baroque-interior"
    | "kids room-interior"
    | "girls room-interior"
    | "boys room-interior"
    | "scandinavian-interior"
    | "french country-interior"
    | "mediterranean-interior"
    | "cyberpunk-interior"
    | "hot pink-interior"
    | "biophilic-interior"
    | "ancient egypt-interior"
    | "pixel-interior"
    | "art deco-interior"
    | "modern-exterior"
    | "minimalistic-exterior"
    | "farmhouse-exterior"
    | "cozy-exterior"
    | "luxury-exterior"
    | "colonial-exterior"
    | "zen-exterior"
    | "asian-exterior"
    | "creepy-exterior"
    | "airstone-exterior"
    | "ancient greek-exterior"
    | "art deco-exterior"
    | "brutalist-exterior"
    | "christmas lights-exterior"
    | "contemporary-exterior"
    | "cottage-exterior"
    | "dutch colonial-exterior"
    | "federal colonial-exterior"
    | "fire-exterior"
    | "french provincial-exterior"
    | "full glass-exterior"
    | "georgian colonial-exterior"
    | "gothic-exterior"
    | "greek revival-exterior"
    | "ice-exterior"
    | "italianate-exterior"
    | "mediterranean-exterior"
    | "midcentury-exterior"
    | "middle eastern-exterior"
    | "minecraft-exterior"
    | "morocco-exterior"
    | "neoclassical-exterior"
    | "spanish-exterior"
    | "tudor-exterior"
    | "underwater-exterior"
    | "winter-exterior"
    | "yard lighting-exterior";
  /**
   * Color palette for furniture and decor
   */
  color_palette:
    | "surprise me"
    | "golden beige"
    | "refined blues"
    | "dusky elegance"
    | "emerald charm"
    | "crimson luxury"
    | "golden sapphire"
    | "soft pastures"
    | "candy sky"
    | "peach meadow"
    | "muted sands"
    | "ocean breeze"
    | "frosted pastels"
    | "spring bloom"
    | "gentle horizon"
    | "seaside breeze"
    | "azure coast"
    | "golden shore"
    | "mediterranean gem"
    | "ocean serenity"
    | "serene blush"
    | "muted horizon"
    | "pastel shores"
    | "dusky calm"
    | "woodland retreat"
    | "meadow glow"
    | "forest canopy"
    | "riverbank calm"
    | "earthy tones"
    | "earthy neutrals"
    | "arctic mist"
    | "aqua drift"
    | "blush bloom"
    | "coral haze"
    | "retro rust"
    | "autumn glow"
    | "rustic charm"
    | "vintage sage"
    | "faded plum"
    | "electric lime"
    | "violet pulse"
    | "neon sorbet"
    | "aqua glow"
    | "fluorescent sunset"
    | "lavender bloom"
    | "petal fresh"
    | "meadow light"
    | "sunny pastures"
    | "frosted mauve"
    | "snowy hearth"
    | "icy blues"
    | "winter twilight"
    | "earthy hues"
    | "stone balance"
    | "neutral sands"
    | "slate shades";
  /**
   * Additional elements to include in the options above (e.g., plants, lighting) Default value: `""`
   */
  additional_elements?: string;
  /**
   * Custom prompt for architectural editing, it overrides above options when used Default value: `""`
   */
  custom_prompt?: string;
  /**
   * The format of the generated image. Choose from: 'jpeg' or 'png'. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type ArchEditOutput = {
  /**
   * Generated image
   */
  image: Image;
  /**
   * Status message with processing details
   */
  status: string;
};
export type ArchStyleInput = {
  /**
   * URL of the image to do architectural styling
   */
  input_image_url: string | Blob | File;
  /**
   * URL of the style image, optional. If given, other parameters are ignored Default value: `""`
   */
  style_image_url?: string | Blob | File;
  /**
   * Type of architecture for appropriate furniture selection
   */
  architecture_type:
    | "living room-interior"
    | "bedroom-interior"
    | "kitchen-interior"
    | "dining room-interior"
    | "bathroom-interior"
    | "laundry room-interior"
    | "home office-interior"
    | "study room-interior"
    | "dorm room-interior"
    | "coffee shop-interior"
    | "gaming room-interior"
    | "restaurant-interior"
    | "office-interior"
    | "attic-interior"
    | "toilet-interior"
    | "other-interior"
    | "house-exterior"
    | "villa-exterior"
    | "backyard-exterior"
    | "courtyard-exterior"
    | "ranch-exterior"
    | "office-exterior"
    | "retail-exterior"
    | "tower-exterior"
    | "apartment-exterior"
    | "school-exterior"
    | "museum-exterior"
    | "commercial-exterior"
    | "residential-exterior"
    | "other-exterior";
  /**
   * Style for furniture and decor
   */
  style:
    | "minimalistic-interior"
    | "farmhouse-interior"
    | "luxury-interior"
    | "modern-interior"
    | "zen-interior"
    | "mid century-interior"
    | "airbnb-interior"
    | "cozy-interior"
    | "rustic-interior"
    | "christmas-interior"
    | "bohemian-interior"
    | "tropical-interior"
    | "industrial-interior"
    | "japanese-interior"
    | "vintage-interior"
    | "loft-interior"
    | "halloween-interior"
    | "soho-interior"
    | "baroque-interior"
    | "kids room-interior"
    | "girls room-interior"
    | "boys room-interior"
    | "scandinavian-interior"
    | "french country-interior"
    | "mediterranean-interior"
    | "cyberpunk-interior"
    | "hot pink-interior"
    | "biophilic-interior"
    | "ancient egypt-interior"
    | "pixel-interior"
    | "art deco-interior"
    | "modern-exterior"
    | "minimalistic-exterior"
    | "farmhouse-exterior"
    | "cozy-exterior"
    | "luxury-exterior"
    | "colonial-exterior"
    | "zen-exterior"
    | "asian-exterior"
    | "creepy-exterior"
    | "airstone-exterior"
    | "ancient greek-exterior"
    | "art deco-exterior"
    | "brutalist-exterior"
    | "christmas lights-exterior"
    | "contemporary-exterior"
    | "cottage-exterior"
    | "dutch colonial-exterior"
    | "federal colonial-exterior"
    | "fire-exterior"
    | "french provincial-exterior"
    | "full glass-exterior"
    | "georgian colonial-exterior"
    | "gothic-exterior"
    | "greek revival-exterior"
    | "ice-exterior"
    | "italianate-exterior"
    | "mediterranean-exterior"
    | "midcentury-exterior"
    | "middle eastern-exterior"
    | "minecraft-exterior"
    | "morocco-exterior"
    | "neoclassical-exterior"
    | "spanish-exterior"
    | "tudor-exterior"
    | "underwater-exterior"
    | "winter-exterior"
    | "yard lighting-exterior";
  /**
   * Color palette for furniture and decor
   */
  color_palette:
    | "surprise me"
    | "golden beige"
    | "refined blues"
    | "dusky elegance"
    | "emerald charm"
    | "crimson luxury"
    | "golden sapphire"
    | "soft pastures"
    | "candy sky"
    | "peach meadow"
    | "muted sands"
    | "ocean breeze"
    | "frosted pastels"
    | "spring bloom"
    | "gentle horizon"
    | "seaside breeze"
    | "azure coast"
    | "golden shore"
    | "mediterranean gem"
    | "ocean serenity"
    | "serene blush"
    | "muted horizon"
    | "pastel shores"
    | "dusky calm"
    | "woodland retreat"
    | "meadow glow"
    | "forest canopy"
    | "riverbank calm"
    | "earthy tones"
    | "earthy neutrals"
    | "arctic mist"
    | "aqua drift"
    | "blush bloom"
    | "coral haze"
    | "retro rust"
    | "autumn glow"
    | "rustic charm"
    | "vintage sage"
    | "faded plum"
    | "electric lime"
    | "violet pulse"
    | "neon sorbet"
    | "aqua glow"
    | "fluorescent sunset"
    | "lavender bloom"
    | "petal fresh"
    | "meadow light"
    | "sunny pastures"
    | "frosted mauve"
    | "snowy hearth"
    | "icy blues"
    | "winter twilight"
    | "earthy hues"
    | "stone balance"
    | "neutral sands"
    | "slate shades";
  /**
   * Additional elements to include in the options above (e.g., plants, lighting) Default value: `""`
   */
  additional_elements?: string;
  /**
   * It gives better rendering quality with more processing time, additional cost is 0.01 USD per image
   */
  enhanced_rendering?: boolean;
  /**
   * Strength of the input image Default value: `0.85`
   */
  input_image_strength?: number;
  /**
   * Custom prompt for architectural editing, it overrides above options when used Default value: `""`
   */
  custom_prompt?: string;
  /**
   * The format of the generated image. Choose from: 'jpeg' or 'png'. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type ArchStyleOutput = {
  /**
   * Generated image
   */
  image: Image;
  /**
   * Status message with processing details
   */
  status: string;
};
export type Audio2VideoInput = {
  /**
   * The avatar to use for the video
   */
  avatar_id:
    | "emily_vertical_primary"
    | "emily_vertical_secondary"
    | "marcus_vertical_primary"
    | "marcus_vertical_secondary"
    | "mira_vertical_primary"
    | "mira_vertical_secondary"
    | "jasmine_vertical_primary"
    | "jasmine_vertical_secondary"
    | "jasmine_vertical_walking"
    | "aisha_vertical_walking"
    | "elena_vertical_primary"
    | "elena_vertical_secondary"
    | "any_male_vertical_primary"
    | "any_female_vertical_primary"
    | "any_male_vertical_secondary"
    | "any_female_vertical_secondary"
    | "any_female_vertical_walking"
    | "emily_primary"
    | "emily_side"
    | "marcus_primary"
    | "marcus_side"
    | "aisha_walking"
    | "elena_primary"
    | "elena_side"
    | "any_male_primary"
    | "any_female_primary"
    | "any_male_side"
    | "any_female_side";
  /**
   *
   */
  audio_url: string | Blob | File;
};
export type AudioCompressorInput = {
  /**
   * URL of the audio file to compress
   */
  audio_url: string | Blob | File;
  /**
   * Threshold level in dB above which compression is applied (-60 to 0) Default value: `-18`
   */
  threshold?: number;
  /**
   * Compression ratio (1 = no compression, higher = more compression) Default value: `3`
   */
  ratio?: number;
  /**
   * Attack time in milliseconds (how fast compression starts) Default value: `5`
   */
  attack?: number;
  /**
   * Release time in milliseconds (how fast compression stops) Default value: `50`
   */
  release?: number;
  /**
   * Makeup gain in dB to compensate for volume reduction Default value: `8`
   */
  makeup?: number;
  /**
   * Knee width in dB for soft knee compression (0 = hard knee) Default value: `2.83`
   */
  knee?: number;
  /**
   * Output audio bitrate Default value: `"192k"`
   */
  output_bitrate?: "128k" | "192k" | "256k" | "320k";
};
export type AudioCompressorOutput = {
  /**
   * The compressed audio file
   */
  audio: AudioFile;
};
export type AudioEnterpriseInput = {
  /**
   * URL or data URI of the audio file to process. Supported formats: wav, mp3, aiff, aac, ogg, flac, m4a.
   */
  audio_url: string | Blob | File;
  /**
   * Prompt to be used for the audio processing
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type AudioEqualizerInput = {
  /**
   * URL of the audio file to equalize
   */
  audio_url: string | Blob | File;
  /**
   * List of EQ bands to apply. Each band has frequency, width, and gain.
   */
  bands: Array<EQBand>;
  /**
   * Output audio bitrate Default value: `"192k"`
   */
  output_bitrate?: "128k" | "192k" | "256k" | "320k";
};
export type AudioEqualizerOutput = {
  /**
   * The equalized audio file
   */
  audio: AudioFile;
};
export type AudioInput = {
  /**
   * The input audio file
   */
  url: string;
};
export type AudioOutput = {
  /**
   * The generated audio file.
   */
  audio: File;
  /**
   * The prompt used to generate the audio.
   */
  prompt: string;
};
export type AudioToAudioInput = {
  /**
   * The prompt to guide the audio generation
   */
  prompt: string;
  /**
   * The audio clip to transform
   */
  audio_url: string | Blob | File;
  /**
   * Sometimes referred to as denoising, this parameter controls how much influence the `audio_url` parameter has on the generated audio. A value of 0 would yield audio that is identical to the input. A value of 1 would be as if you passed in no audio at all. Default value: `0.8`
   */
  strength?: number;
  /**
   * The number of steps to denoise the audio for Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio.
   */
  total_seconds?: number;
  /**
   * How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt). Default value: `1`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   *
   */
  seed?: number;
};
export type AudioToAudioOutput = {
  /**
   * The generated audio clip
   */
  audio: File;
  /**
   * The random seed used for generation
   */
  seed: number;
};
export type AudioUnderstandingInput = {
  /**
   * URL of the audio file to analyze
   */
  audio_url: string | Blob | File;
  /**
   * The question or prompt about the audio content.
   */
  prompt: string;
  /**
   * Whether to request a more detailed analysis of the audio
   */
  detailed_analysis?: boolean;
};
export type AudioUnderstandingOutput = {
  /**
   * The analysis of the audio content based on the prompt
   */
  output: string;
};
export type AuraSrInput = {
  /**
   * URL of the image to upscale.
   */
  image_url: string | Blob | File;
  /**
   * Upscaling factor. More coming soon. Default value: `"4"`
   */
  upscaling_factor?: "4";
  /**
   * Whether to use overlapping tiles for upscaling. Setting this to true helps remove seams but doubles the inference time.
   */
  overlapping_tiles?: boolean;
  /**
   * Checkpoint to use for upscaling. More coming soon. Default value: `"v1"`
   */
  checkpoint?: "v1" | "v2";
};
export type AuraSrOutput = {
  /**
   * Upscaled image
   */
  image: Image;
  /**
   * Timings for each step in the pipeline.
   */
  timings: any;
};
export type AutoSubtitleInput = {
  /**
   * URL of the video file to add automatic subtitles to
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra') Default value: `"en"`
   */
  language?: string;
  /**
   * Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty') Default value: `"Montserrat"`
   */
  font_name?: string;
  /**
   * Font size for subtitles (TikTok style uses larger text) Default value: `100`
   */
  font_size?: number;
  /**
   * Font weight (TikTok style typically uses bold or black) Default value: `"bold"`
   */
  font_weight?: "normal" | "bold" | "black";
  /**
   * Subtitle text color for non-active words Default value: `"white"`
   */
  font_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Color for the currently speaking word (karaoke-style highlight) Default value: `"purple"`
   */
  highlight_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Text stroke/outline width in pixels (0 for no stroke) Default value: `3`
   */
  stroke_width?: number;
  /**
   * Text stroke/outline color Default value: `"black"`
   */
  stroke_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Background color behind text ('none' or 'transparent' for no background) Default value: `"none"`
   */
  background_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta"
    | "none"
    | "transparent";
  /**
   * Background opacity (0.0 = fully transparent, 1.0 = fully opaque)
   */
  background_opacity?: number;
  /**
   * Vertical position of subtitles Default value: `"bottom"`
   */
  position?: "top" | "center" | "bottom";
  /**
   * Vertical offset in pixels (positive = move down, negative = move up) Default value: `75`
   */
  y_offset?: number;
  /**
   * Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences. Default value: `3`
   */
  words_per_subtitle?: number;
  /**
   * Enable animation effects for subtitles (bounce style entrance) Default value: `true`
   */
  enable_animation?: boolean;
};
export type AutoSubtitleOutput = {
  /**
   * The video with automatic subtitles
   */
  video: File;
  /**
   * Full transcription text
   */
  transcription: string;
  /**
   * Number of subtitle segments generated
   */
  subtitle_count: number;
  /**
   * Word-level timing information from transcription service
   */
  words?: Array<any>;
  /**
   * Additional transcription metadata from ElevenLabs (language, segments, etc.)
   */
  transcription_metadata?: any;
};
export type AvatarsAudioToVideoInput = {
  /**
   * The avatar to use for the video
   */
  avatar_id:
    | "emily_vertical_primary"
    | "emily_vertical_secondary"
    | "marcus_vertical_primary"
    | "marcus_vertical_secondary"
    | "mira_vertical_primary"
    | "mira_vertical_secondary"
    | "jasmine_vertical_primary"
    | "jasmine_vertical_secondary"
    | "jasmine_vertical_walking"
    | "aisha_vertical_walking"
    | "elena_vertical_primary"
    | "elena_vertical_secondary"
    | "any_male_vertical_primary"
    | "any_female_vertical_primary"
    | "any_male_vertical_secondary"
    | "any_female_vertical_secondary"
    | "any_female_vertical_walking"
    | "emily_primary"
    | "emily_side"
    | "marcus_primary"
    | "marcus_side"
    | "aisha_walking"
    | "elena_primary"
    | "elena_side"
    | "any_male_primary"
    | "any_female_primary"
    | "any_male_side"
    | "any_female_side";
  /**
   *
   */
  audio_url: string | Blob | File;
};
export type AvatarsAudioToVideoOutput = {
  /**
   *
   */
  video: File;
};
export type AvatarsTextToVideoInput = {
  /**
   * The avatar to use for the video
   */
  avatar_id:
    | "emily_vertical_primary"
    | "emily_vertical_secondary"
    | "marcus_vertical_primary"
    | "marcus_vertical_secondary"
    | "mira_vertical_primary"
    | "mira_vertical_secondary"
    | "jasmine_vertical_primary"
    | "jasmine_vertical_secondary"
    | "jasmine_vertical_walking"
    | "aisha_vertical_walking"
    | "elena_vertical_primary"
    | "elena_vertical_secondary"
    | "any_male_vertical_primary"
    | "any_female_vertical_primary"
    | "any_male_vertical_secondary"
    | "any_female_vertical_secondary"
    | "any_female_vertical_walking"
    | "emily_primary"
    | "emily_side"
    | "marcus_primary"
    | "marcus_side"
    | "aisha_walking"
    | "elena_primary"
    | "elena_side"
    | "any_male_primary"
    | "any_female_primary"
    | "any_male_side"
    | "any_female_side";
  /**
   *
   */
  text: string;
};
export type AvatarsTextToVideoOutput = {
  /**
   *
   */
  video: File;
};
export type BabyVersionInput = {
  /**
   * URL of the image to transform into a baby version.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BabyVersionOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type BackgroundChangeInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The desired background to apply. Default value: `"beach sunset with palm trees"`
   */
  prompt?: string;
};
export type BackgroundChangeOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type BagelEditInput = {
  /**
   * The prompt to edit the image with.
   */
  prompt: string;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * Whether to use thought tokens for generation. If set to true, the model will "think" to potentially improve generation quality. Increases generation time and increases the cost by 20%.
   */
  use_thought?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The image to edit.
   */
  image_url: string | Blob | File;
};
export type BagelEditOutput = {
  /**
   * The edited images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type bagelInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * Whether to use thought tokens for generation. If set to true, the model will "think" to potentially improve generation quality. Increases generation time and increases the cost by 20%.
   */
  use_thought?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type bagelOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type BagelUnderstandInput = {
  /**
   * The image for the query.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to query the image with.
   */
  prompt: string;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
};
export type BagelUnderstandOutput = {
  /**
   * The answer to the query.
   */
  text: string;
  /**
   * The seed used for the generation.
   */
  seed: number;
  /**
   * The query used for the generation.
   */
  prompt: string;
  /**
   * The timings of the generation.
   */
  timings: any;
};
export type BallpointPenSketchInput = {
  /**
   * The prompt to generate a ballpoint pen sketch style image. Use 'b4llp01nt' trigger word for best results.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the ballpoint pen sketch effect. Default value: `1`
   */
  lora_scale?: number;
};
export type BallpointPenSketchOutput = {
  /**
   * The generated ballpoint pen sketch style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type BaseFlux1ImageToInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseFlux1Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseFlux1ReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseImageToInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BaseKontextEditInput = {
  /**
   * The URL of the image to edit.
   *
   * Max width: 14142px, Max height: 14142px, Timeout: 20s
   */
  image_url: string | Blob | File;
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Determines how the output resolution is set for image editing.
   * - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.
   * - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).
   * Apart from these, a few aspect ratios are also supported. Default value: `"match_input"`
   */
  resolution_mode?:
    | "auto"
    | "match_input"
    | "1:1"
    | "16:9"
    | "21:9"
    | "3:2"
    | "2:3"
    | "4:5"
    | "5:4"
    | "3:4"
    | "4:3"
    | "9:16"
    | "9:21";
};
export type BaseKontextImg2ImgInput = {
  /**
   * The prompt for the image to image task.
   */
  prompt: string;
  /**
   * The URL of the image for image-to-image.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`
   */
  strength?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output format Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseKontextInpaintInput = {
  /**
   * The URL of the image to be inpainted.
   */
  image_url: string | Blob | File;
  /**
   * The prompt for the image to image task.
   */
  prompt: string;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The URL of the reference image for inpainting.
   */
  reference_image_url: string | Blob | File;
  /**
   * The URL of the mask for inpainting.
   */
  mask_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`
   */
  strength?: number;
};
export type BaseKontextInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output format Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseKreaFlux1ImageToInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseKreaFlux1Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseKreaFlux1ReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseKreaImageToInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseKreaInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseKreaReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseQwenEditImageInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseQwenEditImg2ImgInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Strength of the image-to-image transformation. Lower values preserve more of the original image. Default value: `0.94`
   */
  strength?: number;
};
export type BaseQwenEditInpaintImageInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The URL of the mask for inpainting
   */
  mask_url: string | Blob | File;
  /**
   * Strength of noising process for inpainting Default value: `0.93`
   */
  strength?: number;
};
export type BaseQwenImageInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2).
   */
  use_turbo?: boolean;
};
export type BaseReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseSRPOFlux1ImageToInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseSRPOFlux1Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseSRPOImageToInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseSRPOInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type BaseTextToImageInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "10:16"
    | "16:10"
    | "9:16"
    | "16:9"
    | "4:3"
    | "3:4"
    | "1:1"
    | "1:3"
    | "3:1"
    | "3:2"
    | "2:3";
  /**
   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The style of the generated image Default value: `"auto"`
   */
  style?: "auto" | "general" | "realistic" | "design" | "render_3D" | "anime";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BboxInput = {
  /**
   * The URL of the image to remove objects from.
   */
  image_url: string | Blob | File;
  /**
   * List of bounding box coordinates to erase (only one box prompt is supported)
   */
  box_prompts?: Array<BBoxPromptBase>;
  /**
   *  Default value: `"best_quality"`
   */
  model?: "low_quality" | "medium_quality" | "high_quality" | "best_quality";
  /**
   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`
   */
  mask_expansion?: number;
};
export type BenV2ImageInput = {
  /**
   * URL of image to be used for background removal
   */
  image_url: string | Blob | File;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
};
export type BenV2ImageOutput = {
  /**
   * The output image after background removal.
   */
  image: Image;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
};
export type BenV2VideoInput = {
  /**
   * URL of video to be used for background removal.
   */
  video_url: string | Blob | File;
  /**
   * Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]
   */
  background_color?: Array<void>;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
};
export type BenV2VideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
};
export type BGRemoveBatchedInput = {
  /**
   * List of image URLs to be processed (maximum 32 images)
   */
  images_data_url: string | Blob | File;
};
export type BGRemoveBatchedOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
};
export type BGRemoveInput = {
  /**
   * Input Image to erase from
   */
  image_url: string | Blob | File;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BGRemoveOutput = {
  /**
   * The generated image
   */
  image: Image;
};
export type BGReplaceInput = {
  /**
   * Input Image to erase from
   */
  image_url: string | Blob | File;
  /**
   * The URL of the reference image to be used for generating the new background. Use "" to leave empty. Either ref_image_url or bg_prompt has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `""`
   */
  ref_image_url?: string | Blob | File;
  /**
   * The prompt you would like to use to generate images.
   */
  prompt?: string;
  /**
   * The negative prompt you would like to use to generate images. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Whether to refine prompt Default value: `true`
   */
  refine_prompt?: boolean;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Whether to use the fast model Default value: `true`
   */
  fast?: boolean;
  /**
   * Number of Images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BGReplaceOutput = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type BirefnetV2VideoInput = {
  /**
   * Model to use for background removal.
   * The 'General Use (Light)' model is the original model used in the BiRefNet repository.
   * The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images.
   * The 'General Use (Heavy)' model is a slower but more accurate model.
   * The 'Matting' model is a model trained specifically for matting images.
   * The 'Portrait' model is a model trained specifically for portrait images.
   * The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304.
   * The 'General Use (Light)' model is recommended for most use cases.
   *
   * The corresponding models are as follows:
   * - 'General Use (Light)': BiRefNet
   * - 'General Use (Light 2K)': BiRefNet_lite-2K
   * - 'General Use (Heavy)': BiRefNet_lite
   * - 'Matting': BiRefNet-matting
   * - 'Portrait': BiRefNet-portrait
   * - 'General Use (Dynamic)': BiRefNet_dynamic Default value: `"General Use (Light)"`
   */
  model?:
    | "General Use (Light)"
    | "General Use (Light 2K)"
    | "General Use (Heavy)"
    | "Matting"
    | "Portrait"
    | "General Use (Dynamic)";
  /**
   * The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model. Default value: `"1024x1024"`
   */
  operating_resolution?: "1024x1024" | "2048x2048" | "2304x2304";
  /**
   * Whether to output the mask used to remove the background
   */
  output_mask?: boolean;
  /**
   * Whether to refine the foreground using the estimated mask Default value: `true`
   */
  refine_foreground?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * URL of the video to remove background from
   */
  video_url: string | Blob | File;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
};
export type BirefnetV2VideoOutput = {
  /**
   * Video with background removed
   */
  video: VideoFile;
  /**
   * Mask used to remove the background
   */
  mask_video?: VideoFile;
};
export type BlendingInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * Instruct what elements you would like to blend in your image.
   */
  instruction: string;
};
export type BlendVideoInput = {
  /**
   * URL of the top layer video
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  top_video_url: string | Blob | File;
  /**
   * URL of the bottom layer video
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  bottom_video_url: string | Blob | File;
  /**
   * Blend mode to use for combining the videos Default value: `"overlay"`
   */
  blend_mode?:
    | "addition"
    | "average"
    | "burn"
    | "darken"
    | "difference"
    | "divide"
    | "dodge"
    | "exclusion"
    | "grainextract"
    | "grainmerge"
    | "hardlight"
    | "lighten"
    | "multiply"
    | "negation"
    | "normal"
    | "overlay"
    | "phoenix"
    | "pinlight"
    | "reflect"
    | "screen"
    | "softlight"
    | "subtract"
    | "vividlight";
  /**
   * Opacity of the top layer (0.0-1.0) Default value: `1`
   */
  opacity?: number;
  /**
   * End output when the shortest input ends Default value: `true`
   */
  shortest?: boolean;
};
export type BlendVideoOutput = {
  /**
   * The blended video output
   */
  video: File;
};
export type BlurInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Type of blur to apply Default value: `"gaussian"`
   */
  blur_type?: "gaussian" | "kuwahara";
  /**
   * Blur radius Default value: `3`
   */
  blur_radius?: number;
  /**
   * Sigma for Gaussian blur Default value: `1`
   */
  blur_sigma?: number;
};
export type BlurOutput = {
  /**
   * The processed images with blur effect
   */
  images: Array<Image>;
};
export type BrEngOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type BriaReimagineInput = {
  /**
   * The prompt you would like to use to generate images.
   */
  prompt: string;
  /**
   * The URL of the structure reference image. Use "" to leave empty. Accepted formats are jpeg, jpg, png, webp. Default value: `""`
   */
  structure_image_url?: string | Blob | File;
  /**
   * The influence of the structure reference on the generated image. Default value: `0.75`
   */
  structure_ref_influence?: number;
  /**
   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `1`
   */
  num_results?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Whether to use the fast model Default value: `true`
   */
  fast?: boolean;
  /**
   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BriaReimagineOutput = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type BriaVideoEraserEraseKeypointsInput = {
  /**
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4. Default value: `"mp4_h264"`
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * auto trim the video, to working duration ( 5s ) Default value: `true`
   */
  auto_trim?: boolean;
  /**
   * If true, audio will be preserved in the output video. Default value: `true`
   */
  preserve_audio?: boolean;
  /**
   * Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}
   */
  keypoints: Array<string>;
  /**
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string | Blob | File;
};
export type BriaVideoEraserEraseKeypointsOutput = {
  /**
   * Final video.
   */
  video: Video | File;
};
export type BriaVideoEraserEraseMaskInput = {
  /**
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4. Default value: `"mp4_h264"`
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * auto trim the video, to working duration ( 5s ) Default value: `true`
   */
  auto_trim?: boolean;
  /**
   * If true, audio will be preserved in the output video. Default value: `true`
   */
  preserve_audio?: boolean;
  /**
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string | Blob | File;
  /**
   * Input video to mask erase object from. duration must be less than 5s.
   */
  mask_video_url: string | Blob | File;
};
export type BriaVideoEraserEraseMaskOutput = {
  /**
   * Final video.
   */
  video: Video | File;
};
export type BriaVideoEraserErasePromptInput = {
  /**
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4. Default value: `"mp4_h264"`
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * auto trim the video, to working duration ( 5s ) Default value: `true`
   */
  auto_trim?: boolean;
  /**
   * If true, audio will be preserved in the output video. Default value: `true`
   */
  preserve_audio?: boolean;
  /**
   * Input prompt to detect object to erase
   */
  prompt: string;
  /**
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string | Blob | File;
};
export type BriaVideoEraserErasePromptOutput = {
  /**
   * Final video.
   */
  video: Video | File;
};
export type BroccoliHaircutInput = {
  /**
   * URL of the image to apply broccoli haircut style.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BroccoliHaircutOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type BrPortugeseOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type BytedanceDreamactorV2Input = {
  /**
   * The URL of the reference image to animate. Supports real people, animation, pets, etc. Format: jpeg, jpg or png. Max size: 4.7 MB. Resolution: between 480x480 and 1920x1080 (larger images will be proportionally reduced).
   */
  image_url: string | Blob | File;
  /**
   * The URL of the driving template video providing motion, facial expressions, and lip movement reference. Max duration: 30 seconds. Format: mp4, mov or webm. Resolution: between 200x200 and 2048x1440. Supports full face and body driving.
   */
  video_url: string | Blob | File;
  /**
   * Whether to crop the first second of the output video. The output has a 1-second transition at the beginning; enable this to remove it. Default value: `true`
   */
  trim_first_second?: boolean;
};
export type BytedanceDreamactorV2Output = {
  /**
   * Generated video file.
   */
  video: File;
};
export type BytedanceDreaminaV31TextToImageInput = {
  /**
   * The text prompt used to generate the image
   */
  prompt: string;
  /**
   * The size of the generated image. Width and height must be between 512 and 2048.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use an LLM to enhance the prompt
   */
  enhance_prompt?: boolean;
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BytedanceDreaminaV31TextToImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceOmnihumanInput = {
  /**
   * The URL of the image used to generate the video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file to generate the video. Audio must be under 30s long.
   */
  audio_url: string | Blob | File;
};
export type BytedanceOmnihumanOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Duration of audio input/video output as used for billing.
   */
  duration: number;
};
export type BytedanceOmnihumanV15Input = {
  /**
   * The text prompt used to guide the video generation.
   */
  prompt?: string;
  /**
   * The URL of the image used to generate the video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file to generate the video. Audio must be under 30s long for 1080p generation and under 60s long for 720p generation.
   */
  audio_url: string | Blob | File;
  /**
   * Generate a video at a faster rate with a slight quality trade-off.
   */
  turbo_mode?: boolean;
  /**
   * The resolution of the generated video. Defaults to 1080p. 720p generation is faster and higher in quality. 1080p generation is limited to 30s audio and 720p generation is limited to 60s audio. Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
};
export type BytedanceOmnihumanV15Output = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Duration of audio input/video output as used for billing.
   */
  duration: number;
};
export type BytedanceSeed3dImageTo3dInput = {
  /**
   * URL of the image for the 3D asset generation.
   */
  image_url: string | Blob | File;
};
export type BytedanceSeed3dImageTo3dOutput = {
  /**
   * The generated 3D model files
   */
  model: File;
  /**
   * The number of tokens used for the 3D model generation
   */
  usage_tokens: number;
};
export type BytedanceSeedanceV15ProImageToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to generate audio for the video Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The URL of the image used to generate video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image the video ends with. Defaults to None.
   */
  end_image_url?: string | Blob | File;
};
export type BytedanceSeedanceV15ProImageToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedanceV15ProTextToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to generate audio for the video Default value: `true`
   */
  generate_audio?: boolean;
};
export type BytedanceSeedanceV15ProTextToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedanceV1LiteImageToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The URL of the image used to generate video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image the video ends with. Defaults to None.
   */
  end_image_url?: string | Blob | File;
};
export type BytedanceSeedanceV1LiteImageToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedanceV1LiteReferenceToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Reference images to generate the video with.
   */
  reference_image_urls: Array<string>;
};
export type BytedanceSeedanceV1LiteReferenceToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedanceV1LiteTextToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "9:21";
  /**
   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type BytedanceSeedanceV1LiteTextToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedanceV1ProFastImageToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The URL of the image used to generate video
   */
  image_url: string | Blob | File;
};
export type BytedanceSeedanceV1ProFastImageToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedanceV1ProFastTextToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type BytedanceSeedanceV1ProFastTextToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedanceV1ProImageToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The URL of the image used to generate video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image the video ends with. Defaults to None.
   */
  end_image_url?: string | Blob | File;
};
export type BytedanceSeedanceV1ProImageToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedanceV1ProTextToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type BytedanceSeedanceV1ProTextToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeededitV3EditImageInput = {
  /**
   * The text prompt used to edit the image
   */
  prompt: string;
  /**
   * URL of the image to be edited.
   */
  image_url: string | Blob | File;
  /**
   * Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation. Default value: `0.5`
   */
  guidance_scale?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BytedanceSeededitV3EditImageOutput = {
  /**
   * Generated image
   */
  image: Image;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedreamV3TextToImageInput = {
  /**
   * The text prompt used to generate the image
   */
  prompt: string;
  /**
   * Use for finer control over the output image size. Will be used over aspect_ratio, if both are provided. Width and height must be between 512 and 2048.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type BytedanceSeedreamV3TextToImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedreamV45EditInput = {
  /**
   * The text prompt used to edit the image
   */
  prompt: string;
  /**
   * The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto_2K"
    | "auto_4K";
  /**
   * Number of separate model generations to be run with the prompt. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15 Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used.
   */
  image_urls: Array<string>;
};
export type BytedanceSeedreamV45EditOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type BytedanceSeedreamV45TextToImageInput = {
  /**
   * The text prompt used to generate the image
   */
  prompt: string;
  /**
   * The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto_2K"
    | "auto_4K";
  /**
   * Number of separate model generations to be run with the prompt. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type BytedanceSeedreamV45TextToImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedreamV4EditInput = {
  /**
   * The text prompt used to edit the image
   */
  prompt: string;
  /**
   * The size of the generated image. The minimum total image area is 921600 pixels. Failing this, the image size will be adjusted to by scaling it up, while maintaining the aspect ratio.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto"
    | "auto_2K"
    | "auto_4K";
  /**
   * Number of separate model generations to be run with the prompt. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15 Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate. Default value: `"standard"`
   */
  enhance_prompt_mode?: "standard" | "fast";
  /**
   * List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used.
   */
  image_urls: Array<string>;
};
export type BytedanceSeedreamV4EditOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceSeedreamV4TextToImageInput = {
  /**
   * The text prompt used to generate the image
   */
  prompt: string;
  /**
   * The size of the generated image. Total pixels must be between 960x960 and 4096x4096.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto"
    | "auto_2K"
    | "auto_4K";
  /**
   * Number of separate model generations to be run with the prompt. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate. Default value: `"standard"`
   */
  enhance_prompt_mode?: "standard" | "fast";
};
export type BytedanceSeedreamV4TextToImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type BytedanceUpscalerUpscaleVideoInput = {
  /**
   * The URL of the video to upscale.
   */
  video_url: string | Blob | File;
  /**
   * The target resolution of the video to upscale. Default value: `"1080p"`
   */
  target_resolution?: "1080p" | "2k" | "4k";
  /**
   * The target FPS of the video to upscale. Default value: `"30fps"`
   */
  target_fps?: "30fps" | "60fps";
};
export type BytedanceUpscalerUpscaleVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Duration of audio input/video output as used for billing.
   */
  duration: number;
};
export type BytedanceVideoStylizeInput = {
  /**
   * The style for your character in the video. Please use a short description.
   */
  style: string;
  /**
   * URL of the image to make the stylized video from.
   */
  image_url: string | Blob | File;
};
export type calligrapherInput = {
  /**
   * Base64-encoded source image with drawn mask layers
   */
  source_image_url: string | Blob | File;
  /**
   * Base64-encoded mask image (optional if using auto_mask_generation)
   */
  mask_image_url?: string | Blob | File;
  /**
   * Optional base64 reference image for style
   */
  reference_image_url?: string | Blob | File;
  /**
   * Text prompt to inpaint or customize
   */
  prompt: string;
  /**
   * How many images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Target image size for generation
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance or strength scale for the model Default value: `1`
   */
  cfg_scale?: number;
  /**
   * Number of inference steps (1-100) Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducibility
   */
  seed?: number;
  /**
   * Whether to prepend context reference to the input Default value: `true`
   */
  use_context?: boolean;
  /**
   * Whether to automatically generate mask from detected text
   */
  auto_mask_generation?: boolean;
  /**
   * Source text to replace (if empty, masks all detected text) Default value: `""`
   */
  source_text?: string;
};
export type calligrapherOutput = {
  /**
   *
   */
  images: Array<Image>;
};
export type cartoonifyInput = {
  /**
   * URL of the image to apply Pixar style to
   */
  image_url: string | Blob | File;
  /**
   * Scale factor for the Pixar effect Default value: `1`
   */
  scale?: number;
  /**
   * Guidance scale for the generation Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to use CFG zero
   */
  use_cfg_zero?: boolean;
  /**
   * The seed for image generation. Same seed with same parameters will generate same image.
   */
  seed?: number;
};
export type cartoonifyOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type CartoonifyOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ChainOfZoomInput = {
  /**
   * Input image to zoom into
   */
  image_url: string | Blob | File;
  /**
   * Zoom scale in powers of 2 Default value: `5`
   */
  scale?: number;
  /**
   * X coordinate of zoom center (0-1) Default value: `0.5`
   */
  center_x?: number;
  /**
   * Y coordinate of zoom center (0-1) Default value: `0.5`
   */
  center_y?: number;
  /**
   * Additional prompt text to guide the zoom enhancement Default value: `""`
   */
  user_prompt?: string;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
};
export type ChainOfZoomOutput = {
  /**
   * List of intermediate images
   */
  images: Array<Image>;
  /**
   * Actual linear zoom scale applied
   */
  scale: number;
  /**
   * Center coordinates used for zoom
   */
  zoom_center: Array<number>;
};
export type ChatEnterpriseInput = {
  /**
   * Prompt to be used for the chat completion
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type ChatInput = {
  /**
   * Prompt to be used for the chat completion
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type ChatOutput = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Whether the output is partial
   */
  partial?: boolean;
  /**
   * Error message if an error occurred
   */
  error?: string;
  /**
   * Usage information
   */
  usage?: CompletionUsage;
};
export type ChatterboxhdSpeechToSpeechInput = {
  /**
   * URL to the source audio file to be voice-converted.
   */
  source_audio_url: string | Blob | File;
  /**
   * The voice to use for the speech-to-speech request. If neither target_voice nor target_voice_audio_url are provided, a random target voice will be used.
   */
  target_voice?:
    | "Aurora"
    | "Blade"
    | "Britney"
    | "Carl"
    | "Cliff"
    | "Richard"
    | "Rico"
    | "Siobhan"
    | "Vicky";
  /**
   * URL to the audio file which represents the voice of the output audio. If provided, this will override the target_voice setting. If neither target_voice nor target_voice_audio_url are provided, the default target voice will be used.
   */
  target_voice_audio_url?: string | Blob | File;
  /**
   * If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz.
   */
  high_quality_audio?: boolean;
};
export type ChatterboxhdSpeechToSpeechOutput = {
  /**
   * The generated voice-converted audio file.
   */
  audio: Audio;
};
export type ChatterboxhdTextToSpeechInput = {
  /**
   * Text to synthesize into speech. Default value: `"My name is Maximus Decimus Meridius, commander of the Armies of the North, General of the Felix Legions and loyal servant to the true emperor, Marcus Aurelius. Father to a murdered son, husband to a murdered wife. And I will have my vengeance, in this life or the next."`
   */
  text?: string;
  /**
   * The voice to use for the TTS request. If neither voice nor audio are provided, a random voice will be used.
   */
  voice?:
    | "Aurora"
    | "Blade"
    | "Britney"
    | "Carl"
    | "Cliff"
    | "Richard"
    | "Rico"
    | "Siobhan"
    | "Vicky";
  /**
   * URL to the audio sample to use as a voice prompt for zero-shot TTS voice cloning. Providing a audio sample will override the voice setting. If neither voice nor audio_url are provided, a random voice will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * Controls emotion exaggeration. Range typically 0.25 to 2.0. Default value: `0.5`
   */
  exaggeration?: number;
  /**
   * Classifier-free guidance scale (CFG) controls the conditioning factor. Range typically 0.2 to 1.0. For expressive or dramatic speech, try lower cfg values (e.g. ~0.3) and increase exaggeration to around 0.7 or higher. If the reference speaker has a fast speaking style, lowering cfg to around 0.3 can improve pacing. Default value: `0.5`
   */
  cfg?: number;
  /**
   * If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz.
   */
  high_quality_audio?: boolean;
  /**
   * Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file. Set to 0 for random seed.
   */
  seed?: number;
  /**
   * Controls the randomness of generation. Range typically 0.05 to 5. Default value: `0.8`
   */
  temperature?: number;
};
export type ChatterboxhdTextToSpeechOutput = {
  /**
   * The generated audio file.
   */
  audio: Audio;
};
export type ChatterboxMultilingualOutput = {
  /**
   * The generated multilingual speech audio file
   */
  audio: File;
};
export type ChatterboxOutput = {
  /**
   * The generated speech audio
   */
  audio: File;
};
export type ChatterboxSpeechToSpeechInput = {
  /**
   *
   */
  source_audio_url: string | Blob | File;
  /**
   * Optional URL to an audio file to use as a reference for the generated speech. If provided, the model will try to match the style and tone of the reference audio.
   */
  target_voice_audio_url?: string | Blob | File;
};
export type ChatterboxSpeechToSpeechOutput = {
  /**
   * The generated speech audio
   */
  audio: File;
};
export type ChatterboxTextToSpeechInput = {
  /**
   * The text to be converted to speech. You can additionally add the following emotive tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>
   */
  text: string;
  /**
   * Optional URL to an audio file to use as a reference for the generated speech. If provided, the model will try to match the style and tone of the reference audio. Default value: `"https://storage.googleapis.com/chatterbox-demo-samples/prompts/male_rickmorty.mp3"`
   */
  audio_url?: string | Blob | File;
  /**
   * Exaggeration factor for the generated speech (0.0 = no exaggeration, 1.0 = maximum exaggeration). Default value: `0.25`
   */
  exaggeration?: number;
  /**
   * Temperature for generation (higher = more creative). Default value: `0.7`
   */
  temperature?: number;
  /**
   *  Default value: `0.5`
   */
  cfg?: number;
  /**
   * Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file. Set to 0 for random seed..
   */
  seed?: number;
};
export type ChatterboxTextToSpeechMultilingualInput = {
  /**
   * The text to be converted to speech (maximum 300 characters). Supports 23 languages including English, French, German, Spanish, Italian, Portuguese, Hindi, Arabic, Chinese, Japanese, Korean, and more.
   */
  text: string;
  /**
   * Language code for synthesis. In case using custom please provide audio url and select custom_audio_language. Default value: `"english"`
   */
  voice?: string;
  /**
   * If using a custom audio URL, specify the language of the audio here. Ignored if voice is not a custom url.
   */
  custom_audio_language?:
    | "english"
    | "arabic"
    | "danish"
    | "german"
    | "greek"
    | "spanish"
    | "finnish"
    | "french"
    | "hebrew"
    | "hindi"
    | "italian"
    | "japanese"
    | "korean"
    | "malay"
    | "dutch"
    | "norwegian"
    | "polish"
    | "portuguese"
    | "russian"
    | "swedish"
    | "swahili"
    | "turkish"
    | "chinese";
  /**
   * Controls speech expressiveness and emotional intensity (0.25-2.0). 0.5 is neutral, higher values increase expressiveness. Extreme values may be unstable. Default value: `0.5`
   */
  exaggeration?: number;
  /**
   * Controls randomness and variation in generation (0.05-5.0). Higher values create more varied speech patterns. Default value: `0.8`
   */
  temperature?: number;
  /**
   * Configuration/pace weight controlling generation guidance (0.0-1.0). Use 0.0 for language transfer to mitigate accent inheritance. Default value: `0.5`
   */
  cfg_scale?: number;
  /**
   * Random seed for reproducible results. Set to 0 for random generation, or provide a specific number for consistent outputs.
   */
  seed?: number;
};
export type ChatterboxTextToSpeechMultilingualOutput = {
  /**
   * The generated multilingual speech audio file
   */
  audio: File;
};
export type ChatterboxTurboOutput = {
  /**
   * The generated speech audio using Chatterbox Turbo
   */
  audio: File;
};
export type ChromaticAberrationInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Red channel shift amount
   */
  red_shift?: number;
  /**
   * Red channel shift direction Default value: `"horizontal"`
   */
  red_direction?: "horizontal" | "vertical";
  /**
   * Green channel shift amount
   */
  green_shift?: number;
  /**
   * Green channel shift direction Default value: `"horizontal"`
   */
  green_direction?: "horizontal" | "vertical";
  /**
   * Blue channel shift amount
   */
  blue_shift?: number;
  /**
   * Blue channel shift direction Default value: `"horizontal"`
   */
  blue_direction?: "horizontal" | "vertical";
};
export type ChromaticAberrationOutput = {
  /**
   * The processed images with chromatic aberration effect
   */
  images: Array<Image>;
};
export type ChronoEditInput = {
  /**
   * The image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale for the inference. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable temporal reasoning.
   */
  enable_temporal_reasoning?: boolean;
  /**
   * The number of temporal reasoning steps to perform. Default value: `8`
   */
  num_temporal_reasoning_steps?: number;
  /**
   * The resolution of the output image. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
  /**
   * Enable turbo mode to use for faster inference. Default value: `true`
   */
  turbo_mode?: boolean;
};
export type ChronoEditLoraGalleryPaintbrushInput = {
  /**
   * The image to edit.
   */
  image_url: string | Blob | File;
  /**
   * Optional mask image where black areas indicate regions to sketch/paint.
   */
  mask_url?: string | Blob | File;
  /**
   * Describe how to transform the sketched regions.
   */
  prompt: string;
  /**
   * Number of denoising steps to run. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The resolution of the output image. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA adapter. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
  /**
   * Enable turbo mode to use faster inference. Default value: `true`
   */
  turbo_mode?: boolean;
  /**
   * Optional additional LoRAs to merge (max 3).
   */
  loras?: Array<ChronoLoraWeight>;
};
export type ChronoEditLoraGalleryPaintbrushOutput = {
  /**
   * The edited image.
   */
  images: Array<ImageFile>;
  /**
   * The prompt used for the inference.
   */
  prompt: string;
  /**
   * The seed for the inference.
   */
  seed: number;
};
export type ChronoEditLoraGalleryUpscalerInput = {
  /**
   * The image to upscale.
   */
  image_url: string | Blob | File;
  /**
   * Target scale factor for the output resolution. Default value: `2`
   */
  upscale_factor?: number;
  /**
   * The guidance scale for the inference. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Number of inference steps for the upscaling pass. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The scale factor for the LoRA adapter. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
  /**
   * Optional additional LoRAs to merge (max 3).
   */
  loras?: Array<ChronoLoraWeight>;
};
export type ChronoEditLoraGalleryUpscalerOutput = {
  /**
   * The edited image.
   */
  images: Array<ImageFile>;
  /**
   * The prompt used for the inference.
   */
  prompt: string;
  /**
   * The seed for the inference.
   */
  seed: number;
};
export type ChronoEditLoraInput = {
  /**
   * The image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale for the inference. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable temporal reasoning.
   */
  enable_temporal_reasoning?: boolean;
  /**
   * The number of temporal reasoning steps to perform. Default value: `8`
   */
  num_temporal_reasoning_steps?: number;
  /**
   * The resolution of the output image. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
  /**
   * Enable turbo mode to use for faster inference. Default value: `true`
   */
  turbo_mode?: boolean;
  /**
   * Optional additional LoRAs to merge for this request (max 3).
   */
  loras?: Array<ChronoLoraWeight>;
};
export type ChronoEditLoraOutput = {
  /**
   * The edited image.
   */
  images: Array<ImageFile>;
  /**
   * The prompt used for the inference.
   */
  prompt: string;
  /**
   * The seed for the inference.
   */
  seed: number;
};
export type ChronoEditOutput = {
  /**
   * The edited image.
   */
  images: Array<ImageFile>;
  /**
   * The prompt used for the inference.
   */
  prompt: string;
  /**
   * The seed for the inference.
   */
  seed: number;
};
export type ChronoEditPaintBrushInput = {
  /**
   * The image to edit.
   */
  image_url: string | Blob | File;
  /**
   * Optional mask image where black areas indicate regions to sketch/paint.
   */
  mask_url?: string | Blob | File;
  /**
   * Describe how to transform the sketched regions.
   */
  prompt: string;
  /**
   * Number of denoising steps to run. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The resolution of the output image. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA adapter. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
  /**
   * Enable turbo mode to use faster inference. Default value: `true`
   */
  turbo_mode?: boolean;
  /**
   * Optional additional LoRAs to merge (max 3).
   */
  loras?: Array<ChronoLoraWeight>;
};
export type ChronoEditUpscalerInput = {
  /**
   * The image to upscale.
   */
  image_url: string | Blob | File;
  /**
   * Target scale factor for the output resolution. Default value: `2`
   */
  upscale_factor?: number;
  /**
   * The guidance scale for the inference. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Number of inference steps for the upscaling pass. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The scale factor for the LoRA adapter. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
  /**
   * Optional additional LoRAs to merge (max 3).
   */
  loras?: Array<ChronoLoraWeight>;
};
export type CityTeleportInput = {
  /**
   * Person photo URL
   */
  person_image_url: string | Blob | File;
  /**
   * Optional city background image URL. When provided, the person will be blended into this custom scene.
   */
  city_image_url?: string | Blob | File;
  /**
   * City name (used when city_image_url is not provided)
   */
  city_name: string;
  /**
   * Type of photo shot Default value: `"medium_shot"`
   */
  photo_shot?:
    | "extreme_close_up"
    | "close_up"
    | "medium_close_up"
    | "medium_shot"
    | "medium_long_shot"
    | "long_shot"
    | "extreme_long_shot"
    | "full_body";
  /**
   * Camera angle for the shot Default value: `"eye_level"`
   */
  camera_angle?:
    | "eye_level"
    | "low_angle"
    | "high_angle"
    | "dutch_angle"
    | "birds_eye_view"
    | "worms_eye_view"
    | "overhead"
    | "side_angle";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type CityTeleportOutput = {
  /**
   * Person teleported to city location
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ClarityUpscalerInput = {
  /**
   * The URL of the image to upscale.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to use for generating the image. Be as descriptive as possible for best results. Default value: `"masterpiece, best quality, highres"`
   */
  prompt?: string;
  /**
   * The upscale factor Default value: `2`
   */
  upscale_factor?: number;
  /**
   * The negative prompt to use. Use it to address details that you don't want in the image. Default value: `"(worst quality, low quality, normal quality:2)"`
   */
  negative_prompt?: string;
  /**
   * The creativity of the model. The higher the creativity, the more the model will deviate from the prompt.
   * Refers to the denoise strength of the sampling. Default value: `0.35`
   */
  creativity?: number;
  /**
   * The resemblance of the upscaled image to the original image. The higher the resemblance, the more the model will try to keep the original image.
   * Refers to the strength of the ControlNet. Default value: `0.6`
   */
  resemblance?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `18`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If set to false, the safety checker will be disabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type ClarityUpscalerOutput = {
  /**
   * The URL of the generated image.
   */
  image: Image;
  /**
   * The seed used to generate the image.
   */
  seed: number;
  /**
   * The timings of the different steps in the workflow.
   */
  timings: any;
};
export type codeformerInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * Weight of the fidelity factor. Default value: `0.5`
   */
  fidelity?: number;
  /**
   * Upscaling factor Default value: `2`
   */
  upscale_factor?: number;
  /**
   * Should faces etc should be aligned.
   */
  aligned?: boolean;
  /**
   * Should only center face be restored
   */
  only_center_face?: boolean;
  /**
   * Should faces be upscaled Default value: `true`
   */
  face_upscale?: boolean;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
};
export type codeformerOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
};
export type Cogvideox5bInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The negative prompt to generate video from Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The LoRAs to use for the image generation. We currently support one lora.
   */
  loras?: Array<LoraWeight>;
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`
   */
  guidance_scale?: number;
  /**
   * Use RIFE for video interpolation Default value: `true`
   */
  use_rife?: boolean;
  /**
   * The target FPS of the video Default value: `16`
   */
  export_fps?: number;
};
export type Cogvideox5bOutput = {
  /**
   * The URL to the generated video
   */
  video: File;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated video. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * The prompt used for generating the video.
   */
  prompt: string;
};
export type cogview4Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type cogview4Output = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type CollectionToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ColorCorrectionInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Color temperature adjustment
   */
  temperature?: number;
  /**
   * Brightness adjustment
   */
  brightness?: number;
  /**
   * Contrast adjustment
   */
  contrast?: number;
  /**
   * Saturation adjustment
   */
  saturation?: number;
  /**
   * Gamma adjustment Default value: `1`
   */
  gamma?: number;
};
export type ColorCorrectionOutput = {
  /**
   * The processed images with color correction
   */
  images: Array<Image>;
};
export type ColorizeInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * Select the color palette or aesthetic for the output image
   */
  color:
    | "contemporary color"
    | "vivid color"
    | "black and white colors"
    | "sepia vintage";
};
export type ColorTintInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Tint strength Default value: `1`
   */
  tint_strength?: number;
  /**
   * Tint color mode Default value: `"sepia"`
   */
  tint_mode?:
    | "sepia"
    | "red"
    | "green"
    | "blue"
    | "cyan"
    | "magenta"
    | "yellow"
    | "purple"
    | "orange"
    | "warm"
    | "cool"
    | "lime"
    | "navy"
    | "vintage"
    | "rose"
    | "teal"
    | "maroon"
    | "peach"
    | "lavender"
    | "olive";
};
export type ColorTintOutput = {
  /**
   * The processed images with color tint effect
   */
  images: Array<Image>;
};
export type CombineInput = {
  /**
   * URL of the video file to use as the video track
   */
  video_url: string | Blob | File;
  /**
   * URL of the audio file to use as the audio track
   */
  audio_url: string | Blob | File;
  /**
   * Offset in seconds for when the audio should start relative to the video
   */
  start_offset?: number;
};
export type CombineOutput = {
  /**
   * Output video with merged audio.
   */
  video: File;
};
export type ComposeOutput = {
  /**
   * URL of the processed video file
   */
  video_url: string | Blob | File;
  /**
   * URL of the video's thumbnail image
   */
  thumbnail_url: string | Blob | File;
};
export type CompressImageInput = {
  /**
   * The URL of the image to compress
   *
   * Max file size: 9.5MB, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Compression quality (1-100, higher = better quality, larger file) Default value: `99`
   */
  quality?: number;
  /**
   * Maximum width in pixels (resizes if larger, maintains aspect ratio) Default value: `470`
   */
  max_width?: number;
  /**
   * Maximum height in pixels (resizes if larger, maintains aspect ratio)
   */
  max_height?: number;
  /**
   * Output format (jpg recommended for compression) Default value: `"jpg"`
   */
  output_format?: "jpg" | "jpeg" | "webp" | "png";
  /**
   * Apply additional optimization (slightly slower but better compression) Default value: `true`
   */
  optimize?: boolean;
};
export type CompressImageOutput = {
  /**
   * Compressed image
   */
  image: Image;
  /**
   * Original image size in bytes
   */
  original_size: number;
  /**
   * Compressed image size in bytes
   */
  compressed_size: number;
  /**
   * Compression ratio (compressed/original)
   */
  compression_ratio: number;
};
export type ConcatImageInput = {
  /**
   * List of image URLs to concatenate
   */
  image_urls: Array<string>;
  /**
   * Direction of concatenation Default value: `"horizontal"`
   */
  direction?: "horizontal" | "vertical";
  /**
   * Spacing between images in pixels
   */
  spacing?: number;
  /**
   * Background color for spacing Default value: `"white"`
   */
  background_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta"
    | "transparent";
  /**
   * Alignment of images Default value: `"center"`
   */
  alignment?: "start" | "center" | "end";
  /**
   * Output format for the concatenated image Default value: `"png"`
   */
  output_format?: "png" | "jpg" | "jpeg" | "webp";
};
export type ConcatImageOutput = {
  /**
   * Concatenated image
   */
  image: Image;
};
export type ControlNetUnionInput = {
  /**
   * URL of the image to be used as the control image.
   */
  control_image_url: string | Blob | File;
  /**
   * URL of the mask for the control image.
   */
  mask_image_url?: string | Blob | File;
  /**
   * Control Mode for Flux Controlnet Union. Supported values are:
   * - canny: Uses the edges for guided generation.
   * - tile: Uses the tiles for guided generation.
   * - depth: Utilizes a grayscale depth map for guided generation.
   * - blur: Adds a blur to the image.
   * - pose: Uses the pose of the image for guided generation.
   * - gray: Converts the image to grayscale.
   * - low-quality: Converts the image to a low-quality image.
   */
  control_mode:
    | "canny"
    | "tile"
    | "depth"
    | "blur"
    | "pose"
    | "gray"
    | "low-quality";
  /**
   * The scale of the control net weight. This is used to scale the control net weight
   * before merging it with the base model. Default value: `1`
   */
  conditioning_scale?: number;
  /**
   * Threshold for mask. Default value: `0.5`
   */
  mask_threshold?: number;
  /**
   * The percentage of the image to start applying the controlnet in terms of the total timesteps.
   */
  start_percentage?: number;
  /**
   * The percentage of the image to end applying the controlnet in terms of the total timesteps. Default value: `1`
   */
  end_percentage?: number;
};
export type ConvertFormatInput = {
  /**
   * URL of 3D file to convert (FBX, OBJ, GLB). Max size: 60MB.
   */
  input_file_url: string | Blob | File;
  /**
   * Target output format. STL: 3D printing. USDZ: AR/iOS. FBX: animation. MP4/GIF: preview video/animation.
   */
  output_format: "STL" | "USDZ" | "FBX" | "MP4" | "GIF";
};
export type ConvertFormatOutput = {
  /**
   * Converted file in the requested format.
   */
  result_file: File;
};
export type CreateVoiceInput = {
  /**
   * URL of the voice audio file. Supports .mp3/.wav audio or .mp4/.mov video. Duration must be 5-30 seconds with clean, single-voice audio.
   */
  voice_url: string | Blob | File;
};
export type CreateVoiceOutput = {
  /**
   * Unique identifier for the created voice
   */
  voice_id: string;
};
export type CreatifyAuroraInput = {
  /**
   * The URL of the image file to be used for video generation.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file to be used for video generation.
   */
  audio_url: string | Blob | File;
  /**
   * A text prompt to guide the video generation process.
   */
  prompt?: string;
  /**
   * Guidance scale to be used for text prompt adherence. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Guidance scale to be used for audio adherence. Default value: `2`
   */
  audio_guidance_scale?: number;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
};
export type CreatifyAuroraOutput = {
  /**
   * The generated video file.
   */
  video: VideoFile;
};
export type CropImageInput = {
  /**
   * The URL of the image to crop
   *
   * Max file size: 9.5MB, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * X coordinate as percentage of image width (0-100)
   */
  x_percent?: number;
  /**
   * Y coordinate as percentage of image height (0-100)
   */
  y_percent?: number;
  /**
   * Width as percentage of image width (0-100) Default value: `100`
   */
  width_percent?: number;
  /**
   * Height as percentage of image height (0-100) Default value: `100`
   */
  height_percent?: number;
  /**
   * Output format for the cropped image Default value: `"png"`
   */
  output_format?: "png" | "jpg" | "jpeg" | "webp";
};
export type CropImageOutput = {
  /**
   * Cropped image
   */
  image: Image;
};
export type CrystalUpscaleInput = {
  /**
   * URL to the input image
   */
  image_url: string | Blob | File;
  /**
   * Scale factor Default value: `2`
   */
  scale_factor?: number;
  /**
   * Creativity level for upscaling
   */
  creativity?: number;
};
export type CrystalUpscaleOutput = {
  /**
   * List of upscaled images
   */
  images: Array<Image>;
};
export type CrystalUpscalerInput = {
  /**
   * URL to the input image
   */
  image_url: string | Blob | File;
  /**
   * Scale factor Default value: `2`
   */
  scale_factor?: number;
  /**
   * Creativity level for upscaling
   */
  creativity?: number;
};
export type CrystalUpscalerOutput = {
  /**
   * List of upscaled images
   */
  images: Array<Image>;
};
export type CrystalVideoUpscaleInput = {
  /**
   * URL to the input video.
   */
  video_url: string | Blob | File;
  /**
   * Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution. Default value: `2`
   */
  scale_factor?: number;
};
export type CrystalVideoUpscaleOutput = {
  /**
   * URL to the upscaled video
   */
  video: VideoFile;
};
export type CrystalVideoUpscalerInput = {
  /**
   * URL to the input video.
   */
  video_url: string | Blob | File;
  /**
   * Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution. Default value: `2`
   */
  scale_factor?: number;
};
export type CrystalVideoUpscalerOutput = {
  /**
   * URL to the upscaled video
   */
  video: VideoFile;
};
export type Csm1bInput = {
  /**
   * The text to generate an audio from.
   */
  scene: Array<Turn>;
  /**
   * The context to generate an audio from.
   */
  context?: Array<Speaker>;
};
export type Csm1bOutput = {
  /**
   * The generated audio.
   */
  audio: File | string;
};
export type ddcolorInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * seed to be used for generation
   */
  seed?: number;
};
export type ddcolorOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
};
export type DecartLucy5bImageToVideoInput = {
  /**
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * Resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history. Default value: `true`
   */
  sync_mode?: boolean;
};
export type DecartLucy5bImageToVideoOutput = {
  /**
   * The generated MP4 video with H.264 encoding
   */
  video: File;
};
export type deepfilternet3Input = {
  /**
   * The URL of the audio to enhance.
   */
  audio_url: string | Blob | File;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format for the output audio. Default value: `"mp3"`
   */
  audio_format?: "mp3" | "aac" | "m4a" | "ogg" | "opus" | "flac" | "wav";
  /**
   * The bitrate of the output audio. Default value: `"192k"`
   */
  bitrate?: string;
};
export type deepfilternet3Output = {
  /**
   * The audio file that was enhanced.
   */
  audio_file: AudioFile;
  /**
   * Timings for each step in the pipeline.
   */
  timings: DeepFilterNetTimings;
};
export type demucsInput = {
  /**
   * URL of the audio file to separate into stems
   */
  audio_url: string | Blob | File;
  /**
   * Demucs model to use for separation Default value: `"htdemucs_6s"`
   */
  model?:
    | "htdemucs"
    | "htdemucs_ft"
    | "htdemucs_6s"
    | "hdemucs_mmi"
    | "mdx"
    | "mdx_extra"
    | "mdx_q"
    | "mdx_extra_q";
  /**
   * Specific stems to extract. If None, extracts all available stems. Available stems depend on model: vocals, drums, bass, other, guitar, piano (for 6s model)
   */
  stems?: Array<"vocals" | "drums" | "bass" | "other" | "guitar" | "piano">;
  /**
   * Length in seconds of each segment for processing. Smaller values use less memory but may reduce quality. Default is model-specific.
   */
  segment_length?: number;
  /**
   * Number of random shifts for equivariant stabilization. Higher values improve quality but increase processing time. Default value: `1`
   */
  shifts?: number;
  /**
   * Overlap between segments (0.0 to 1.0). Higher values may improve quality but increase processing time. Default value: `0.25`
   */
  overlap?: number;
  /**
   * Output audio format for the separated stems Default value: `"mp3"`
   */
  output_format?: "wav" | "mp3";
};
export type demucsOutput = {
  /**
   * Separated vocals audio file
   */
  vocals?: File;
  /**
   * Separated drums audio file
   */
  drums?: File;
  /**
   * Separated bass audio file
   */
  bass?: File;
  /**
   * Separated other instruments audio file
   */
  other?: File;
  /**
   * Separated guitar audio file (only available for 6s models)
   */
  guitar?: File;
  /**
   * Separated piano audio file (only available for 6s models)
   */
  piano?: File;
};
export type DepthLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The image to use for control lora. This is used to control the style of the generated image.
   */
  control_lora_image_url: string | Blob | File;
  /**
   * The strength of the control lora. Default value: `1`
   */
  control_lora_strength?: number;
  /**
   * If set to true, the input image will be preprocessed to extract depth information.
   * This is useful for generating depth maps from images. Default value: `true`
   */
  preprocess_depth?: boolean;
};
export type DesaturateInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Desaturation factor Default value: `1`
   */
  desaturate_factor?: number;
  /**
   * Desaturation method Default value: `"luminance (Rec.709)"`
   */
  desaturate_method?:
    | "luminance (Rec.709)"
    | "luminance (Rec.601)"
    | "average"
    | "lightness";
};
export type DesaturateOutput = {
  /**
   * The processed images with desaturation effect
   */
  images: Array<Image>;
};
export type DetectionInput = {
  /**
   * Image URL to be processed
   */
  image_url: string | Blob | File;
  /**
   * Type of detection to perform
   */
  task_type: "bbox_detection" | "point_detection" | "gaze_detection";
  /**
   * Text description of what to detect
   */
  detection_prompt: string;
  /**
   * Whether to use ensemble for gaze detection
   */
  use_ensemble?: boolean;
  /**
   * Whether to combine points into a single point for point detection. This has no effect for bbox detection or gaze detection.
   */
  combine_points?: boolean;
  /**
   * Whether to show visualization for detection Default value: `true`
   */
  show_visualization?: boolean;
};
export type DetectionOutput = {
  /**
   * Output image with detection visualization
   */
  image?: Image;
  /**
   * Detection results as text
   */
  text_output: string;
};
export type DevImageToImageInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type DevReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type DevTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type DiaCloneOutput = {
  /**
   * The generated speech audio
   */
  audio: File;
};
export type DiaOutput = {
  /**
   * The generated speech audio
   */
  audio: File;
};
export type DiaTtsInput = {
  /**
   * The text to be converted to speech.
   */
  text: string;
};
export type DiaTtsOutput = {
  /**
   * The generated speech audio
   */
  audio: File;
};
export type DiaTtsVoiceCloneInput = {
  /**
   * The text to be converted to speech.
   */
  text: string;
  /**
   * The URL of the reference audio file.
   */
  ref_audio_url: string | Blob | File;
  /**
   * The reference text to be used for TTS.
   */
  ref_text: string;
};
export type DiaTtsVoiceCloneOutput = {
  /**
   * The generated speech audio
   */
  audio: File;
};
export type DictOutput = {
  /**
   * The value of the measurement.
   */
  value?: unknown;
};
export type DifferentialDiffusionInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  control_loras?: Array<ControlLoraWeight>;
  /**
   * The controlnets to use for the image generation. Only one controlnet is supported at the moment.
   */
  controlnets?: Array<ControlNet>;
  /**
   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.
   */
  controlnet_unions?: Array<ControlNetUnion>;
  /**
   * IP-Adapter to use for image generation.
   */
  ip_adapters?: Array<IPAdapter>;
  /**
   * EasyControl Inputs to use for image generation.
   */
  easycontrols?: Array<EasyControlWeight>;
  /**
   * Use an image input to influence the generation. Can be used to fill images in masked areas.
   */
  fill_image?: ImageFillInput;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  real_cfg_scale?: number;
  /**
   * Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.
   * If using XLabs IP-Adapter v1, this will be turned on!.
   */
  use_real_cfg?: boolean;
  /**
   * Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.
   */
  use_cfg_zero?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * URL of Image for Reference-Only
   */
  reference_image_url?: string | Blob | File;
  /**
   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`
   */
  reference_strength?: number;
  /**
   * The percentage of the total timesteps when the reference guidance is to bestarted.
   */
  reference_start?: number;
  /**
   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`
   */
  reference_end?: number;
  /**
   * Base shift for the scheduled timesteps Default value: `0.5`
   */
  base_shift?: number;
  /**
   * Max shift for the scheduled timesteps Default value: `1.15`
   */
  max_shift?: number;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * Specifies whether beta sigmas ought to be used.
   */
  use_beta_schedule?: boolean;
  /**
   * Sigmas schedule for the denoising process.
   */
  sigma_schedule?: "sgm_uniform";
  /**
   * Scheduler for the denoising process. Default value: `"euler"`
   */
  scheduler?: "euler" | "dpmpp_2m";
  /**
   * Negative prompt to steer the image generation away from unwanted features.
   * By default, we will be using NAG for processing the negative prompt. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The scale for NAG. Higher values will result in a image that is more distant
   * to the negative prompt. Default value: `3`
   */
  nag_scale?: number;
  /**
   * The tau for NAG. Controls the normalization of the hidden state.
   * Higher values will result in a less aggressive normalization,
   * but may also lead to unexpected changes with respect to the original image.
   * Not recommended to change this value. Default value: `2.5`
   */
  nag_tau?: number;
  /**
   * The alpha value for NAG. This value is used as a final weighting
   * factor for steering the normalized guidance (positive and negative prompts)
   * in the direction of the positive prompt. Higher values will result in less
   * steering on the normalized guidance where lower values will result in
   * considering the positive prompt guidance more. Default value: `0.25`
   */
  nag_alpha?: number;
  /**
   * The proportion of steps to apply NAG. After the specified proportion
   * of steps has been iterated, the remaining steps will use original
   * attention processors in FLUX. Default value: `0.25`
   */
  nag_end?: number;
  /**
   * URL of image to use as initial image.
   */
  image_url: string | Blob | File;
  /**
   * URL of change map.
   */
  change_map_image_url: string | Blob | File;
  /**
   * The strength to use for differential diffusion. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
};
export type diffrhythmInput = {
  /**
   * The prompt to generate the song from. Must have two sections. Sections start with either [chorus] or a [verse].
   */
  lyrics: string;
  /**
   * The URL of the reference audio to use for the music generation.
   */
  reference_audio_url?: string | Blob | File;
  /**
   * The style prompt to use for the music generation.
   */
  style_prompt?: string;
  /**
   * The duration of the music to generate. Default value: `"95s"`
   */
  music_duration?: "95s" | "285s";
  /**
   * The CFG strength to use for the music generation. Default value: `4`
   */
  cfg_strength?: number;
  /**
   * The scheduler to use for the music generation. Default value: `"euler"`
   */
  scheduler?: "euler" | "midpoint" | "rk4" | "implicit_adams";
  /**
   * The number of inference steps to use for the music generation. Default value: `32`
   */
  num_inference_steps?: number;
};
export type diffrhythmOutput = {
  /**
   * Generated music file.
   */
  audio: File;
};
export type DigitalComicArtInput = {
  /**
   * The prompt to generate a digital comic art style image. Use 'd1g1t4l' trigger word for best results.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the digital comic art effect. Default value: `1`
   */
  lora_scale?: number;
};
export type DigitalComicArtOutput = {
  /**
   * The generated digital comic art style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type DissolveInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * URL of second image for dissolve
   */
  dissolve_image_url: string | Blob | File;
  /**
   * Dissolve blend factor Default value: `0.5`
   */
  dissolve_factor?: number;
};
export type DissolveOutput = {
  /**
   * The processed images with dissolve effect
   */
  images: Array<Image>;
};
export type DistilledExtendVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`
   */
  constant_rate_factor?: number;
  /**
   * Video to be extended.
   */
  video: VideoConditioningInput;
};
export type DistilledImageToVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`
   */
  constant_rate_factor?: number;
  /**
   * Image URL for Image-to-Video task
   */
  image_url: string | Blob | File;
};
export type DistilledMultiConditioningVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`
   */
  constant_rate_factor?: number;
  /**
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
};
export type DistilledTextToVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9, 1:1 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type DocresDewarpInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
};
export type DocresDewarpOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
};
export type docresInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * Task to perform
   */
  task: "deshadowing" | "appearance" | "deblurring" | "binarization";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
};
export type DocResInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * Task to perform
   */
  task: "deshadowing" | "appearance" | "deblurring" | "binarization";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
};
export type docresOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
};
export type DodgeBurnInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Dodge and burn intensity Default value: `0.5`
   */
  dodge_burn_intensity?: number;
  /**
   * Dodge and burn mode Default value: `"dodge"`
   */
  dodge_burn_mode?:
    | "dodge"
    | "burn"
    | "dodge_and_burn"
    | "burn_and_dodge"
    | "color_dodge"
    | "color_burn"
    | "linear_dodge"
    | "linear_burn";
};
export type DodgeBurnOutput = {
  /**
   * The processed images with dodge and burn effect
   */
  images: Array<Image>;
};
export type DrctSuperResolutionInput = {
  /**
   * URL of the image to upscale.
   */
  image_url: string | Blob | File;
  /**
   * Upscaling factor. Default value: `"4"`
   */
  upscale_factor?: "4";
};
export type DrctSuperResolutionOutput = {
  /**
   * Upscaled image
   */
  image: Image;
};
export type DreamActor2Input = {
  /**
   * The URL of the reference image to animate. Supports real people, animation, pets, etc. Format: jpeg, jpg or png. Max size: 4.7 MB. Resolution: between 480x480 and 1920x1080 (larger images will be proportionally reduced).
   */
  image_url: string | Blob | File;
  /**
   * The URL of the driving template video providing motion, facial expressions, and lip movement reference. Max duration: 30 seconds. Format: mp4, mov or webm. Resolution: between 200x200 and 2048x1440. Supports full face and body driving.
   */
  video_url: string | Blob | File;
  /**
   * Whether to crop the first second of the output video. The output has a 1-second transition at the beginning; enable this to remove it. Default value: `true`
   */
  trim_first_second?: boolean;
};
export type DreamActor2Output = {
  /**
   * Generated video file.
   */
  video: File;
};
export type DreaminaInput = {
  /**
   * The text prompt used to generate the image
   */
  prompt: string;
  /**
   * The size of the generated image. Width and height must be between 512 and 2048.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use an LLM to enhance the prompt
   */
  enhance_prompt?: boolean;
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type DreaminaOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type dreamoInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * URL of first reference image to use for generation.
   */
  first_image_url?: string | Blob | File;
  /**
   * URL of second reference image to use for generation.
   */
  second_image_url?: string | Blob | File;
  /**
   * Task for first reference image (ip/id/style). Default value: `"ip"`
   */
  first_reference_task?: "ip" | "id" | "style";
  /**
   * Task for second reference image (ip/id/style). Default value: `"ip"`
   */
  second_reference_task?: "ip" | "id" | "style";
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `12`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * Resolution for reference images. Default value: `512`
   */
  ref_resolution?: number;
  /**
   * The weight of the CFG loss. Default value: `1`
   */
  true_cfg?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type Dreamomni2EditInput = {
  /**
   * List of URLs of input images for editing.
   */
  image_urls: Array<string>;
  /**
   * The prompt to edit the image.
   */
  prompt: string;
};
export type Dreamomni2EditOutput = {
  /**
   * Generated image
   */
  image: Image;
};
export type dreamoOutput = {
  /**
   * The URLs of the generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used to generate the image.
   */
  prompt: string;
};
export type DubbingAudioOutput = {
  /**
   * The dubbed audio file.
   */
  audio: File;
  /**
   * The target language of the dubbed content
   */
  target_lang: string;
};
export type DubbingVideoOutput = {
  /**
   * The dubbed video file. Will be populated if video_url was provided in the request.
   */
  video: File;
  /**
   * The target language of the dubbed content
   */
  target_lang: string;
};
export type DWPoseInput = {
  /**
   * URL of the image to be processed
   */
  image_url: string | Blob | File;
  /**
   * Mode of drawing the pose on the image. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'. Default value: `"body-pose"`
   */
  draw_mode?:
    | "full-pose"
    | "body-pose"
    | "face-pose"
    | "hand-pose"
    | "face-hand-mask"
    | "face-mask"
    | "hand-mask";
};
export type DWPoseOutput = {
  /**
   * The predicted pose image
   */
  image: Image;
};
export type DwposeVideoInput = {
  /**
   * URL of video to be used for pose estimation
   */
  video_url: string | Blob | File;
  /**
   * Mode of drawing the pose on the video. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'. Default value: `"body-pose"`
   */
  draw_mode?:
    | "full-pose"
    | "body-pose"
    | "face-pose"
    | "hand-pose"
    | "face-hand-mask"
    | "face-mask"
    | "hand-mask";
};
export type DwposeVideoOutput = {
  /**
   * The output video with pose estimation.
   */
  video: File;
};
export type EchomimicV3Input = {
  /**
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio to use as a reference for the video generation.
   */
  audio_url: string | Blob | File;
  /**
   * The prompt to use for the video generation.
   */
  prompt: string;
  /**
   * The negative prompt to use for the video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of frames to generate at once. Default value: `121`
   */
  num_frames_per_generation?: number;
  /**
   * The guidance scale to use for the video generation. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * The audio guidance scale to use for the video generation. Default value: `2.5`
   */
  audio_guidance_scale?: number;
  /**
   * The seed to use for the video generation.
   */
  seed?: number;
};
export type EchomimicV3Output = {
  /**
   * The generated video file.
   */
  video: File;
};
export type EditImageInput = {
  /**
   * The prompt to fill the masked part of the image.
   */
  prompt: string;
  /**
   * The image URL to generate an image from. Needs to match the dimensions of the mask.
   */
  image_url: string | Blob | File;
  /**
   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.
   */
  mask_url: string | Blob | File;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The style of the generated image Default value: `"auto"`
   */
  style?: "auto" | "general" | "realistic" | "design" | "render_3D" | "anime";
  /**
   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type EditImageLoraInput = {
  /**
   * The prompt to edit the image with.
   */
  prompt: string;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. If None, uses the input image dimensions.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The URLs of the images to edit.
   */
  image_urls: Array<string>;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
};
export type edittoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. Required for inpainting.
   */
  video_url: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
};
export type edittoOutput = {
  /**
   * The generated image to video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type EffectInput = {
  /**
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string | Blob | File;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
};
export type EffectOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ElementInput = {
  /**
   * The frontal image of the element (main view). Optional.
   */
  frontal_image_url?: string | Blob | File;
  /**
   * Additional reference images from different angles. 0-3 images supported. Optional.
   */
  reference_image_urls?: Array<string>;
};
export type ElementsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ElevenlabsAudioIsolationInput = {
  /**
   * URL of the audio file to isolate voice from
   */
  audio_url?: string | Blob | File;
  /**
   * Video file to use for audio isolation. Either `audio_url` or `video_url` must be provided.
   */
  video_url?: string | Blob | File;
};
export type ElevenlabsAudioIsolationOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.
   */
  timestamps?: Array<void>;
};
export type ElevenlabsDubbingInput = {
  /**
   * URL of the audio file to dub. Either audio_url or video_url must be provided.
   */
  audio_url?: string | Blob | File;
  /**
   * URL of the video file to dub. Either audio_url or video_url must be provided. If both are provided, video_url takes priority.
   */
  video_url?: string | Blob | File;
  /**
   * Target language code for dubbing (ISO 639-1)
   */
  target_lang: string;
  /**
   * Source language code. If not provided, will be auto-detected.
   */
  source_lang?: string;
  /**
   * Number of speakers in the audio. If not provided, will be auto-detected.
   */
  num_speakers?: number;
  /**
   * Whether to use the highest resolution for dubbing. Default value: `true`
   */
  highest_resolution?: boolean;
};
export type ElevenlabsDubbingOutput = {
  /**
   * The dubbed video file. Will be populated if video_url was provided in the request.
   */
  video: File;
  /**
   * The target language of the dubbed content
   */
  target_lang: string;
};
export type ElevenlabsMusicInput = {
  /**
   * The text prompt describing the music to generate
   */
  prompt?: string;
  /**
   * The composition plan for the music
   */
  composition_plan?: MusicCompositionPlan;
  /**
   * The length of the song to generate in milliseconds. Used only in conjunction with prompt. Must be between 3000ms and 600000ms. Optional - if not provided, the model will choose a length based on the prompt.
   */
  music_length_ms?: number;
  /**
   * If true, guarantees that the generated song will be instrumental. If false, the song may or may not be instrumental depending on the prompt. Can only be used with prompt.
   */
  force_instrumental?: boolean;
  /**
   * Controls how strictly section durations in the composition_plan are enforced. It will only have an effect if it is used with composition_plan. When set to true, the model will precisely respect each section's duration_ms from the plan. When set to false, the model may adjust individual section durations which will generally lead to better generation quality and improved latency, while always preserving the total song duration from the plan. Default value: `true`
   */
  respect_sections_durations?: boolean;
  /**
   * Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs. Default value: `"mp3_44100_128"`
   */
  output_format?:
    | "mp3_22050_32"
    | "mp3_44100_32"
    | "mp3_44100_64"
    | "mp3_44100_96"
    | "mp3_44100_128"
    | "mp3_44100_192"
    | "pcm_8000"
    | "pcm_16000"
    | "pcm_22050"
    | "pcm_24000"
    | "pcm_44100"
    | "pcm_48000"
    | "ulaw_8000"
    | "alaw_8000"
    | "opus_48000_32"
    | "opus_48000_64"
    | "opus_48000_96"
    | "opus_48000_128"
    | "opus_48000_192";
};
export type ElevenlabsMusicOutput = {
  /**
   * The generated music audio file in MP3 format
   */
  audio: File;
};
export type ElevenlabsSoundEffectsV2Input = {
  /**
   * The text describing the sound effect to generate
   */
  text: string;
  /**
   * Duration in seconds (0.5-22). If None, optimal duration will be determined from prompt.
   */
  duration_seconds?: number;
  /**
   * How closely to follow the prompt (0-1). Higher values mean less variation. Default value: `0.3`
   */
  prompt_influence?: number;
  /**
   * Output format of the generated audio. Formatted as codec_sample_rate_bitrate. Default value: `"mp3_44100_128"`
   */
  output_format?:
    | "mp3_22050_32"
    | "mp3_44100_32"
    | "mp3_44100_64"
    | "mp3_44100_96"
    | "mp3_44100_128"
    | "mp3_44100_192"
    | "pcm_8000"
    | "pcm_16000"
    | "pcm_22050"
    | "pcm_24000"
    | "pcm_44100"
    | "pcm_48000"
    | "ulaw_8000"
    | "alaw_8000"
    | "opus_48000_32"
    | "opus_48000_64"
    | "opus_48000_96"
    | "opus_48000_128"
    | "opus_48000_192";
  /**
   * Whether to create a sound effect that loops smoothly.
   */
  loop?: boolean;
};
export type ElevenlabsSoundEffectsV2Output = {
  /**
   * The generated sound effect audio file in MP3 format
   */
  audio: File;
};
export type ElevenlabsSpeechToTextInput = {
  /**
   * URL of the audio file to transcribe
   */
  audio_url: string | Blob | File;
  /**
   * Language code of the audio
   */
  language_code?: string;
  /**
   * Tag audio events like laughter, applause, etc. Default value: `true`
   */
  tag_audio_events?: boolean;
  /**
   * Whether to annotate who is speaking Default value: `true`
   */
  diarize?: boolean;
};
export type ElevenlabsSpeechToTextOutput = {
  /**
   * The full transcribed text
   */
  text: string;
  /**
   * Detected or specified language code
   */
  language_code: string;
  /**
   * Confidence in language detection
   */
  language_probability: number;
  /**
   * Word-level transcription details
   */
  words: Array<TranscriptionWord>;
};
export type ElevenlabsSpeechToTextScribeV2Input = {
  /**
   * URL of the audio file to transcribe
   */
  audio_url: string | Blob | File;
  /**
   * Language code of the audio
   */
  language_code?: string;
  /**
   * Tag audio events like laughter, applause, etc. Default value: `true`
   */
  tag_audio_events?: boolean;
  /**
   * Whether to annotate who is speaking Default value: `true`
   */
  diarize?: boolean;
  /**
   * Words or sentences to bias the model towards transcribing. Up to 100 keyterms, max 50 characters each. Adds 30% premium over base transcription price.
   */
  keyterms?: Array<string>;
};
export type ElevenlabsSpeechToTextScribeV2Output = {
  /**
   * The full transcribed text
   */
  text: string;
  /**
   * Detected or specified language code
   */
  language_code: string;
  /**
   * Confidence in language detection
   */
  language_probability: number;
  /**
   * Word-level transcription details
   */
  words: Array<TranscriptionWord>;
};
export type ElevenlabsTextToDialogueElevenV3Input = {
  /**
   * A list of dialogue inputs, each containing text and a voice ID which will be converted into speech.
   */
  inputs: Array<DialogueBlock>;
  /**
   * Determines how stable the voice is and the randomness between each generation. Lower values introduce broader emotional range for the voice. Higher values can result in a monotonous voice with limited emotion. Must be one of 0.0, 0.5, 1.0, else it will be rounded to the nearest value.
   */
  stability?: number;
  /**
   * This setting boosts the similarity to the original speaker. Using this setting requires a slightly higher computational load, which in turn increases latency.
   */
  use_speaker_boost?: boolean;
  /**
   * A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
   */
  pronunciation_dictionary_locators?: Array<PronunciationDictionaryLocator>;
  /**
   * Random seed for reproducibility.
   */
  seed?: number;
  /**
   * Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model.
   */
  language_code?: string;
};
export type ElevenlabsTextToDialogueElevenV3Output = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Random seed for reproducibility.
   */
  seed: number;
};
export type ElevenlabsTtsElevenV3Input = {
  /**
   * The text to convert to speech
   */
  text: string;
  /**
   * The voice to use for speech generation Default value: `"Rachel"`
   */
  voice?: string;
  /**
   * Voice stability (0-1) Default value: `0.5`
   */
  stability?: number;
  /**
   * Similarity boost (0-1) Default value: `0.75`
   */
  similarity_boost?: number;
  /**
   * Style exaggeration (0-1)
   */
  style?: number;
  /**
   * Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality. Default value: `1`
   */
  speed?: number;
  /**
   * Whether to return timestamps for each word in the generated speech
   */
  timestamps?: boolean;
  /**
   * Language code (ISO 639-1) used to enforce a language for the model.
   */
  language_code?: string;
  /**
   * This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped. Default value: `"auto"`
   */
  apply_text_normalization?: "auto" | "on" | "off";
};
export type ElevenlabsTtsElevenV3Output = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.
   */
  timestamps?: Array<void>;
};
export type ElevenlabsTtsMultilingualV2Input = {
  /**
   * The text to convert to speech
   */
  text: string;
  /**
   * The voice to use for speech generation Default value: `"Rachel"`
   */
  voice?: string;
  /**
   * Voice stability (0-1) Default value: `0.5`
   */
  stability?: number;
  /**
   * Similarity boost (0-1) Default value: `0.75`
   */
  similarity_boost?: number;
  /**
   * Style exaggeration (0-1)
   */
  style?: number;
  /**
   * Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality. Default value: `1`
   */
  speed?: number;
  /**
   * Whether to return timestamps for each word in the generated speech
   */
  timestamps?: boolean;
  /**
   * The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
   */
  previous_text?: string;
  /**
   * The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
   */
  next_text?: string;
  /**
   * Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model.
   */
  language_code?: string;
  /**
   * This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped. Default value: `"auto"`
   */
  apply_text_normalization?: "auto" | "on" | "off";
};
export type ElevenlabsTtsMultilingualV2Output = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.
   */
  timestamps?: Array<void>;
};
export type ElevenlabsTtsTurboV25Input = {
  /**
   * The text to convert to speech
   */
  text: string;
  /**
   * The voice to use for speech generation Default value: `"Rachel"`
   */
  voice?: string;
  /**
   * Voice stability (0-1) Default value: `0.5`
   */
  stability?: number;
  /**
   * Similarity boost (0-1) Default value: `0.75`
   */
  similarity_boost?: number;
  /**
   * Style exaggeration (0-1)
   */
  style?: number;
  /**
   * Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality. Default value: `1`
   */
  speed?: number;
  /**
   * Whether to return timestamps for each word in the generated speech
   */
  timestamps?: boolean;
  /**
   * The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
   */
  previous_text?: string;
  /**
   * The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
   */
  next_text?: string;
  /**
   * Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model.
   */
  language_code?: string;
  /**
   * This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped. Default value: `"auto"`
   */
  apply_text_normalization?: "auto" | "on" | "off";
};
export type ElevenlabsTtsTurboV25Output = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.
   */
  timestamps?: Array<void>;
};
export type ElevenlabsVoiceChangerInput = {
  /**
   * The input audio file
   */
  audio_url: string | Blob | File;
  /**
   * The voice to use for speech generation Default value: `"Rachel"`
   */
  voice?: string;
  /**
   * If set, will remove the background noise from your audio input using our audio isolation model.
   */
  remove_background_noise?: boolean;
  /**
   * Random seed for reproducibility.
   */
  seed?: number;
  /**
   * Output format of the generated audio. Formatted as codec_sample_rate_bitrate. Default value: `"mp3_44100_128"`
   */
  output_format?:
    | "mp3_22050_32"
    | "mp3_44100_32"
    | "mp3_44100_64"
    | "mp3_44100_96"
    | "mp3_44100_128"
    | "mp3_44100_192"
    | "pcm_8000"
    | "pcm_16000"
    | "pcm_22050"
    | "pcm_24000"
    | "pcm_44100"
    | "pcm_48000"
    | "ulaw_8000"
    | "alaw_8000"
    | "opus_48000_32"
    | "opus_48000_64"
    | "opus_48000_96"
    | "opus_48000_128"
    | "opus_48000_192";
};
export type ElevenlabsVoiceChangerOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Random seed for reproducibility.
   */
  seed: number;
};
export type Emu35EditOutput = {
  /**
   * The edited image.
   */
  images: Array<ImageFile>;
  /**
   * The seed for the inference.
   */
  seed: number;
};
export type Emu35ImageEditImageInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The resolution of the output image. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the output image. Default value: `"auto"`
   */
  aspect_ratio?:
    | "auto"
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
  /**
   * The image to edit.
   */
  image_url: string | Blob | File;
};
export type Emu35ImageEditImageOutput = {
  /**
   * The edited image.
   */
  images: Array<ImageFile>;
  /**
   * The seed for the inference.
   */
  seed: number;
};
export type Emu35ImageEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The resolution of the output image. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the output image. Default value: `"auto"`
   */
  aspect_ratio?:
    | "auto"
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
  /**
   * The image to edit.
   */
  image_url: string | Blob | File;
};
export type Emu35ImageInput = {
  /**
   * The prompt to create the image.
   */
  prompt: string;
  /**
   * The resolution of the output image. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the output image. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
};
export type Emu35ImageTextToImageInput = {
  /**
   * The prompt to create the image.
   */
  prompt: string;
  /**
   * The resolution of the output image. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the output image. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The seed for the inference.
   */
  seed?: number;
  /**
   * The format of the output image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Whether to return the image in sync mode.
   */
  sync_mode?: boolean;
};
export type Emu35ImageTextToImageOutput = {
  /**
   * The edited image.
   */
  images: Array<ImageFile>;
  /**
   * The seed for the inference.
   */
  seed: number;
};
export type Emu35Output = {
  /**
   * The edited image.
   */
  images: Array<ImageFile>;
  /**
   * The seed for the inference.
   */
  seed: number;
};
export type EraseByTextInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The name of the object to remove.
   */
  object_name: string;
};
export type EraserInput = {
  /**
   * Input Image to erase from
   */
  image_url: string | Blob | File;
  /**
   * The URL of the binary mask image that represents the area that will be cleaned.
   */
  mask_url: string | Blob | File;
  /**
   * You can use this parameter to specify the type of the input mask from the list. 'manual' opttion should be used in cases in which the mask had been generated by a user (e.g. with a brush tool), and 'automatic' mask type should be used when mask had been generated by an algorithm like 'SAM'. Default value: `"manual"`
   */
  mask_type?: "manual" | "automatic";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, attempts to preserve the alpha channel of the input image.
   */
  preserve_alpha?: boolean;
};
export type EraserOutput = {
  /**
   * The generated image
   */
  image: Image;
};
export type EvfSamInput = {
  /**
   * The prompt to generate segmentation from.
   */
  prompt: string;
  /**
   * Areas to exclude from segmentation (will be subtracted from prompt results)
   */
  negative_prompt?: string;
  /**
   * Enable semantic level segmentation for body parts, background or multi objects
   */
  semantic_type?: boolean;
  /**
   * URL of the input image
   */
  image_url: string | Blob | File;
  /**
   * Output only the binary mask instead of masked image Default value: `true`
   */
  mask_only?: boolean;
  /**
   * Use GroundingDINO instead of SAM for segmentation
   */
  use_grounding_dino?: boolean;
  /**
   * Invert the mask (background becomes foreground and vice versa)
   */
  revert_mask?: boolean;
  /**
   * Apply Gaussian blur to the mask. Value determines kernel size (must be odd number)
   */
  blur_mask?: number;
  /**
   * Expand/dilate the mask by specified pixels
   */
  expand_mask?: number;
  /**
   * Fill holes in the mask using morphological operations
   */
  fill_holes?: boolean;
};
export type EvfSamOutput = {
  /**
   * The segmented output image
   */
  image: File;
};
export type ExpressionChangeInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The desired facial expression to apply. Default value: `"sad"`
   */
  prompt?: string;
};
export type ExpressionChangeOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ExtendInput = {
  /**
   * A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics.
   */
  prompt?: string;
  /**
   * Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.
   */
  tags?: Array<string>;
  /**
   * The lyrics sung in the generated song. An empty string will generate an instrumental track.
   */
  lyrics_prompt?: string;
  /**
   * The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.
   */
  seed?: number;
  /**
   * Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.) Default value: `1.8`
   */
  prompt_strength?: number;
  /**
   * Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7. Default value: `0.7`
   */
  balance_strength?: number;
  /**
   * Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed. Default value: `1`
   */
  num_songs?: number;
  /**
   *  Default value: `"wav"`
   */
  output_format?: "flac" | "mp3" | "wav" | "ogg" | "m4a";
  /**
   * The bit rate to use for mp3 and m4a formats. Not available for other formats.
   */
  output_bit_rate?: "128" | "192" | "256" | "320";
  /**
   * The URL of the audio file to alter. Must be a valid publicly accessible URL.
   */
  audio_url: string | Blob | File;
  /**
   * Add more to the beginning (left) or end (right) of the song
   */
  side: "left" | "right";
  /**
   * Duration in seconds to extend the song. If not provided, will attempt to automatically determine.
   */
  extend_duration?: number;
  /**
   * Duration in seconds to crop from the selected side before extending from that side.
   */
  crop_duration?: number;
};
export type ExtendOutput = {
  /**
   * The extended video
   */
  video: File;
};
export type ExtendVideoConditioningInput = {
  /**
   * URL of video to use as conditioning
   */
  video_url: string | Blob | File;
  /**
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number;
  /**
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning. Default value: `1`
   */
  strength?: number;
  /**
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean;
  /**
   * Maximum number of frames to use from the video. If None, all frames will be used. Default value: `1441`
   */
  max_num_frames?: number;
  /**
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean;
  /**
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True. Default value: `24`
   */
  target_fps?: number;
  /**
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean;
};
export type ExtendVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Video to be extended.
   */
  video: VideoConditioningInput;
};
export type ExtendVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type ExtractAudioInput = {
  /**
   * URL of the video file to extract audio from
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Output audio format Default value: `"mp3"`
   */
  audio_format?: "mp3" | "wav" | "aac" | "flac";
  /**
   * Audio bitrate Default value: `"192k"`
   */
  audio_bitrate?: "128k" | "192k" | "256k" | "320k";
};
export type ExtractAudioOutput = {
  /**
   * The extracted audio file
   */
  audio: AudioFile;
};
export type ExtractNthFrameInput = {
  /**
   * URL of the video file to extract frames from
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Extract every Nth frame (e.g., 3 = every 3rd frame, 12 = every 12th frame) Default value: `12`
   */
  frame_interval?: number;
  /**
   * Output format for extracted frames Default value: `"png"`
   */
  output_format?: "png" | "jpg" | "jpeg" | "webp";
  /**
   * Maximum number of frames to extract Default value: `100`
   */
  max_frames?: number;
  /**
   * Quality for jpg/webp output (1-100) Default value: `95`
   */
  quality?: number;
};
export type ExtractNthFrameOutput = {
  /**
   * Array of extracted frame images
   */
  images: Array<Image>;
  /**
   * Total number of frames extracted
   */
  frame_count: number;
};
export type Fabric10FastInput = {
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   *
   */
  audio_url: string | Blob | File;
  /**
   * Resolution
   */
  resolution: "720p" | "480p";
};
export type Fabric10FastOutput = {
  /**
   *
   */
  video: File;
};
export type Fabric10Input = {
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   *
   */
  audio_url: string | Blob | File;
  /**
   * Resolution
   */
  resolution: "720p" | "480p";
};
export type Fabric10Output = {
  /**
   *
   */
  video: File;
};
export type Fabric10TextInput = {
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   *
   */
  text: string;
  /**
   * Optional additional voice description. The primary voice description is auto-generated from the image. You can use simple descriptors like 'British accent' or 'Confident' or provide a detailed description like 'Confident male voice, mid-20s, with notes of...'
   */
  voice_description?: string;
  /**
   * Resolution
   */
  resolution: "720p" | "480p";
};
export type Fabric10TextOutput = {
  /**
   *
   */
  video: File;
};
export type FabricOneLipsyncInput = {
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   *
   */
  audio_url: string | Blob | File;
  /**
   * Resolution
   */
  resolution: "720p" | "480p";
};
export type FabricOneOutput = {
  /**
   *
   */
  video: File;
};
export type FabricOneTextInput = {
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   *
   */
  text: string;
  /**
   * Optional additional voice description. The primary voice description is auto-generated from the image. You can use simple descriptors like 'British accent' or 'Confident' or provide a detailed description like 'Confident male voice, mid-20s, with notes of...'
   */
  voice_description?: string;
  /**
   * Resolution
   */
  resolution: "720p" | "480p";
};
export type FabricOneTextOutput = {
  /**
   *
   */
  video: File;
};
export type FaceEnhancementOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type FaceFusionImageOutput = {
  /**
   * Generated image result
   */
  image: Image;
  /**
   * Optional processing duration in milliseconds
   */
  processing_time_ms?: number;
};
export type FaceFusionVideoOutput = {
  /**
   * Generated video result
   */
  video: Video;
  /**
   * Optional processing duration in milliseconds
   */
  processing_time_ms?: number;
  /**
   * Warning message if video was modified (e.g., truncated or FPS reduced)
   */
  warning?: string;
};
export type FaceToFullPortraitInput = {
  /**
   * The URL of the cropped face image. Provide a close-up face photo.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the full portrait you want to generate from the face. Include clothing, setting, pose, and style details. Default value: `"Photography. A portrait of the person in professional attire with natural lighting"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type FaceToFullPortraitOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type FashnTryonV15Input = {
  /**
   * URL or base64 of the model image
   */
  model_image: string;
  /**
   * URL or base64 of the garment image
   */
  garment_image: string;
  /**
   * Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment. Default value: `"auto"`
   */
  category?: "tops" | "bottoms" | "one-pieces" | "auto";
  /**
   * Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results. Default value: `"balanced"`
   */
  mode?: "performance" | "balanced" | "quality";
  /**
   * Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type. Default value: `"auto"`
   */
  garment_photo_type?: "auto" | "model" | "flat-lay";
  /**
   * Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear. Default value: `"permissive"`
   */
  moderation_level?: "none" | "permissive" | "conservative";
  /**
   * Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results.
   */
  seed?: number;
  /**
   * Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result. Default value: `1`
   */
  num_samples?: number;
  /**
   * Disables human parsing on the model image. Default value: `true`
   */
  segmentation_free?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster Default value: `"png"`
   */
  output_format?: "png" | "jpeg";
};
export type FashnTryonV15Output = {
  /**
   *
   */
  images: Array<File>;
};
export type FashnTryonV16Input = {
  /**
   * URL or base64 of the model image
   */
  model_image: string;
  /**
   * URL or base64 of the garment image
   */
  garment_image: string;
  /**
   * Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment. Default value: `"auto"`
   */
  category?: "tops" | "bottoms" | "one-pieces" | "auto";
  /**
   * Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results. Default value: `"balanced"`
   */
  mode?: "performance" | "balanced" | "quality";
  /**
   * Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type. Default value: `"auto"`
   */
  garment_photo_type?: "auto" | "model" | "flat-lay";
  /**
   * Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear. Default value: `"permissive"`
   */
  moderation_level?: "none" | "permissive" | "conservative";
  /**
   * Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results.
   */
  seed?: number;
  /**
   * Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result. Default value: `1`
   */
  num_samples?: number;
  /**
   * Disables human parsing on the model image. Default value: `true`
   */
  segmentation_free?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster Default value: `"png"`
   */
  output_format?: "png" | "jpeg";
};
export type FashnTryonV16Output = {
  /**
   *
   */
  images: Array<File>;
};
export type FastGeneralRembgInput = {
  /**
   *
   */
  video_url: string | Blob | File;
  /**
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality. Default value: `"vp9"`
   */
  output_codec?: "vp9" | "h264";
  /**
   * Improves the quality of the extracted object's edges. Default value: `true`
   */
  refine_foreground_edges?: boolean;
  /**
   * Set to False if the subject is not a person. Default value: `true`
   */
  subject_is_person?: boolean;
};
export type FastGeneralRembgOutput = {
  /**
   *
   */
  video: Array<File>;
};
export type FastImageToVideoHailuo02Input = {
  /**
   *
   */
  prompt: string;
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `"6"`
   */
  duration?: "6" | "10";
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type FfmpegApiComposeInput = {
  /**
   * List of tracks to be combined into the final media
   */
  tracks: Array<Track>;
};
export type FfmpegApiComposeOutput = {
  /**
   * URL of the processed video file
   */
  video_url: string | Blob | File;
  /**
   * URL of the video's thumbnail image
   */
  thumbnail_url: string | Blob | File;
};
export type FfmpegApiExtractFrameInput = {
  /**
   * URL of the video file to use as the video track
   */
  video_url: string | Blob | File;
  /**
   * Type of frame to extract: first, middle, or last frame of the video Default value: `"first"`
   */
  frame_type?: "first" | "middle" | "last";
};
export type FfmpegApiExtractFrameOutput = {
  /**
   *
   */
  images: Array<Image>;
};
export type FfmpegApiLoudnormInput = {
  /**
   * URL of the audio file to normalize
   */
  audio_url: string | Blob | File;
  /**
   * Integrated loudness target in LUFS. Default value: `-18`
   */
  integrated_loudness?: number;
  /**
   * Maximum true peak in dBTP. Default value: `-0.1`
   */
  true_peak?: number;
  /**
   * Loudness range target in LU Default value: `7`
   */
  loudness_range?: number;
  /**
   * Offset gain in dB applied before the true-peak limiter
   */
  offset?: number;
  /**
   * Use linear normalization mode (single-pass). If false, uses dynamic mode (two-pass for better quality).
   */
  linear?: boolean;
  /**
   * Treat mono input files as dual-mono for correct EBU R128 measurement on stereo systems
   */
  dual_mono?: boolean;
  /**
   * Return loudness measurement summary with the normalized audio
   */
  print_summary?: boolean;
  /**
   * Measured integrated loudness of input file in LUFS. Required for linear mode.
   */
  measured_i?: number;
  /**
   * Measured loudness range of input file in LU. Required for linear mode.
   */
  measured_lra?: number;
  /**
   * Measured true peak of input file in dBTP. Required for linear mode.
   */
  measured_tp?: number;
  /**
   * Measured threshold of input file in LUFS. Required for linear mode.
   */
  measured_thresh?: number;
};
export type FfmpegApiLoudnormOutput = {
  /**
   * Normalized audio file
   */
  audio: File;
  /**
   * Structured loudness measurement summary (if requested)
   */
  summary?: LoudnormSummary;
};
export type FfmpegApiMergeAudiosInput = {
  /**
   * List of audio URLs to merge in order. The 0th stream of the audio will be considered as the merge candidate.
   */
  audio_urls: Array<string>;
  /**
   * Output format of the combined audio. If not used, will be determined automatically using FFMPEG. Formatted as codec_sample_rate_bitrate.
   */
  output_format?:
    | "mp3_22050_32"
    | "mp3_44100_32"
    | "mp3_44100_64"
    | "mp3_44100_96"
    | "mp3_44100_128"
    | "mp3_44100_192"
    | "pcm_8000"
    | "pcm_16000"
    | "pcm_22050"
    | "pcm_24000"
    | "pcm_44100"
    | "pcm_48000"
    | "ulaw_8000"
    | "alaw_8000"
    | "opus_48000_32"
    | "opus_48000_64"
    | "opus_48000_96"
    | "opus_48000_128"
    | "opus_48000_192";
};
export type FfmpegApiMergeAudiosOutput = {
  /**
   * Merged audio file
   */
  audio: File;
};
export type FfmpegApiMergeAudioVideoInput = {
  /**
   * URL of the video file to use as the video track
   */
  video_url: string | Blob | File;
  /**
   * URL of the audio file to use as the audio track
   */
  audio_url: string | Blob | File;
  /**
   * Offset in seconds for when the audio should start relative to the video
   */
  start_offset?: number;
};
export type FfmpegApiMergeAudioVideoOutput = {
  /**
   * Output video with merged audio.
   */
  video: File;
};
export type FfmpegApiMergeVideosInput = {
  /**
   * List of video URLs to merge in order
   */
  video_urls: Array<string>;
  /**
   * Target FPS for the output video. If not provided, uses the lowest FPS from input videos.
   */
  target_fps?: number;
  /**
   * Resolution of the final video. Width and height must be between 512 and 2048.
   */
  resolution?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
};
export type FfmpegApiMergeVideosOutput = {
  /**
   * Merged video file
   */
  video: File;
  /**
   * Metadata about the merged video including original video info
   */
  metadata: unknown;
};
export type FfmpegApiMetadataInput = {
  /**
   * URL of the media file (video or audio) to analyze
   */
  media_url: string | Blob | File;
  /**
   * Whether to extract the start and end frames for videos. Note that when true the request will be slower.
   */
  extract_frames?: boolean;
};
export type FfmpegApiMetadataOutput = {
  /**
   * Metadata for the analyzed media file (either Video or Audio)
   */
  media: Video | Audio;
};
export type FfmpegApiWaveformInput = {
  /**
   * URL of the audio file to analyze
   */
  media_url: string | Blob | File;
  /**
   * Controls how many points are sampled per second of audio. Lower values (e.g. 1-2) create a coarser waveform, higher values (e.g. 4-10) create a more detailed one. Default value: `4`
   */
  points_per_second?: number;
  /**
   * Number of decimal places for the waveform values. Higher values provide more precision but increase payload size. Default value: `2`
   */
  precision?: number;
  /**
   * Size of the smoothing window. Higher values create a smoother waveform. Must be an odd number. Default value: `3`
   */
  smoothing_window?: number;
};
export type FfmpegApiWaveformOutput = {
  /**
   * Normalized waveform data as an array of values between -1 and 1. The number of points is determined by audio duration × points_per_second.
   */
  waveform: Array<number>;
  /**
   * Duration of the audio in seconds
   */
  duration: number;
  /**
   * Number of points in the waveform data
   */
  points: number;
  /**
   * Number of decimal places used in the waveform values
   */
  precision: number;
};
export type FiboEditAddObjectByTextInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The full natural language command describing what to add and where.
   */
  instruction: string;
};
export type FiboEditAddObjectByTextOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditBlendInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * Instruct what elements you would like to blend in your image.
   */
  instruction: string;
};
export type FiboEditBlendOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditColorizeInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * Select the color palette or aesthetic for the output image
   */
  color:
    | "contemporary color"
    | "vivid color"
    | "black and white colors"
    | "sepia vintage";
};
export type FiboEditColorizeOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditEditInput = {
  /**
   * Reference image (file or URL).
   */
  image_url?: string | Blob | File;
  /**
   * Mask image (file or URL). Optional
   */
  mask_url?: string | Blob | File;
  /**
   * Instruction for image editing.
   */
  instruction?: string;
  /**
   * The structured prompt to generate an image from.
   */
  structured_instruction?: StructuredInstruction;
  /**
   * Random seed for reproducibility. Default value: `5555`
   */
  seed?: number;
  /**
   * Number of inference steps. Default value: `50`
   */
  steps_num?: number;
  /**
   * Negative prompt for image generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for text. Default value: `5`
   */
  guidance_scale?: number | number;
  /**
   * If true, returns the image directly in the response (increases latency).
   */
  sync_mode?: boolean;
};
export type FiboEditEditOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditEditStructuredInstructionInput = {
  /**
   * Reference image (file or URL).
   */
  image_url?: string | Blob | File;
  /**
   * Reference image mask (file or URL). Optional.
   */
  mask_url?: string | Blob | File;
  /**
   * Instruction for image editing.
   */
  instruction?: string;
  /**
   * Random seed for reproducibility. Default value: `5555`
   */
  seed?: number;
  /**
   * If true, returns the image directly in the response (increases latency).
   */
  sync_mode?: boolean;
};
export type FiboEditEraseByTextInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The name of the object to remove.
   */
  object_name: string;
};
export type FiboEditEraseByTextOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditRelightInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * Where the light comes from.
   */
  light_direction: "front" | "side" | "bottom" | "top-down";
  /**
   * The quality/style/time of day.
   */
  light_type:
    | "midday"
    | "blue hour light"
    | "low-angle sunlight"
    | "sunrise light"
    | "spotlight on subject"
    | "overcast light"
    | "soft overcast daylight lighting"
    | "cloud-filtered lighting"
    | "fog-diffused lighting"
    | "moonlight lighting"
    | "starlight nighttime"
    | "soft bokeh lighting"
    | "harsh studio lighting";
};
export type FiboEditRelightOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditReplaceObjectByTextInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The full natural language command describing what to replace.
   */
  instruction: string;
};
export type FiboEditReplaceObjectByTextOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditReseasonInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The desired season.
   */
  season: "spring" | "summer" | "autumn" | "winter";
};
export type FiboEditReseasonOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditRestoreInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
};
export type FiboEditRestoreOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditRestyleInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * Select the desired artistic style for the output image.
   */
  style:
    | "3D Render"
    | "Cubism"
    | "Oil Painting"
    | "Anime"
    | "Cartoon"
    | "Coloring Book"
    | "Retro Ad"
    | "Pop Art Halftone"
    | "Vector Art"
    | "Story Board"
    | "Art Nouveau"
    | "Cross Etching"
    | "Wood Cut";
};
export type FiboEditRestyleOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditRewriteTextInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The new text string to appear in the image.
   */
  new_text: string;
};
export type FiboEditRewriteTextOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboEditSketchToColoredImageInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
};
export type FiboEditSketchToColoredImageOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<Image>;
  /**
   * Current instruction.
   */
  structured_instruction: any;
};
export type FiboGenerateInput = {
  /**
   * Prompt for image generation.
   */
  prompt?: string;
  /**
   * The structured prompt to generate an image from.
   */
  structured_prompt?: StructuredPrompt;
  /**
   * Reference image (file or URL).
   */
  image_url?: string | Blob | File;
  /**
   * Random seed for reproducibility. Default value: `5555`
   */
  seed?: number;
  /**
   * Number of inference steps. Default value: `50`
   */
  steps_num?: number;
  /**
   * Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9 Default value: `"1:1"`
   */
  aspect_ratio?:
    | "1:1"
    | "2:3"
    | "3:2"
    | "3:4"
    | "4:3"
    | "4:5"
    | "5:4"
    | "9:16"
    | "16:9";
  /**
   * Negative prompt for image generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for text. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * If true, returns the image directly in the response (increases latency).
   */
  sync_mode?: boolean;
};
export type FiboGenerateOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<any>;
  /**
   * Current prompt.
   */
  structured_prompt: any;
};
export type FiboGenerateStructuredPromptInput = {
  /**
   * Prompt for image generation.
   */
  prompt?: string;
  /**
   * The structured prompt to generate an image from.
   */
  structured_prompt?: StructuredPrompt;
  /**
   * Reference image (file or URL).
   */
  image_url?: string | Blob | File;
  /**
   * Random seed for reproducibility. Default value: `5555`
   */
  seed?: number;
};
export type FiboLiteGenerateInput = {
  /**
   * Prompt for image generation.
   */
  prompt?: string;
  /**
   * The structured prompt to generate an image from.
   */
  structured_prompt?: StructuredPrompt;
  /**
   * Reference image (file or URL).
   */
  image_url?: string | Blob | File;
  /**
   * Random seed for reproducibility. Default value: `5555`
   */
  seed?: number;
  /**
   * Number of inference steps for Fibo Lite. Default value: `8`
   */
  steps_num?: number;
  /**
   * Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9 Default value: `"1:1"`
   */
  aspect_ratio?:
    | "1:1"
    | "2:3"
    | "3:2"
    | "3:4"
    | "4:3"
    | "4:5"
    | "5:4"
    | "9:16"
    | "16:9";
  /**
   * If true, returns the image directly in the response (increases latency).
   */
  sync_mode?: boolean;
};
export type FiboLiteGenerateOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<any>;
  /**
   * Current prompt.
   */
  structured_prompt: any;
};
export type FiboLiteGenerateStructuredPromptInput = {
  /**
   * Prompt for image generation.
   */
  prompt?: string;
  /**
   * The structured prompt to generate an image from.
   */
  structured_prompt?: StructuredPrompt;
  /**
   * Reference image (file or URL).
   */
  image_url?: string | Blob | File;
  /**
   * Random seed for reproducibility. Default value: `5555`
   */
  seed?: number;
};
export type FiboLiteGenerateStructuredPromptLiteInput = {
  /**
   * Prompt for image generation.
   */
  prompt?: string;
  /**
   * The structured prompt to generate an image from.
   */
  structured_prompt?: BriaFiboVlmStructuredprompt;
  /**
   * Reference image (file or URL).
   */
  image_url?: string | Blob | File;
  /**
   * Random seed for reproducibility. Default value: `5555`
   */
  seed?: number;
};
export type FILMImageInput = {
  /**
   * The URL of the first image to use as the starting point for interpolation.
   */
  start_image_url: string | Blob | File;
  /**
   * The URL of the second image to use as the ending point for interpolation.
   */
  end_image_url: string | Blob | File;
  /**
   * The type of output to generate; either individual images or a video. Default value: `"images"`
   */
  output_type?: "images" | "video";
  /**
   * The format of the output images. Only applicable if output_type is 'images'. Default value: `"jpeg"`
   */
  image_format?: "png" | "jpeg";
  /**
   * The quality of the output video. Only applicable if output_type is 'video'. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Only applicable if output_type is 'video'. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * The number of frames to generate between the input images. Default value: `1`
   */
  num_frames?: number;
  /**
   * Whether to include the start image in the output.
   */
  include_start?: boolean;
  /**
   * Whether to include the end image in the output.
   */
  include_end?: boolean;
  /**
   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `8`
   */
  fps?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type FILMImageOutput = {
  /**
   * The generated frames as individual images.
   */
  images?: Array<ImageFile>;
  /**
   * The generated video file, if output_type is 'video'.
   */
  video?: VideoFile;
};
export type filmInput = {
  /**
   * The URL of the first image to use as the starting point for interpolation.
   */
  start_image_url: string | Blob | File;
  /**
   * The URL of the second image to use as the ending point for interpolation.
   */
  end_image_url: string | Blob | File;
  /**
   * The type of output to generate; either individual images or a video. Default value: `"images"`
   */
  output_type?: "images" | "video";
  /**
   * The format of the output images. Only applicable if output_type is 'images'. Default value: `"jpeg"`
   */
  image_format?: "png" | "jpeg";
  /**
   * The quality of the output video. Only applicable if output_type is 'video'. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Only applicable if output_type is 'video'. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * The number of frames to generate between the input images. Default value: `1`
   */
  num_frames?: number;
  /**
   * Whether to include the start image in the output.
   */
  include_start?: boolean;
  /**
   * Whether to include the end image in the output.
   */
  include_end?: boolean;
  /**
   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `8`
   */
  fps?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type filmOutput = {
  /**
   * The generated frames as individual images.
   */
  images?: Array<ImageFile>;
  /**
   * The generated video file, if output_type is 'video'.
   */
  video?: VideoFile;
};
export type FilmVideoInput = {
  /**
   * The URL of the video to use for interpolation.
   */
  video_url: string | Blob | File;
  /**
   * The number of frames to generate between the input video frames. Default value: `1`
   */
  num_frames?: number;
  /**
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean;
  /**
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used. Default value: `true`
   */
  use_calculated_fps?: boolean;
  /**
   * Frames per second for the output video. Only applicable if use_calculated_fps is False. Default value: `8`
   */
  fps?: number;
  /**
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The quality of the output video. Only applicable if output_type is 'video'. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Only applicable if output_type is 'video'. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
};
export type FILMVideoInput = {
  /**
   * The URL of the video to use for interpolation.
   */
  video_url: string | Blob | File;
  /**
   * The number of frames to generate between the input video frames. Default value: `1`
   */
  num_frames?: number;
  /**
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean;
  /**
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used. Default value: `true`
   */
  use_calculated_fps?: boolean;
  /**
   * Frames per second for the output video. Only applicable if use_calculated_fps is False. Default value: `8`
   */
  fps?: number;
  /**
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The quality of the output video. Only applicable if output_type is 'video'. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Only applicable if output_type is 'video'. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
};
export type FilmVideoOutput = {
  /**
   * The generated video file with interpolated frames.
   */
  video: VideoFile;
};
export type FILMVideoOutput = {
  /**
   * The generated video file with interpolated frames.
   */
  video: VideoFile;
};
export type FinegrainEraserBboxInput = {
  /**
   * URL of the image to edit
   */
  image_url: string | Blob | File;
  /**
   * List of bounding box coordinates to erase (only one box prompt is supported)
   */
  box_prompts: Array<BoxPromptBase>;
  /**
   * Erase quality mode Default value: `"standard"`
   */
  mode?: "express" | "standard" | "premium";
  /**
   * Random seed for reproducible generation
   */
  seed?: number;
};
export type FinegrainEraserBboxOutput = {
  /**
   * The edited image with content erased
   */
  image: File;
  /**
   * Seed used for generation
   */
  used_seed: number;
};
export type FinegrainEraserInput = {
  /**
   * URL of the image to edit
   */
  image_url: string | Blob | File;
  /**
   * Text description of what to erase
   */
  prompt: string;
  /**
   * Erase quality mode Default value: `"standard"`
   */
  mode?: "express" | "standard" | "premium";
  /**
   * Random seed for reproducible generation
   */
  seed?: number;
};
export type FinegrainEraserMaskInput = {
  /**
   * URL of the image to edit
   */
  image_url: string | Blob | File;
  /**
   * URL of the mask image. Should be a binary mask where white (255) indicates areas to erase
   */
  mask_url: string | Blob | File;
  /**
   * Erase quality mode Default value: `"standard"`
   */
  mode?: "express" | "standard" | "premium";
  /**
   * Random seed for reproducible generation
   */
  seed?: number;
};
export type FinegrainEraserMaskOutput = {
  /**
   * The edited image with content erased
   */
  image: File;
  /**
   * Seed used for generation
   */
  used_seed: number;
};
export type FinegrainEraserOutput = {
  /**
   * The edited image with content erased
   */
  image: File;
  /**
   * Seed used for generation
   */
  used_seed: number;
};
export type FlashvsrUpscaleVideoInput = {
  /**
   * Upscaling factor to be used. Default value: `2`
   */
  upscale_factor?: number;
  /**
   * The random seed used for the generation process.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned inline and not stored in history.
   */
  sync_mode?: boolean;
  /**
   * The input video to be upscaled
   */
  video_url: string | Blob | File;
  /**
   * Acceleration mode for VAE decoding. Options: regular (best quality), high (balanced), full (fastest). More accerleation means longer duration videos can be processed too. Default value: `"regular"`
   */
  acceleration?: "regular" | "high" | "full";
  /**
   * Color correction enabled. Default value: `true`
   */
  color_fix?: boolean;
  /**
   * Quality level for tile blending (0-100). Controls overlap between tiles to prevent grid artifacts. Higher values provide better quality with more overlap. Recommended: 70-85 for high-res videos, 50-70 for faster processing. Default value: `70`
   */
  quality?: number;
  /**
   * Copy the original audio tracks into the upscaled video using FFmpeg when possible.
   */
  preserve_audio?: boolean;
  /**
   * The format of the output video. Default value: `"X264 (.mp4)"`
   */
  output_format?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the output video. Default value: `"high"`
   */
  output_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Default value: `"balanced"`
   */
  output_write_mode?: "fast" | "balanced" | "small";
};
export type FlashvsrUpscaleVideoOutput = {
  /**
   * Upscaled video file after processing
   */
  video: File;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
};
export type FLiteStandardInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative Prompt for generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type FLiteStandardOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FLiteTextureInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative Prompt for generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type FLiteTextureOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FloatOutput = {
  /**
   * The value of the measurement.
   */
  value: number;
};
export type floweditInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * Prompt of the image to be used.
   */
  source_prompt: string;
  /**
   * Prompt of the image to be made.
   */
  target_prompt: string;
  /**
   * Random seed for reproducible generation. If set none, a random seed will be used.
   */
  seed?: number;
  /**
   * Steps for which the model should run. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for the source. Default value: `1.5`
   */
  src_guidance_scale?: number;
  /**
   * Guidance scale for target. Default value: `5.5`
   */
  tar_guidance_scale?: number;
  /**
   * Average step count Default value: `1`
   */
  n_avg?: number;
  /**
   * Control the strength of the edit Default value: `23`
   */
  n_max?: number;
  /**
   * Minimum step for improved style edits
   */
  n_min?: number;
};
export type floweditOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
};
export type Flux1DevImageToImageInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1DevImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux1DevInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1DevOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux1DevReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1DevReduxOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux1KreaImageToImageInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1KreaImageToImageOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux1KreaInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1KreaOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux1KreaReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1KreaReduxOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux1SchnellInput = {
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1SchnellOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux1SchnellReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1SchnellReduxOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux1SrpoImageToImageInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1SrpoImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux1SrpoInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type Flux1SrpoOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2EditImageInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
   */
  image_urls: Array<string>;
};
export type Flux2EditImageLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URsL of the images for editing. A maximum of 3 images are allowed, if more are provided, only the first 3 will be used.
   */
  image_urls: Array<string>;
  /**
   * List of LoRA weights to apply (maximum 3). Each LoRA can be a URL, HuggingFace repo ID, or local path.
   */
  loras?: Array<LoRAInput>;
};
export type Flux2EditImageLoRAOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2EditImageOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2EditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
   */
  image_urls: Array<string>;
};
export type Flux2EditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2FlashEditImageInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
   */
  image_urls: Array<string>;
};
export type Flux2FlashEditImageOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2FlashEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
   */
  image_urls: Array<string>;
};
export type Flux2FlashEditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2FlashInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Flux2FlashOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2FlashT2IOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2FlashTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Flux2FlexEditInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. If `auto`, the size will be determined by the model. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to expand the prompt using the model's own knowledge. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * List of URLs of input images for editing
   */
  image_urls: Array<string>;
  /**
   * The guidance scale to use for the generation. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
};
export type Flux2FlexEditOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The seed used for the generation.
   */
  seed: number;
};
export type Flux2FlexImageEditInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. If `auto`, the size will be determined by the model. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to expand the prompt using the model's own knowledge. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * List of URLs of input images for editing
   */
  image_urls: Array<string>;
  /**
   * The guidance scale to use for the generation. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
};
export type Flux2FlexInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to expand the prompt using the model's own knowledge. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The guidance scale to use for the generation. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
};
export type Flux2FlexOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The seed used for the generation.
   */
  seed: number;
};
export type Flux2FlexTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to expand the prompt using the model's own knowledge. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The guidance scale to use for the generation. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
};
export type Flux2Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Flux2Klein4bBaseEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
};
export type Flux2Klein4bBaseEditLoraInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<FalAiFlux2KleinLorainput>;
};
export type Flux2Klein4bBaseEditLoraOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein4bBaseEditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein4bBaseInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Flux2Klein4bBaseLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<FalAiFlux2KleinLorainput>;
};
export type Flux2Klein4bBaseLoraOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein4bBaseOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein4bBaseTrainerEditInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain up to four reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate applied to trainable parameters. Default value: `0.00005`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Dictates the naming scheme for the output weights Default value: `"fal"`
   */
  output_lora_format?: "fal" | "comfy";
};
export type Flux2Klein4bBaseTrainerEditOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type Flux2Klein4bBaseTrainerInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate applied to trainable parameters. Default value: `0.00005`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Dictates the naming scheme for the output weights Default value: `"fal"`
   */
  output_lora_format?: "fal" | "comfy";
};
export type Flux2Klein4bBaseTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type Flux2Klein4bEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
};
export type Flux2Klein4bEditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein4bInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Flux2Klein4bOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein9bBaseEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
};
export type Flux2Klein9bBaseEditLoraInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<FalAiFlux2KleinLorainput>;
};
export type Flux2Klein9bBaseEditLoraOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein9bBaseEditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein9bBaseInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Flux2Klein9bBaseLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<FalAiFlux2KleinLorainput>;
};
export type Flux2Klein9bBaseLoraOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein9bBaseOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein9bBaseTrainerEditInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain up to four reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate applied to trainable parameters. Default value: `0.00005`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Dictates the naming scheme for the output weights Default value: `"fal"`
   */
  output_lora_format?: "fal" | "comfy";
};
export type Flux2Klein9bBaseTrainerEditOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type Flux2Klein9bBaseTrainerInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate applied to trainable parameters. Default value: `0.00005`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Dictates the naming scheme for the output weights Default value: `"fal"`
   */
  output_lora_format?: "fal" | "comfy";
};
export type Flux2Klein9bBaseTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type Flux2Klein9bEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
};
export type Flux2Klein9bEditOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2Klein9bInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Flux2Klein9bOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2LoraEditInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URsL of the images for editing. A maximum of 3 images are allowed, if more are provided, only the first 3 will be used.
   */
  image_urls: Array<string>;
  /**
   * List of LoRA weights to apply (maximum 3). Each LoRA can be a URL, HuggingFace repo ID, or local path.
   */
  loras?: Array<LoRAInput>;
};
export type Flux2LoraEditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2LoraGalleryAddBackgroundInput = {
  /**
   * The URLs of the images. Provide an image with a white or clean background.
   */
  image_urls: Array<string>;
  /**
   * The prompt describing the background to add. Must start with 'Add Background' followed by your description. Default value: `"Add Background forest"`
   */
  prompt?: string;
  /**
   * The size of the generated image. If not provided, the size of the input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the add background effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGalleryAddBackgroundOutput = {
  /**
   * The generated images with added background
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGalleryApartmentStagingInput = {
  /**
   * The URL of the empty room image to furnish.
   */
  image_urls: Array<string>;
  /**
   * The prompt to generate a furnished room. Use 'furnish this room' for best results.
   */
  prompt: string;
  /**
   * The size of the generated image. If not provided, the size of the input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the apartment staging effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGalleryApartmentStagingOutput = {
  /**
   * The generated furnished room images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGalleryBallpointPenSketchInput = {
  /**
   * The prompt to generate a ballpoint pen sketch style image. Use 'b4llp01nt' trigger word for best results.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the ballpoint pen sketch effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGalleryBallpointPenSketchOutput = {
  /**
   * The generated ballpoint pen sketch style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGalleryDigitalComicArtInput = {
  /**
   * The prompt to generate a digital comic art style image. Use 'd1g1t4l' trigger word for best results.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the digital comic art effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGalleryDigitalComicArtOutput = {
  /**
   * The generated digital comic art style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGalleryFaceToFullPortraitInput = {
  /**
   * The URL of the cropped face image.
   */
  image_urls: Array<string>;
  /**
   * The prompt describing the full portrait to generate from the face. Default value: `"Face to full portrait"`
   */
  prompt?: string;
  /**
   * The size of the generated image. If not provided, the size of the input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the face to full portrait effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGalleryFaceToFullPortraitOutput = {
  /**
   * The generated full portrait images from face
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGalleryHdrStyleInput = {
  /**
   * The prompt to generate an HDR style image. The trigger word 'Hyp3rRe4list1c' will be automatically prepended.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the HDR style effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGalleryHdrStyleOutput = {
  /**
   * The generated HDR style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGalleryMultipleAnglesInput = {
  /**
   * The URL of the image to adjust camera angle for.
   */
  image_urls: Array<string>;
  /**
   * Horizontal rotation angle around the object in degrees. 0°=front view, 90°=right side, 180°=back view, 270°=left side, 360°=front view again.
   */
  horizontal_angle?: number;
  /**
   * Vertical camera angle in degrees. 0°=eye-level shot, 30°=elevated shot, 60°=high-angle shot (looking down from above).
   */
  vertical_angle?: number;
  /**
   * Camera zoom/distance. 0=wide shot (far away), 5=medium shot (normal), 10=close-up (very close). Default value: `5`
   */
  zoom?: number;
  /**
   * The size of the generated image. If not provided, the size of the input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility.
   */
  seed?: number;
  /**
   * If True, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image. Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the multiple angles effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGalleryMultipleAnglesOutput = {
  /**
   * The generated images with multiple camera angles
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGalleryRealismInput = {
  /**
   * The prompt to generate a realistic image with natural lighting and authentic details.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the realism effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGalleryRealismOutput = {
  /**
   * The generated realistic style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGallerySatelliteViewStyleInput = {
  /**
   * The prompt to generate a satellite/aerial view style image.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the satellite view style effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGallerySatelliteViewStyleOutput = {
  /**
   * The generated satellite view style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGallerySepiaVintageInput = {
  /**
   * The prompt to generate a sepia vintage photography style image.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the sepia vintage photography effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGallerySepiaVintageOutput = {
  /**
   * The generated sepia vintage photography style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraGalleryVirtualTryonInput = {
  /**
   * The URLs of the images for virtual try-on. Provide person image and clothing image.
   */
  image_urls: Array<string>;
  /**
   * The prompt to generate a virtual try-on image.
   */
  prompt: string;
  /**
   * The size of the generated image. If not provided, the size of the input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the virtual try-on effect. Default value: `1`
   */
  lora_scale?: number;
};
export type Flux2LoraGalleryVirtualTryonOutput = {
  /**
   * The generated virtual try-on images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type Flux2LoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * List of LoRA weights to apply (maximum 3). Each LoRA can be a URL, HuggingFace repo ID, or local path.
   */
  loras?: Array<LoRAInput>;
};
export type Flux2LoraOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2MaxEditInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. If `auto`, the size will be determined by the model. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * List of URLs of input images for editing
   */
  image_urls: Array<string>;
};
export type Flux2MaxEditOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The seed used for the generation.
   */
  seed: number;
};
export type Flux2MaxImageEditInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. If `auto`, the size will be determined by the model. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * List of URLs of input images for editing
   */
  image_urls: Array<string>;
};
export type Flux2MaxInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type Flux2MaxOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The seed used for the generation.
   */
  seed: number;
};
export type Flux2MaxTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type Flux2Output = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2ProEditInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. If `auto`, the size will be determined by the model. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * List of URLs of input images for editing
   */
  image_urls: Array<string>;
};
export type Flux2ProEditOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The seed used for the generation.
   */
  seed: number;
};
export type Flux2ProImageEditInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. If `auto`, the size will be determined by the model. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * List of URLs of input images for editing
   */
  image_urls: Array<string>;
};
export type Flux2ProInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type Flux2ProOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The seed used for the generation.
   */
  seed: number;
};
export type Flux2ProTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5";
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type Flux2T2ILoRAOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2T2IOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2TextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Flux2TextToImageLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * List of LoRA weights to apply (maximum 3). Each LoRA can be a URL, HuggingFace repo ID, or local path.
   */
  loras?: Array<LoRAInput>;
};
export type Flux2TrainerEditInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain up to four reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate applied to trainable parameters. Default value: `0.00005`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Dictates the naming scheme for the output weights Default value: `"fal"`
   */
  output_lora_format?: "fal" | "comfy";
};
export type Flux2TrainerEditOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type Flux2TrainerInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate applied to trainable parameters. Default value: `0.00005`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Dictates the naming scheme for the output weights Default value: `"fal"`
   */
  output_lora_format?: "fal" | "comfy";
};
export type Flux2TrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type Flux2TrainerV2EditInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain up to four reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate applied to trainable parameters. Default value: `0.00005`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Dictates the naming scheme for the output weights Default value: `"fal"`
   */
  output_lora_format?: "fal" | "comfy";
};
export type Flux2TrainerV2EditOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type Flux2TrainerV2Input = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate applied to trainable parameters. Default value: `0.00005`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Dictates the naming scheme for the output weights Default value: `"fal"`
   */
  output_lora_format?: "fal" | "comfy";
};
export type Flux2TrainerV2Output = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type Flux2TurboEditImageInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
   */
  image_urls: Array<string>;
};
export type Flux2TurboEditImageOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2TurboEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
   */
  image_urls: Array<string>;
};
export type Flux2TurboEditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2TurboInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Flux2TurboOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2TurboT2IOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Flux2TurboTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the prompt will be expanded for better results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type FluxControlLoraCannyImageToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The image to use for control lora. This is used to control the style of the generated image.
   */
  control_lora_image_url?: string | Blob | File;
  /**
   * The strength of the control lora. Default value: `1`
   */
  control_lora_strength?: number;
  /**
   * URL of image to use for inpainting. or img2img
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
};
export type FluxControlLoraCannyImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxControlLoraCannyInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The image to use for control lora. This is used to control the style of the generated image.
   */
  control_lora_image_url?: string | Blob | File;
  /**
   * The strength of the control lora. Default value: `1`
   */
  control_lora_strength?: number;
};
export type FluxControlLoraCannyOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxControlLoraDepthImageToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The image to use for control lora. This is used to control the style of the generated image.
   */
  control_lora_image_url: string | Blob | File;
  /**
   * The strength of the control lora. Default value: `1`
   */
  control_lora_strength?: number;
  /**
   * URL of image to use for inpainting. or img2img
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
};
export type FluxControlLoraDepthImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxControlLoraDepthInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The image to use for control lora. This is used to control the style of the generated image.
   */
  control_lora_image_url: string | Blob | File;
  /**
   * The strength of the control lora. Default value: `1`
   */
  control_lora_strength?: number;
  /**
   * If set to true, the input image will be preprocessed to extract depth information.
   * This is useful for generating depth maps from images. Default value: `true`
   */
  preprocess_depth?: boolean;
};
export type FluxControlLoraDepthOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxDevImageToImageInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type FluxDevImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxDevInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type FluxDevOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxGeneralInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  control_loras?: Array<ControlLoraWeight>;
  /**
   * The controlnets to use for the image generation. Only one controlnet is supported at the moment.
   */
  controlnets?: Array<ControlNet>;
  /**
   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.
   */
  controlnet_unions?: Array<ControlNetUnion>;
  /**
   * IP-Adapter to use for image generation.
   */
  ip_adapters?: Array<IPAdapter>;
  /**
   * EasyControl Inputs to use for image generation.
   */
  easycontrols?: Array<EasyControlWeight>;
  /**
   * Use an image input to influence the generation. Can be used to fill images in masked areas.
   */
  fill_image?: ImageFillInput;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  real_cfg_scale?: number;
  /**
   * Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.
   * If using XLabs IP-Adapter v1, this will be turned on!.
   */
  use_real_cfg?: boolean;
  /**
   * Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.
   */
  use_cfg_zero?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * URL of Image for Reference-Only
   */
  reference_image_url?: string | Blob | File;
  /**
   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`
   */
  reference_strength?: number;
  /**
   * The percentage of the total timesteps when the reference guidance is to bestarted.
   */
  reference_start?: number;
  /**
   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`
   */
  reference_end?: number;
  /**
   * Base shift for the scheduled timesteps Default value: `0.5`
   */
  base_shift?: number;
  /**
   * Max shift for the scheduled timesteps Default value: `1.15`
   */
  max_shift?: number;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * Specifies whether beta sigmas ought to be used.
   */
  use_beta_schedule?: boolean;
  /**
   * Sigmas schedule for the denoising process.
   */
  sigma_schedule?: "sgm_uniform";
  /**
   * Scheduler for the denoising process. Default value: `"euler"`
   */
  scheduler?: "euler" | "dpmpp_2m";
  /**
   * Negative prompt to steer the image generation away from unwanted features.
   * By default, we will be using NAG for processing the negative prompt. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The scale for NAG. Higher values will result in a image that is more distant
   * to the negative prompt. Default value: `3`
   */
  nag_scale?: number;
  /**
   * The tau for NAG. Controls the normalization of the hidden state.
   * Higher values will result in a less aggressive normalization,
   * but may also lead to unexpected changes with respect to the original image.
   * Not recommended to change this value. Default value: `2.5`
   */
  nag_tau?: number;
  /**
   * The alpha value for NAG. This value is used as a final weighting
   * factor for steering the normalized guidance (positive and negative prompts)
   * in the direction of the positive prompt. Higher values will result in less
   * steering on the normalized guidance where lower values will result in
   * considering the positive prompt guidance more. Default value: `0.25`
   */
  nag_alpha?: number;
  /**
   * The proportion of steps to apply NAG. After the specified proportion
   * of steps has been iterated, the remaining steps will use original
   * attention processors in FLUX. Default value: `0.25`
   */
  nag_end?: number;
};
export type FluxGeneralOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKontextDevInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output format Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Determines how the output resolution is set for image editing.
   * - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.
   * - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).
   * Apart from these, a few aspect ratios are also supported. Default value: `"match_input"`
   */
  resolution_mode?:
    | "auto"
    | "match_input"
    | "1:1"
    | "16:9"
    | "21:9"
    | "3:2"
    | "2:3"
    | "4:5"
    | "5:4"
    | "3:4"
    | "4:3"
    | "9:16"
    | "9:21";
};
export type FluxKontextDevOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKontextInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
};
export type FluxKontextLoraInpaintInput = {
  /**
   * The URL of the image to be inpainted.
   */
  image_url: string | Blob | File;
  /**
   * The prompt for the image to image task.
   */
  prompt: string;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The URL of the reference image for inpainting.
   */
  reference_image_url: string | Blob | File;
  /**
   * The URL of the mask for inpainting.
   */
  mask_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`
   */
  strength?: number;
};
export type FluxKontextLoraInpaintOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKontextLoraInput = {
  /**
   * The URL of the image to edit.
   *
   * Max width: 14142px, Max height: 14142px, Timeout: 20s
   */
  image_url: string | Blob | File;
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Determines how the output resolution is set for image editing.
   * - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.
   * - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).
   * Apart from these, a few aspect ratios are also supported. Default value: `"match_input"`
   */
  resolution_mode?:
    | "auto"
    | "match_input"
    | "1:1"
    | "16:9"
    | "21:9"
    | "3:2"
    | "2:3"
    | "4:5"
    | "5:4"
    | "3:4"
    | "4:3"
    | "9:16"
    | "9:21";
};
export type FluxKontextLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKontextLoraTextToImageInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type FluxKontextLoraTextToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKontextMultiInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Image prompt for the omni model.
   */
  image_urls: Array<string>;
};
export type FluxKontextOutput = {
  /**
   * The generated image files info.
   */
  images: Array<FalToolkitImageImageImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKontextTrainerInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Number of steps to train for Default value: `1000`
   */
  steps?: number;
  /**
   *  Default value: `0.0001`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Dictates the naming scheme for the output weights Default value: `"fal"`
   */
  output_lora_format?: "fal" | "comfy";
};
export type FluxKontextTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type FluxKreaImageToImageInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type FluxKreaImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKreaInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type FluxKreaLoraImageToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * URL of image to use for inpainting. or img2img
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
};
export type FluxKreaLoraImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKreaLoraInpaintingInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * URL of image to use for inpainting. or img2img
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
  /**
   * The mask to area to Inpaint in.
   */
  mask_url: string | Blob | File;
};
export type FluxKreaLoraInpaintingOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKreaLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type FluxKreaLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKreaLoraStreamInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type FluxKreaLoraStreamOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKreaOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKreaReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type FluxKreaReduxOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxKreaTrainerInput = {
  /**
   * URL to zip archive with images. Try to use at least 4 images in general the more the better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
   */
  images_data_url: string | Blob | File;
  /**
   * Trigger word to be used in the captions. If None, a trigger word will not be used.
   * If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.
   */
  trigger_word?: string;
  /**
   * If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible. Default value: `true`
   */
  create_masks?: boolean;
  /**
   * Number of steps to train the LoRA on.
   */
  steps?: number;
  /**
   * If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.
   */
  is_style?: boolean;
  /**
   * Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.
   */
  is_input_format_already_preprocessed?: boolean;
  /**
   * The format of the archive. If not specified, the format will be inferred from the URL.
   */
  data_archive_format?: string;
};
export type FluxKreaTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the training configuration file.
   */
  config_file: File;
  /**
   * URL to the preprocessed images.
   */
  debug_preprocessed_output?: File;
};
export type FluxLoraCannyInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * URL of image to use for canny input
   */
  image_url: string | Blob | File;
};
export type FluxLoraCannyOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxLoraFastTrainingInput = {
  /**
   * URL to zip archive with images. Try to use at least 4 images in general the more the better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
   */
  images_data_url: string | Blob | File;
  /**
   * Trigger word to be used in the captions. If None, a trigger word will not be used.
   * If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.
   */
  trigger_word?: string;
  /**
   * If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible. Default value: `true`
   */
  create_masks?: boolean;
  /**
   * Number of steps to train the LoRA on.
   */
  steps?: number;
  /**
   * If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.
   */
  is_style?: boolean;
  /**
   * Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.
   */
  is_input_format_already_preprocessed?: boolean;
  /**
   * The format of the archive. If not specified, the format will be inferred from the URL.
   */
  data_archive_format?: string;
};
export type FluxLoraFastTrainingOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the training configuration file.
   */
  config_file: File;
  /**
   * URL to the preprocessed images.
   */
  debug_preprocessed_output?: File;
};
export type FluxLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type FluxLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxLoraPortraitTrainerInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
   *
   * The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.
   */
  images_data_url: string | Blob | File;
  /**
   * Trigger phrase to be used in the captions. If None, a trigger word will not be used.
   * If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.
   */
  trigger_phrase?: string;
  /**
   * Learning rate to use for training. Default value: `0.00009`
   */
  learning_rate?: number;
  /**
   * Number of steps to train the LoRA on. Default value: `2500`
   */
  steps?: number;
  /**
   * If True, multiresolution training will be used. Default value: `true`
   */
  multiresolution_training?: boolean;
  /**
   * If True, the subject will be cropped from the image. Default value: `true`
   */
  subject_crop?: boolean;
  /**
   * The format of the archive. If not specified, the format will be inferred from the URL.
   */
  data_archive_format?: string;
  /**
   * URL to a checkpoint to resume training from. Default value: `""`
   */
  resume_from_checkpoint?: string;
  /**
   * If True, masks will be created for the subject.
   */
  create_masks?: boolean;
};
export type FluxLoraPortraitTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the training configuration file.
   */
  config_file: File;
};
export type FluxLoraStreamInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type FluxLoraStreamOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxMultiIDOutput = {
  /**
   * The generated image files info
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The final prompt used for generation
   */
  prompt: string;
};
export type FluxProCannyControlFinetunedInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The control image URL to generate the Canny edge map from.
   */
  control_image_url: string | Blob | File;
  /**
   * References your specific model
   */
  finetune_id: string;
  /**
   * Controls finetune influence.
   * Increase this value if your target concept isn't showing up strongly enough.
   * The optimal setting depends on your finetune and prompt
   */
  finetune_strength: number;
};
export type FluxProCannyControlInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The control image URL to generate the Canny edge map from.
   */
  control_image_url: string | Blob | File;
};
export type FluxProDepthControlFinetunedInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `15`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The control image URL to generate the depth map from.
   */
  control_image_url: string | Blob | File;
  /**
   * References your specific model
   */
  finetune_id: string;
  /**
   * Controls finetune influence.
   * Increase this value if your target concept isn't showing up strongly enough.
   * The optimal setting depends on your finetune and prompt
   */
  finetune_strength: number;
};
export type FluxProDepthControlInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The control image URL to generate the depth map from.
   */
  control_image_url: string | Blob | File;
};
export type FluxProFillFinetunedInput = {
  /**
   * The prompt to fill the masked part of the image.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The image URL to generate an image from. Needs to match the dimensions of the mask.
   */
  image_url: string | Blob | File;
  /**
   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.
   */
  mask_url: string | Blob | File;
  /**
   * References your specific model
   */
  finetune_id: string;
  /**
   * Controls finetune influence.
   * Increase this value if your target concept isn't showing up strongly enough.
   * The optimal setting depends on your finetune and prompt
   */
  finetune_strength: number;
};
export type FluxProFillInput = {
  /**
   * The prompt to fill the masked part of the image.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The image URL to generate an image from. Needs to match the dimensions of the mask.
   */
  image_url: string | Blob | File;
  /**
   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.
   */
  mask_url: string | Blob | File;
};
export type FluxProKontextInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
};
export type FluxProKontextMaxInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
};
export type FluxProKontextMaxMultiInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Image prompt for the omni model.
   */
  image_urls: Array<string>;
};
export type FluxProKontextMaxMultiOutput = {
  /**
   * The generated image files info.
   */
  images: Array<RegistryImageFastSdxlModelsImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxProKontextMaxOutput = {
  /**
   * The generated image files info.
   */
  images: Array<FalToolkitImageImageImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxProKontextMaxTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
};
export type FluxProKontextMaxTextToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<RegistryImageFastSdxlModelsImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxProKontextMultiInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * Image prompt for the omni model.
   */
  image_urls: Array<string>;
};
export type FluxProKontextMultiOutput = {
  /**
   * The generated image files info.
   */
  images: Array<RegistryImageFastSdxlModelsImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxProKontextOutput = {
  /**
   * The generated image files info.
   */
  images: Array<FalToolkitImageImageImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxProKontextTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
};
export type FluxProKontextTextToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<RegistryImageFastSdxlModelsImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxProOutpaintInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The image URL to expand using outpainting
   */
  image_url: string | Blob | File;
  /**
   * Pixels to expand at the top
   */
  expand_top?: number;
  /**
   * Pixels to expand at the bottom
   */
  expand_bottom?: number;
  /**
   * Pixels to expand on the left
   */
  expand_left?: number;
  /**
   * Pixels to expand on the right
   */
  expand_right?: number;
};
export type FluxProPlusTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
};
export type FluxProTextToImageFinetunedInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * References your specific model
   */
  finetune_id: string;
  /**
   * Controls finetune influence.
   * Increase this value if your target concept isn't showing up strongly enough.
   * The optimal setting depends on your finetune and prompt
   */
  finetune_strength: number;
};
export type FluxProTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
};
export type FluxProUltraTextToImageFinetunedInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The image URL to generate an image from.
   */
  image_url?: string | Blob | File;
  /**
   * The strength of the image prompt, between 0 and 1. Default value: `0.1`
   */
  image_prompt_strength?: number;
  /**
   * The aspect ratio of the generated image. Default value: `16:9`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21"
    | string;
  /**
   * Generate less processed, more natural-looking images.
   */
  raw?: boolean;
  /**
   * References your specific model
   */
  finetune_id: string;
  /**
   * Controls finetune influence.
   * Increase this value if your target concept isn't showing up strongly enough.
   * The optimal setting depends on your finetune and prompt
   */
  finetune_strength: number;
};
export type FluxProUltraTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The image URL to generate an image from.
   */
  image_url?: string | Blob | File;
  /**
   * The strength of the image prompt, between 0 and 1. Default value: `0.1`
   */
  image_prompt_strength?: number;
  /**
   * The aspect ratio of the generated image. Default value: `16:9`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21"
    | string;
  /**
   * Generate less processed, more natural-looking images.
   */
  raw?: boolean;
};
export type FluxProV11Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
};
export type FluxProV11Output = {
  /**
   * The generated image files info.
   */
  images: Array<RegistryImageFastSdxlModelsImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxProV11UltraFinetunedInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The image URL to generate an image from.
   */
  image_url?: string | Blob | File;
  /**
   * The strength of the image prompt, between 0 and 1. Default value: `0.1`
   */
  image_prompt_strength?: number;
  /**
   * The aspect ratio of the generated image. Default value: `16:9`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21"
    | string;
  /**
   * Generate less processed, more natural-looking images.
   */
  raw?: boolean;
  /**
   * References your specific model
   */
  finetune_id: string;
  /**
   * Controls finetune influence.
   * Increase this value if your target concept isn't showing up strongly enough.
   * The optimal setting depends on your finetune and prompt
   */
  finetune_strength: number;
};
export type FluxProV11UltraFinetunedOutput = {
  /**
   * The generated image files info.
   */
  images: Array<RegistryImageFastSdxlModelsImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxProV11UltraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The image URL to generate an image from.
   */
  image_url?: string | Blob | File;
  /**
   * The strength of the image prompt, between 0 and 1. Default value: `0.1`
   */
  image_prompt_strength?: number;
  /**
   * The aspect ratio of the generated image. Default value: `16:9`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21"
    | string;
  /**
   * Generate less processed, more natural-looking images.
   */
  raw?: boolean;
};
export type FluxProV11UltraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<RegistryImageFastSdxlModelsImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxProV1FillFinetunedInput = {
  /**
   * The prompt to fill the masked part of the image.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
  /**
   * The image URL to generate an image from. Needs to match the dimensions of the mask.
   */
  image_url: string | Blob | File;
  /**
   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.
   */
  mask_url: string | Blob | File;
  /**
   * References your specific model
   */
  finetune_id: string;
  /**
   * Controls finetune influence.
   * Increase this value if your target concept isn't showing up strongly enough.
   * The optimal setting depends on your finetune and prompt
   */
  finetune_strength: number;
};
export type FluxProV1FillFinetunedOutput = {
  /**
   * The generated image files info.
   */
  images: Array<RegistryImageFastSdxlModelsImage>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxSingleIDOutput = {
  /**
   * The generated image files info
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The final prompt used for generation
   */
  prompt: string;
};
export type FluxSrpoImageToImageInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type FluxSrpoImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxSrpoInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type FluxSrpoOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type FluxVisionUpscalerInput = {
  /**
   * The URL of the image to upscale.
   */
  image_url: string | Blob | File;
  /**
   * The upscale factor (1-4x). Default value: `2`
   */
  upscale_factor?: number;
  /**
   * The seed to use for the upscale. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The creativity of the model. The higher the creativity, the more the model will deviate from the original. Refers to the denoise strength of the sampling. Default value: `0.3`
   */
  creativity?: number;
  /**
   * CFG/guidance scale (1-4). Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance?: number;
  /**
   * Number of inference steps (4-50). Default value: `20`
   */
  steps?: number;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type FluxVisionUpscalerOutput = {
  /**
   * The URL of the generated image.
   */
  image: Image;
  /**
   * The seed used to generate the image.
   */
  seed: number;
  /**
   * The timings of the different steps in the workflow.
   */
  timings: any;
  /**
   * The VLM-generated caption describing the upscaled image.
   */
  caption: string;
};
export type FrameInput = {
  /**
   * URL of the video file to use as the video track
   */
  video_url: string | Blob | File;
  /**
   * Type of frame to extract: first, middle, or last frame of the video Default value: `"first"`
   */
  frame_type?: "first" | "middle" | "last";
};
export type FrameOutput = {
  /**
   *
   */
  images: Array<Image>;
};
export type FramepackF1Input = {
  /**
   * Text prompt for video generation (max 500 characters).
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * URL of the image input.
   */
  image_url: string | Blob | File;
  /**
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * The aspect ratio of the video to generate. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations. Default value: `"480p"`
   */
  resolution?: "720p" | "480p";
  /**
   * Classifier-Free Guidance scale for the generation. Default value: `1`
   */
  cfg_scale?: number;
  /**
   * Guidance scale for the generation. Default value: `10`
   */
  guidance_scale?: number;
  /**
   * The number of frames to generate. Default value: `180`
   */
  num_frames?: number;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};
export type FramepackF1Output = {
  /**
   *
   */
  video: File;
  /**
   * The seed used for generating the video.
   */
  seed: number;
};
export type FramepackFlf2vInput = {
  /**
   * Text prompt for video generation (max 500 characters).
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * URL of the image input.
   */
  image_url: string | Blob | File;
  /**
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * The aspect ratio of the video to generate. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations. Default value: `"480p"`
   */
  resolution?: "720p" | "480p";
  /**
   * Classifier-Free Guidance scale for the generation. Default value: `1`
   */
  cfg_scale?: number;
  /**
   * Guidance scale for the generation. Default value: `10`
   */
  guidance_scale?: number;
  /**
   * The number of frames to generate. Default value: `240`
   */
  num_frames?: number;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * URL of the end image input.
   */
  end_image_url: string | Blob | File;
  /**
   * Determines the influence of the final frame on the generated video. Higher values result in the output being more heavily influenced by the last frame. Default value: `0.8`
   */
  strength?: number;
};
export type FramepackFlf2vOutput = {
  /**
   *
   */
  video: File;
  /**
   * The seed used for generating the video.
   */
  seed: number;
};
export type framepackInput = {
  /**
   * Text prompt for video generation (max 500 characters).
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * URL of the image input.
   */
  image_url: string | Blob | File;
  /**
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * The aspect ratio of the video to generate. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations. Default value: `"480p"`
   */
  resolution?: "720p" | "480p";
  /**
   * Classifier-Free Guidance scale for the generation. Default value: `1`
   */
  cfg_scale?: number;
  /**
   * Guidance scale for the generation. Default value: `10`
   */
  guidance_scale?: number;
  /**
   * The number of frames to generate. Default value: `180`
   */
  num_frames?: number;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};
export type framepackOutput = {
  /**
   *
   */
  video: File;
  /**
   * The seed used for generating the video.
   */
  seed: number;
};
export type FrenchOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type Gemini25FlashImageEditInput = {
  /**
   * The prompt for image editing.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"auto"`
   */
  aspect_ratio?:
    | "auto"
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The URLs of the images to use for image-to-image generation or image editing.
   */
  image_urls: Array<string>;
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
};
export type Gemini25FlashImageEditOutput = {
  /**
   * The edited images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type Gemini25FlashImageInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
};
export type Gemini25FlashImageOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type Gemini3ProImagePreviewEditInput = {
  /**
   * The prompt for image editing.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * The aspect ratio of the generated image. Default value: `auto`
   */
  aspect_ratio?:
    | "auto"
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict. Default value: `"4"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The URLs of the images to use for image-to-image generation or image editing.
   */
  image_urls: Array<string>;
  /**
   * The resolution of the image to generate. Default value: `"1K"`
   */
  resolution?: "1K" | "2K" | "4K";
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
  /**
   * Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image.
   */
  enable_web_search?: boolean;
};
export type Gemini3ProImagePreviewEditOutput = {
  /**
   * The edited images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type Gemini3ProImagePreviewInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * The aspect ratio of the generated image. Use "auto" to let the model decide based on the prompt. Default value: `1:1`
   */
  aspect_ratio?:
    | "auto"
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict. Default value: `"4"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The resolution of the image to generate. Default value: `"1K"`
   */
  resolution?: "1K" | "2K" | "4K";
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
  /**
   * Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image.
   */
  enable_web_search?: boolean;
};
export type Gemini3ProImagePreviewOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type GeminiFlashEditInput = {
  /**
   * The prompt for image generation or editing
   */
  prompt: string;
  /**
   * Optional URL of an input image for editing. If not provided, generates a new image.
   */
  image_url: string | Blob | File;
};
export type GeminiFlashEditMultiInput = {
  /**
   * The prompt for image generation or editing
   */
  prompt: string;
  /**
   * List of URLs of input images for editing
   */
  input_image_urls: Array<string>;
};
export type GeminiFlashEditMultiOutput = {
  /**
   * The generated or edited image
   */
  image: Image;
  /**
   * Text description or response from Gemini
   */
  description: string;
};
export type GeminiFlashEditOutput = {
  /**
   * The generated or edited image
   */
  image: Image;
  /**
   * Text description or response from Gemini
   */
  description: string;
};
export type GeneralRembgInput = {
  /**
   *
   */
  video_url: string | Blob | File;
  /**
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality. Default value: `"vp9"`
   */
  output_codec?: "vp9" | "h264";
  /**
   * Improves the quality of the extracted object's edges. Default value: `true`
   */
  refine_foreground_edges?: boolean;
  /**
   * Set to False if the subject is not a person. Default value: `true`
   */
  subject_is_person?: boolean;
};
export type GeneralRembgOutput = {
  /**
   *
   */
  video: Array<File>;
};
export type GenerateInput = {
  /**
   * A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics.
   */
  prompt?: string;
  /**
   * Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.
   */
  tags?: Array<string>;
  /**
   * The lyrics sung in the generated song. An empty string will generate an instrumental track.
   */
  lyrics_prompt?: string;
  /**
   * The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.
   */
  seed?: number;
  /**
   * Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.) Default value: `2`
   */
  prompt_strength?: number;
  /**
   * Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7. Default value: `0.7`
   */
  balance_strength?: number;
  /**
   * Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed. Default value: `1`
   */
  num_songs?: number;
  /**
   *  Default value: `"wav"`
   */
  output_format?: "flac" | "mp3" | "wav" | "ogg" | "m4a";
  /**
   * The bit rate to use for mp3 and m4a formats. Not available for other formats.
   */
  output_bit_rate?: "128" | "192" | "256" | "320";
  /**
   * The beats per minute of the song. This can be set to an integer or the literal string "auto" to pick a suitable bpm based on the tags. Set bpm to null to not condition the model on bpm information. Default value: `auto`
   */
  bpm?: number | string;
};
export type GenerateOutput = {
  /**
   * The seed used for generation. This can be used to generate an identical song by passing the same parameters with this seed in a future request.
   */
  seed: number;
  /**
   * The style tags used for generation.
   */
  tags?: Array<string>;
  /**
   * The lyrics used for generation.
   */
  lyrics?: string;
  /**
   * The generated audio files.
   */
  audio: Array<File>;
};
export type GenFillInput = {
  /**
   * Input Image to erase from
   */
  image_url: string | Blob | File;
  /**
   * The URL of the binary mask image that represents the area that will be cleaned.
   */
  mask_url: string | Blob | File;
  /**
   * The prompt you would like to use to generate images.
   */
  prompt: string;
  /**
   * The negative prompt you would like to use to generate images. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Number of Images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type GenFillOutput = {
  /**
   * Generated Images
   */
  images: Array<Image>;
};
export type ghiblifyInput = {
  /**
   * The URL of the image to upscale.
   */
  image_url: string | Blob | File;
  /**
   * The seed to use for the upscale. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type ghiblifyOutput = {
  /**
   * The URL of the generated image.
   */
  image: Image;
};
export type GlmImageImageToImageInput = {
  /**
   * Text prompt for image generation.
   */
  prompt: string;
  /**
   * Output image size. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "portrait_3_2"
    | "landscape_3_2"
    | "portrait_hd"
    | "landscape_hd";
  /**
   * Number of diffusion denoising steps. More steps generally produce higher quality images. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Higher values make the model follow the prompt more closely. Default value: `1.5`
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducibility. The same seed with the same prompt will produce the same image.
   */
  seed?: number;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Enable NSFW safety checking on the generated images. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output image format. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If True, the image will be returned as a base64 data URI instead of a URL.
   */
  sync_mode?: boolean;
  /**
   * If True, the prompt will be enhanced using an LLM for more detailed and higher quality results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL(s) of the condition image(s) for image-to-image generation. Supports up to 4 URLs for multi-image references.
   */
  image_urls: Array<string>;
};
export type GlmImageImageToImageOutput = {
  /**
   * List of URLs to the generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type GlmImageInput = {
  /**
   * Text prompt for image generation.
   */
  prompt: string;
  /**
   * Output image size. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "portrait_3_2"
    | "landscape_3_2"
    | "portrait_hd"
    | "landscape_hd";
  /**
   * Number of diffusion denoising steps. More steps generally produce higher quality images. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Higher values make the model follow the prompt more closely. Default value: `1.5`
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducibility. The same seed with the same prompt will produce the same image.
   */
  seed?: number;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Enable NSFW safety checking on the generated images. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output image format. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If True, the image will be returned as a base64 data URI instead of a URL.
   */
  sync_mode?: boolean;
  /**
   * If True, the prompt will be enhanced using an LLM for more detailed and higher quality results.
   */
  enable_prompt_expansion?: boolean;
};
export type GlmImageOutput = {
  /**
   * List of URLs to the generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type GlmImageToImageInput = {
  /**
   * Text prompt for image generation.
   */
  prompt: string;
  /**
   * Output image size. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "portrait_3_2"
    | "landscape_3_2"
    | "portrait_hd"
    | "landscape_hd";
  /**
   * Number of diffusion denoising steps. More steps generally produce higher quality images. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Higher values make the model follow the prompt more closely. Default value: `1.5`
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducibility. The same seed with the same prompt will produce the same image.
   */
  seed?: number;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Enable NSFW safety checking on the generated images. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output image format. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If True, the image will be returned as a base64 data URI instead of a URL.
   */
  sync_mode?: boolean;
  /**
   * If True, the prompt will be enhanced using an LLM for more detailed and higher quality results.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL(s) of the condition image(s) for image-to-image generation. Supports up to 4 URLs for multi-image references.
   */
  image_urls: Array<string>;
};
export type GlmImageToImageOutput = {
  /**
   * List of URLs to the generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type GlowInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Glow intensity Default value: `1`
   */
  glow_intensity?: number;
  /**
   * Glow blur radius Default value: `5`
   */
  glow_radius?: number;
};
export type GlowOutput = {
  /**
   * The processed images with glow effect
   */
  images: Array<Image>;
};
export type GotOcrV2Input = {
  /**
   * URL of images.
   */
  input_image_urls?: Array<string>;
  /**
   * Generate the output in formatted mode.
   */
  do_format?: boolean;
  /**
   * Use provided images to generate a single output.
   */
  multi_page?: boolean;
};
export type GotOcrV2Output = {
  /**
   * Generated output
   */
  outputs: Array<string>;
};
export type GptImage15EditInput = {
  /**
   * The prompt for image generation
   */
  prompt: string;
  /**
   * The URLs of the images to use as a reference for the generation.
   */
  image_urls: Array<string>;
  /**
   * Aspect ratio for the generated image Default value: `"auto"`
   */
  image_size?: "auto" | "1024x1024" | "1536x1024" | "1024x1536";
  /**
   * Background for the generated image Default value: `"auto"`
   */
  background?: "auto" | "transparent" | "opaque";
  /**
   * Quality for the generated image Default value: `"high"`
   */
  quality?: "low" | "medium" | "high";
  /**
   * Input fidelity for the generated image Default value: `"high"`
   */
  input_fidelity?: "low" | "high";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the images Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The URL of the mask image to use for the generation. This indicates what part of the image to edit.
   */
  mask_image_url?: string | Blob | File;
};
export type GptImage15EditOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
};
export type GptImage15Input = {
  /**
   * The prompt for image generation
   */
  prompt: string;
  /**
   * Aspect ratio for the generated image Default value: `"1024x1024"`
   */
  image_size?: "1024x1024" | "1536x1024" | "1024x1536";
  /**
   * Background for the generated image Default value: `"auto"`
   */
  background?: "auto" | "transparent" | "opaque";
  /**
   * Quality for the generated image Default value: `"high"`
   */
  quality?: "low" | "medium" | "high";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the images Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type GptImage15Output = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
};
export type GptImage1EditImageInput = {
  /**
   * The prompt for image generation
   */
  prompt: string;
  /**
   * The URLs of the images to use as a reference for the generation.
   */
  image_urls: Array<string>;
  /**
   * Aspect ratio for the generated image Default value: `"auto"`
   */
  image_size?: "auto" | "1024x1024" | "1536x1024" | "1024x1536";
  /**
   * Background for the generated image Default value: `"auto"`
   */
  background?: "auto" | "transparent" | "opaque";
  /**
   * Quality for the generated image Default value: `"auto"`
   */
  quality?: "auto" | "low" | "medium" | "high";
  /**
   * Input fidelity for the generated image Default value: `"high"`
   */
  input_fidelity?: "low" | "high";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the images Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type GptImage1EditImageOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
};
export type GptImage1MiniEditInput = {
  /**
   * The prompt for image generation
   */
  prompt: string;
  /**
   * The URLs of the images to use as a reference for the generation.
   */
  image_urls: Array<string>;
  /**
   * Aspect ratio for the generated image Default value: `"auto"`
   */
  image_size?: "auto" | "1024x1024" | "1536x1024" | "1024x1536";
  /**
   * Background for the generated image Default value: `"auto"`
   */
  background?: "auto" | "transparent" | "opaque";
  /**
   * Quality for the generated image Default value: `"auto"`
   */
  quality?: "auto" | "low" | "medium" | "high";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the images Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type GptImage1MiniEditOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
};
export type GptImage1MiniInput = {
  /**
   * The prompt for image generation
   */
  prompt: string;
  /**
   * Aspect ratio for the generated image Default value: `"auto"`
   */
  image_size?: "auto" | "1024x1024" | "1536x1024" | "1024x1536";
  /**
   * Background for the generated image Default value: `"auto"`
   */
  background?: "auto" | "transparent" | "opaque";
  /**
   * Quality for the generated image Default value: `"auto"`
   */
  quality?: "auto" | "low" | "medium" | "high";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the images Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type GptImage1MiniOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
};
export type GptImage1TextToImageInput = {
  /**
   * The prompt for image generation
   */
  prompt: string;
  /**
   * Aspect ratio for the generated image Default value: `"auto"`
   */
  image_size?: "auto" | "1024x1024" | "1536x1024" | "1024x1536";
  /**
   * Background for the generated image Default value: `"auto"`
   */
  background?: "auto" | "transparent" | "opaque";
  /**
   * Quality for the generated image Default value: `"auto"`
   */
  quality?: "auto" | "low" | "medium" | "high";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the images Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type GptImage1TextToImageOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
};
export type GrainInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Film grain intensity Default value: `0.4`
   */
  grain_intensity?: number;
  /**
   * Film grain scale Default value: `10`
   */
  grain_scale?: number;
  /**
   * Style of film grain to apply Default value: `"modern"`
   */
  grain_style?:
    | "modern"
    | "analog"
    | "kodak"
    | "fuji"
    | "cinematic"
    | "newspaper";
};
export type GrainOutput = {
  /**
   * The processed images with grain effect
   */
  images: Array<Image>;
};
export type GreenScreenRembgInput = {
  /**
   *
   */
  video_url: string | Blob | File;
  /**
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality. Default value: `"vp9"`
   */
  output_codec?: "vp9" | "h264";
  /**
   * Increase the value if green spots remain in the video, decrease if color changes are noticed on the extracted subject. Default value: `0.8`
   */
  spill_suppression_strength?: number;
};
export type GreenScreenRembgOutput = {
  /**
   *
   */
  video: Array<File>;
};
export type GrokImagineImageEditInput = {
  /**
   * Text description of the desired image.
   */
  prompt: string;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * URL of the image to edit.
   */
  image_url: string | Blob | File;
};
export type GrokImagineImageEditOutput = {
  /**
   * The URL of the edited image.
   */
  images: Array<ImageFile>;
  /**
   * The enhanced prompt that was used to generate the image.
   */
  revised_prompt: string;
};
export type GrokImagineImageInput = {
  /**
   * Text description of the desired image.
   */
  prompt: string;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "2:1"
    | "20:9"
    | "19.5:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:19.5"
    | "9:20"
    | "1:2";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type GrokImagineImageOutput = {
  /**
   * The URL of the generated image.
   */
  images: Array<ImageFile>;
  /**
   * The enhanced prompt that was used to generate the image.
   */
  revised_prompt: string;
};
export type GrokImagineVideoEditVideoInput = {
  /**
   * Text description of the desired edit.
   */
  prompt: string;
  /**
   * URL of the input video to edit. The video will be resized to a maximum area of 854x480 pixels and truncated to 8 seconds.
   */
  video_url: string | Blob | File;
  /**
   * Resolution of the output video. Default value: `"auto"`
   */
  resolution?: "auto" | "480p" | "720p";
};
export type GrokImagineVideoEditVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
};
export type GrokImagineVideoImageToVideoInput = {
  /**
   * Text description of desired changes or motion in the video.
   */
  prompt: string;
  /**
   * Video duration in seconds. Default value: `6`
   */
  duration?: number;
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?:
    | "auto"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16";
  /**
   * Resolution of the output video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * URL of the input image for video generation.
   */
  image_url: string | Blob | File;
};
export type GrokImagineVideoImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
};
export type GrokImagineVideoTextToVideoInput = {
  /**
   * Text description of the desired video.
   */
  prompt: string;
  /**
   * Video duration in seconds. Default value: `6`
   */
  duration?: number;
  /**
   * Aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "3:2" | "1:1" | "2:3" | "3:4" | "9:16";
  /**
   * Resolution of the output video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
};
export type GrokImagineVideoTextToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
};
export type GroupPhotoInput = {
  /**
   * The URLs of the images to combine into a group photo. Provide 2 or more individual portrait images.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the group photo scene, setting, and style. The model will maintain character consistency and add vintage effects like grain, blur, and retro filters. Default value: `"Two people standing next to each other outside with a landscape background"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type GroupPhotoOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type GuidanceInput = {
  /**
   * The image that should be used as guidance, in base64 format, with the method defined in guidance_method_1. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB. If more then one guidance method is used, all guidance images must be of the same aspect ratio, and this will be the aspect ratio of the generated results. If guidance_method_1 is selected, an image must be provided.
   */
  image_url: string | Blob | File;
  /**
   * Which guidance type you would like to include in the generation. Up to 4 guidance methods can be combined during a single inference. This parameter is optional.
   */
  method?:
    | "controlnet_canny"
    | "controlnet_depth"
    | "controlnet_recoloring"
    | "controlnet_color_grid";
  /**
   * Impact of the guidance. Default value: `1`
   */
  scale?: number;
};
export type HairChangeInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The desired hair style to apply. Default value: `"bald"`
   */
  prompt?: string;
};
export type HairChangeOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type HdrStyleInput = {
  /**
   * The prompt to generate an HDR style image. The trigger word 'Hyp3rRe4list1c' will be automatically prepended.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the HDR style effect. Default value: `1`
   */
  lora_scale?: number;
};
export type HdrStyleOutput = {
  /**
   * The generated HDR style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type HeadshotInput = {
  /**
   * Portrait image URL to convert to professional headshot
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"professional"`
   */
  background_style?: "professional" | "corporate" | "clean" | "gradient";
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type HeadshotOutput = {
  /**
   * Professional headshot image
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type HidreamE11Input = {
  /**
   * The description of the target image after your edits have been made. Leave this blank to allow the model to use its own imagination.
   */
  target_image_description?: string;
  /**
   * The prompt to generate an image from.
   */
  prompt?: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `"low resolution, blur"`
   */
  negative_prompt?: string;
  /**
   * URL of an input image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your initial image when looking for a related image to show you. Default value: `2`
   */
  image_guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type HidreamE11Output = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type HidreamE1FullInput = {
  /**
   * The description of the target image after your edits have been made. Leave this blank to allow the model to use its own imagination.
   */
  target_image_description?: string;
  /**
   * The prompt to generate an image from.
   */
  prompt?: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `"low resolution, blur"`
   */
  negative_prompt?: string;
  /**
   * URL of an input image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your initial image when looking for a related image to show you. Default value: `2`
   */
  image_guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type HidreamE1FullOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type HidreamI1DevInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type HidreamI1DevOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type HidreamI1FastInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `16`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type HidreamI1FastOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type HidreamI1FullImageToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. Setting to None uses the input image's size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name.
   */
  loras?: Array<LoraWeight>;
  /**
   * The image URL to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * Denoising strength for image-to-image generation. Default value: `0.75`
   */
  strength?: number;
};
export type HidreamI1FullImageToImageOutput = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type HidreamI1FullInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name.
   */
  loras?: Array<LoraWeight>;
};
export type HindiOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type Hunyuan3DInput = {
  /**
   * URL of image to use while generating the 3D model.
   */
  input_image_url: string | Blob | File;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for the model. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Octree resolution for the model. Default value: `256`
   */
  octree_resolution?: number;
  /**
   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
   */
  textured_mesh?: boolean;
};
export type Hunyuan3dV21Input = {
  /**
   * URL of image to use while generating the 3D model.
   */
  input_image_url: string | Blob | File;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for the model. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Octree resolution for the model. Default value: `256`
   */
  octree_resolution?: number;
  /**
   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
   */
  textured_mesh?: boolean;
};
export type Hunyuan3dV21Output = {
  /**
   * Generated 3D object.
   */
  model_glb: File;
  /**
   * Generated 3D object with PBR materials.
   */
  model_glb_pbr?: File;
  /**
   * Generated 3D object assets zip.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type Hunyuan3dV2Input = {
  /**
   * URL of image to use while generating the 3D model.
   */
  input_image_url: string | Blob | File;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for the model. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Octree resolution for the model. Default value: `256`
   */
  octree_resolution?: number;
  /**
   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
   */
  textured_mesh?: boolean;
};
export type Hunyuan3dV2MiniInput = {
  /**
   * URL of image to use while generating the 3D model.
   */
  input_image_url: string | Blob | File;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for the model. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Octree resolution for the model. Default value: `256`
   */
  octree_resolution?: number;
  /**
   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
   */
  textured_mesh?: boolean;
};
export type Hunyuan3dV2MiniOutput = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type Hunyuan3dV2MiniTurboInput = {
  /**
   * URL of image to use while generating the 3D model.
   */
  input_image_url: string | Blob | File;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for the model. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Octree resolution for the model. Default value: `256`
   */
  octree_resolution?: number;
  /**
   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
   */
  textured_mesh?: boolean;
};
export type Hunyuan3dV2MiniTurboOutput = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type Hunyuan3dV2MultiViewInput = {
  /**
   * URL of image to use while generating the 3D model.
   */
  front_image_url: string | Blob | File;
  /**
   * URL of image to use while generating the 3D model.
   */
  back_image_url: string | Blob | File;
  /**
   * URL of image to use while generating the 3D model.
   */
  left_image_url: string | Blob | File;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for the model. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Octree resolution for the model. Default value: `256`
   */
  octree_resolution?: number;
  /**
   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
   */
  textured_mesh?: boolean;
};
export type Hunyuan3dV2MultiViewOutput = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type Hunyuan3dV2MultiViewTurboInput = {
  /**
   * URL of image to use while generating the 3D model.
   */
  front_image_url: string | Blob | File;
  /**
   * URL of image to use while generating the 3D model.
   */
  back_image_url: string | Blob | File;
  /**
   * URL of image to use while generating the 3D model.
   */
  left_image_url: string | Blob | File;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for the model. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Octree resolution for the model. Default value: `256`
   */
  octree_resolution?: number;
  /**
   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
   */
  textured_mesh?: boolean;
};
export type Hunyuan3dV2MultiViewTurboOutput = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type Hunyuan3dV2Output = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type Hunyuan3dV2TurboInput = {
  /**
   * URL of image to use while generating the 3D model.
   */
  input_image_url: string | Blob | File;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * Number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for the model. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Octree resolution for the model. Default value: `256`
   */
  octree_resolution?: number;
  /**
   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.
   */
  textured_mesh?: boolean;
};
export type Hunyuan3dV2TurboOutput = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type Hunyuan3dV31PartInput = {
  /**
   * URL of FBX file to split into parts. ONLY FBX format supported. Max size: 100MB, face count ≤30,000. Recommended: AIGC-generated models.
   */
  input_file_url: string | Blob | File;
};
export type Hunyuan3dV31PartOutput = {
  /**
   * List of generated part files in FBX format
   */
  result_files: Array<File>;
};
export type Hunyuan3dV31ProImageTo3dInput = {
  /**
   * Front view image URL. Resolution: 128-5000px, max 8MB, formats: JPG/PNG/WEBP. Tips: simple background, single object, object >50% of frame.
   */
  input_image_url: string | Blob | File;
  /**
   * Optional back/rear view image URL (JPG/PNG recommended).
   */
  back_image_url?: string | Blob | File;
  /**
   * Optional left side view image URL (JPG/PNG recommended).
   */
  left_image_url?: string | Blob | File;
  /**
   * Optional right side view image URL (JPG/PNG recommended).
   */
  right_image_url?: string | Blob | File;
  /**
   * Optional top view image URL (v3.1 exclusive, JPG/PNG recommended).
   */
  top_image_url?: string | Blob | File;
  /**
   * Optional bottom view image URL (v3.1 exclusive, JPG/PNG recommended).
   */
  bottom_image_url?: string | Blob | File;
  /**
   * Optional left-front 45 degree angle view image URL (v3.1 exclusive, JPG/PNG recommended).
   */
  left_front_image_url?: string | Blob | File;
  /**
   * Optional right-front 45 degree angle view image URL (v3.1 exclusive, JPG/PNG recommended).
   */
  right_front_image_url?: string | Blob | File;
  /**
   * Generation task type. Normal: textured model. Geometry: geometry-only white model (no textures). LowPoly/Sketch are not available in v3.1. Default value: `"Normal"`
   */
  generate_type?: "Normal" | "Geometry";
  /**
   * Enable PBR material generation (metallic, roughness, normal textures). Ignored when generate_type is Geometry.
   */
  enable_pbr?: boolean;
  /**
   * Target polygon face count. Range: 40,000-1,500,000. Default: 500,000. Default value: `500000`
   */
  face_count?: number;
};
export type Hunyuan3dV31ProImageTo3dOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * The seed used for generation
   */
  seed?: number;
};
export type Hunyuan3dV31ProTextTo3dInput = {
  /**
   * Text description of the 3D content to generate. Max 1024 UTF-8 characters.
   */
  prompt: string;
  /**
   * Generation task type. Normal: textured model. Geometry: geometry-only white model (no textures). LowPoly/Sketch are not available in v3.1. Default value: `"Normal"`
   */
  generate_type?: "Normal" | "Geometry";
  /**
   * Enable PBR material generation (metallic, roughness, normal textures). Ignored when generate_type is Geometry.
   */
  enable_pbr?: boolean;
  /**
   * Target polygon face count. Range: 40,000-1,500,000. Default: 500,000. Default value: `500000`
   */
  face_count?: number;
};
export type Hunyuan3dV31ProTextTo3dOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * The seed used for generation
   */
  seed?: number;
};
export type Hunyuan3dV31RapidImageTo3dInput = {
  /**
   * Front view image URL. Resolution: 128-5000px, max 8MB (recommended ≤6MB for base64 encoding), formats: JPG/PNG/WEBP. Tips: simple background, single object, object >50% of frame.
   */
  input_image_url: string | Blob | File;
  /**
   * Enable PBR material generation (metallic, roughness, normal textures). Does not take effect when enable_geometry is True.
   */
  enable_pbr?: boolean;
  /**
   * Generate geometry-only white model without textures. When enabled, enable_pbr is ignored and OBJ is not supported (default output is GLB).
   */
  enable_geometry?: boolean;
};
export type Hunyuan3dV31RapidImageTo3dOutput = {
  /**
   * Generated 3D model file. Contains GLB if available, otherwise OBJ.
   */
  model_glb?: File;
  /**
   * MTL material file for the OBJ model.
   */
  material_mtl?: File;
  /**
   * Texture image for the 3D model.
   */
  texture?: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats.
   */
  model_urls: ModelUrls;
};
export type Hunyuan3dV31RapidTextTo3dInput = {
  /**
   * Text description of the 3D content to generate. Max 200 UTF-8 characters.
   */
  prompt: string;
  /**
   * Enable PBR material generation (metallic, roughness, normal textures). Does not take effect when enable_geometry is True.
   */
  enable_pbr?: boolean;
  /**
   * Generate geometry-only white model without textures. When enabled, enable_pbr is ignored and OBJ is not supported (default output is GLB).
   */
  enable_geometry?: boolean;
};
export type Hunyuan3dV31RapidTextTo3dOutput = {
  /**
   * Generated 3D model in OBJ format.
   */
  model_obj?: File;
  /**
   * MTL material file for the OBJ model.
   */
  material_mtl?: File;
  /**
   * Texture image for the 3D model.
   */
  texture?: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats.
   */
  model_urls: ModelUrls;
};
export type Hunyuan3dV31SmartTopologyInput = {
  /**
   * URL of GLB or OBJ file to optimize topology. Max size: 200MB. Default value: `"https://v3b.fal.media/files/b/0a8c09c0/VYDiCTcDGK55qY2-idGbX_model.glb"`
   */
  input_file_url?: string | Blob | File;
  /**
   * Input 3D file format. Default value: `"glb"`
   */
  input_file_type?: "glb" | "obj";
  /**
   * Output polygon type. triangle: triangular faces only. quadrilateral: mixed quad and triangle faces. Default value: `"triangle"`
   */
  polygon_type?: "triangle" | "quadrilateral";
  /**
   * Target polygon density. high: more detail/polygons, medium: balanced, low: fewer polygons. Default value: `"medium"`
   */
  face_level?: "high" | "medium" | "low";
};
export type Hunyuan3dV31SmartTopologyOutput = {
  /**
   * Processed 3D model with optimized topology (primary file).
   */
  model_glb: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
};
export type Hunyuan3dV3ImageTo3dInput = {
  /**
   * URL of image to use while generating the 3D model.
   */
  input_image_url: string | Blob | File;
  /**
   * Optional back view image URL for better 3D reconstruction.
   */
  back_image_url?: string | Blob | File;
  /**
   * Optional left view image URL for better 3D reconstruction.
   */
  left_image_url?: string | Blob | File;
  /**
   * Optional right view image URL for better 3D reconstruction.
   */
  right_image_url?: string | Blob | File;
  /**
   * Whether to enable PBR material generation. Does not take effect when generate_type is Geometry.
   */
  enable_pbr?: boolean;
  /**
   * Target face count. Range: 40000-1500000 Default value: `500000`
   */
  face_count?: number;
  /**
   * Generation type. Normal: textured model. LowPoly: polygon reduction. Geometry: white model without texture. Default value: `"Normal"`
   */
  generate_type?: "Normal" | "LowPoly" | "Geometry";
  /**
   * Polygon type. Only takes effect when GenerateType is LowPoly. Default value: `"triangle"`
   */
  polygon_type?: "triangle" | "quadrilateral";
};
export type Hunyuan3dV3ImageTo3dOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * The seed used for generation
   */
  seed?: number;
};
export type Hunyuan3dV3SketchTo3dInput = {
  /**
   * URL of sketch or line art image to transform into a 3D model. Image resolution must be between 128x128 and 5000x5000 pixels.
   */
  input_image_url: string | Blob | File;
  /**
   * Text prompt describing the 3D content attributes such as color, category, and material.
   */
  prompt: string;
  /**
   * Whether to enable PBR material generation.
   */
  enable_pbr?: boolean;
  /**
   * Target face count. Range: 40000-1500000 Default value: `500000`
   */
  face_count?: number;
};
export type Hunyuan3dV3SketchTo3dOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * The seed used for generation
   */
  seed?: number;
};
export type Hunyuan3dV3TextTo3dInput = {
  /**
   * Text description of the 3D content to generate. Supports up to 1024 UTF-8 characters.
   */
  prompt: string;
  /**
   * Whether to enable PBR material generation
   */
  enable_pbr?: boolean;
  /**
   * Target face count. Range: 40000-1500000 Default value: `500000`
   */
  face_count?: number;
  /**
   * Generation type. Normal: textured model. LowPoly: polygon reduction. Geometry: white model without texture. Default value: `"Normal"`
   */
  generate_type?: "Normal" | "LowPoly" | "Geometry";
  /**
   * Polygon type. Only takes effect when GenerateType is LowPoly. Default value: `"triangle"`
   */
  polygon_type?: "triangle" | "quadrilateral";
};
export type Hunyuan3dV3TextTo3dOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * The seed used for generation
   */
  seed?: number;
};
export type HunyuanAvatarInput = {
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * The URL of the reference image.
   */
  image_url: string | Blob | File;
  /**
   * Text prompt describing the scene. Default value: `"A cat is singing."`
   */
  text?: string;
  /**
   * Number of video frames to generate at 25 FPS. If greater than the input audio length, it will capped to the length of the input audio. Default value: `129`
   */
  num_frames?: number;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * If true, the video will be generated faster with no noticeable degradation in the visual quality. Default value: `true`
   */
  turbo_mode?: boolean;
  /**
   * Random seed for generation.
   */
  seed?: number;
};
export type HunyuanAvatarOutput = {
  /**
   * The generated video with the avatar animation.
   */
  video: File;
};
export type HunyuanCustomInput = {
  /**
   * Text prompt for video generation (max 500 characters).
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion, blurring, text, subtitles, static, picture, black border."`
   */
  negative_prompt?: string;
  /**
   * URL of the image input.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to run. Lower gets faster results, higher gets better results. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * The aspect ratio of the video to generate. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations. Default value: `"512p"`
   */
  resolution?: "512p" | "720p";
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * Classifier-Free Guidance scale for the generation. Default value: `7.5`
   */
  cfg_scale?: number;
  /**
   * The number of frames to generate. Default value: `129`
   */
  num_frames?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type HunyuanCustomOutput = {
  /**
   *
   */
  video: File;
  /**
   * The seed used for generating the video.
   */
  seed: number;
};
export type HunyuanImageToImageInput = {
  /**
   * The text prompt for image-to-image.
   */
  prompt: string;
  /**
   * The negative prompt to guide the image generation away from certain concepts. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The desired size of the generated image. By default, attempts to preserve the original size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Number of denoising steps. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Controls how much the model adheres to the prompt. Higher values mean stricter adherence. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducible results. If None, a random seed is used.
   */
  seed?: number;
  /**
   * Enable prompt enhancement for potentially better results. Default value: `true`
   */
  use_reprompt?: boolean;
  /**
   * Enable the refiner model for improved image quality.
   */
  use_refiner?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * URL of the Image for Image-to-Image
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for image-to-image. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.6`
   */
  strength?: number;
};
export type HunyuanImageToImageOutput = {
  /**
   * A list of the generated images.
   */
  images: Array<Image>;
  /**
   * The base seed used for the generation process.
   */
  seed: number;
};
export type HunyuanImageV21TextToImageInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to guide the image generation away from certain concepts. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The desired size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Number of denoising steps. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Controls how much the model adheres to the prompt. Higher values mean stricter adherence. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducible results. If None, a random seed is used.
   */
  seed?: number;
  /**
   * Enable prompt enhancement for potentially better results. Default value: `true`
   */
  use_reprompt?: boolean;
  /**
   * Enable the refiner model for improved image quality.
   */
  use_refiner?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type HunyuanImageV21TextToImageOutput = {
  /**
   * A list of the generated images.
   */
  images: Array<Image>;
  /**
   * The base seed used for the generation process.
   */
  seed: number;
};
export type HunyuanImageV3InstructEditInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The desired size of the generated image. If auto, image size will be determined by the model. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Controls how much the model adheres to the prompt. Higher values mean stricter adherence. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducible results. If None, a random seed is used.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URLs of the images to use as a reference for the generation. A maximum of 2 images are supported.
   */
  image_urls: Array<string>;
};
export type HunyuanImageV3InstructEditOutput = {
  /**
   * A list of the generated images.
   */
  images: Array<Image>;
  /**
   * The base seed used for the generation process.
   */
  seed: number;
};
export type HunyuanImageV3InstructTextToImageInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The desired size of the generated image. If auto, image size will be determined by the model. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Controls how much the model adheres to the prompt. Higher values mean stricter adherence. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducible results. If None, a random seed is used.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type HunyuanImageV3InstructTextToImageOutput = {
  /**
   * A list of the generated images.
   */
  images: Array<Image>;
  /**
   * The base seed used for the generation process.
   */
  seed: number;
};
export type HunyuanImageV3TextToImageInput = {
  /**
   * The text prompt for image-to-image.
   */
  prompt: string;
  /**
   * The negative prompt to guide the image generation away from certain concepts. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The desired size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Number of denoising steps. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Controls how much the model adheres to the prompt. Higher values mean stricter adherence. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducible results. If None, a random seed is used.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
};
export type HunyuanImageV3TextToImageOutput = {
  /**
   * A list of the generated images.
   */
  images: Array<Image>;
  /**
   * The base seed used for the generation process.
   */
  seed: number;
};
export type HunyuanMotionFastInput = {
  /**
   * Text prompt describing the motion to generate.
   */
  prompt: string;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * Motion duration in seconds (0.5-12.0). Default value: `5`
   */
  duration?: number;
  /**
   * Classifier-free guidance scale. Higher = more faithful to prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Output format: 'fbx' for animation files, 'dict' for raw JSON. Default value: `"fbx"`
   */
  output_format?: "fbx" | "dict";
};
export type HunyuanMotionFastOutput = {
  /**
   * Generated FBX animation file.
   */
  fbx_file?: File;
  /**
   * Generated motion data as JSON.
   */
  motion_json?: File;
  /**
   * Seed used for generation.
   */
  seed: number;
};
export type HunyuanMotionInput = {
  /**
   * Text prompt describing the motion to generate.
   */
  prompt: string;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * Motion duration in seconds (0.5-12.0). Default value: `5`
   */
  duration?: number;
  /**
   * Classifier-free guidance scale. Higher = more faithful to prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Output format: 'fbx' for animation files, 'dict' for raw JSON. Default value: `"fbx"`
   */
  output_format?: "fbx" | "dict";
};
export type HunyuanMotionOutput = {
  /**
   * Generated FBX animation file.
   */
  fbx_file?: File;
  /**
   * Generated motion data as JSON.
   */
  motion_json?: File;
  /**
   * Seed used for generation.
   */
  seed: number;
};
export type HunyuanPartInput = {
  /**
   * URL of the 3D model file (.glb or .obj) to process for segmentation.
   */
  model_file_url: string | Blob | File;
  /**
   * X coordinate of the point prompt for segmentation (normalized space -1 to 1).
   */
  point_prompt_x?: number;
  /**
   * Y coordinate of the point prompt for segmentation (normalized space -1 to 1).
   */
  point_prompt_y?: number;
  /**
   * Z coordinate of the point prompt for segmentation (normalized space -1 to 1).
   */
  point_prompt_z?: number;
  /**
   * Number of points to sample from the mesh. Default value: `100000`
   */
  point_num?: number;
  /**
   * Whether to use normal information for segmentation. Default value: `true`
   */
  use_normal?: boolean;
  /**
   * Standard deviation of noise to add to sampled points.
   */
  noise_std?: number;
  /**
   * The same seed and input will produce the same segmentation results.
   */
  seed?: number;
};
export type HunyuanPartOutput = {
  /**
   * Segmented 3D mesh with mask applied.
   */
  segmented_mesh: File;
  /**
   * Mesh showing segmentation mask 1.
   */
  mask_1_mesh: File;
  /**
   * Mesh showing segmentation mask 2.
   */
  mask_2_mesh: File;
  /**
   * Mesh showing segmentation mask 3.
   */
  mask_3_mesh: File;
  /**
   * Index of the best mask (1, 2, or 3) based on IoU score.
   */
  best_mask_index: number;
  /**
   * IoU scores for each of the three masks.
   */
  iou_scores: Array<number>;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type HunyuanPortraitInput = {
  /**
   * The URL of the driving video.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the source image.
   */
  image_url: string | Blob | File;
  /**
   * Random seed for generation. If None, a random seed will be used.
   */
  seed?: number;
  /**
   * Whether to use ArcFace for face recognition. Default value: `true`
   */
  use_arcface?: boolean;
};
export type HunyuanPortraitOutput = {
  /**
   * The generated video with the portrait animation.
   */
  video: File;
};
export type HunyuanTextToImageInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to guide the image generation away from certain concepts. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The desired size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Number of denoising steps. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Controls how much the model adheres to the prompt. Higher values mean stricter adherence. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducible results. If None, a random seed is used.
   */
  seed?: number;
  /**
   * Enable prompt enhancement for potentially better results. Default value: `true`
   */
  use_reprompt?: boolean;
  /**
   * Enable the refiner model for improved image quality.
   */
  use_refiner?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type HunyuanTextToImageOutput = {
  /**
   * A list of the generated images.
   */
  images: Array<Image>;
  /**
   * The base seed used for the generation process.
   */
  seed: number;
};
export type HunyuanTextToImageV3Output = {
  /**
   * A list of the generated images.
   */
  images: Array<Image>;
  /**
   * The base seed used for the generation process.
   */
  seed: number;
};
export type HunyuanVideoFoleyInput = {
  /**
   * The URL of the video to generate audio for.
   */
  video_url: string | Blob | File;
  /**
   * Text description of the desired audio (optional).
   */
  text_prompt: string;
  /**
   * Negative prompt to avoid certain audio characteristics. Default value: `"noisy, harsh"`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for audio generation. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for generation. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
};
export type HunyuanVideoFoleyOutput = {
  /**
   * List of generated video files with audio.
   */
  video: File;
};
export type HunyuanVideoImageToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * URL of the image input.
   */
  image_url: string | Blob | File;
  /**
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * The aspect ratio of the video to generate. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video to generate. Default value: `"720p"`
   */
  resolution?: "720p";
  /**
   * The number of frames to generate. Default value: `"129"`
   */
  num_frames?: "129";
  /**
   * Turning on I2V Stability reduces hallucination but also reduces motion.
   */
  i2v_stability?: boolean;
};
export type HunyuanVideoImageToVideoOutput = {
  /**
   *
   */
  video: File;
  /**
   * The seed used for generating the video.
   */
  seed: number;
};
export type HunyuanVideoImg2vidLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL to the image to generate the video from. The image must be 960x544 or it will get cropped and resized to that size.
   */
  image_url: string | Blob | File;
  /**
   * The seed to use for generating the video.
   */
  seed?: number;
};
export type HunyuanVideoImg2vidLoraOutput = {
  /**
   * The generated video
   */
  video: File;
  /**
   * The seed used for generating the video.
   */
  seed: number;
};
export type HunyuanVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean;
  /**
   * The aspect ratio of the video to generate. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video to generate. Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * The number of frames to generate. Default value: `"129"`
   */
  num_frames?: "129" | "85";
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
};
export type HunyuanVideoLoraOutput = {
  /**
   *
   */
  video: File;
  /**
   * The seed used for generating the video.
   */
  seed: number;
};
export type HunyuanVideoLoraTrainingInput = {
  /**
   * URL to zip archive with images. Try to use at least 4 images in general the more the better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
   */
  images_data_url: string | Blob | File;
  /**
   * Number of steps to train the LoRA on.
   */
  steps: number;
  /**
   * The trigger word to use. Default value: `""`
   */
  trigger_word?: string;
  /**
   * Learning rate to use for training. Default value: `0.0001`
   */
  learning_rate?: number;
  /**
   * Whether to generate captions for the images. Default value: `true`
   */
  do_caption?: boolean;
  /**
   * The format of the archive. If not specified, the format will be inferred from the URL.
   */
  data_archive_format?: string;
};
export type HunyuanVideoLoraTrainingOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the lora configuration file.
   */
  config_file: File;
};
export type HunyuanVideoLoraVideoToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean;
  /**
   * The aspect ratio of the video to generate. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video to generate. Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * The number of frames to generate. Default value: `"129"`
   */
  num_frames?: "129" | "85";
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * URL of the video
   */
  video_url: string | Blob | File;
  /**
   * Strength of video-to-video Default value: `0.75`
   */
  strength?: number;
};
export type HunyuanVideoLoraVideoToVideoOutput = {
  /**
   *
   */
  video: File;
  /**
   * The seed used for generating the video.
   */
  seed: number;
};
export type HunyuanVideoV15ImageToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The negative prompt to guide what not to generate. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducibility.
   */
  seed?: number;
  /**
   * The aspect ratio of the video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video. Default value: `"480p"`
   */
  resolution?: "480p";
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * Enable prompt expansion to enhance the input prompt. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of the reference image for image-to-video generation.
   */
  image_url: string | Blob | File;
};
export type HunyuanVideoV15ImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type HunyuanVideoV15TextToVideoInput = {
  /**
   * The prompt to generate the video.
   */
  prompt: string;
  /**
   * The negative prompt to guide what not to generate. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducibility.
   */
  seed?: number;
  /**
   * The aspect ratio of the video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video. Default value: `"480p"`
   */
  resolution?: "480p";
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * Enable prompt expansion to enhance the input prompt. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
};
export type HunyuanVideoV15TextToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type HunyuanVideoVideoToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The number of inference steps to run. Lower gets faster results, higher gets better results. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean;
  /**
   * The aspect ratio of the video to generate. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The resolution of the video to generate. Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * The number of frames to generate. Default value: `"129"`
   */
  num_frames?: "129" | "85";
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * URL of the video input.
   */
  video_url: string | Blob | File;
  /**
   * Strength for Video-to-Video Default value: `0.85`
   */
  strength?: number;
};
export type HunyuanVideoVideoToVideoOutput = {
  /**
   *
   */
  video: File;
  /**
   * The seed used for generating the video.
   */
  seed: number;
};
export type HunyuanWorldImageToWorldInput = {
  /**
   * The URL of the image to convert to a world.
   */
  image_url: string | Blob | File;
  /**
   * Labels for the first foreground object.
   */
  labels_fg1: string;
  /**
   * Labels for the second foreground object.
   */
  labels_fg2: string;
  /**
   * Classes to use for the world generation.
   */
  classes: string;
  /**
   * Whether to export DRC (Dynamic Resource Configuration).
   */
  export_drc?: boolean;
};
export type HunyuanWorldImageToWorldOutput = {
  /**
   * The generated world.
   */
  world_file: File;
};
export type HunyuanWorldInput = {
  /**
   * The URL of the image to convert to a panorama.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to use for the panorama generation.
   */
  prompt: string;
};
export type HunyuanWorldOutput = {
  /**
   * The generated panorama image.
   */
  image: Image;
};
export type Hyper3dRodinV2Input = {
  /**
   * A textual prompt to guide model generation. Optional for Image-to-3D mode - if empty, AI will generate a prompt based on your images. Default value: `""`
   */
  prompt?: string;
  /**
   * URL of images to use while generating the 3D model. Required for Image-to-3D mode. Up to 5 images allowed.
   */
  input_image_urls?: Array<string>;
  /**
   * When enabled, preserves the transparency channel from input images during 3D generation.
   */
  use_original_alpha?: boolean;
  /**
   * Seed value for randomization, ranging from 0 to 65535. Optional.
   */
  seed?: number;
  /**
   * Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb. Default value: `"glb"`
   */
  geometry_file_format?: "glb" | "usdz" | "fbx" | "obj" | "stl";
  /**
   * Material type. PBR: Physically-based materials with realistic lighting. Shaded: Simple materials with baked lighting. All: Both types included. Default value: `"All"`
   */
  material?: "PBR" | "Shaded" | "All";
  /**
   * Combined quality and mesh type selection. Quad = smooth surfaces, Triangle = detailed geometry. These corresponds to `mesh_mode` (if the option contains 'Triangle', mesh_mode is 'Raw', otherwise 'Quad') and `quality_override` (the numeric part of the option) parameters in Hyper3D API. Default value: `"500K Triangle"`
   */
  quality_mesh_option?:
    | "4K Quad"
    | "8K Quad"
    | "18K Quad"
    | "50K Quad"
    | "2K Triangle"
    | "20K Triangle"
    | "150K Triangle"
    | "500K Triangle";
  /**
   * Generate characters in T-pose or A-pose format, making them easier to rig and animate in 3D software.
   */
  TAPose?: boolean;
  /**
   * An array that specifies the bounding box dimensions [width, height, length].
   */
  bbox_condition?: Array<number>;
  /**
   * The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost **triple the billable units**.
   */
  addons?: "HighPack";
  /**
   * Generate a preview render image of the 3D model along with the model files.
   */
  preview_render?: boolean;
};
export type Hyper3dRodinV2Output = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
  /**
   * Generated textures for the 3D object.
   */
  textures: Array<Image>;
};
export type I2IO3ImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type I2IV3ImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type I2VDirectorOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type I2VLiveOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type I2VOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type IdentifyFaceInput = {
  /**
   * The URL of the video. Used to specify the video and determine whether it can be used for lip-sync services. Supported video formats: .mp4/.mov, file size ≤100MB, duration 2s-60s, resolution 720p or 1080p, with both width and height between 512px-2160px.
   */
  video_url: string | Blob | File;
};
export type IdentifyFaceOutput = {
  /**
   * List of detected faces in the video with their time ranges.
   */
  face_data: Array<FaceData>;
  /**
   * The session id of the lip-sync task
   */
  session_id: string;
};
export type IdeogramCharacterEditInput = {
  /**
   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
   */
  image_urls?: Array<string>;
  /**
   * The rendering speed to use. Default value: `"BALANCED"`
   */
  rendering_speed?: "TURBO" | "BALANCED" | "QUALITY";
  /**
   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)
   */
  color_palette?: ColorPalette;
  /**
   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
   */
  style_codes?: Array<string>;
  /**
   * The style type to generate with. Cannot be used with style_codes. Default value: `"AUTO"`
   */
  style?: "AUTO" | "REALISTIC" | "FICTION";
  /**
   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The prompt to fill the masked part of the image.
   */
  prompt: string;
  /**
   * The image URL to generate an image from. MUST have the exact same dimensions (width and height) as the mask image.
   */
  image_url: string | Blob | File;
  /**
   * The mask URL to inpaint the image. MUST have the exact same dimensions (width and height) as the input image.
   */
  mask_url: string | Blob | File;
  /**
   * A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format
   */
  reference_image_urls: Array<string>;
  /**
   * A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format
   */
  reference_mask_urls?: Array<string>;
};
export type IdeogramCharacterEditOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramCharacterInput = {
  /**
   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
   */
  image_urls?: Array<string>;
  /**
   * The rendering speed to use. Default value: `"BALANCED"`
   */
  rendering_speed?: "TURBO" | "BALANCED" | "QUALITY";
  /**
   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)
   */
  color_palette?: ColorPalette;
  /**
   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
   */
  style_codes?: Array<string>;
  /**
   * The style type to generate with. Cannot be used with style_codes. Default value: `"AUTO"`
   */
  style?: "AUTO" | "REALISTIC" | "FICTION";
  /**
   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The prompt to fill the masked part of the image.
   */
  prompt: string;
  /**
   * The resolution of the generated image Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format
   */
  reference_image_urls: Array<string>;
  /**
   * A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format
   */
  reference_mask_urls?: Array<string>;
};
export type IdeogramCharacterOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramCharacterRemixInput = {
  /**
   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
   */
  image_urls?: Array<string>;
  /**
   * The rendering speed to use. Default value: `"BALANCED"`
   */
  rendering_speed?: "TURBO" | "BALANCED" | "QUALITY";
  /**
   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)
   */
  color_palette?: ColorPalette;
  /**
   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
   */
  style_codes?: Array<string>;
  /**
   * The style type to generate with. Cannot be used with style_codes. Default value: `"AUTO"`
   */
  style?: "AUTO" | "REALISTIC" | "FICTION";
  /**
   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The prompt to remix the image with
   */
  prompt: string;
  /**
   * The image URL to remix
   */
  image_url: string | Blob | File;
  /**
   * Strength of the input image in the remix Default value: `0.8`
   */
  strength?: number;
  /**
   * The resolution of the generated image Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format
   */
  reference_image_urls: Array<string>;
  /**
   * A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format
   */
  reference_mask_urls?: Array<string>;
};
export type IdeogramCharacterRemixOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramUpscaleInput = {
  /**
   * The image URL to upscale
   */
  image_url: string | Blob | File;
  /**
   * The prompt to upscale the image with Default value: `""`
   */
  prompt?: string;
  /**
   * The resemblance of the upscaled image to the original image Default value: `50`
   */
  resemblance?: number;
  /**
   * The detail of the upscaled image Default value: `50`
   */
  detail?: number;
  /**
   * Whether to expand the prompt with MagicPrompt functionality.
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type IdeogramUpscaleOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV2aInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "10:16"
    | "16:10"
    | "9:16"
    | "16:9"
    | "4:3"
    | "3:4"
    | "1:1"
    | "1:3"
    | "3:1"
    | "3:2"
    | "2:3";
  /**
   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The style of the generated image Default value: `"auto"`
   */
  style?: "auto" | "general" | "realistic" | "design" | "render_3D" | "anime";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type IdeogramV2aOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV2aRemixInput = {
  /**
   * The prompt to remix the image with
   */
  prompt: string;
  /**
   * The image URL to remix
   */
  image_url: string | Blob | File;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "10:16"
    | "16:10"
    | "9:16"
    | "16:9"
    | "4:3"
    | "3:4"
    | "1:1"
    | "1:3"
    | "3:1"
    | "3:2"
    | "2:3";
  /**
   * Strength of the input image in the remix Default value: `0.8`
   */
  strength?: number;
  /**
   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The style of the generated image Default value: `"auto"`
   */
  style?: "auto" | "general" | "realistic" | "design" | "render_3D" | "anime";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type IdeogramV2aRemixOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV2aTurboInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "10:16"
    | "16:10"
    | "9:16"
    | "16:9"
    | "4:3"
    | "3:4"
    | "1:1"
    | "1:3"
    | "3:1"
    | "3:2"
    | "2:3";
  /**
   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The style of the generated image Default value: `"auto"`
   */
  style?: "auto" | "general" | "realistic" | "design" | "render_3D" | "anime";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type IdeogramV2aTurboOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV2aTurboRemixInput = {
  /**
   * The prompt to remix the image with
   */
  prompt: string;
  /**
   * The image URL to remix
   */
  image_url: string | Blob | File;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "10:16"
    | "16:10"
    | "9:16"
    | "16:9"
    | "4:3"
    | "3:4"
    | "1:1"
    | "1:3"
    | "3:1"
    | "3:2"
    | "2:3";
  /**
   * Strength of the input image in the remix Default value: `0.8`
   */
  strength?: number;
  /**
   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The style of the generated image Default value: `"auto"`
   */
  style?: "auto" | "general" | "realistic" | "design" | "render_3D" | "anime";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type IdeogramV2aTurboRemixOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV2Input = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "10:16"
    | "16:10"
    | "9:16"
    | "16:9"
    | "4:3"
    | "3:4"
    | "1:1"
    | "1:3"
    | "3:1"
    | "3:2"
    | "2:3";
  /**
   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The style of the generated image Default value: `"auto"`
   */
  style?: "auto" | "general" | "realistic" | "design" | "render_3D" | "anime";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * A negative prompt to avoid in the generated image Default value: `""`
   */
  negative_prompt?: string;
};
export type IdeogramV2Output = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV3EditInput = {
  /**
   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
   */
  image_urls?: Array<string>;
  /**
   * The rendering speed to use. Default value: `"BALANCED"`
   */
  rendering_speed?: "TURBO" | "BALANCED" | "QUALITY";
  /**
   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)
   */
  color_palette?: ColorPalette;
  /**
   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
   */
  style_codes?: Array<string>;
  /**
   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Style preset for generation. The chosen style preset will guide the generation.
   */
  style_preset?:
    | "80S_ILLUSTRATION"
    | "90S_NOSTALGIA"
    | "ABSTRACT_ORGANIC"
    | "ANALOG_NOSTALGIA"
    | "ART_BRUT"
    | "ART_DECO"
    | "ART_POSTER"
    | "AURA"
    | "AVANT_GARDE"
    | "BAUHAUS"
    | "BLUEPRINT"
    | "BLURRY_MOTION"
    | "BRIGHT_ART"
    | "C4D_CARTOON"
    | "CHILDRENS_BOOK"
    | "COLLAGE"
    | "COLORING_BOOK_I"
    | "COLORING_BOOK_II"
    | "CUBISM"
    | "DARK_AURA"
    | "DOODLE"
    | "DOUBLE_EXPOSURE"
    | "DRAMATIC_CINEMA"
    | "EDITORIAL"
    | "EMOTIONAL_MINIMAL"
    | "ETHEREAL_PARTY"
    | "EXPIRED_FILM"
    | "FLAT_ART"
    | "FLAT_VECTOR"
    | "FOREST_REVERIE"
    | "GEO_MINIMALIST"
    | "GLASS_PRISM"
    | "GOLDEN_HOUR"
    | "GRAFFITI_I"
    | "GRAFFITI_II"
    | "HALFTONE_PRINT"
    | "HIGH_CONTRAST"
    | "HIPPIE_ERA"
    | "ICONIC"
    | "JAPANDI_FUSION"
    | "JAZZY"
    | "LONG_EXPOSURE"
    | "MAGAZINE_EDITORIAL"
    | "MINIMAL_ILLUSTRATION"
    | "MIXED_MEDIA"
    | "MONOCHROME"
    | "NIGHTLIFE"
    | "OIL_PAINTING"
    | "OLD_CARTOONS"
    | "PAINT_GESTURE"
    | "POP_ART"
    | "RETRO_ETCHING"
    | "RIVIERA_POP"
    | "SPOTLIGHT_80S"
    | "STYLIZED_RED"
    | "SURREAL_COLLAGE"
    | "TRAVEL_POSTER"
    | "VINTAGE_GEO"
    | "VINTAGE_POSTER"
    | "WATERCOLOR"
    | "WEIRD"
    | "WOODBLOCK_PRINT";
  /**
   * The prompt to fill the masked part of the image.
   */
  prompt: string;
  /**
   * The image URL to generate an image from. MUST have the exact same dimensions (width and height) as the mask image.
   */
  image_url: string | Blob | File;
  /**
   * The mask URL to inpaint the image. MUST have the exact same dimensions (width and height) as the input image.
   */
  mask_url: string | Blob | File;
};
export type IdeogramV3EditOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV3Input = {
  /**
   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
   */
  image_urls?: Array<string>;
  /**
   * The rendering speed to use. Default value: `"BALANCED"`
   */
  rendering_speed?: "TURBO" | "BALANCED" | "QUALITY";
  /**
   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)
   */
  color_palette?: ColorPalette;
  /**
   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
   */
  style_codes?: Array<string>;
  /**
   * The style type to generate with. Cannot be used with style_codes.
   */
  style?: "AUTO" | "GENERAL" | "REALISTIC" | "DESIGN";
  /**
   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Style preset for generation. The chosen style preset will guide the generation.
   */
  style_preset?:
    | "80S_ILLUSTRATION"
    | "90S_NOSTALGIA"
    | "ABSTRACT_ORGANIC"
    | "ANALOG_NOSTALGIA"
    | "ART_BRUT"
    | "ART_DECO"
    | "ART_POSTER"
    | "AURA"
    | "AVANT_GARDE"
    | "BAUHAUS"
    | "BLUEPRINT"
    | "BLURRY_MOTION"
    | "BRIGHT_ART"
    | "C4D_CARTOON"
    | "CHILDRENS_BOOK"
    | "COLLAGE"
    | "COLORING_BOOK_I"
    | "COLORING_BOOK_II"
    | "CUBISM"
    | "DARK_AURA"
    | "DOODLE"
    | "DOUBLE_EXPOSURE"
    | "DRAMATIC_CINEMA"
    | "EDITORIAL"
    | "EMOTIONAL_MINIMAL"
    | "ETHEREAL_PARTY"
    | "EXPIRED_FILM"
    | "FLAT_ART"
    | "FLAT_VECTOR"
    | "FOREST_REVERIE"
    | "GEO_MINIMALIST"
    | "GLASS_PRISM"
    | "GOLDEN_HOUR"
    | "GRAFFITI_I"
    | "GRAFFITI_II"
    | "HALFTONE_PRINT"
    | "HIGH_CONTRAST"
    | "HIPPIE_ERA"
    | "ICONIC"
    | "JAPANDI_FUSION"
    | "JAZZY"
    | "LONG_EXPOSURE"
    | "MAGAZINE_EDITORIAL"
    | "MINIMAL_ILLUSTRATION"
    | "MIXED_MEDIA"
    | "MONOCHROME"
    | "NIGHTLIFE"
    | "OIL_PAINTING"
    | "OLD_CARTOONS"
    | "PAINT_GESTURE"
    | "POP_ART"
    | "RETRO_ETCHING"
    | "RIVIERA_POP"
    | "SPOTLIGHT_80S"
    | "STYLIZED_RED"
    | "SURREAL_COLLAGE"
    | "TRAVEL_POSTER"
    | "VINTAGE_GEO"
    | "VINTAGE_POSTER"
    | "WATERCOLOR"
    | "WEIRD"
    | "WOODBLOCK_PRINT";
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated image Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt. Default value: `""`
   */
  negative_prompt?: string;
};
export type IdeogramV3Output = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV3ReframeInput = {
  /**
   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
   */
  image_urls?: Array<string>;
  /**
   * The rendering speed to use. Default value: `"BALANCED"`
   */
  rendering_speed?: "TURBO" | "BALANCED" | "QUALITY";
  /**
   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)
   */
  color_palette?: ColorPalette;
  /**
   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
   */
  style_codes?: Array<string>;
  /**
   * The style type to generate with. Cannot be used with style_codes.
   */
  style?: "AUTO" | "GENERAL" | "REALISTIC" | "DESIGN";
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Style preset for generation. The chosen style preset will guide the generation.
   */
  style_preset?:
    | "80S_ILLUSTRATION"
    | "90S_NOSTALGIA"
    | "ABSTRACT_ORGANIC"
    | "ANALOG_NOSTALGIA"
    | "ART_BRUT"
    | "ART_DECO"
    | "ART_POSTER"
    | "AURA"
    | "AVANT_GARDE"
    | "BAUHAUS"
    | "BLUEPRINT"
    | "BLURRY_MOTION"
    | "BRIGHT_ART"
    | "C4D_CARTOON"
    | "CHILDRENS_BOOK"
    | "COLLAGE"
    | "COLORING_BOOK_I"
    | "COLORING_BOOK_II"
    | "CUBISM"
    | "DARK_AURA"
    | "DOODLE"
    | "DOUBLE_EXPOSURE"
    | "DRAMATIC_CINEMA"
    | "EDITORIAL"
    | "EMOTIONAL_MINIMAL"
    | "ETHEREAL_PARTY"
    | "EXPIRED_FILM"
    | "FLAT_ART"
    | "FLAT_VECTOR"
    | "FOREST_REVERIE"
    | "GEO_MINIMALIST"
    | "GLASS_PRISM"
    | "GOLDEN_HOUR"
    | "GRAFFITI_I"
    | "GRAFFITI_II"
    | "HALFTONE_PRINT"
    | "HIGH_CONTRAST"
    | "HIPPIE_ERA"
    | "ICONIC"
    | "JAPANDI_FUSION"
    | "JAZZY"
    | "LONG_EXPOSURE"
    | "MAGAZINE_EDITORIAL"
    | "MINIMAL_ILLUSTRATION"
    | "MIXED_MEDIA"
    | "MONOCHROME"
    | "NIGHTLIFE"
    | "OIL_PAINTING"
    | "OLD_CARTOONS"
    | "PAINT_GESTURE"
    | "POP_ART"
    | "RETRO_ETCHING"
    | "RIVIERA_POP"
    | "SPOTLIGHT_80S"
    | "STYLIZED_RED"
    | "SURREAL_COLLAGE"
    | "TRAVEL_POSTER"
    | "VINTAGE_GEO"
    | "VINTAGE_POSTER"
    | "WATERCOLOR"
    | "WEIRD"
    | "WOODBLOCK_PRINT";
  /**
   * The image URL to reframe
   */
  image_url: string | Blob | File;
  /**
   * The resolution for the reframed output image
   */
  image_size:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
};
export type IdeogramV3ReframeOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV3RemixInput = {
  /**
   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
   */
  image_urls?: Array<string>;
  /**
   * The rendering speed to use. Default value: `"BALANCED"`
   */
  rendering_speed?: "TURBO" | "BALANCED" | "QUALITY";
  /**
   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)
   */
  color_palette?: ColorPalette;
  /**
   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
   */
  style_codes?: Array<string>;
  /**
   * The style type to generate with. Cannot be used with style_codes.
   */
  style?: "AUTO" | "GENERAL" | "REALISTIC" | "DESIGN";
  /**
   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The prompt to remix the image with
   */
  prompt: string;
  /**
   * The image URL to remix
   */
  image_url: string | Blob | File;
  /**
   * Strength of the input image in the remix Default value: `0.8`
   */
  strength?: number;
  /**
   * The resolution of the generated image Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt. Default value: `""`
   */
  negative_prompt?: string;
};
export type IdeogramV3RemixOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type IdeogramV3ReplaceBackgroundInput = {
  /**
   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format
   */
  image_urls?: Array<string>;
  /**
   * The rendering speed to use. Default value: `"BALANCED"`
   */
  rendering_speed?: "TURBO" | "BALANCED" | "QUALITY";
  /**
   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)
   */
  color_palette?: ColorPalette;
  /**
   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style
   */
  style_codes?: Array<string>;
  /**
   * The style type to generate with. Cannot be used with style_codes.
   */
  style?: "AUTO" | "GENERAL" | "REALISTIC" | "DESIGN";
  /**
   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Style preset for generation. The chosen style preset will guide the generation.
   */
  style_preset?:
    | "80S_ILLUSTRATION"
    | "90S_NOSTALGIA"
    | "ABSTRACT_ORGANIC"
    | "ANALOG_NOSTALGIA"
    | "ART_BRUT"
    | "ART_DECO"
    | "ART_POSTER"
    | "AURA"
    | "AVANT_GARDE"
    | "BAUHAUS"
    | "BLUEPRINT"
    | "BLURRY_MOTION"
    | "BRIGHT_ART"
    | "C4D_CARTOON"
    | "CHILDRENS_BOOK"
    | "COLLAGE"
    | "COLORING_BOOK_I"
    | "COLORING_BOOK_II"
    | "CUBISM"
    | "DARK_AURA"
    | "DOODLE"
    | "DOUBLE_EXPOSURE"
    | "DRAMATIC_CINEMA"
    | "EDITORIAL"
    | "EMOTIONAL_MINIMAL"
    | "ETHEREAL_PARTY"
    | "EXPIRED_FILM"
    | "FLAT_ART"
    | "FLAT_VECTOR"
    | "FOREST_REVERIE"
    | "GEO_MINIMALIST"
    | "GLASS_PRISM"
    | "GOLDEN_HOUR"
    | "GRAFFITI_I"
    | "GRAFFITI_II"
    | "HALFTONE_PRINT"
    | "HIGH_CONTRAST"
    | "HIPPIE_ERA"
    | "ICONIC"
    | "JAPANDI_FUSION"
    | "JAZZY"
    | "LONG_EXPOSURE"
    | "MAGAZINE_EDITORIAL"
    | "MINIMAL_ILLUSTRATION"
    | "MIXED_MEDIA"
    | "MONOCHROME"
    | "NIGHTLIFE"
    | "OIL_PAINTING"
    | "OLD_CARTOONS"
    | "PAINT_GESTURE"
    | "POP_ART"
    | "RETRO_ETCHING"
    | "RIVIERA_POP"
    | "SPOTLIGHT_80S"
    | "STYLIZED_RED"
    | "SURREAL_COLLAGE"
    | "TRAVEL_POSTER"
    | "VINTAGE_GEO"
    | "VINTAGE_POSTER"
    | "WATERCOLOR"
    | "WEIRD"
    | "WOODBLOCK_PRINT";
  /**
   * Cyber punk city with neon lights and skyscrappers
   */
  prompt: string;
  /**
   * The image URL whose background needs to be replaced
   */
  image_url: string | Blob | File;
};
export type IdeogramV3ReplaceBackgroundOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type image2pixelInput = {
  /**
   * The image URL to process into improved pixel art
   */
  image_url: string | Blob | File;
  /**
   * Maximum number of colors in the output palette. Set None to disable limit. Default value: `32`
   */
  max_colors?: number;
  /**
   * Enable automatic detection of optimal number of colors.
   */
  auto_color_detect?: boolean;
  /**
   * Optional fixed color palette as hex strings (e.g., ['#000000', '#ffffff']).
   */
  fixed_palette?: Array<string>;
  /**
   * Scale detection method to use. Default value: `"auto"`
   */
  detect_method?: "auto" | "runs" | "edge";
  /**
   * Force a specific pixel scale. If None, auto-detect.
   */
  scale?: number;
  /**
   * Downscaling method to produce the pixel-art output. Default value: `"dominant"`
   */
  downscale_method?:
    | "dominant"
    | "median"
    | "mode"
    | "mean"
    | "content-adaptive";
  /**
   * Trim borders of the image.
   */
  trim_borders?: boolean;
  /**
   * Remove background of the image. This will check for contiguous color regions from the edges after correction and make them transparent.
   */
  transparent_background?: boolean;
  /**
   * Apply morphological operations to remove noise.
   */
  cleanup_morph?: boolean;
  /**
   * Remove isolated diagonal pixels (jaggy edge cleanup).
   */
  cleanup_jaggy?: boolean;
  /**
   * Align output to the pixel grid. Default value: `true`
   */
  snap_grid?: boolean;
  /**
   * Alpha binarization threshold (0-255). Default value: `128`
   */
  alpha_threshold?: number;
  /**
   * Dominant color threshold (0.0-1.0). Default value: `0.05`
   */
  dominant_color_threshold?: number;
  /**
   * Background tolerance (0-255).
   */
  background_tolerance?: number;
  /**
   * Controls where to flood-fill from when removing the background. Default value: `"corners"`
   */
  background_mode?: "edges" | "corners" | "midpoints";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type image2pixelOutput = {
  /**
   * The detected pixel scale of the input.
   */
  pixel_scale: number;
  /**
   * The palette of the processed media.
   */
  palette: Array<string>;
  /**
   * The number of colors in the processed media.
   */
  num_colors: number;
  /**
   * The processed pixel-art image (PNG) and the scaled image (PNG).
   */
  images: Array<ImageFile>;
};
export type image2svgInput = {
  /**
   * The image to convert to SVG
   */
  image_url: string | Blob | File;
  /**
   * Choose between color or binary (black and white) output Default value: `"color"`
   */
  colormode?: "color" | "binary";
  /**
   * Hierarchical mode: stacked or cutout Default value: `"stacked"`
   */
  hierarchical?: "stacked" | "cutout";
  /**
   * Mode: spline (curved) or polygon (straight lines) Default value: `"spline"`
   */
  mode?: "spline" | "polygon";
  /**
   * Filter out small speckles and noise Default value: `4`
   */
  filter_speckle?: number;
  /**
   * Color quantization level Default value: `6`
   */
  color_precision?: number;
  /**
   * Layer difference threshold for hierarchical mode Default value: `16`
   */
  layer_difference?: number;
  /**
   * Corner detection threshold in degrees Default value: `60`
   */
  corner_threshold?: number;
  /**
   * Length threshold for curves/lines Default value: `4`
   */
  length_threshold?: number;
  /**
   * Maximum number of iterations for optimization Default value: `10`
   */
  max_iterations?: number;
  /**
   * Splice threshold for joining paths Default value: `45`
   */
  splice_threshold?: number;
  /**
   * Decimal precision for path coordinates Default value: `3`
   */
  path_precision?: number;
};
export type image2svgOutput = {
  /**
   * The converted SVG file
   */
  images: Array<File>;
};
export type ImageAppsV2AgeModifyInput = {
  /**
   * Portrait image URL for age modification
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `30`
   */
  target_age?: number;
  /**
   *  Default value: `true`
   */
  preserve_identity?: boolean;
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2AgeModifyOutput = {
  /**
   * Portrait with modified age
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2CityTeleportInput = {
  /**
   * Person photo URL
   */
  person_image_url: string | Blob | File;
  /**
   * Optional city background image URL. When provided, the person will be blended into this custom scene.
   */
  city_image_url?: string | Blob | File;
  /**
   * City name (used when city_image_url is not provided)
   */
  city_name: string;
  /**
   * Type of photo shot Default value: `"medium_shot"`
   */
  photo_shot?:
    | "extreme_close_up"
    | "close_up"
    | "medium_close_up"
    | "medium_shot"
    | "medium_long_shot"
    | "long_shot"
    | "extreme_long_shot"
    | "full_body";
  /**
   * Camera angle for the shot Default value: `"eye_level"`
   */
  camera_angle?:
    | "eye_level"
    | "low_angle"
    | "high_angle"
    | "dutch_angle"
    | "birds_eye_view"
    | "worms_eye_view"
    | "overhead"
    | "side_angle";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2CityTeleportOutput = {
  /**
   * Person teleported to city location
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2ExpressionChangeInput = {
  /**
   * Portrait image URL for expression change
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"smile"`
   */
  target_expression?:
    | "smile"
    | "surprise"
    | "glare"
    | "panic"
    | "shyness"
    | "laugh"
    | "cry"
    | "angry"
    | "sad"
    | "happy"
    | "excited"
    | "shocked"
    | "confused"
    | "focused"
    | "dreamy"
    | "serious"
    | "playful"
    | "mysterious"
    | "confident"
    | "thoughtful";
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2ExpressionChangeOutput = {
  /**
   * Portrait with changed expression
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2HairChangeInput = {
  /**
   * Portrait image URL for hair change
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"long_hair"`
   */
  target_hairstyle?:
    | "short_hair"
    | "medium_long_hair"
    | "long_hair"
    | "curly_hair"
    | "wavy_hair"
    | "high_ponytail"
    | "bun"
    | "bob_cut"
    | "pixie_cut"
    | "braids"
    | "straight_hair"
    | "afro"
    | "dreadlocks"
    | "buzz_cut"
    | "mohawk"
    | "bangs"
    | "side_part"
    | "middle_part";
  /**
   *  Default value: `"natural"`
   */
  hair_color?:
    | "black"
    | "dark_brown"
    | "light_brown"
    | "blonde"
    | "platinum_blonde"
    | "red"
    | "auburn"
    | "gray"
    | "silver"
    | "blue"
    | "green"
    | "purple"
    | "pink"
    | "rainbow"
    | "natural"
    | "highlights"
    | "ombre"
    | "balayage";
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2HairChangeOutput = {
  /**
   * Portrait with changed hair
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2HeadshotPhotoInput = {
  /**
   * Portrait image URL to convert to professional headshot
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"professional"`
   */
  background_style?: "professional" | "corporate" | "clean" | "gradient";
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2HeadshotPhotoOutput = {
  /**
   * Professional headshot image
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2MakeupApplicationInput = {
  /**
   * Portrait image URL for makeup application
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"natural"`
   */
  makeup_style?:
    | "natural"
    | "glamorous"
    | "smoky_eyes"
    | "bold_lips"
    | "no_makeup"
    | "remove_makeup"
    | "dramatic"
    | "bridal"
    | "professional"
    | "korean_style"
    | "artistic";
  /**
   *  Default value: `"medium"`
   */
  intensity?: "light" | "medium" | "heavy" | "dramatic";
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2MakeupApplicationOutput = {
  /**
   * Portrait with applied makeup
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2ObjectRemovalInput = {
  /**
   * Image URL containing object to remove
   */
  image_url: string | Blob | File;
  /**
   * Object to remove
   */
  object_to_remove: string;
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2ObjectRemovalOutput = {
  /**
   * Image with object removed
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2OutpaintInput = {
  /**
   * Image URL to outpaint
   */
  image_url: string | Blob | File;
  /**
   * Number of pixels to add as black margin on the left side (0-700).
   */
  expand_left?: number;
  /**
   * Number of pixels to add as black margin on the right side (0-700).
   */
  expand_right?: number;
  /**
   * Number of pixels to add as black margin on the top side (0-700).
   */
  expand_top?: number;
  /**
   * Number of pixels to add as black margin on the bottom side (0-700). Default value: `400`
   */
  expand_bottom?: number;
  /**
   * Percentage to zoom out the image. If set, the image will be scaled down by this percentage and black margins will be added to maintain original size. Example: 50 means the image will be 50% of original size with black margins filling the rest. Default value: `20`
   */
  zoom_out_percentage?: number;
  /**
   * Optional prompt to guide the outpainting. If provided, it will be appended to the base outpaint instruction. Example: 'with a beautiful sunset in the background' Default value: `""`
   */
  prompt?: string;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If True, the function will wait for the image to be generated and uploaded before returning the response. If False, the function will return immediately and the image will be generated asynchronously.
   */
  sync_mode?: boolean;
  /**
   * The format of the output image. Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "jpg" | "webp";
};
export type ImageAppsV2OutpaintOutput = {
  /**
   * Outpainted image with extended scene
   */
  images: Array<Image>;
};
export type ImageAppsV2PerspectiveInput = {
  /**
   * Image URL for perspective change
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"front"`
   */
  target_perspective?:
    | "front"
    | "left_side"
    | "right_side"
    | "back"
    | "top_down"
    | "bottom_up"
    | "birds_eye"
    | "three_quarter_left"
    | "three_quarter_right";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2PerspectiveOutput = {
  /**
   * Image with changed perspective
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2PhotographyEffectsInput = {
  /**
   * Image URL for photography effects
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"film"`
   */
  effect_type?:
    | "film"
    | "vintage_film"
    | "portrait_photography"
    | "fashion_photography"
    | "street_photography"
    | "sepia_tone"
    | "film_grain"
    | "light_leaks"
    | "vignette_effect"
    | "instant_camera"
    | "golden_hour"
    | "dramatic_lighting"
    | "soft_focus"
    | "bokeh_effect"
    | "high_contrast"
    | "double_exposure";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2PhotographyEffectsOutput = {
  /**
   * Image with photography effects
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2PhotoRestorationInput = {
  /**
   * Old or damaged photo URL to restore
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `true`
   */
  enhance_resolution?: boolean;
  /**
   *  Default value: `true`
   */
  fix_colors?: boolean;
  /**
   *  Default value: `true`
   */
  remove_scratches?: boolean;
  /**
   * Aspect ratio for 4K output (default: 4:3 for classic photos)
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2PhotoRestorationOutput = {
  /**
   * Restored photo
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2PortraitEnhanceInput = {
  /**
   * Portrait image URL to enhance
   */
  image_url: string | Blob | File;
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2PortraitEnhanceOutput = {
  /**
   * Enhanced portrait
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2ProductHoldingInput = {
  /**
   * Image URL of the person who will hold the product
   */
  person_image_url: string | Blob | File;
  /**
   * Image URL of the product to be held by the person
   */
  product_image_url: string | Blob | File;
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2ProductHoldingOutput = {
  /**
   * Person holding the product naturally
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2ProductPhotographyInput = {
  /**
   * Image URL of the product to create professional studio photography
   */
  product_image_url: string | Blob | File;
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2ProductPhotographyOutput = {
  /**
   * Professional studio product photography
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2RelightingInput = {
  /**
   * Image URL for relighting
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"natural"`
   */
  lighting_style?:
    | "natural"
    | "studio"
    | "golden_hour"
    | "blue_hour"
    | "dramatic"
    | "soft"
    | "hard"
    | "backlight"
    | "side_light"
    | "front_light"
    | "rim_light"
    | "sunset"
    | "sunrise"
    | "neon"
    | "candlelight"
    | "moonlight"
    | "spotlight"
    | "ambient";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2RelightingOutput = {
  /**
   * Image with new lighting
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2StyleTransferInput = {
  /**
   * Image URL for style transfer
   */
  image_url: string | Blob | File;
  /**
   * Optional reference image URL. When provided, the style will be inferred from this image instead of the selected preset style.
   */
  style_reference_image_url?: string | Blob | File;
  /**
   *  Default value: `"impressionist"`
   */
  target_style?:
    | "anime_character"
    | "cartoon_3d"
    | "hand_drawn_animation"
    | "cyberpunk_future"
    | "anime_game_style"
    | "comic_book_animation"
    | "animated_series"
    | "cartoon_animation"
    | "lofi_aesthetic"
    | "cottagecore"
    | "dark_academia"
    | "y2k"
    | "vaporwave"
    | "liminal_space"
    | "weirdcore"
    | "dreamcore"
    | "synthwave"
    | "outrun"
    | "photorealistic"
    | "hyperrealistic"
    | "digital_art"
    | "concept_art"
    | "impressionist"
    | "anime"
    | "pixel_art"
    | "claymation";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2StyleTransferOutput = {
  /**
   * Image with transferred style
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2TextureTransformInput = {
  /**
   * Image URL for texture transformation
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"marble"`
   */
  target_texture?:
    | "cotton"
    | "denim"
    | "wool"
    | "felt"
    | "wood"
    | "leather"
    | "velvet"
    | "stone"
    | "marble"
    | "ceramic"
    | "concrete"
    | "brick"
    | "clay"
    | "foam"
    | "glass"
    | "metal"
    | "silk"
    | "fabric"
    | "crystal"
    | "rubber"
    | "plastic"
    | "lace";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2TextureTransformOutput = {
  /**
   * Image with transformed texture
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageAppsV2VirtualTryOnInput = {
  /**
   * Person photo URL
   */
  person_image_url: string | Blob | File;
  /**
   * Clothing photo URL
   */
  clothing_image_url: string | Blob | File;
  /**
   *  Default value: `true`
   */
  preserve_pose?: boolean;
  /**
   * Aspect ratio for 4K output (default: 3:4 for fashion)
   */
  aspect_ratio?: AspectRatio;
};
export type ImageAppsV2VirtualTryOnOutput = {
  /**
   * Person wearing the virtual clothing
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ImageChatOutput = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Dictionary of label: mask image
   */
  masks: Array<Image>;
};
export type ImageConditioningInput = {
  /**
   * URL of image to use as conditioning
   */
  image_url: string | Blob | File;
  /**
   * Frame number of the image from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num: number;
};
export type ImageDetectionInput = {
  /**
   * URL pointing to an image to analyze for AI generation.(Max: 3000 characters)
   */
  image_url: string | Blob | File;
};
export type ImageEditingAgeProgressionInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The age change to apply. Default value: `"20 years older"`
   */
  prompt?: string;
};
export type ImageEditingAgeProgressionOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingBabyVersionInput = {
  /**
   * URL of the image to transform into a baby version.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingBabyVersionOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingBackgroundChangeInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The desired background to apply. Default value: `"beach sunset with palm trees"`
   */
  prompt?: string;
};
export type ImageEditingBackgroundChangeOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingBroccoliHaircutInput = {
  /**
   * URL of the image to apply broccoli haircut style.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingBroccoliHaircutOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingCartoonifyInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingCartoonifyOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingColorCorrectionInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingColorCorrectionOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingExpressionChangeInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The desired facial expression to apply. Default value: `"sad"`
   */
  prompt?: string;
};
export type ImageEditingExpressionChangeOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingFaceEnhancementInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingFaceEnhancementOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingHairChangeInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The desired hair style to apply. Default value: `"bald"`
   */
  prompt?: string;
};
export type ImageEditingHairChangeOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingObjectRemovalInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Specify which objects to remove from the image. Default value: `"background people"`
   */
  prompt?: string;
};
export type ImageEditingObjectRemovalOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingPhotoRestorationInput = {
  /**
   * URL of the old or damaged photo to restore.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingPhotoRestorationOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingPlushieStyleInput = {
  /**
   * URL of the image to convert to plushie style.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingPlushieStyleOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingProfessionalPhotoInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingProfessionalPhotoOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingRealismInput = {
  /**
   * URL of the image to enhance with realism details.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `0.6`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingRealismOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingReframeInput = {
  /**
   * URL of the old or damaged photo to restore.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The desired aspect ratio for the reframed image. Default value: `"16:9"`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingReframeOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingRetouchInput = {
  /**
   * URL of the image to retouch.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingRetouchOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingSceneCompositionInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Describe the scene where you want to place the subject. Default value: `"enchanted forest"`
   */
  prompt?: string;
};
export type ImageEditingSceneCompositionOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingStyleTransferInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The artistic style to apply. Default value: `"Van Gogh's Starry Night"`
   */
  prompt?: string;
};
export type ImageEditingStyleTransferOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingTextRemovalInput = {
  /**
   * URL of the image containing text to be removed.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingTextRemovalOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingTimeOfDayInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The time of day to transform the scene to. Default value: `"golden hour"`
   */
  prompt?: string;
};
export type ImageEditingTimeOfDayOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingWeatherEffectInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The weather effect to apply. Default value: `"heavy snowfall"`
   */
  prompt?: string;
};
export type ImageEditingWeatherEffectOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingWojakStyleInput = {
  /**
   * URL of the image to convert to wojak style.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageEditingWojakStyleOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditingYoutubeThumbnailsInput = {
  /**
   * URL of the image to convert to YouTube thumbnail style.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `0.5`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The text to include in the YouTube thumbnail. Default value: `"Generate youtube thumbnails"`
   */
  prompt?: string;
};
export type ImageEditingYoutubeThumbnailsOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ImageEditInput = {
  /**
   * The prompt to edit the image with.
   */
  prompt: string;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * Whether to use thought tokens for generation. If set to true, the model will "think" to potentially improve generation quality. Increases generation time and increases the cost by 20%.
   */
  use_thought?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The image to edit.
   */
  image_url: string | Blob | File;
};
export type ImageEditOutput = {
  /**
   * The edited images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ImageExpansionInput = {
  /**
   * The URL of the input image.
   */
  image_url: string | Blob | File;
  /**
   * The desired size of the final image, after the expansion. should have an area of less than 5000x5000 pixels.
   */
  canvas_size: Array<number>;
  /**
   * The desired aspect ratio of the final image. Will be used over original_image_size and original_image_location if provided.
   */
  aspect_ratio?:
    | "1:1"
    | "2:3"
    | "3:2"
    | "3:4"
    | "4:3"
    | "4:5"
    | "5:4"
    | "9:16"
    | "16:9";
  /**
   * The desired size of the original image, inside the full canvas. Ensure that the ratio of input image foreground or main subject to the canvas area is greater than 15% to achieve optimal results. Will be ignored if aspect_ratio is provided.
   */
  original_image_size?: Array<number>;
  /**
   * The desired location of the original image, inside the full canvas. Provide the location of the upper left corner of the original image. The location can also be outside the canvas (the original image will be cropped). Will be ignored if aspect_ratio is provided.
   */
  original_image_location?: Array<number>;
  /**
   * Text on which you wish to base the image expansion. This parameter is optional. Bria currently supports prompts in English only, excluding special characters. Default value: `""`
   */
  prompt?: string;
  /**
   * You can choose whether you want your generated expension to be random or predictable. You can recreate the same result in the future by using the seed value of a result from the response. You can exclude this parameter if you are not interested in recreating your results. This parameter is optional.
   */
  seed?: number;
  /**
   * The negative prompt you would like to use to generate images. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ImageExpansionOutput = {
  /**
   * The generated image
   */
  image: Image;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type ImageFillInput = {
  /**
   * URLs of images to be filled for redux prompting
   */
  fill_image_url?: string | Blob | File | Array<string | Blob | File>;
};
export type ImageGenInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
  /**
   * Whether to use thought tokens for generation. If set to true, the model will "think" to potentially improve generation quality. Increases generation time and increases the cost by 20%.
   */
  use_thought?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type ImageGridInput = {
  /**
   * List of image URLs to arrange in grid
   */
  image_urls: Array<string>;
  /**
   * Number of columns in the grid Default value: `2`
   */
  columns?: number;
  /**
   * Width of each cell in pixels (if not set, uses first image width)
   */
  cell_width?: number;
  /**
   * Height of each cell in pixels (if not set, uses first image height)
   */
  cell_height?: number;
  /**
   * Spacing between cells in pixels
   */
  spacing?: number;
  /**
   * Background color for empty cells and spacing Default value: `"white"`
   */
  background_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta"
    | "transparent";
  /**
   * How images fit in cells Default value: `"cover"`
   */
  fit_mode?: "cover" | "contain" | "stretch";
  /**
   * Output format for the grid image Default value: `"png"`
   */
  output_format?: "png" | "jpg" | "jpeg" | "webp";
};
export type ImageGridOutput = {
  /**
   * Grid layout image
   */
  image: Image;
};
export type ImageInput = {
  /**
   * Prompt to be used for the chat completion
   */
  prompt: string;
  /**
   * Url for the Input image.
   */
  image_url: string | Blob | File;
};
export type ImageMultiMeasurementInput = {
  /**
   * The measurements to use for the measurement.
   */
  measurements: Array<"arniqa" | "clip_iqa" | "musiq" | "nima" | "lapvar">;
  /**
   * The inputs to use for the measurement.
   */
  inputs: Array<ImageInput>;
};
export type Imagen3FastInput = {
  /**
   * The text prompt describing what you want to see
   */
  prompt: string;
  /**
   * A description of what to discourage in the generated images Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?: "1:1" | "16:9" | "9:16" | "3:4" | "4:3";
  /**
   * Number of images to generate (1-4) Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducible generation
   */
  seed?: number;
};
export type Imagen3FastOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type imagen3Input = {
  /**
   * The text prompt describing what you want to see
   */
  prompt: string;
  /**
   * A description of what to discourage in the generated images Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?: "1:1" | "16:9" | "9:16" | "3:4" | "4:3";
  /**
   * Number of images to generate (1-4) Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducible generation
   */
  seed?: number;
};
export type imagen3Output = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type Imagen4PreviewFastInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?: "1:1" | "16:9" | "9:16" | "4:3" | "3:4";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type Imagen4PreviewFastOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type Imagen4PreviewInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?: "1:1" | "16:9" | "9:16" | "4:3" | "3:4";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The resolution of the generated image. Default value: `"1K"`
   */
  resolution?: "1K" | "2K";
};
export type Imagen4PreviewOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type Imagen4PreviewUltraInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?: "1:1" | "16:9" | "9:16" | "4:3" | "3:4";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The resolution of the generated image. Default value: `"1K"`
   */
  resolution?: "1K" | "2K";
};
export type Imagen4PreviewUltraOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type Imagen4TextToImageFastInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?: "1:1" | "16:9" | "9:16" | "4:3" | "3:4";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type Imagen4TextToImageFastOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type Imagen4TextToImageInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?: "1:1" | "16:9" | "9:16" | "4:3" | "3:4";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The resolution of the generated image. Default value: `"1K"`
   */
  resolution?: "1K" | "2K";
};
export type Imagen4TextToImageOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type Imagen4TextToImageUltraInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?: "1:1" | "16:9" | "9:16" | "4:3" | "3:4";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The resolution of the generated image. Default value: `"1K"`
   */
  resolution?: "1K" | "2K";
};
export type Imagen4TextToImageUltraOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type ImageOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ImageProcessingInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Enable film grain effect
   */
  enable_grain?: boolean;
  /**
   * Film grain intensity (when enabled) Default value: `0.4`
   */
  grain_intensity?: number;
  /**
   * Film grain scale (when enabled) Default value: `10`
   */
  grain_scale?: number;
  /**
   * Style of film grain to apply Default value: `"modern"`
   */
  grain_style?:
    | "modern"
    | "analog"
    | "kodak"
    | "fuji"
    | "cinematic"
    | "newspaper";
  /**
   * Enable color correction
   */
  enable_color_correction?: boolean;
  /**
   * Color temperature adjustment
   */
  temperature?: number;
  /**
   * Brightness adjustment
   */
  brightness?: number;
  /**
   * Contrast adjustment
   */
  contrast?: number;
  /**
   * Saturation adjustment
   */
  saturation?: number;
  /**
   * Gamma adjustment Default value: `1`
   */
  gamma?: number;
  /**
   * Enable chromatic aberration
   */
  enable_chromatic?: boolean;
  /**
   * Red channel shift amount
   */
  red_shift?: number;
  /**
   * Red channel shift direction Default value: `"horizontal"`
   */
  red_direction?: "horizontal" | "vertical";
  /**
   * Green channel shift amount
   */
  green_shift?: number;
  /**
   * Green channel shift direction Default value: `"horizontal"`
   */
  green_direction?: "horizontal" | "vertical";
  /**
   * Blue channel shift amount
   */
  blue_shift?: number;
  /**
   * Blue channel shift direction Default value: `"horizontal"`
   */
  blue_direction?: "horizontal" | "vertical";
  /**
   * Enable blur effect
   */
  enable_blur?: boolean;
  /**
   * Type of blur to apply Default value: `"gaussian"`
   */
  blur_type?: "gaussian" | "kuwahara";
  /**
   * Blur radius Default value: `3`
   */
  blur_radius?: number;
  /**
   * Sigma for Gaussian blur Default value: `1`
   */
  blur_sigma?: number;
  /**
   * Enable vignette effect
   */
  enable_vignette?: boolean;
  /**
   * Vignette strength (when enabled) Default value: `0.5`
   */
  vignette_strength?: number;
  /**
   * Enable parabolize effect
   */
  enable_parabolize?: boolean;
  /**
   * Parabolize coefficient Default value: `1`
   */
  parabolize_coeff?: number;
  /**
   * Vertex X position Default value: `0.5`
   */
  vertex_x?: number;
  /**
   * Vertex Y position Default value: `0.5`
   */
  vertex_y?: number;
  /**
   * Enable color tint effect
   */
  enable_tint?: boolean;
  /**
   * Tint strength Default value: `1`
   */
  tint_strength?: number;
  /**
   * Tint color mode Default value: `"sepia"`
   */
  tint_mode?:
    | "sepia"
    | "red"
    | "green"
    | "blue"
    | "cyan"
    | "magenta"
    | "yellow"
    | "purple"
    | "orange"
    | "warm"
    | "cool"
    | "lime"
    | "navy"
    | "vintage"
    | "rose"
    | "teal"
    | "maroon"
    | "peach"
    | "lavender"
    | "olive";
  /**
   * Enable dissolve effect
   */
  enable_dissolve?: boolean;
  /**
   * URL of second image for dissolve Default value: `""`
   */
  dissolve_image_url?: string | Blob | File;
  /**
   * Dissolve blend factor Default value: `0.5`
   */
  dissolve_factor?: number;
  /**
   * Enable dodge and burn effect
   */
  enable_dodge_burn?: boolean;
  /**
   * Dodge and burn intensity Default value: `0.5`
   */
  dodge_burn_intensity?: number;
  /**
   * Dodge and burn mode Default value: `"dodge"`
   */
  dodge_burn_mode?:
    | "dodge"
    | "burn"
    | "dodge_and_burn"
    | "burn_and_dodge"
    | "color_dodge"
    | "color_burn"
    | "linear_dodge"
    | "linear_burn";
  /**
   * Enable glow effect
   */
  enable_glow?: boolean;
  /**
   * Glow intensity Default value: `1`
   */
  glow_intensity?: number;
  /**
   * Glow blur radius Default value: `5`
   */
  glow_radius?: number;
  /**
   * Enable sharpen effect
   */
  enable_sharpen?: boolean;
  /**
   * Type of sharpening to apply Default value: `"basic"`
   */
  sharpen_mode?: "basic" | "smart" | "cas";
  /**
   * Sharpen radius (for basic mode) Default value: `1`
   */
  sharpen_radius?: number;
  /**
   * Sharpen strength (for basic mode) Default value: `1`
   */
  sharpen_alpha?: number;
  /**
   * Noise radius for smart sharpen Default value: `7`
   */
  noise_radius?: number;
  /**
   * Edge preservation factor Default value: `0.75`
   */
  preserve_edges?: number;
  /**
   * Smart sharpen strength Default value: `5`
   */
  smart_sharpen_strength?: number;
  /**
   * Smart sharpen blend ratio Default value: `0.5`
   */
  smart_sharpen_ratio?: number;
  /**
   * CAS sharpening amount Default value: `0.8`
   */
  cas_amount?: number;
  /**
   * Enable solarize effect
   */
  enable_solarize?: boolean;
  /**
   * Solarize threshold Default value: `0.5`
   */
  solarize_threshold?: number;
  /**
   * Enable desaturation effect
   */
  enable_desaturate?: boolean;
  /**
   * Desaturation factor Default value: `1`
   */
  desaturate_factor?: number;
  /**
   * Desaturation method Default value: `"luminance (Rec.709)"`
   */
  desaturate_method?:
    | "luminance (Rec.709)"
    | "luminance (Rec.601)"
    | "average"
    | "lightness";
};
export type ImageReferenceMeasurementInput = {
  /**
   * The measurements to use for the measurement.
   */
  measurements: Array<"dists" | "mse" | "lpips" | "sdi" | "ssim">;
  /**
   * The inputs to use for the measurement.
   */
  inputs: Array<ReferenceImageInput>;
};
export type ImageTo3dInput = {
  /**
   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.
   */
  seed?: number;
  /**
   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.
   */
  face_limit?: number;
  /**
   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.
   */
  pbr?: boolean;
  /**
   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `"standard"`
   */
  texture?: "no" | "standard" | "HD";
  /**
   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.
   */
  texture_seed?: number;
  /**
   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.
   */
  auto_size?: boolean;
  /**
   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.
   */
  quad?: boolean;
  /**
   * Determines the prioritization of texture alignment in the 3D model. The default value is original_image. Default value: `"original_image"`
   */
  texture_alignment?: "original_image" | "geometry";
  /**
   * Set orientation=align_image to automatically rotate the model to align the original image. The default value is default. Default value: `"default"`
   */
  orientation?: "default" | "align_image";
  /**
   * URL of the image to use for model generation.
   */
  image_url: string | Blob | File;
};
export type ImageTo3DInput = {
  /**
   * Image URL or base64 data URI for 3D model creation. Supports .jpg, .jpeg, and .png formats. Also supports AVIF and HEIF formats which will be automatically converted.
   */
  image_url: string | Blob | File;
  /**
   * Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry. Default value: `"triangle"`
   */
  topology?: "quad" | "triangle";
  /**
   * Target number of polygons in the generated model Default value: `30000`
   */
  target_polycount?: number;
  /**
   * Controls symmetry behavior during model generation. Off disables symmetry, Auto determines it automatically, On enforces symmetry. Default value: `"auto"`
   */
  symmetry_mode?: "off" | "auto" | "on";
  /**
   * Whether to enable the remesh phase Default value: `true`
   */
  should_remesh?: boolean;
  /**
   * Whether to generate textures Default value: `true`
   */
  should_texture?: boolean;
  /**
   * Generate PBR Maps (metallic, roughness, normal) in addition to base color
   */
  enable_pbr?: boolean;
  /**
   * Whether to generate the model in an A/T pose
   */
  is_a_t_pose?: boolean;
  /**
   * Text prompt to guide the texturing process
   */
  texture_prompt?: string;
  /**
   * 2D image to guide the texturing process
   */
  texture_image_url?: string | Blob | File;
  /**
   * If set to true, input data will be checked for safety before processing. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type ImageTo3DOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * Array of texture file objects, matching Meshy API structure
   */
  texture_urls?: Array<TextureFiles>;
  /**
   * The seed used for generation (if available)
   */
  seed?: number;
};
export type ImageToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The image to use for control lora. This is used to control the style of the generated image.
   */
  control_lora_image_url: string | Blob | File;
  /**
   * The strength of the control lora. Default value: `1`
   */
  control_lora_strength?: number;
  /**
   * URL of image to use for inpainting. or img2img
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
};
export type ImageToImageOutput = {
  /**
   * The generated images
   */
  images: Array<File>;
};
export type ImageToImageTurboInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you.
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * URL of Image for Image-to-Image
   */
  image_url: string | Blob | File;
  /**
   * Strength for Image-to-Image. Default value: `0.83`
   */
  strength?: number;
};
export type ImageToVideoHailuo02FastOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoHailuo02Output = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The negative prompt to generate video from Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The LoRAs to use for the image generation. We currently support one lora.
   */
  loras?: Array<LoraWeight>;
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`
   */
  guidance_scale?: number;
  /**
   * Use RIFE for video interpolation Default value: `true`
   */
  use_rife?: boolean;
  /**
   * The target FPS of the video Default value: `16`
   */
  export_fps?: number;
  /**
   * The URL to the image to generate the video from.
   */
  image_url: string | Blob | File;
};
export type ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoTurboInput = {
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `5`
   */
  duration?: number;
};
export type ImageToVideov21Input = {
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `5`
   */
  duration?: number;
};
export type ImageToVideoV21MasterOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoV21Output = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoV21ProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoV21StandardOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoV25ProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoV25StandardOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoV26ProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoV2MasterOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoV3ProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageToVideoV3StandardOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ImageUnderstandingInput = {
  /**
   * The image for the query.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to query the image with.
   */
  prompt: string;
  /**
   * The seed to use for the generation.
   */
  seed?: number;
};
export type ImageUpscaleOutput = {
  /**
   * The upscaled image.
   */
  image: File;
};
export type Imagineart15PreviewTextToImageInput = {
  /**
   * Text prompt describing the desired image
   */
  prompt: string;
  /**
   * Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3 Default value: `"1:1"`
   */
  aspect_ratio?:
    | "1:1"
    | "16:9"
    | "9:16"
    | "4:3"
    | "3:4"
    | "3:1"
    | "1:3"
    | "3:2"
    | "2:3";
  /**
   * Seed for the image generation
   */
  seed?: number;
};
export type Imagineart15PreviewTextToImageOutput = {
  /**
   * Generated image
   */
  images: Array<ImageOutput>;
};
export type Imagineart15ProPreviewTextToImageInput = {
  /**
   * Text prompt describing the desired image
   */
  prompt: string;
  /**
   * Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3 Default value: `"1:1"`
   */
  aspect_ratio?:
    | "1:1"
    | "16:9"
    | "9:16"
    | "4:3"
    | "3:4"
    | "3:1"
    | "1:3"
    | "3:2"
    | "2:3";
  /**
   * Seed for the image generation
   */
  seed?: number;
};
export type Imagineart15ProPreviewTextToImageOutput = {
  /**
   * Generated image
   */
  images: Array<Image>;
};
export type Img2ImgOutput = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ImpulseResponseInput = {
  /**
   * URL of the main audio file to process
   */
  audio_url: string | Blob | File;
  /**
   * URL of the impulse response WAV file (reverb/effect profile)
   */
  impulse_response_url: string | Blob | File;
  /**
   * Level of the original (dry) signal in the mix (0.0-1.0) Default value: `0.7`
   */
  dry_level?: number;
  /**
   * Level of the processed (wet) signal in the mix (0.0-1.0) Default value: `0.3`
   */
  wet_level?: number;
  /**
   * Target integrated loudness in LUFS (typically -24 to -14) Default value: `-18`
   */
  loudness_i?: number;
  /**
   * Loudness Range target in LU (typically 5-15) Default value: `8`
   */
  loudness_lra?: number;
  /**
   * Maximum true peak in dBTP (typically -2 to -1) Default value: `-1.5`
   */
  loudness_tp?: number;
  /**
   * Output audio bitrate Default value: `"192k"`
   */
  output_bitrate?: "128k" | "192k" | "256k" | "320k";
};
export type ImpulseResponseOutput = {
  /**
   * The processed audio file with reverb applied
   */
  audio: AudioFile;
};
export type IndexTts2TextToSpeechInput = {
  /**
   * The audio file to generate the speech from.
   */
  audio_url: string | Blob | File;
  /**
   * The speech prompt to generate
   */
  prompt: string;
  /**
   * The emotional reference audio file to extract the style from.
   */
  emotional_audio_url?: string | Blob | File;
  /**
   * The strength of the emotional style transfer. Higher values result in stronger emotional influence. Default value: `1`
   */
  strength?: number;
  /**
   * The strengths of individual emotions for fine-grained control.
   */
  emotional_strengths?: EmotionalStrengths;
  /**
   * Whether to use the `prompt` to calculate emotional strengths, if enabled it will overwrite the `emotional_strengths` values. If `emotion_prompt` is provided, it will be used to instead of `prompt` to extract the emotional style.
   */
  should_use_prompt_for_emotion?: boolean;
  /**
   * The emotional prompt to influence the emotional style. Must be used together with should_use_prompt_for_emotion.
   */
  emotion_prompt?: string;
};
export type IndexTts2TextToSpeechOutput = {
  /**
   * The generated audio file in base64 format.
   */
  audio: File;
};
export type infinitalkInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 41 to 721. Default value: `145`
   */
  num_frames?: number;
  /**
   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `42`
   */
  seed?: number;
  /**
   * The acceleration level to use for generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type infinitalkOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type InfinitalkSingleTextInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The text input to guide video generation.
   */
  text_input: string;
  /**
   * The voice to use for speech generation
   */
  voice:
    | "Aria"
    | "Roger"
    | "Sarah"
    | "Laura"
    | "Charlie"
    | "George"
    | "Callum"
    | "River"
    | "Liam"
    | "Charlotte"
    | "Alice"
    | "Matilda"
    | "Will"
    | "Jessica"
    | "Eric"
    | "Chris"
    | "Brian"
    | "Daniel"
    | "Lily"
    | "Bill";
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 41 to 721. Default value: `145`
   */
  num_frames?: number;
  /**
   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `42`
   */
  seed?: number;
  /**
   * The acceleration level to use for generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type InfinitalkSingleTextOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type InfinitalkVideoToVideoInput = {
  /**
   * URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `145`
   */
  num_frames?: number;
  /**
   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `42`
   */
  seed?: number;
  /**
   * The acceleration level to use for generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type InfinitalkVideoToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type InfinityStarTextToVideoInput = {
  /**
   * Text prompt for generating the video
   */
  prompt: string;
  /**
   * Negative prompt to guide what to avoid in generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Number of inference steps Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for generation Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Tau value for video scale Default value: `0.4`
   */
  tau_video?: number;
  /**
   * Whether to use APG Default value: `true`
   */
  use_apg?: boolean;
  /**
   * Aspect ratio of the generated output Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * Random seed for reproducibility. Leave empty for random generation.
   */
  seed?: number;
  /**
   * Whether to use an LLM to enhance the prompt. Default value: `true`
   */
  enhance_prompt?: boolean;
};
export type InfinityStarTextToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
};
export type InpaintInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * URL of image to use for inpainting. or img2img
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
  /**
   * The mask to area to Inpaint in.
   */
  mask_url: string | Blob | File;
};
export type InpaintOutput = {
  /**
   * The generated audio files.
   */
  audio: Array<File>;
  /**
   * The seed used for generation. This can be used to generate an identical song by passing the same parameters with this seed in a future request.
   */
  seed: number;
};
export type InpaintTurboInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you.
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * URL of Image for inpainting
   */
  image_url: string | Blob | File;
  /**
   * Strength for Image-to-Image. Default value: `0.83`
   */
  strength?: number;
  /**
   * URL of mask image for inpainting.
   */
  mask_image_url: string | Blob | File;
};
export type Input = {
  /**
   * List of tracks to be combined into the final media
   */
  tracks: Array<Track>;
};
export type InstantCharacterInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The image URL to generate an image from. Needs to match the dimensions of the mask.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The scale of the subject image. Higher values will make the subject image more prominent in the generated image. Default value: `1`
   */
  scale?: number;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type InstantCharacterOutput = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type IntegrateProductInput = {
  /**
   * The URL of the image with product to integrate into background.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe how to blend and integrate the product/element into the background. The model will automatically correct perspective, lighting and shadows for natural integration. Default value: `"Blend and integrate the product into the background"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type IntegrateProductOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type InterleaveVideoInput = {
  /**
   * List of video URLs to interleave in order
   */
  video_urls: Array<string>;
};
export type InterleaveVideoOutput = {
  /**
   * The interleaved video output
   */
  video: File;
};
export type InvisibleWatermarkInput = {
  /**
   * URL of image to be watermarked or decoded
   */
  image_url: string | Blob | File;
  /**
   * Text to use as watermark (for encoding only) Default value: `"watermark"`
   */
  watermark?: string;
  /**
   * Whether to decode a watermark from the image instead of encoding
   */
  decode?: boolean;
  /**
   * Length of watermark bits to decode (required when decode=True)
   */
  length?: number;
};
export type InvisibleWatermarkOutput = {
  /**
   * The watermarked image file info (when encoding)
   */
  image?: Image;
  /**
   * The extracted watermark text (when decoding)
   */
  extracted_watermark?: string;
  /**
   * Length of the watermark bits used (helpful for future decoding)
   */
  length?: number;
};
export type Isaac01Input = {
  /**
   * Image URL to be processed
   */
  image_url: string | Blob | File;
  /**
   * Prompt to be used for the image
   */
  prompt: string;
  /**
   * Response style to be used for the image.
   *
   * - text: Model will output text. Good for descriptions and captioning.
   * - box: Model will output a combination of text and bounding boxes. Good for
   * localization.
   * - point: Model will output a combination of text and points. Good for counting many
   * objects.
   * - polygon: Model will output a combination of text and polygons. Good for granular
   * segmentation. Default value: `"text"`
   */
  response_style?: "text" | "box" | "point" | "polygon";
};
export type Isaac01Output = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Whether the output is partial
   */
  partial?: boolean;
  /**
   * Error message if an error occurred
   */
  error?: string;
  /**
   * Usage information
   */
  usage?: CompletionUsage;
};
export type ItalianOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type janusInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `square`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Controls randomness in the generation. Higher values make output more random. Default value: `1`
   */
  temperature?: number;
  /**
   * Classifier Free Guidance scale - how closely to follow the prompt. Default value: `5`
   */
  cfg_weight?: number;
  /**
   * Number of images to generate in parallel. Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type janusOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type JapaneseOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type JuggernautFluxBaseImageToImageInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type JuggernautFluxBaseImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type JuggernautFluxBaseInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type JuggernautFluxBaseOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type JuggernautFluxLightningInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type JuggernautFluxLightningOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type JuggernautFluxLoraInpaintingInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * URL of image to use for inpainting. or img2img
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
  /**
   * The mask to area to Inpaint in.
   */
  mask_url: string | Blob | File;
};
export type JuggernautFluxLoraInpaintingOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type JuggernautFluxLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type JuggernautFluxLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type JuggernautFluxProImageToImageInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`
   */
  strength?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type JuggernautFluxProImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type JuggernautFluxProInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type JuggernautFluxProOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Kandinsky5ProImageToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string | Blob | File;
  /**
   * Video resolution: 512p or 1024p. Default value: `"512P"`
   */
  resolution?: "512P" | "1024P";
  /**
   * Video duration. Default value: `"5s"`
   */
  duration?: "5s";
  /**
   *  Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for faster generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
};
export type Kandinsky5ProImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video?: File;
};
export type Kandinsky5ProTextToVideoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video resolution: 512p or 1024p. Default value: `"512P"`
   */
  resolution?: "512P" | "1024P";
  /**
   * Aspect ratio of the generated video. One of (3:2, 1:1, 2:3). Default value: `"3:2"`
   */
  aspect_ratio?: "3:2" | "1:1" | "2:3";
  /**
   * The length of the video to generate (5s or 10s) Default value: `"5s"`
   */
  duration?: "5s";
  /**
   * The number of inference steps. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for faster generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
};
export type Kandinsky5ProTextToVideoOutput = {
  /**
   * The generated video file.
   */
  video?: File;
};
export type Kandinsky5TextToVideoDistillInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768). Default value: `"768x512"`
   */
  resolution?: "768x512";
  /**
   * Aspect ratio of the generated video. One of (3:2, 1:1, 2:3). Default value: `"3:2"`
   */
  aspect_ratio?: "3:2" | "1:1" | "2:3";
  /**
   * The length of the video to generate (5s or 10s) Default value: `"5s"`
   */
  duration?: "5s" | "10s";
};
export type Kandinsky5TextToVideoDistillOutput = {
  /**
   * The generated video file.
   */
  video?: File;
};
export type Kandinsky5TextToVideoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768). Default value: `"768x512"`
   */
  resolution?: "768x512";
  /**
   * Aspect ratio of the generated video. One of (3:2, 1:1, 2:3). Default value: `"3:2"`
   */
  aspect_ratio?: "3:2" | "1:1" | "2:3";
  /**
   * The length of the video to generate (5s or 10s) Default value: `"5s"`
   */
  duration?: "5s" | "10s";
  /**
   * The number of inference steps. Default value: `30`
   */
  num_inference_steps?: number;
};
export type Kandinsky5TextToVideoOutput = {
  /**
   * The generated video file.
   */
  video?: File;
};
export type Klein4BBaseEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
};
export type Klein4BBaseEditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Klein4BBaseInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Klein4BDistilledEditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Klein4BDistilledT2IOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Klein4BT2IOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Klein9BBaseEditOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Klein9BBaseInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Klein9BDistilledInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type Klein9BDistilledT2IOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Klein9BEditImageInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
};
export type Klein9BT2IOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type KleinBaseEditLoRAInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<FalAiFlux2KleinLorainput>;
};
export type KleinBaseLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<FalAiFlux2KleinLorainput>;
};
export type KleinDistilledEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. If not provided, uses the input image size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed.
   */
  image_urls: Array<string>;
};
export type KleinDistilledInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type KleinT2IOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type KlingImageO1Input = {
  /**
   * Text prompt for image generation. Reference images using @Image1, @Image2, etc. (or @Image if only one image). Max 2500 characters.
   */
  prompt: string;
  /**
   * List of reference images. Reference images in prompt using @Image1, @Image2, etc. (1-indexed). Max 10 images.
   */
  image_urls: Array<string>;
  /**
   * Elements (characters/objects) to include in the image. Reference in prompt as @Element1, @Element2, etc. Maximum 10 total (elements + reference images).
   */
  elements?: Array<ElementInput>;
  /**
   * Image generation resolution. 1K: standard, 2K: high-res. Default value: `"1K"`
   */
  resolution?: "1K" | "2K";
  /**
   * Number of images to generate (1-9). Default value: `1`
   */
  num_images?: number;
  /**
   * Aspect ratio of generated images. 'auto' intelligently determines based on input content. Default value: `"auto"`
   */
  aspect_ratio?:
    | "auto"
    | "16:9"
    | "9:16"
    | "1:1"
    | "4:3"
    | "3:4"
    | "3:2"
    | "2:3"
    | "21:9";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type KlingImageO1Output = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type KlingImageO3ImageToImageInput = {
  /**
   * Text prompt for image generation. Reference images using @Image1, @Image2, etc. (or @Image if only one image). Max 2500 characters.
   */
  prompt: string;
  /**
   * List of reference images. Reference images in prompt using @Image1, @Image2, etc. (1-indexed). Max 10 images.
   */
  image_urls: Array<string>;
  /**
   * Optional: Elements (characters/objects) for face control. Reference in prompt as @Element1, @Element2, etc.
   */
  elements?: Array<ElementInput>;
  /**
   * Image generation resolution. 1K: standard, 2K: high-res, 4K: ultra high-res. Default value: `"1K"`
   */
  resolution?: "1K" | "2K" | "4K";
  /**
   * Result type. 'single' for one image, 'series' for a series of related images. Default value: `"single"`
   */
  result_type?: "single" | "series";
  /**
   * Number of images to generate (1-9). Only used when result_type is 'single'. Default value: `1`
   */
  num_images?: number;
  /**
   * Number of images in series (2-9). Only used when result_type is 'series'.
   */
  series_amount?: number;
  /**
   * Aspect ratio of generated images. 'auto' intelligently determines based on input content. Default value: `"auto"`
   */
  aspect_ratio?:
    | "16:9"
    | "9:16"
    | "1:1"
    | "4:3"
    | "3:4"
    | "3:2"
    | "2:3"
    | "21:9"
    | "auto";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
};
export type KlingImageO3ImageToImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type KlingImageO3TextToImageInput = {
  /**
   * Text prompt for image generation. Max 2500 characters.
   */
  prompt: string;
  /**
   * Optional: Elements (characters/objects) for face control. Reference in prompt as @Element1, @Element2, etc.
   */
  elements?: Array<ElementInput>;
  /**
   * Image generation resolution. 1K: standard, 2K: high-res, 4K: ultra high-res. Default value: `"1K"`
   */
  resolution?: "1K" | "2K" | "4K";
  /**
   * Result type. 'single' for one image, 'series' for a series of related images. Default value: `"single"`
   */
  result_type?: "single" | "series";
  /**
   * Number of images to generate (1-9). Only used when result_type is 'single'. Default value: `1`
   */
  num_images?: number;
  /**
   * Number of images in series (2-9). Only used when result_type is 'series'.
   */
  series_amount?: number;
  /**
   * Aspect ratio of generated images. Default value: `"16:9"`
   */
  aspect_ratio?:
    | "16:9"
    | "9:16"
    | "1:1"
    | "4:3"
    | "3:4"
    | "3:2"
    | "2:3"
    | "21:9";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
};
export type KlingImageO3TextToImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type KlingImageV3ImageToImageInput = {
  /**
   * Text prompt for image generation. Max 2500 characters.
   */
  prompt: string;
  /**
   * Reference image for image-to-image generation.
   */
  image_url: string | Blob | File;
  /**
   * Optional: Elements (characters/objects) to include in the image for face control.
   */
  elements?: Array<ElementInput>;
  /**
   * Image generation resolution. 1K: standard, 2K: high-res. Default value: `"1K"`
   */
  resolution?: "1K" | "2K";
  /**
   * Number of images to generate (1-9). Default value: `1`
   */
  num_images?: number;
  /**
   * Aspect ratio of generated images. Default value: `"16:9"`
   */
  aspect_ratio?:
    | "16:9"
    | "9:16"
    | "1:1"
    | "4:3"
    | "3:4"
    | "3:2"
    | "2:3"
    | "21:9";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
};
export type KlingImageV3ImageToImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type KlingImageV3TextToImageInput = {
  /**
   * Text prompt for image generation. Max 2500 characters.
   */
  prompt: string;
  /**
   * Negative text prompt. It is recommended to supplement negative prompt information through negative sentences directly within positive prompts.
   */
  negative_prompt?: string;
  /**
   * Optional: Elements (characters/objects) to include in the image for face control. Each element can have a frontal image and optionally reference images.
   */
  elements?: Array<ElementInput>;
  /**
   * Image generation resolution. 1K: standard, 2K: high-res. Default value: `"1K"`
   */
  resolution?: "1K" | "2K";
  /**
   * Number of images to generate (1-9). Default value: `1`
   */
  num_images?: number;
  /**
   * Aspect ratio of generated images. Default value: `"16:9"`
   */
  aspect_ratio?:
    | "16:9"
    | "9:16"
    | "1:1"
    | "4:3"
    | "3:4"
    | "3:2"
    | "2:3"
    | "21:9";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
};
export type KlingImageV3TextToImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type KlingV15KolorsVirtualTryOnInput = {
  /**
   * Url for the human image.
   */
  human_image_url: string | Blob | File;
  /**
   * Url to the garment image.
   */
  garment_image_url: string | Blob | File;
  /**
   * If true, the function will return the image in the response.
   */
  sync_mode?: boolean;
};
export type KlingV15KolorsVirtualTryOnOutput = {
  /**
   * The output image.
   */
  image: Image;
};
export type KlingV1I2VOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingV3ComboElementInput = {
  /**
   * The frontal image of the element (main view).
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  frontal_image_url?: string | Blob | File;
  /**
   * Additional reference images from different angles. 1-3 images supported. At least one image is required.
   */
  reference_image_urls?: Array<string>;
  /**
   * The video URL of the element. A request can only have one element with a video.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url?: string | Blob | File;
};
export type KlingV3ImageElementInput = {
  /**
   * The frontal image of the element (main view).
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  frontal_image_url?: string | Blob | File;
  /**
   * Additional reference images from different angles. 1-3 images supported. At least one image is required.
   */
  reference_image_urls?: Array<string>;
};
export type KlingVideoAiAvatarV2ProInput = {
  /**
   * The URL of the image to use as your avatar
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * The prompt to use for the video generation. Default value: `"."`
   */
  prompt?: string;
};
export type KlingVideoAiAvatarV2ProOutput = {
  /**
   * The generated video
   */
  video: File;
  /**
   * Duration of the output video in seconds.
   */
  duration: number;
};
export type KlingVideoAiAvatarV2StandardInput = {
  /**
   * The URL of the image to use as your avatar
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * The prompt to use for the video generation. Default value: `"."`
   */
  prompt?: string;
};
export type KlingVideoAiAvatarV2StandardOutput = {
  /**
   * The generated video
   */
  video: File;
  /**
   * Duration of the output video in seconds.
   */
  duration: number;
};
export type KlingVideoCreateVoiceInput = {
  /**
   * URL of the voice audio file. Supports .mp3/.wav audio or .mp4/.mov video. Duration must be 5-30 seconds with clean, single-voice audio.
   */
  voice_url: string | Blob | File;
};
export type KlingVideoCreateVoiceOutput = {
  /**
   * Unique identifier for the created voice
   */
  voice_id: string;
};
export type KlingVideoLipsyncAudioToVideoInput = {
  /**
   * The URL of the video to generate the lip sync for. Supports .mp4/.mov, ≤100MB, 2–10s, 720p/1080p only, width/height 720–1920px.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the audio to generate the lip sync for. Minimum duration is 2s and maximum duration is 60s. Maximum file size is 5MB.
   */
  audio_url: string | Blob | File;
};
export type KlingVideoLipsyncAudioToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoLipsyncTextToVideoInput = {
  /**
   * The URL of the video to generate the lip sync for. Supports .mp4/.mov, ≤100MB, 2-60s, 720p/1080p only, width/height 720–1920px. If validation fails, an error is returned.
   */
  video_url: string | Blob | File;
  /**
   * Text content for lip-sync video generation. Max 120 characters.
   */
  text: string;
  /**
   * Voice ID to use for speech synthesis
   */
  voice_id:
    | "genshin_vindi2"
    | "zhinen_xuesheng"
    | "AOT"
    | "ai_shatang"
    | "genshin_klee2"
    | "genshin_kirara"
    | "ai_kaiya"
    | "oversea_male1"
    | "ai_chenjiahao_712"
    | "girlfriend_4_speech02"
    | "chat1_female_new-3"
    | "chat_0407_5-1"
    | "cartoon-boy-07"
    | "uk_boy1"
    | "cartoon-girl-01"
    | "PeppaPig_platform"
    | "ai_huangzhong_712"
    | "ai_huangyaoshi_712"
    | "ai_laoguowang_712"
    | "chengshu_jiejie"
    | "you_pingjing"
    | "calm_story1"
    | "uk_man2"
    | "laopopo_speech02"
    | "heainainai_speech02"
    | "reader_en_m-v1"
    | "commercial_lady_en_f-v1"
    | "tiyuxi_xuedi"
    | "tiexin_nanyou"
    | "girlfriend_1_speech02"
    | "girlfriend_2_speech02"
    | "zhuxi_speech02"
    | "uk_oldman3"
    | "dongbeilaotie_speech02"
    | "chongqingxiaohuo_speech02"
    | "chuanmeizi_speech02"
    | "chaoshandashu_speech02"
    | "ai_taiwan_man2_speech02"
    | "xianzhanggui_speech02"
    | "tianjinjiejie_speech02"
    | "diyinnansang_DB_CN_M_04-v2"
    | "yizhipiannan-v1"
    | "guanxiaofang-v2"
    | "tianmeixuemei-v1"
    | "daopianyansang-v1"
    | "mengwa-v1";
  /**
   * The voice language corresponding to the Voice ID Default value: `"en"`
   */
  voice_language?: "zh" | "en";
  /**
   * Speech rate for Text to Video generation Default value: `1`
   */
  voice_speed?: number;
};
export type KlingVideoLipsyncTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoO1ImageToVideoInput = {
  /**
   * Use @Image1 to reference the start frame, @Image2 to reference the end frame.
   */
  prompt: string;
  /**
   * Image to use as the first frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  start_image_url: string | Blob | File;
  /**
   * Image to use as the last frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  end_image_url?: string | Blob | File;
  /**
   * Video duration in seconds. Default value: `"5"`
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
};
export type KlingVideoO1ImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO1ReferenceToVideoInput = {
  /**
   * Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image).
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image).
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * Video duration in seconds. Default value: `"5"`
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
  /**
   * The aspect ratio of the generated video frame. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
};
export type KlingVideoO1ReferenceToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO1StandardImageToVideoInput = {
  /**
   * Use @Image1 to reference the start frame, @Image2 to reference the end frame.
   */
  prompt: string;
  /**
   * Image to use as the first frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  start_image_url: string | Blob | File;
  /**
   * Image to use as the last frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  end_image_url?: string | Blob | File;
  /**
   * Video duration in seconds. Default value: `"5"`
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
};
export type KlingVideoO1StandardImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO1StandardReferenceToVideoInput = {
  /**
   * Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image).
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image).
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * Video duration in seconds. Default value: `"5"`
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
  /**
   * The aspect ratio of the generated video frame. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
};
export type KlingVideoO1StandardReferenceToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO1StandardVideoToVideoEditInput = {
  /**
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
};
export type KlingVideoO1StandardVideoToVideoEditOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO1StandardVideoToVideoReferenceInput = {
  /**
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Video duration in seconds. Default value: `"5"`
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
};
export type KlingVideoO1StandardVideoToVideoReferenceOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO1VideoToVideoEditInput = {
  /**
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
};
export type KlingVideoO1VideoToVideoEditOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO1VideoToVideoReferenceInput = {
  /**
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Video duration in seconds. Default value: `"5"`
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
};
export type KlingVideoO1VideoToVideoReferenceOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3ProImageToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * URL of the start frame image.
   */
  image_url: string | Blob | File;
  /**
   * URL of the end frame image (optional).
   */
  end_image_url?: string | Blob | File;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type KlingVideoO3ProImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3ProReferenceToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * Image to use as the first frame of the video.
   */
  start_image_url?: string | Blob | File;
  /**
   * Image to use as the last frame of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ComboElementInput>;
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * The aspect ratio of the generated video frame. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
};
export type KlingVideoO3ProReferenceToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3ProTextToVideoInput = {
  /**
   * Text prompt for video generation. Required unless multi_prompt is provided.
   */
  prompt?: string;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice
   */
  voice_ids?: Array<string>;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type KlingVideoO3ProTextToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3ProVideoToVideoEditInput = {
  /**
   * Text prompt for video generation. Reference video as @Video1.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.
   */
  video_url: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Whether to keep the original audio from the reference video. Default value: `true`
   */
  keep_audio?: boolean;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ImageElementInput>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type KlingVideoO3ProVideoToVideoEditOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3ProVideoToVideoReferenceInput = {
  /**
   * Text prompt for video generation. Reference video as @Video1.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.
   */
  video_url: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Whether to keep the original audio from the reference video. Default value: `true`
   */
  keep_audio?: boolean;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ImageElementInput>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * Aspect ratio. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Video duration in seconds (3-15s for reference video).
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
};
export type KlingVideoO3ProVideoToVideoReferenceOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3StandardImageToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * URL of the start frame image.
   */
  image_url: string | Blob | File;
  /**
   * URL of the end frame image (optional).
   */
  end_image_url?: string | Blob | File;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type KlingVideoO3StandardImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3StandardReferenceToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * Image to use as the first frame of the video.
   */
  start_image_url?: string | Blob | File;
  /**
   * Image to use as the last frame of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ComboElementInput>;
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * The aspect ratio of the generated video frame. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
};
export type KlingVideoO3StandardReferenceToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3StandardTextToVideoInput = {
  /**
   * Text prompt for video generation. Required unless multi_prompt is provided.
   */
  prompt?: string;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice
   */
  voice_ids?: Array<string>;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type KlingVideoO3StandardTextToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3StandardVideoToVideoEditInput = {
  /**
   * Text prompt for video generation. Reference video as @Video1.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.
   */
  video_url: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Whether to keep the original audio from the reference video. Default value: `true`
   */
  keep_audio?: boolean;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ImageElementInput>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type KlingVideoO3StandardVideoToVideoEditOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoO3StandardVideoToVideoReferenceInput = {
  /**
   * Text prompt for video generation. Reference video as @Video1.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.
   */
  video_url: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Whether to keep the original audio from the reference video. Default value: `true`
   */
  keep_audio?: boolean;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ImageElementInput>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * Aspect ratio. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Video duration in seconds (3-15s for reference video).
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
};
export type KlingVideoO3StandardVideoToVideoReferenceOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type KlingVideoV15ProEffectsInput = {
  /**
   * URL of images to be used for hug, kiss or heart_gesture video.
   */
  input_image_urls?: Array<string>;
  /**
   * The effect scene to use for the video generation
   */
  effect_scene:
    | "hug"
    | "kiss"
    | "heart_gesture"
    | "squish"
    | "expansion"
    | "fuzzyfuzzy"
    | "bloombloom"
    | "dizzydizzy"
    | "jelly_press"
    | "jelly_slice"
    | "jelly_squish"
    | "jelly_jiggle"
    | "pixelpixel"
    | "yearbook"
    | "instant_film"
    | "anime_figure"
    | "rocketrocket"
    | "fly_fly"
    | "disappear"
    | "lightning_power"
    | "bullet_time"
    | "bullet_time_360"
    | "media_interview"
    | "day_to_night"
    | "let's_ride"
    | "jumpdrop"
    | "swish_swish"
    | "running_man"
    | "jazz_jazz"
    | "swing_swing"
    | "skateskate"
    | "building_sweater"
    | "pure_white_wings"
    | "black_wings"
    | "golden_wing"
    | "pink_pink_wings"
    | "rampage_ape"
    | "a_list_look"
    | "countdown_teleport"
    | "firework_2026"
    | "instant_christmas"
    | "birthday_star"
    | "firework"
    | "celebration"
    | "tiger_hug_pro"
    | "pet_lion_pro"
    | "guardian_spirit"
    | "squeeze_scream"
    | "inner_voice"
    | "memory_alive"
    | "guess_what"
    | "eagle_snatch"
    | "hug_from_past"
    | "instant_kid"
    | "dollar_rain"
    | "cry_cry"
    | "building_collapse"
    | "mushroom"
    | "jesus_hug"
    | "shark_alert"
    | "lie_flat"
    | "polar_bear_hug"
    | "brown_bear_hug"
    | "office_escape_plow"
    | "watermelon_bomb"
    | "boss_coming"
    | "wig_out"
    | "car_explosion"
    | "tiger_hug"
    | "siblings"
    | "construction_worker"
    | "snatched"
    | "felt_felt"
    | "plushcut"
    | "drunk_dance"
    | "drunk_dance_pet"
    | "daoma_dance"
    | "bouncy_dance"
    | "smooth_sailing_dance"
    | "new_year_greeting"
    | "lion_dance"
    | "prosperity"
    | "great_success"
    | "golden_horse_fortune"
    | "red_packet_box"
    | "lucky_horse_year"
    | "lucky_red_packet"
    | "lucky_money_come"
    | "lion_dance_pet"
    | "dumpling_making_pet"
    | "fish_making_pet"
    | "pet_red_packet"
    | "lantern_glow"
    | "expression_challenge"
    | "overdrive"
    | "heart_gesture_dance"
    | "poping"
    | "martial_arts"
    | "running"
    | "nezha"
    | "motorcycle_dance"
    | "subject_3_dance"
    | "ghost_step_dance"
    | "phantom_jewel"
    | "zoom_out"
    | "cheers_2026"
    | "kiss_pro"
    | "fight_pro"
    | "hug_pro"
    | "heart_gesture_pro"
    | "dollar_rain_pro"
    | "pet_bee_pro"
    | "santa_random_surprise"
    | "magic_match_tree"
    | "happy_birthday"
    | "thumbs_up_pro"
    | "surprise_bouquet"
    | "bouquet_drop"
    | "3d_cartoon_1_pro"
    | "glamour_photo_shoot"
    | "box_of_joy"
    | "first_toast_of_the_year"
    | "my_santa_pic"
    | "santa_gift"
    | "steampunk_christmas"
    | "snowglobe"
    | "christmas_photo_shoot"
    | "ornament_crash"
    | "santa_express"
    | "particle_santa_surround"
    | "coronation_of_frost"
    | "spark_in_the_snow"
    | "scarlet_and_snow"
    | "cozy_toon_wrap"
    | "bullet_time_lite"
    | "magic_cloak"
    | "balloon_parade"
    | "jumping_ginger_joy"
    | "c4d_cartoon_pro"
    | "venomous_spider"
    | "throne_of_king"
    | "luminous_elf"
    | "woodland_elf"
    | "japanese_anime_1"
    | "american_comics"
    | "snowboarding"
    | "witch_transform"
    | "vampire_transform"
    | "pumpkin_head_transform"
    | "demon_transform"
    | "mummy_transform"
    | "zombie_transform"
    | "cute_pumpkin_transform"
    | "cute_ghost_transform"
    | "knock_knock_halloween"
    | "halloween_escape"
    | "baseball"
    | "trampoline"
    | "trampoline_night"
    | "pucker_up"
    | "feed_mooncake"
    | "flyer"
    | "dishwasher"
    | "pet_chinese_opera"
    | "magic_fireball"
    | "gallery_ring"
    | "pet_moto_rider"
    | "muscle_pet"
    | "pet_delivery"
    | "mythic_style"
    | "steampunk"
    | "3d_cartoon_2"
    | "pet_chef"
    | "santa_gifts"
    | "santa_hug"
    | "girlfriend"
    | "boyfriend"
    | "heart_gesture_1"
    | "pet_wizard"
    | "smoke_smoke"
    | "gun_shot"
    | "double_gun"
    | "pet_warrior"
    | "long_hair"
    | "pet_dance"
    | "wool_curly"
    | "pet_bee"
    | "marry_me"
    | "piggy_morph"
    | "ski_ski"
    | "magic_broom"
    | "splashsplash"
    | "surfsurf"
    | "fairy_wing"
    | "angel_wing"
    | "dark_wing"
    | "emoji";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
};
export type KlingVideoV15ProEffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV16ProEffectsInput = {
  /**
   * URL of images to be used for hug, kiss or heart_gesture video.
   */
  input_image_urls?: Array<string>;
  /**
   * The effect scene to use for the video generation
   */
  effect_scene:
    | "hug"
    | "kiss"
    | "heart_gesture"
    | "squish"
    | "expansion"
    | "fuzzyfuzzy"
    | "bloombloom"
    | "dizzydizzy"
    | "jelly_press"
    | "jelly_slice"
    | "jelly_squish"
    | "jelly_jiggle"
    | "pixelpixel"
    | "yearbook"
    | "instant_film"
    | "anime_figure"
    | "rocketrocket"
    | "fly_fly"
    | "disappear"
    | "lightning_power"
    | "bullet_time"
    | "bullet_time_360"
    | "media_interview"
    | "day_to_night"
    | "let's_ride"
    | "jumpdrop"
    | "swish_swish"
    | "running_man"
    | "jazz_jazz"
    | "swing_swing"
    | "skateskate"
    | "building_sweater"
    | "pure_white_wings"
    | "black_wings"
    | "golden_wing"
    | "pink_pink_wings"
    | "rampage_ape"
    | "a_list_look"
    | "countdown_teleport"
    | "firework_2026"
    | "instant_christmas"
    | "birthday_star"
    | "firework"
    | "celebration"
    | "tiger_hug_pro"
    | "pet_lion_pro"
    | "guardian_spirit"
    | "squeeze_scream"
    | "inner_voice"
    | "memory_alive"
    | "guess_what"
    | "eagle_snatch"
    | "hug_from_past"
    | "instant_kid"
    | "dollar_rain"
    | "cry_cry"
    | "building_collapse"
    | "mushroom"
    | "jesus_hug"
    | "shark_alert"
    | "lie_flat"
    | "polar_bear_hug"
    | "brown_bear_hug"
    | "office_escape_plow"
    | "watermelon_bomb"
    | "boss_coming"
    | "wig_out"
    | "car_explosion"
    | "tiger_hug"
    | "siblings"
    | "construction_worker"
    | "snatched"
    | "felt_felt"
    | "plushcut"
    | "drunk_dance"
    | "drunk_dance_pet"
    | "daoma_dance"
    | "bouncy_dance"
    | "smooth_sailing_dance"
    | "new_year_greeting"
    | "lion_dance"
    | "prosperity"
    | "great_success"
    | "golden_horse_fortune"
    | "red_packet_box"
    | "lucky_horse_year"
    | "lucky_red_packet"
    | "lucky_money_come"
    | "lion_dance_pet"
    | "dumpling_making_pet"
    | "fish_making_pet"
    | "pet_red_packet"
    | "lantern_glow"
    | "expression_challenge"
    | "overdrive"
    | "heart_gesture_dance"
    | "poping"
    | "martial_arts"
    | "running"
    | "nezha"
    | "motorcycle_dance"
    | "subject_3_dance"
    | "ghost_step_dance"
    | "phantom_jewel"
    | "zoom_out"
    | "cheers_2026"
    | "kiss_pro"
    | "fight_pro"
    | "hug_pro"
    | "heart_gesture_pro"
    | "dollar_rain_pro"
    | "pet_bee_pro"
    | "santa_random_surprise"
    | "magic_match_tree"
    | "happy_birthday"
    | "thumbs_up_pro"
    | "surprise_bouquet"
    | "bouquet_drop"
    | "3d_cartoon_1_pro"
    | "glamour_photo_shoot"
    | "box_of_joy"
    | "first_toast_of_the_year"
    | "my_santa_pic"
    | "santa_gift"
    | "steampunk_christmas"
    | "snowglobe"
    | "christmas_photo_shoot"
    | "ornament_crash"
    | "santa_express"
    | "particle_santa_surround"
    | "coronation_of_frost"
    | "spark_in_the_snow"
    | "scarlet_and_snow"
    | "cozy_toon_wrap"
    | "bullet_time_lite"
    | "magic_cloak"
    | "balloon_parade"
    | "jumping_ginger_joy"
    | "c4d_cartoon_pro"
    | "venomous_spider"
    | "throne_of_king"
    | "luminous_elf"
    | "woodland_elf"
    | "japanese_anime_1"
    | "american_comics"
    | "snowboarding"
    | "witch_transform"
    | "vampire_transform"
    | "pumpkin_head_transform"
    | "demon_transform"
    | "mummy_transform"
    | "zombie_transform"
    | "cute_pumpkin_transform"
    | "cute_ghost_transform"
    | "knock_knock_halloween"
    | "halloween_escape"
    | "baseball"
    | "trampoline"
    | "trampoline_night"
    | "pucker_up"
    | "feed_mooncake"
    | "flyer"
    | "dishwasher"
    | "pet_chinese_opera"
    | "magic_fireball"
    | "gallery_ring"
    | "pet_moto_rider"
    | "muscle_pet"
    | "pet_delivery"
    | "mythic_style"
    | "steampunk"
    | "3d_cartoon_2"
    | "pet_chef"
    | "santa_gifts"
    | "santa_hug"
    | "girlfriend"
    | "boyfriend"
    | "heart_gesture_1"
    | "pet_wizard"
    | "smoke_smoke"
    | "gun_shot"
    | "double_gun"
    | "pet_warrior"
    | "long_hair"
    | "pet_dance"
    | "wool_curly"
    | "pet_bee"
    | "marry_me"
    | "piggy_morph"
    | "ski_ski"
    | "magic_broom"
    | "splashsplash"
    | "surfsurf"
    | "fairy_wing"
    | "angel_wing"
    | "dark_wing"
    | "emoji";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
};
export type KlingVideoV16ProEffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV16ProElementsInput = {
  /**
   *
   */
  prompt: string;
  /**
   * List of image URLs to use for video generation. Supports up to 4 images.
   */
  input_image_urls: Array<string>;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
};
export type KlingVideoV16ProElementsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV16ProImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * URL of the image to be used for the end of the video
   */
  tail_image_url?: string | Blob | File;
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV16ProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV16ProTextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV16ProTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV16StandardEffectsInput = {
  /**
   * URL of images to be used for hug, kiss or heart_gesture video.
   */
  input_image_urls?: Array<string>;
  /**
   * The effect scene to use for the video generation
   */
  effect_scene:
    | "hug"
    | "kiss"
    | "heart_gesture"
    | "squish"
    | "expansion"
    | "fuzzyfuzzy"
    | "bloombloom"
    | "dizzydizzy"
    | "jelly_press"
    | "jelly_slice"
    | "jelly_squish"
    | "jelly_jiggle"
    | "pixelpixel"
    | "yearbook"
    | "instant_film"
    | "anime_figure"
    | "rocketrocket"
    | "fly_fly"
    | "disappear"
    | "lightning_power"
    | "bullet_time"
    | "bullet_time_360"
    | "media_interview"
    | "day_to_night"
    | "let's_ride"
    | "jumpdrop"
    | "swish_swish"
    | "running_man"
    | "jazz_jazz"
    | "swing_swing"
    | "skateskate"
    | "building_sweater"
    | "pure_white_wings"
    | "black_wings"
    | "golden_wing"
    | "pink_pink_wings"
    | "rampage_ape"
    | "a_list_look"
    | "countdown_teleport"
    | "firework_2026"
    | "instant_christmas"
    | "birthday_star"
    | "firework"
    | "celebration"
    | "tiger_hug_pro"
    | "pet_lion_pro"
    | "guardian_spirit"
    | "squeeze_scream"
    | "inner_voice"
    | "memory_alive"
    | "guess_what"
    | "eagle_snatch"
    | "hug_from_past"
    | "instant_kid"
    | "dollar_rain"
    | "cry_cry"
    | "building_collapse"
    | "mushroom"
    | "jesus_hug"
    | "shark_alert"
    | "lie_flat"
    | "polar_bear_hug"
    | "brown_bear_hug"
    | "office_escape_plow"
    | "watermelon_bomb"
    | "boss_coming"
    | "wig_out"
    | "car_explosion"
    | "tiger_hug"
    | "siblings"
    | "construction_worker"
    | "snatched"
    | "felt_felt"
    | "plushcut"
    | "drunk_dance"
    | "drunk_dance_pet"
    | "daoma_dance"
    | "bouncy_dance"
    | "smooth_sailing_dance"
    | "new_year_greeting"
    | "lion_dance"
    | "prosperity"
    | "great_success"
    | "golden_horse_fortune"
    | "red_packet_box"
    | "lucky_horse_year"
    | "lucky_red_packet"
    | "lucky_money_come"
    | "lion_dance_pet"
    | "dumpling_making_pet"
    | "fish_making_pet"
    | "pet_red_packet"
    | "lantern_glow"
    | "expression_challenge"
    | "overdrive"
    | "heart_gesture_dance"
    | "poping"
    | "martial_arts"
    | "running"
    | "nezha"
    | "motorcycle_dance"
    | "subject_3_dance"
    | "ghost_step_dance"
    | "phantom_jewel"
    | "zoom_out"
    | "cheers_2026"
    | "kiss_pro"
    | "fight_pro"
    | "hug_pro"
    | "heart_gesture_pro"
    | "dollar_rain_pro"
    | "pet_bee_pro"
    | "santa_random_surprise"
    | "magic_match_tree"
    | "happy_birthday"
    | "thumbs_up_pro"
    | "surprise_bouquet"
    | "bouquet_drop"
    | "3d_cartoon_1_pro"
    | "glamour_photo_shoot"
    | "box_of_joy"
    | "first_toast_of_the_year"
    | "my_santa_pic"
    | "santa_gift"
    | "steampunk_christmas"
    | "snowglobe"
    | "christmas_photo_shoot"
    | "ornament_crash"
    | "santa_express"
    | "particle_santa_surround"
    | "coronation_of_frost"
    | "spark_in_the_snow"
    | "scarlet_and_snow"
    | "cozy_toon_wrap"
    | "bullet_time_lite"
    | "magic_cloak"
    | "balloon_parade"
    | "jumping_ginger_joy"
    | "c4d_cartoon_pro"
    | "venomous_spider"
    | "throne_of_king"
    | "luminous_elf"
    | "woodland_elf"
    | "japanese_anime_1"
    | "american_comics"
    | "snowboarding"
    | "witch_transform"
    | "vampire_transform"
    | "pumpkin_head_transform"
    | "demon_transform"
    | "mummy_transform"
    | "zombie_transform"
    | "cute_pumpkin_transform"
    | "cute_ghost_transform"
    | "knock_knock_halloween"
    | "halloween_escape"
    | "baseball"
    | "trampoline"
    | "trampoline_night"
    | "pucker_up"
    | "feed_mooncake"
    | "flyer"
    | "dishwasher"
    | "pet_chinese_opera"
    | "magic_fireball"
    | "gallery_ring"
    | "pet_moto_rider"
    | "muscle_pet"
    | "pet_delivery"
    | "mythic_style"
    | "steampunk"
    | "3d_cartoon_2"
    | "pet_chef"
    | "santa_gifts"
    | "santa_hug"
    | "girlfriend"
    | "boyfriend"
    | "heart_gesture_1"
    | "pet_wizard"
    | "smoke_smoke"
    | "gun_shot"
    | "double_gun"
    | "pet_warrior"
    | "long_hair"
    | "pet_dance"
    | "wool_curly"
    | "pet_bee"
    | "marry_me"
    | "piggy_morph"
    | "ski_ski"
    | "magic_broom"
    | "splashsplash"
    | "surfsurf"
    | "fairy_wing"
    | "angel_wing"
    | "dark_wing"
    | "emoji";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
};
export type KlingVideoV16StandardEffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV16StandardElementsInput = {
  /**
   *
   */
  prompt: string;
  /**
   * List of image URLs to use for video generation. Supports up to 4 images.
   */
  input_image_urls: Array<string>;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
};
export type KlingVideoV16StandardElementsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV1ProAiAvatarInput = {
  /**
   * The URL of the image to use as your avatar
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * The prompt to use for the video generation. Default value: `"."`
   */
  prompt?: string;
};
export type KlingVideoV1ProAiAvatarOutput = {
  /**
   * The generated video
   */
  video: File;
  /**
   * Duration of the output video in seconds.
   */
  duration: number;
};
export type KlingVideoV1StandardAiAvatarInput = {
  /**
   * The URL of the image to use as your avatar
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * The prompt to use for the video generation. Default value: `"."`
   */
  prompt?: string;
};
export type KlingVideoV1StandardAiAvatarOutput = {
  /**
   * The generated video
   */
  video: File;
  /**
   * Duration of the output video in seconds.
   */
  duration: number;
};
export type KlingVideoV1StandardEffectsInput = {
  /**
   * URL of images to be used for hug, kiss or heart_gesture video.
   */
  input_image_urls?: Array<string>;
  /**
   * The effect scene to use for the video generation
   */
  effect_scene:
    | "hug"
    | "kiss"
    | "heart_gesture"
    | "squish"
    | "expansion"
    | "fuzzyfuzzy"
    | "bloombloom"
    | "dizzydizzy"
    | "jelly_press"
    | "jelly_slice"
    | "jelly_squish"
    | "jelly_jiggle"
    | "pixelpixel"
    | "yearbook"
    | "instant_film"
    | "anime_figure"
    | "rocketrocket"
    | "fly_fly"
    | "disappear"
    | "lightning_power"
    | "bullet_time"
    | "bullet_time_360"
    | "media_interview"
    | "day_to_night"
    | "let's_ride"
    | "jumpdrop"
    | "swish_swish"
    | "running_man"
    | "jazz_jazz"
    | "swing_swing"
    | "skateskate"
    | "building_sweater"
    | "pure_white_wings"
    | "black_wings"
    | "golden_wing"
    | "pink_pink_wings"
    | "rampage_ape"
    | "a_list_look"
    | "countdown_teleport"
    | "firework_2026"
    | "instant_christmas"
    | "birthday_star"
    | "firework"
    | "celebration"
    | "tiger_hug_pro"
    | "pet_lion_pro"
    | "guardian_spirit"
    | "squeeze_scream"
    | "inner_voice"
    | "memory_alive"
    | "guess_what"
    | "eagle_snatch"
    | "hug_from_past"
    | "instant_kid"
    | "dollar_rain"
    | "cry_cry"
    | "building_collapse"
    | "mushroom"
    | "jesus_hug"
    | "shark_alert"
    | "lie_flat"
    | "polar_bear_hug"
    | "brown_bear_hug"
    | "office_escape_plow"
    | "watermelon_bomb"
    | "boss_coming"
    | "wig_out"
    | "car_explosion"
    | "tiger_hug"
    | "siblings"
    | "construction_worker"
    | "snatched"
    | "felt_felt"
    | "plushcut"
    | "drunk_dance"
    | "drunk_dance_pet"
    | "daoma_dance"
    | "bouncy_dance"
    | "smooth_sailing_dance"
    | "new_year_greeting"
    | "lion_dance"
    | "prosperity"
    | "great_success"
    | "golden_horse_fortune"
    | "red_packet_box"
    | "lucky_horse_year"
    | "lucky_red_packet"
    | "lucky_money_come"
    | "lion_dance_pet"
    | "dumpling_making_pet"
    | "fish_making_pet"
    | "pet_red_packet"
    | "lantern_glow"
    | "expression_challenge"
    | "overdrive"
    | "heart_gesture_dance"
    | "poping"
    | "martial_arts"
    | "running"
    | "nezha"
    | "motorcycle_dance"
    | "subject_3_dance"
    | "ghost_step_dance"
    | "phantom_jewel"
    | "zoom_out"
    | "cheers_2026"
    | "kiss_pro"
    | "fight_pro"
    | "hug_pro"
    | "heart_gesture_pro"
    | "dollar_rain_pro"
    | "pet_bee_pro"
    | "santa_random_surprise"
    | "magic_match_tree"
    | "happy_birthday"
    | "thumbs_up_pro"
    | "surprise_bouquet"
    | "bouquet_drop"
    | "3d_cartoon_1_pro"
    | "glamour_photo_shoot"
    | "box_of_joy"
    | "first_toast_of_the_year"
    | "my_santa_pic"
    | "santa_gift"
    | "steampunk_christmas"
    | "snowglobe"
    | "christmas_photo_shoot"
    | "ornament_crash"
    | "santa_express"
    | "particle_santa_surround"
    | "coronation_of_frost"
    | "spark_in_the_snow"
    | "scarlet_and_snow"
    | "cozy_toon_wrap"
    | "bullet_time_lite"
    | "magic_cloak"
    | "balloon_parade"
    | "jumping_ginger_joy"
    | "c4d_cartoon_pro"
    | "venomous_spider"
    | "throne_of_king"
    | "luminous_elf"
    | "woodland_elf"
    | "japanese_anime_1"
    | "american_comics"
    | "snowboarding"
    | "witch_transform"
    | "vampire_transform"
    | "pumpkin_head_transform"
    | "demon_transform"
    | "mummy_transform"
    | "zombie_transform"
    | "cute_pumpkin_transform"
    | "cute_ghost_transform"
    | "knock_knock_halloween"
    | "halloween_escape"
    | "baseball"
    | "trampoline"
    | "trampoline_night"
    | "pucker_up"
    | "feed_mooncake"
    | "flyer"
    | "dishwasher"
    | "pet_chinese_opera"
    | "magic_fireball"
    | "gallery_ring"
    | "pet_moto_rider"
    | "muscle_pet"
    | "pet_delivery"
    | "mythic_style"
    | "steampunk"
    | "3d_cartoon_2"
    | "pet_chef"
    | "santa_gifts"
    | "santa_hug"
    | "girlfriend"
    | "boyfriend"
    | "heart_gesture_1"
    | "pet_wizard"
    | "smoke_smoke"
    | "gun_shot"
    | "double_gun"
    | "pet_warrior"
    | "long_hair"
    | "pet_dance"
    | "wool_curly"
    | "pet_bee"
    | "marry_me"
    | "piggy_morph"
    | "ski_ski"
    | "magic_broom"
    | "splashsplash"
    | "surfsurf"
    | "fairy_wing"
    | "angel_wing"
    | "dark_wing"
    | "emoji";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
};
export type KlingVideoV1StandardEffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV1TtsInput = {
  /**
   * The text to be converted to speech
   */
  text: string;
  /**
   * The voice ID to use for speech synthesis Default value: `"genshin_vindi2"`
   */
  voice_id?:
    | "genshin_vindi2"
    | "zhinen_xuesheng"
    | "AOT"
    | "ai_shatang"
    | "genshin_klee2"
    | "genshin_kirara"
    | "ai_kaiya"
    | "oversea_male1"
    | "ai_chenjiahao_712"
    | "girlfriend_4_speech02"
    | "chat1_female_new-3"
    | "chat_0407_5-1"
    | "cartoon-boy-07"
    | "uk_boy1"
    | "cartoon-girl-01"
    | "PeppaPig_platform"
    | "ai_huangzhong_712"
    | "ai_huangyaoshi_712"
    | "ai_laoguowang_712"
    | "chengshu_jiejie"
    | "you_pingjing"
    | "calm_story1"
    | "uk_man2"
    | "laopopo_speech02"
    | "heainainai_speech02"
    | "reader_en_m-v1"
    | "commercial_lady_en_f-v1"
    | "tiyuxi_xuedi"
    | "tiexin_nanyou"
    | "girlfriend_1_speech02"
    | "girlfriend_2_speech02"
    | "zhuxi_speech02"
    | "uk_oldman3"
    | "dongbeilaotie_speech02"
    | "chongqingxiaohuo_speech02"
    | "chuanmeizi_speech02"
    | "chaoshandashu_speech02"
    | "ai_taiwan_man2_speech02"
    | "xianzhanggui_speech02"
    | "tianjinjiejie_speech02"
    | "diyinnansang_DB_CN_M_04-v2"
    | "yizhipiannan-v1"
    | "guanxiaofang-v2"
    | "tianmeixuemei-v1"
    | "daopianyansang-v1"
    | "mengwa-v1";
  /**
   * Rate of speech Default value: `1`
   */
  voice_speed?: number;
};
export type KlingVideoV1TtsOutput = {
  /**
   * The generated audio
   */
  audio: File;
};
export type KlingVideoV21MasterImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * URL of the image to be used for the video
   */
  image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV21MasterImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV21MasterTextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV21MasterTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV21ProImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * URL of the image to be used for the video
   */
  image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
  /**
   * URL of the image to be used for the end of the video
   */
  tail_image_url?: string | Blob | File;
};
export type KlingVideoV21ProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV21StandardImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * URL of the image to be used for the video
   */
  image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV21StandardImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV25TurboProImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * URL of the image to be used for the video
   */
  image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
  /**
   * URL of the image to be used for the end of the video
   */
  tail_image_url?: string | Blob | File;
};
export type KlingVideoV25TurboProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV25TurboProTextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV25TurboProTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV25TurboStandardImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * URL of the image to be used for the video
   */
  image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV25TurboStandardImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV26ProImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * URL of the image to be used for the video
   */
  start_image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice
   */
  voice_ids?: Array<string>;
  /**
   * URL of the image to be used for the end of the video
   */
  end_image_url?: string | Blob | File;
};
export type KlingVideoV26ProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV26ProMotionControlInput = {
  /**
   *
   */
  prompt?: string;
  /**
   * Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.
   */
  image_url: string | Blob | File;
  /**
   * Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.
   */
  video_url: string | Blob | File;
  /**
   * Whether to keep the original sound from the reference video. Default value: `true`
   */
  keep_original_sound?: boolean;
  /**
   * Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).
   */
  character_orientation: "image" | "video";
};
export type KlingVideoV26ProMotionControlOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV26ProTextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
  /**
   * Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase. Default value: `true`
   */
  generate_audio?: boolean;
};
export type KlingVideoV26ProTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV26StandardMotionControlInput = {
  /**
   *
   */
  prompt?: string;
  /**
   * Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.
   */
  image_url: string | Blob | File;
  /**
   * Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.
   */
  video_url: string | Blob | File;
  /**
   * Whether to keep the original sound from the reference video. Default value: `true`
   */
  keep_original_sound?: boolean;
  /**
   * Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).
   */
  character_orientation: "image" | "video";
};
export type KlingVideoV26StandardMotionControlOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV2MasterImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * URL of the image to be used for the video
   */
  image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV2MasterImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV2MasterTextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV2MasterTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV3ProImageToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * List of prompts for multi-shot video generation. If provided, divides the video into multiple shots.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * URL of the image to be used for the video
   */
  start_image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * URL of the image to be used for the end of the video
   */
  end_image_url?: string | Blob | File;
  /**
   * Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice
   */
  voice_ids?: Array<string>;
  /**
   * Elements (characters/objects) to include in the video. Each example can either be an image set (frontal + reference images) or a video. Reference in prompt as @Element1, @Element2, etc.
   */
  elements?: Array<KlingV3ComboElementInput>;
  /**
   * The type of multi-shot video generation. Required when multi_prompt is provided. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV3ProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV3ProTextToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * List of prompts for multi-shot video generation. If provided, overrides the single prompt and divides the video into multiple shots with specified prompts and durations.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice
   */
  voice_ids?: Array<string>;
  /**
   * The type of multi-shot video generation Default value: `"customize"`
   */
  shot_type?: "customize" | "intelligent";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV3ProTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV3StandardImageToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * List of prompts for multi-shot video generation. If provided, divides the video into multiple shots.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * URL of the image to be used for the video
   */
  start_image_url: string | Blob | File;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * URL of the image to be used for the end of the video
   */
  end_image_url?: string | Blob | File;
  /**
   * Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice
   */
  voice_ids?: Array<string>;
  /**
   * Elements (characters/objects) to include in the video. Each example can either be an image set (frontal + reference images) or a video. Reference in prompt as @Element1, @Element2, etc.
   */
  elements?: Array<KlingV3ComboElementInput>;
  /**
   * The type of multi-shot video generation. Required when multi_prompt is provided. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV3StandardImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoV3StandardTextToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * List of prompts for multi-shot video generation. If provided, overrides the single prompt and divides the video into multiple shots with specified prompts and durations.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice
   */
  voice_ids?: Array<string>;
  /**
   * The type of multi-shot video generation Default value: `"customize"`
   */
  shot_type?: "customize" | "intelligent";
  /**
   * The aspect ratio of the generated video frame Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   *  Default value: `"blur, distort, and low quality"`
   */
  negative_prompt?: string;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt. Default value: `0.5`
   */
  cfg_scale?: number;
};
export type KlingVideoV3StandardTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type KlingVideoVideoToAudioInput = {
  /**
   * The video URL to extract audio from. Only .mp4/.mov formats are supported. File size does not exceed 100MB. Video duration between 3.0s and 20.0s.
   */
  video_url: string | Blob | File;
  /**
   * Sound effect prompt. Cannot exceed 200 characters. Default value: `"Car tires screech as they accelerate in a drag race"`
   */
  sound_effect_prompt?: string;
  /**
   * Background music prompt. Cannot exceed 200 characters. Default value: `"intense car race"`
   */
  background_music_prompt?: string;
  /**
   * Enable ASMR mode. This mode enhances detailed sound effects and is suitable for highly immersive content scenarios.
   */
  asmr_mode?: boolean;
};
export type KlingVideoVideoToAudioOutput = {
  /**
   * The original video with dubbed audio applied
   */
  video: File;
  /**
   * The extracted/generated audio from the video in MP3 format
   */
  audio: File;
};
export type KokoroAmericanEnglishInput = {
  /**
   *  Default value: `""`
   */
  prompt?: string;
  /**
   * Voice ID for the desired voice. Default value: `"af_heart"`
   */
  voice?:
    | "af_heart"
    | "af_alloy"
    | "af_aoede"
    | "af_bella"
    | "af_jessica"
    | "af_kore"
    | "af_nicole"
    | "af_nova"
    | "af_river"
    | "af_sarah"
    | "af_sky"
    | "am_adam"
    | "am_echo"
    | "am_eric"
    | "am_fenrir"
    | "am_liam"
    | "am_michael"
    | "am_onyx"
    | "am_puck"
    | "am_santa";
  /**
   * Speed of the generated audio. Default is 1.0. Default value: `1`
   */
  speed?: number;
};
export type KokoroAmericanEnglishOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type KokoroBrazilianPortugueseInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Voice ID for the desired voice.
   */
  voice: "pf_dora" | "pm_alex" | "pm_santa";
  /**
   * Speed of the generated audio. Default is 1.0. Default value: `1`
   */
  speed?: number;
};
export type KokoroBrazilianPortugueseOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type KokoroBritishEnglishInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Voice ID for the desired voice.
   */
  voice:
    | "bf_alice"
    | "bf_emma"
    | "bf_isabella"
    | "bf_lily"
    | "bm_daniel"
    | "bm_fable"
    | "bm_george"
    | "bm_lewis";
  /**
   * Speed of the generated audio. Default is 1.0. Default value: `1`
   */
  speed?: number;
};
export type KokoroBritishEnglishOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type KokoroFrenchInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Voice ID for the desired voice.
   */
  voice: "ff_siwis";
  /**
   * Speed of the generated audio. Default is 1.0. Default value: `1`
   */
  speed?: number;
};
export type KokoroFrenchOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type KokoroHindiInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Voice ID for the desired voice.
   */
  voice: "hf_alpha" | "hf_beta" | "hm_omega" | "hm_psi";
  /**
   * Speed of the generated audio. Default is 1.0. Default value: `1`
   */
  speed?: number;
};
export type KokoroHindiOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type KokoroItalianInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Voice ID for the desired voice.
   */
  voice: "if_sara" | "im_nicola";
  /**
   * Speed of the generated audio. Default is 1.0. Default value: `1`
   */
  speed?: number;
};
export type KokoroItalianOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type KokoroJapaneseInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Voice ID for the desired voice.
   */
  voice: "jf_alpha" | "jf_gongitsune" | "jf_nezumi" | "jf_tebukuro" | "jm_kumo";
  /**
   * Speed of the generated audio. Default is 1.0. Default value: `1`
   */
  speed?: number;
};
export type KokoroJapaneseOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type KokoroMandarinChineseInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Voice ID for the desired voice.
   */
  voice:
    | "zf_xiaobei"
    | "zf_xiaoni"
    | "zf_xiaoxiao"
    | "zf_xiaoyi"
    | "zm_yunjian"
    | "zm_yunxi"
    | "zm_yunxia"
    | "zm_yunyang";
  /**
   * Speed of the generated audio. Default is 1.0. Default value: `1`
   */
  speed?: number;
};
export type KokoroMandarinChineseOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type KokoroSpanishInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Voice ID for the desired voice.
   */
  voice: "ef_dora" | "em_alex" | "em_santa";
  /**
   * Speed of the generated audio. Default is 1.0. Default value: `1`
   */
  speed?: number;
};
export type KokoroSpanishOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type KontextEditOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type KontextImg2ImgOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type KontextInpaintOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type KontextT2IOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type KontextText2ImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type KreaOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type KreaReduxOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type KreaWan14bTextToVideoInput = {
  /**
   * Prompt for the video-to-video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be a multiple of 12 plus 6, for example 6, 18, 30, 42, etc. Default value: `78`
   */
  num_frames?: number;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed for the video-to-video generation.
   */
  seed?: number;
};
export type KreaWan14bTextToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
};
export type KreaWan14bVideoToVideoInput = {
  /**
   * Prompt for the video-to-video generation.
   */
  prompt: string;
  /**
   * Denoising strength for the video-to-video generation. 0.0 preserves the original, 1.0 completely remakes the video. Default value: `0.85`
   */
  strength?: number;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of the input video. Currently, only outputs of 16:9 aspect ratio and 480p resolution are supported. Video duration should be less than 1000 frames at 16fps, and output frames will be 6 plus a multiple of 12, for example 18, 30, 42, etc.
   */
  video_url: string | Blob | File;
  /**
   * Seed for the video-to-video generation.
   */
  seed?: number;
};
export type KreaWan14bVideoToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
};
export type latentsyncInput = {
  /**
   * The URL of the video to generate the lip sync for.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the audio to generate the lip sync for.
   */
  audio_url: string | Blob | File;
  /**
   * Guidance scale for the model inference Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Random seed for generation. If None, a random seed will be used.
   */
  seed?: number;
  /**
   * Video loop mode when audio is longer than video. Options: pingpong, loop
   */
  loop_mode?: "pingpong" | "loop";
};
export type latentsyncOutput = {
  /**
   * The generated video with the lip sync.
   */
  video: File;
};
export type LightingRestorationInput = {
  /**
   * The URL of the image to restore lighting for.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
};
export type LightingRestorationOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type LightxRecameraInput = {
  /**
   * URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * Optional text prompt. If omitted, Light-X will auto-caption the video.
   */
  prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Camera control mode. Default value: `"traj"`
   */
  camera?: "traj" | "target";
  /**
   * Camera motion mode. Default value: `"gradual"`
   */
  mode?: "gradual" | "bullet" | "direct" | "dolly-zoom";
  /**
   * Camera trajectory parameters (required for recamera mode).
   */
  trajectory?: TrajectoryParameters;
  /**
   * Target camera pose [theta, phi, radius, x, y] (required when camera='target').
   */
  target_pose?: Array<number>;
};
export type LightxRecameraOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * Optional: normalized/processed input video (if produced by the pipeline).
   */
  input_video?: File;
  /**
   * Optional: visualization/debug video (if produced by the pipeline).
   */
  viz_video?: File;
};
export type LightxRelightInput = {
  /**
   * URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * Optional text prompt. If omitted, Light-X will auto-caption the video.
   */
  prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Relight condition type. Default value: `"ic"`
   */
  relit_cond_type?: "ic" | "ref" | "hdr" | "bg";
  /**
   * Relighting parameters (required for relight_condition_type='ic'). Not used for 'bg' (which expects a background image URL instead).
   */
  relight_parameters?: RelightParameters;
  /**
   * URL of conditioning image. Required for relight_condition_type='ref'/'hdr'. Also required for relight_condition_type='bg' (background image).
   */
  relit_cond_img_url?: string | Blob | File;
  /**
   * Frame index to use as referencen to relight the video with reference.
   */
  ref_id?: number;
};
export type LightxRelightOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * Optional: normalized/processed input video (if produced by the pipeline).
   */
  input_video?: File;
  /**
   * Optional: visualization/debug video (if produced by the pipeline).
   */
  viz_video?: File;
};
export type LipsyncA2VOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type lipsyncInput = {
  /**
   *
   */
  video_url: string | Blob | File;
  /**
   *
   */
  audio_url: string | Blob | File;
};
export type LipSyncInput = {
  /**
   * The model to use for lipsyncing Default value: `"lipsync-1.9.0-beta"`
   */
  model?: "lipsync-1.8.0" | "lipsync-1.7.1" | "lipsync-1.9.0-beta";
  /**
   * URL of the input video
   */
  video_url: string | Blob | File;
  /**
   * URL of the input audio
   */
  audio_url: string | Blob | File;
  /**
   * Lipsync mode when audio and video durations are out of sync. Default value: `"cut_off"`
   */
  sync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
};
export type lipsyncOutput = {
  /**
   *
   */
  video: File;
};
export type LipsyncOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type LipSyncOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type LipSyncV2Input = {
  /**
   * The model to use for lipsyncing. `lipsync-2-pro` will cost roughly 1.67 times as much as `lipsync-2` for the same duration. Default value: `"lipsync-2"`
   */
  model?: "lipsync-2" | "lipsync-2-pro";
  /**
   * URL of the input video
   */
  video_url: string | Blob | File;
  /**
   * URL of the input audio
   */
  audio_url: string | Blob | File;
  /**
   * Lipsync mode when audio and video durations are out of sync. Default value: `"cut_off"`
   */
  sync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
};
export type LipSyncV2Output = {
  /**
   * The generated video
   */
  video: File;
};
export type LipSyncV2ProInput = {
  /**
   * URL of the input video
   */
  video_url: string | Blob | File;
  /**
   * URL of the input audio
   */
  audio_url: string | Blob | File;
  /**
   * Lipsync mode when audio and video durations are out of sync. Default value: `"cut_off"`
   */
  sync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
};
export type LipSyncV2ProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type LiveAvatarInput = {
  /**
   * The URL of the reference image for avatar generation. The character in this image will be animated.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the driving audio file (WAV or MP3). The avatar will be animated to match this audio.
   */
  audio_url: string | Blob | File;
  /**
   * A text prompt describing the scene and character. Helps guide the video generation style and context.
   */
  prompt: string;
  /**
   * Number of video clips to generate. Each clip is approximately 3 seconds. Set higher for longer videos. Default value: `10`
   */
  num_clips?: number;
  /**
   * Number of frames per clip. Must be a multiple of 4. Higher values = smoother but slower generation. Default value: `48`
   */
  frames_per_clip?: number;
  /**
   * Classifier-free guidance scale. Higher values follow the prompt more closely.
   */
  guidance_scale?: number;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * Enable safety checker for content moderation. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Acceleration level for faster video decoding Default value: `"none"`
   */
  acceleration?: "none" | "light" | "regular" | "high";
};
export type LiveAvatarOutput = {
  /**
   * The generated avatar video file with synchronized audio.
   */
  video: VideoFile;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LongcatImageEditInput = {
  /**
   * The prompt to edit the image with.
   */
  prompt: string;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
};
export type LongcatImageEditOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type LongcatImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type LongcatImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type LongcatMultiAvatarImageAudioToVideoInput = {
  /**
   * The URL of the image containing two speakers.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file for person 1 (left side). Default value: `"https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_man.WAV"`
   */
  audio_url_person1?: string;
  /**
   * The URL of the audio file for person 2 (right side). Default value: `"https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_woman.WAV"`
   */
  audio_url_person2?: string;
  /**
   * The prompt to guide the video generation. Default value: `"Two people are having a conversation with natural expressions and movements."`
   */
  prompt?: string;
  /**
   * The negative prompt to avoid in the video generation. Default value: `"Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * Bounding box for person 1. If not provided, defaults to left half of image.
   */
  bbox_person1?: BoundingBox;
  /**
   * Bounding box for person 2. If not provided, defaults to right half of image.
   */
  bbox_person2?: BoundingBox;
  /**
   * How to combine the two audio tracks. 'para' (parallel) plays both simultaneously, 'add' (sequential) plays person 1 first then person 2. Default value: `"para"`
   */
  audio_type?: "para" | "add";
  /**
   * The number of inference steps to use. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The text guidance scale for classifier-free guidance. Default value: `4`
   */
  text_guidance_scale?: number;
  /**
   * The audio guidance scale. Higher values may lead to exaggerated mouth movements. Default value: `4`
   */
  audio_guidance_scale?: number;
  /**
   * Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each. Default value: `1`
   */
  num_segments?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type LongcatMultiAvatarImageAudioToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LongcatSingleAvatarAudioToVideoInput = {
  /**
   * The URL of the audio file to drive the avatar.
   */
  audio_url: string | Blob | File;
  /**
   * The prompt to guide the video generation. Default value: `"A person is talking naturally with natural expressions and movements."`
   */
  prompt?: string;
  /**
   * The negative prompt to avoid in the video generation. Default value: `"Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to use. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The text guidance scale for classifier-free guidance. Default value: `4`
   */
  text_guidance_scale?: number;
  /**
   * The audio guidance scale. Higher values may lead to exaggerated mouth movements. Default value: `4`
   */
  audio_guidance_scale?: number;
  /**
   * Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each. Default value: `1`
   */
  num_segments?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type LongcatSingleAvatarAudioToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LongcatSingleAvatarImageAudioToVideoInput = {
  /**
   * The URL of the image to animate.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file to drive the avatar.
   */
  audio_url: string | Blob | File;
  /**
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * The negative prompt to avoid in the video generation. Default value: `"Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to use. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The text guidance scale for classifier-free guidance. Default value: `4`
   */
  text_guidance_scale?: number;
  /**
   * The audio guidance scale. Higher values may lead to exaggerated mouth movements. Default value: `4`
   */
  audio_guidance_scale?: number;
  /**
   * Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second. Default value: `"480p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each. Default value: `1`
   */
  num_segments?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type LongcatSingleAvatarImageAudioToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LongcatVideoDistilledImageToVideo480pInput = {
  /**
   * The URL of the image to generate a video from.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to guide the video generation. Default value: `"First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k"`
   */
  prompt?: string;
  /**
   * The number of frames to generate. Default value: `162`
   */
  num_frames?: number;
  /**
   * The number of inference steps to use. Default value: `12`
   */
  num_inference_steps?: number;
  /**
   * The frame rate of the generated video. Default value: `15`
   */
  fps?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type LongcatVideoDistilledImageToVideo480pOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video: File;
};
export type LongcatVideoDistilledImageToVideo720pInput = {
  /**
   * The URL of the image to generate a video from.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to guide the video generation. Default value: `"First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k"`
   */
  prompt?: string;
  /**
   * The number of frames to generate. Default value: `162`
   */
  num_frames?: number;
  /**
   * The number of inference steps to use. Default value: `12`
   */
  num_inference_steps?: number;
  /**
   * The number of inference steps to use for refinement. Default value: `12`
   */
  num_refine_inference_steps?: number;
  /**
   * The frame rate of the generated video. Default value: `30`
   */
  fps?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type LongcatVideoDistilledImageToVideo720pOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video: File;
};
export type LongcatVideoDistilledTextToVideo480pInput = {
  /**
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `162`
   */
  num_frames?: number;
  /**
   * The number of inference steps to use. Default value: `12`
   */
  num_inference_steps?: number;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * The frame rate of the generated video. Default value: `15`
   */
  fps?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type LongcatVideoDistilledTextToVideo480pOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video: File;
};
export type LongcatVideoDistilledTextToVideo720pInput = {
  /**
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `162`
   */
  num_frames?: number;
  /**
   * The number of inference steps to use. Default value: `12`
   */
  num_inference_steps?: number;
  /**
   * The number of inference steps to use for refinement. Default value: `12`
   */
  num_refine_inference_steps?: number;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * The frame rate of the generated video. Default value: `30`
   */
  fps?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type LongcatVideoDistilledTextToVideo720pOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video: File;
};
export type LongcatVideoImageToVideo480pInput = {
  /**
   * The URL of the image to generate a video from.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to guide the video generation. Default value: `"First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k"`
   */
  prompt?: string;
  /**
   * The negative prompt to use for the video generation. Default value: `"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * The number of frames to generate. Default value: `162`
   */
  num_frames?: number;
  /**
   * The number of inference steps to use for the video generation. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the video generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The frame rate of the generated video. Default value: `15`
   */
  fps?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The acceleration level to use for the video generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
};
export type LongcatVideoImageToVideo480pOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video: File;
};
export type LongcatVideoImageToVideo720pInput = {
  /**
   * The URL of the image to generate a video from.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to guide the video generation. Default value: `"First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k"`
   */
  prompt?: string;
  /**
   * The negative prompt to use for the video generation. Default value: `"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * The number of frames to generate. Default value: `162`
   */
  num_frames?: number;
  /**
   * The number of inference steps to use for the video generation. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The number of inference steps to use for refinement. Default value: `40`
   */
  num_refine_inference_steps?: number;
  /**
   * The guidance scale to use for the video generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The frame rate of the generated video. Default value: `30`
   */
  fps?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The acceleration level to use for the video generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
};
export type LongcatVideoImageToVideo720pOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video: File;
};
export type LongcatVideoTextToVideo480pInput = {
  /**
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * The negative prompt to use for the video generation. Default value: `"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * The number of frames to generate. Default value: `162`
   */
  num_frames?: number;
  /**
   * The number of inference steps to use for the video generation. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the video generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * The frame rate of the generated video. Default value: `15`
   */
  fps?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The acceleration level to use for the video generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
};
export type LongcatVideoTextToVideo480pOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video: File;
};
export type LongcatVideoTextToVideo720pInput = {
  /**
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * The negative prompt to use for the video generation. Default value: `"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * The number of frames to generate. Default value: `162`
   */
  num_frames?: number;
  /**
   * The number of inference steps to use for the video generation. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The number of inference steps to use for refinement. Default value: `40`
   */
  num_refine_inference_steps?: number;
  /**
   * The guidance scale to use for the video generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * The frame rate of the generated video. Default value: `30`
   */
  fps?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The acceleration level to use for the video generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
};
export type LongcatVideoTextToVideo720pOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video: File;
};
export type LoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
};
export type LoRAInput = {
  /**
   * URL, HuggingFace repo ID (owner/repo) to lora weights.
   */
  path: string;
  /**
   * Scale factor for LoRA application (0.0 to 4.0). Default value: `1`
   */
  scale?: number;
};
export type LoudnormInput = {
  /**
   * URL of the audio file to normalize
   */
  audio_url: string | Blob | File;
  /**
   * Integrated loudness target in LUFS. Default value: `-18`
   */
  integrated_loudness?: number;
  /**
   * Maximum true peak in dBTP. Default value: `-0.1`
   */
  true_peak?: number;
  /**
   * Loudness range target in LU Default value: `7`
   */
  loudness_range?: number;
  /**
   * Offset gain in dB applied before the true-peak limiter
   */
  offset?: number;
  /**
   * Use linear normalization mode (single-pass). If false, uses dynamic mode (two-pass for better quality).
   */
  linear?: boolean;
  /**
   * Treat mono input files as dual-mono for correct EBU R128 measurement on stereo systems
   */
  dual_mono?: boolean;
  /**
   * Return loudness measurement summary with the normalized audio
   */
  print_summary?: boolean;
  /**
   * Measured integrated loudness of input file in LUFS. Required for linear mode.
   */
  measured_i?: number;
  /**
   * Measured loudness range of input file in LU. Required for linear mode.
   */
  measured_lra?: number;
  /**
   * Measured true peak of input file in dBTP. Required for linear mode.
   */
  measured_tp?: number;
  /**
   * Measured threshold of input file in LUFS. Required for linear mode.
   */
  measured_thresh?: number;
};
export type LoudnormOutput = {
  /**
   * Normalized audio file
   */
  audio: File;
  /**
   * Structured loudness measurement summary (if requested)
   */
  summary?: LoudnormSummary;
};
export type Ltx219bAudioToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the audio to generate the video from.
   */
  audio_url: string | Blob | File;
  /**
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_audio_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification. Default value: `1`
   */
  audio_strength?: number;
  /**
   * Whether to preprocess the audio before using it as conditioning. Default value: `true`
   */
  preprocess_audio?: boolean;
};
export type Ltx219bAudioToVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the audio to generate the video from.
   */
  audio_url: string | Blob | File;
  /**
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_audio_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification. Default value: `1`
   */
  audio_strength?: number;
  /**
   * Whether to preprocess the audio before using it as conditioning. Default value: `true`
   */
  preprocess_audio?: boolean;
};
export type Ltx219bAudioToVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bAudioToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledAudioToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the audio to generate the video from.
   */
  audio_url: string | Blob | File;
  /**
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_audio_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification. Default value: `1`
   */
  audio_strength?: number;
  /**
   * Whether to preprocess the audio before using it as conditioning. Default value: `true`
   */
  preprocess_audio?: boolean;
};
export type Ltx219bDistilledAudioToVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the audio to generate the video from.
   */
  audio_url: string | Blob | File;
  /**
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_audio_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification. Default value: `1`
   */
  audio_strength?: number;
  /**
   * Whether to preprocess the audio before using it as conditioning. Default value: `true`
   */
  preprocess_audio?: boolean;
};
export type Ltx219bDistilledAudioToVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledAudioToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledExtendVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to extend.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning. Default value: `"forward"`
   */
  extend_direction?: "forward" | "backward";
  /**
   * The number of frames to use as context for the extension. Default value: `25`
   */
  num_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type Ltx219bDistilledExtendVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to extend.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning. Default value: `"forward"`
   */
  extend_direction?: "forward" | "backward";
  /**
   * The number of frames to use as context for the extension. Default value: `25`
   */
  num_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type Ltx219bDistilledExtendVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledExtendVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledImageToVideoInput = {
  /**
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * The URL of the image to generate the video from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image. Default value: `"forward"`
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type Ltx219bDistilledImageToVideoLoraInput = {
  /**
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * The URL of the image to generate the video from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image. Default value: `"forward"`
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type Ltx219bDistilledImageToVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledTextToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type Ltx219bDistilledTextToVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
};
export type Ltx219bDistilledTextToVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledTextToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledVideoToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to generate the video from.
   */
  video_url: string | Blob | File;
  /**
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_video_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type. Default value: `"none"`
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.) Default value: `"match_preprocessor"`
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA. Default value: `1`
   */
  ic_lora_scale?: number;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type Ltx219bDistilledVideoToVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to generate the video from.
   */
  video_url: string | Blob | File;
  /**
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_video_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type. Default value: `"none"`
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.) Default value: `"match_preprocessor"`
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA. Default value: `1`
   */
  ic_lora_scale?: number;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type Ltx219bDistilledVideoToVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bDistilledVideoToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bExtendVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to extend.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning. Default value: `"forward"`
   */
  extend_direction?: "forward" | "backward";
  /**
   * The number of frames to use as context for the extension. Default value: `25`
   */
  num_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type Ltx219bExtendVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to extend.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning. Default value: `"forward"`
   */
  extend_direction?: "forward" | "backward";
  /**
   * The number of frames to use as context for the extension. Default value: `25`
   */
  num_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type Ltx219bExtendVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bExtendVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bImageToVideoInput = {
  /**
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * The URL of the image to generate the video from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image. Default value: `"forward"`
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type Ltx219bImageToVideoLoraInput = {
  /**
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * The URL of the image to generate the video from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image. Default value: `"forward"`
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type Ltx219bImageToVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bTextToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type Ltx219bTextToVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
};
export type Ltx219bTextToVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bTextToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bVideoToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to generate the video from.
   */
  video_url: string | Blob | File;
  /**
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_video_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type. Default value: `"none"`
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.) Default value: `"match_preprocessor"`
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA. Default value: `1`
   */
  ic_lora_scale?: number;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type Ltx219bVideoToVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to generate the video from.
   */
  video_url: string | Blob | File;
  /**
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_video_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type. Default value: `"none"`
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.) Default value: `"match_preprocessor"`
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA. Default value: `1`
   */
  ic_lora_scale?: number;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type Ltx219bVideoToVideoLoraOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx219bVideoToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type LTX2AudioToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the audio to generate the video from.
   */
  audio_url: string | Blob | File;
  /**
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_audio_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification. Default value: `1`
   */
  audio_strength?: number;
  /**
   * Whether to preprocess the audio before using it as conditioning. Default value: `true`
   */
  preprocess_audio?: boolean;
};
export type LTX2AudioToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type LTX2DistilledAudioToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the audio to generate the video from.
   */
  audio_url: string | Blob | File;
  /**
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_audio_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification. Default value: `1`
   */
  audio_strength?: number;
  /**
   * Whether to preprocess the audio before using it as conditioning. Default value: `true`
   */
  preprocess_audio?: boolean;
};
export type LTX2DistilledExtendVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to extend.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning. Default value: `"forward"`
   */
  extend_direction?: "forward" | "backward";
  /**
   * The number of frames to use as context for the extension. Default value: `25`
   */
  num_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type LTX2DistilledImageToVideoInput = {
  /**
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * The URL of the image to generate the video from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image. Default value: `"forward"`
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type LTX2DistilledRetakeVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to retake.
   */
  video_url: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The frame of the video to begin retaking from Default value: `24`
   */
  start_frame?: number;
  /**
   * The retake mode to use for the retake Default value: `"replace_audio_and_video"`
   */
  retake_mode?: "replace_audio" | "replace_video" | "replace_audio_and_video";
  /**
   * The number of frames to use as start context for the retake. Default value: `25`
   */
  num_start_context_frames?: number;
  /**
   * The number of frames to use as end context for the retake. Default value: `25`
   */
  num_end_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type LTX2DistilledTextToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type LTX2DistilledVideoToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to generate the video from.
   */
  video_url: string | Blob | File;
  /**
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_video_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type. Default value: `"none"`
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.) Default value: `"match_preprocessor"`
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA. Default value: `1`
   */
  ic_lora_scale?: number;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type LTX2ExtendVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to extend.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning. Default value: `"forward"`
   */
  extend_direction?: "forward" | "backward";
  /**
   * The number of frames to use as context for the extension. Default value: `25`
   */
  num_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type LTX2ExtendVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx2ImageToVideoFastInput = {
  /**
   * URL of the image to generate the video from. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to generate the video from
   */
  prompt: string;
  /**
   * The duration of the generated video in seconds. The fast model supports 6-20 seconds. Note: Durations longer than 10 seconds (12, 14, 16, 18, 20) are only supported with 25 FPS and 1080p resolution. Default value: `"6"`
   */
  duration?: "6" | "8" | "10" | "12" | "14" | "16" | "18" | "20";
  /**
   * The resolution of the generated video Default value: `"1080p"`
   */
  resolution?: "1080p" | "1440p" | "2160p";
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9";
  /**
   * The frames per second of the generated video Default value: `"25"`
   */
  fps?: "25" | "50";
  /**
   * Whether to generate audio for the generated video Default value: `true`
   */
  generate_audio?: boolean;
};
export type Ltx2ImageToVideoFastOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
};
export type Ltx2ImageToVideoInput = {
  /**
   * URL of the image to generate the video from. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to generate the video from
   */
  prompt: string;
  /**
   * The duration of the generated video in seconds Default value: `"6"`
   */
  duration?: "6" | "8" | "10";
  /**
   * The resolution of the generated video Default value: `"1080p"`
   */
  resolution?: "1080p" | "1440p" | "2160p";
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9";
  /**
   * The frames per second of the generated video Default value: `"25"`
   */
  fps?: "25" | "50";
  /**
   * Whether to generate audio for the generated video Default value: `true`
   */
  generate_audio?: boolean;
};
export type LTX2ImageToVideoInput = {
  /**
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * The URL of the image to generate the video from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image. Default value: `"forward"`
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type Ltx2ImageToVideoOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
};
export type LTX2ImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type LTX2LoRAAudioToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the audio to generate the video from.
   */
  audio_url: string | Blob | File;
  /**
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_audio_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification. Default value: `1`
   */
  audio_strength?: number;
  /**
   * Whether to preprocess the audio before using it as conditioning. Default value: `true`
   */
  preprocess_audio?: boolean;
};
export type LTX2LoRADistilledAudioToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the audio to generate the video from.
   */
  audio_url: string | Blob | File;
  /**
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_audio_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification. Default value: `1`
   */
  audio_strength?: number;
  /**
   * Whether to preprocess the audio before using it as conditioning. Default value: `true`
   */
  preprocess_audio?: boolean;
};
export type LTX2LoRADistilledExtendVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to extend.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning. Default value: `"forward"`
   */
  extend_direction?: "forward" | "backward";
  /**
   * The number of frames to use as context for the extension. Default value: `25`
   */
  num_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type LTX2LoRADistilledImageToVideoInput = {
  /**
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * The URL of the image to generate the video from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image. Default value: `"forward"`
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type LTX2LoRADistilledRetakeVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to retake.
   */
  video_url: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The frame of the video to begin retaking from Default value: `24`
   */
  start_frame?: number;
  /**
   * The retake mode to use for the retake Default value: `"replace_audio_and_video"`
   */
  retake_mode?: "replace_audio" | "replace_video" | "replace_audio_and_video";
  /**
   * The number of frames to use as start context for the retake. Default value: `25`
   */
  num_start_context_frames?: number;
  /**
   * The number of frames to use as end context for the retake. Default value: `25`
   */
  num_end_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type LTX2LoRADistilledTextToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
};
export type LTX2LoRADistilledVideoToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to generate the video from.
   */
  video_url: string | Blob | File;
  /**
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_video_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The acceleration level to use. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type. Default value: `"none"`
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.) Default value: `"match_preprocessor"`
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA. Default value: `1`
   */
  ic_lora_scale?: number;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type LTX2LoRAExtendVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to extend.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning. Default value: `"forward"`
   */
  extend_direction?: "forward" | "backward";
  /**
   * The number of frames to use as context for the extension. Default value: `25`
   */
  num_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type LTX2LoRAImageToVideoInput = {
  /**
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * The URL of the image to generate the video from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image. Default value: `"forward"`
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
};
export type LTX2LoRATextToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
};
export type LTX2LoRAVideoToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to generate the video from.
   */
  video_url: string | Blob | File;
  /**
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_video_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRAInput>;
  /**
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type. Default value: `"none"`
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.) Default value: `"match_preprocessor"`
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA. Default value: `1`
   */
  ic_lora_scale?: number;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type Ltx2RetakeVideoInput = {
  /**
   * The URL of the video to retake
   */
  video_url: string | Blob | File;
  /**
   * The prompt to retake the video with
   */
  prompt: string;
  /**
   * The start time of the video to retake in seconds
   */
  start_time?: number;
  /**
   * The duration of the video to retake in seconds Default value: `5`
   */
  duration?: number;
  /**
   * The retake mode to use for the retake Default value: `"replace_audio_and_video"`
   */
  retake_mode?: "replace_audio" | "replace_video" | "replace_audio_and_video";
};
export type LTX2RetakeVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to retake.
   */
  video_url: string | Blob | File;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The frame of the video to begin retaking from Default value: `24`
   */
  start_frame?: number;
  /**
   * The retake mode to use for the retake Default value: `"replace_audio_and_video"`
   */
  retake_mode?: "replace_audio" | "replace_video" | "replace_audio_and_video";
  /**
   * The number of frames to use as start context for the retake. Default value: `25`
   */
  num_start_context_frames?: number;
  /**
   * The number of frames to use as end context for the retake. Default value: `25`
   */
  num_end_context_frames?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type Ltx2RetakeVideoOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
};
export type Ltx2TextToVideoFastInput = {
  /**
   * The prompt to generate the video from
   */
  prompt: string;
  /**
   * The duration of the generated video in seconds. The fast model supports 6-20 seconds. Note: Durations longer than 10 seconds (12, 14, 16, 18, 20) are only supported with 25 FPS and 1080p resolution. Default value: `"6"`
   */
  duration?: "6" | "8" | "10" | "12" | "14" | "16" | "18" | "20";
  /**
   * The resolution of the generated video Default value: `"1080p"`
   */
  resolution?: "1080p" | "1440p" | "2160p";
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9";
  /**
   * The frames per second of the generated video Default value: `"25"`
   */
  fps?: "25" | "50";
  /**
   * Whether to generate audio for the generated video Default value: `true`
   */
  generate_audio?: boolean;
};
export type Ltx2TextToVideoFastOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
};
export type Ltx2TextToVideoInput = {
  /**
   * The prompt to generate the video from
   */
  prompt: string;
  /**
   * The duration of the generated video in seconds Default value: `"6"`
   */
  duration?: "6" | "8" | "10";
  /**
   * The resolution of the generated video Default value: `"1080p"`
   */
  resolution?: "1080p" | "1440p" | "2160p";
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9";
  /**
   * The frames per second of the generated video Default value: `"25"`
   */
  fps?: "25" | "50";
  /**
   * Whether to generate audio for the generated video Default value: `true`
   */
  generate_audio?: boolean;
};
export type LTX2TextToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `landscape_4_3`
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type Ltx2TextToVideoOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
};
export type LTX2TextToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx2V2vTrainerInput = {
  /**
   * URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
   *
   * **Supported video formats:** .mp4, .mov, .avi, .mkv
   * **Supported image formats:** .png, .jpg, .jpeg
   *
   * Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
   *
   * The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
   */
  training_data_url: string | Blob | File;
  /**
   * The rank of the LoRA adaptation. Higher values increase capacity but use more memory. Default value: `"32"`
   */
  rank?: "8" | "16" | "32" | "64" | "128";
  /**
   * The number of training steps. Default value: `2000`
   */
  number_of_steps?: number;
  /**
   * Learning rate for optimization. Higher values can lead to faster training but may cause overfitting. Default value: `0.0002`
   */
  learning_rate?: number;
  /**
   * Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97). Default value: `89`
   */
  number_of_frames?: number;
  /**
   * Target frames per second for the video. Default value: `25`
   */
  frame_rate?: number;
  /**
   * Resolution to use for training. Higher resolutions require more memory. Default value: `"medium"`
   */
  resolution?: "low" | "medium" | "high";
  /**
   * Aspect ratio to use for training. Default value: `"1:1"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * A phrase that will trigger the LoRA style. Will be prepended to captions during training. Default value: `""`
   */
  trigger_phrase?: string;
  /**
   * If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
   */
  auto_scale_input?: boolean;
  /**
   * If true, videos above a certain duration threshold will be split into scenes. Default value: `true`
   */
  split_input_into_scenes?: boolean;
  /**
   * The duration threshold in seconds. If a video is longer than this, it will be split into scenes. Default value: `30`
   */
  split_input_duration_threshold?: number;
  /**
   * Probability of conditioning on the first frame during training. Lower values work better for video-to-video transformation. Default value: `0.1`
   */
  first_frame_conditioning_p?: number;
  /**
   * A list of validation inputs with prompts and reference videos.
   */
  validation?: Array<V2VValidation>;
  /**
   * A negative prompt to use for validation. Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  validation_negative_prompt?: string;
  /**
   * The number of frames in validation videos. Default value: `89`
   */
  validation_number_of_frames?: number;
  /**
   * Target frames per second for validation videos. Default value: `25`
   */
  validation_frame_rate?: number;
  /**
   * The resolution to use for validation. Default value: `"high"`
   */
  validation_resolution?: "low" | "medium" | "high";
  /**
   * The aspect ratio to use for validation. Default value: `"1:1"`
   */
  validation_aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0. Default value: `1`
   */
  stg_scale?: number;
};
export type Ltx2V2vTrainerOutput = {
  /**
   * The URL to the validation videos (with reference videos side-by-side), if any.
   */
  video: File;
  /**
   * URL to the trained IC-LoRA weights (.safetensors).
   */
  lora_file: File;
  /**
   * Configuration used for setting up inference endpoints.
   */
  config_file: File;
  /**
   * URL to the debug dataset archive containing decoded videos.
   */
  debug_dataset?: File;
};
export type LTX2VideoToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The URL of the video to generate the video from.
   */
  video_url: string | Blob | File;
  /**
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | Blob | File;
  /**
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames. Default value: `true`
   */
  match_video_length?: boolean;
  /**
   * The number of frames to generate. Default value: `121`
   */
  num_frames?: number;
  /**
   * The size of the generated video. Default value: `auto`
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details. Default value: `true`
   */
  use_multiscale?: boolean;
  /**
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The frames per second of the generated video. Default value: `25`
   */
  fps?: number;
  /**
   * The guidance scale to use. Default value: `3`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to use. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `"none"`
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera. Default value: `1`
   */
  camera_lora_scale?: number;
  /**
   * The negative prompt to generate the video from. Default value: `"blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts."`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to enable prompt expansion. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The output type of the generated video. Default value: `"X264 (.mp4)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type. Default value: `"none"`
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.) Default value: `"match_preprocessor"`
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA. Default value: `1`
   */
  ic_lora_scale?: number;
  /**
   * The strength of the image to use for the video generation. Default value: `1`
   */
  image_strength?: number;
  /**
   * The strength of the end image to use for the video generation. Default value: `1`
   */
  end_image_strength?: number;
  /**
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content. Default value: `1`
   */
  video_strength?: number;
  /**
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content. Default value: `1`
   */
  audio_strength?: number;
};
export type LTX2VideoToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
  /**
   * The seed used for the random number generator.
   */
  seed: number;
  /**
   * The prompt used for the generation.
   */
  prompt: string;
};
export type Ltx2VideoTrainerInput = {
  /**
   * URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
   *
   * **Supported video formats:** .mp4, .mov, .avi, .mkv
   * **Supported image formats:** .png, .jpg, .jpeg
   *
   * Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
   *
   * The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
   */
  training_data_url: string | Blob | File;
  /**
   * The rank of the LoRA adaptation. Higher values increase capacity but use more memory. Default value: `"32"`
   */
  rank?: "8" | "16" | "32" | "64" | "128";
  /**
   * The number of training steps. Default value: `2000`
   */
  number_of_steps?: number;
  /**
   * Learning rate for optimization. Higher values can lead to faster training but may cause overfitting. Default value: `0.0002`
   */
  learning_rate?: number;
  /**
   * Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97). Default value: `89`
   */
  number_of_frames?: number;
  /**
   * Target frames per second for the video. Default value: `25`
   */
  frame_rate?: number;
  /**
   * Resolution to use for training. Higher resolutions require more memory. Default value: `"medium"`
   */
  resolution?: "low" | "medium" | "high";
  /**
   * Aspect ratio to use for training. Default value: `"1:1"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * A phrase that will trigger the LoRA style. Will be prepended to captions during training. Default value: `""`
   */
  trigger_phrase?: string;
  /**
   * If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
   */
  auto_scale_input?: boolean;
  /**
   * If true, videos above a certain duration threshold will be split into scenes. Default value: `true`
   */
  split_input_into_scenes?: boolean;
  /**
   * The duration threshold in seconds. If a video is longer than this, it will be split into scenes. Default value: `30`
   */
  split_input_duration_threshold?: number;
  /**
   * Enable joint audio-video training. If None (default), automatically detects whether input videos have audio. Set to True to force audio training, or False to disable.
   */
  with_audio?: boolean;
  /**
   * Normalize audio peak amplitude to a consistent level. Recommended for consistent audio levels across the dataset. Default value: `true`
   */
  audio_normalize?: boolean;
  /**
   * When audio duration doesn't match video duration, stretch/compress audio without changing pitch. If disabled, audio is trimmed or padded with silence. Default value: `true`
   */
  audio_preserve_pitch?: boolean;
  /**
   * Probability of conditioning on the first frame during training. Higher values improve image-to-video performance. Default value: `0.5`
   */
  first_frame_conditioning_p?: number;
  /**
   * A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.
   */
  validation?: Array<Validation>;
  /**
   * A negative prompt to use for validation. Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  validation_negative_prompt?: string;
  /**
   * The number of frames in validation videos. Default value: `89`
   */
  validation_number_of_frames?: number;
  /**
   * Target frames per second for validation videos. Default value: `25`
   */
  validation_frame_rate?: number;
  /**
   * The resolution to use for validation. Default value: `"high"`
   */
  validation_resolution?: "low" | "medium" | "high";
  /**
   * The aspect ratio to use for validation. Default value: `"1:1"`
   */
  validation_aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0. Default value: `1`
   */
  stg_scale?: number;
  /**
   * Whether to generate audio in validation samples. Default value: `true`
   */
  generate_audio_in_validation?: boolean;
};
export type Ltx2VideoTrainerOutput = {
  /**
   * The URL to the validation videos, if any.
   */
  video: File;
  /**
   * URL to the trained LoRA weights (.safetensors).
   */
  lora_file: File;
  /**
   * Configuration used for setting up inference endpoints.
   */
  config_file: File;
  /**
   * URL to the debug dataset archive containing decoded videos and audio.
   */
  debug_dataset?: File;
};
export type Ltxv13b098DistilledExtendInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `24`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean;
  /**
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution. Default value: `0.5`
   */
  temporal_adain_factor?: number;
  /**
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `29`
   */
  constant_rate_factor?: number;
  /**
   * Video to be extended.
   */
  video: ExtendVideoConditioningInput;
};
export type Ltxv13b098DistilledExtendOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type Ltxv13b098DistilledImageToVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `24`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean;
  /**
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution. Default value: `0.5`
   */
  temporal_adain_factor?: number;
  /**
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `29`
   */
  constant_rate_factor?: number;
  /**
   * Image URL for Image-to-Video task
   */
  image_url: string | Blob | File;
};
export type Ltxv13b098DistilledImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type Ltxv13b098DistilledInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `24`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean;
  /**
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution. Default value: `0.5`
   */
  temporal_adain_factor?: number;
  /**
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number;
};
export type Ltxv13b098DistilledMulticonditioningInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `24`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean;
  /**
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution. Default value: `0.5`
   */
  temporal_adain_factor?: number;
  /**
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `29`
   */
  constant_rate_factor?: number;
  /**
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
};
export type Ltxv13b098DistilledMulticonditioningOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type Ltxv13b098DistilledOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideo13bDevExtendInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `30`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `3`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `30`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `17`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`
   */
  constant_rate_factor?: number;
  /**
   * Video to be extended.
   */
  video: VideoConditioningInput;
};
export type LtxVideo13bDevExtendOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideo13bDevImageToVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `30`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `3`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `30`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `17`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Image URL for Image-to-Video task
   */
  image_url: string | Blob | File;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`
   */
  constant_rate_factor?: number;
};
export type LtxVideo13bDevImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideo13bDevInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9, 1:1 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `30`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `3`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `30`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `17`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type LtxVideo13bDevMulticonditioningInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `30`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `3`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `30`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `17`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`
   */
  constant_rate_factor?: number;
  /**
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
};
export type LtxVideo13bDevMulticonditioningOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideo13bDevOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideo13bDistilledExtendInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`
   */
  constant_rate_factor?: number;
  /**
   * Video to be extended.
   */
  video: VideoConditioningInput;
};
export type LtxVideo13bDistilledExtendOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideo13bDistilledImageToVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`
   */
  constant_rate_factor?: number;
  /**
   * Image URL for Image-to-Video task
   */
  image_url: string | Blob | File;
};
export type LtxVideo13bDistilledImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideo13bDistilledInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9, 1:1 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type LtxVideo13bDistilledMulticonditioningInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * LoRA weights to use for generation
   */
  loras?: Array<LoRAWeight>;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The number of frames in the video. Default value: `121`
   */
  num_frames?: number;
  /**
   * Number of inference steps during the first pass. Default value: `8`
   */
  first_pass_num_inference_steps?: number;
  /**
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`
   */
  first_pass_skip_final_steps?: number;
  /**
   * Number of inference steps during the second pass. Default value: `8`
   */
  second_pass_num_inference_steps?: number;
  /**
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`
   */
  second_pass_skip_initial_steps?: number;
  /**
   * The frame rate of the video. Default value: `30`
   */
  frame_rate?: number;
  /**
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`
   */
  constant_rate_factor?: number;
  /**
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
};
export type LtxVideo13bDistilledMulticonditioningOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideo13bDistilledOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideoLoraImageToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Default value: `"blurry, low quality, low resolution, inconsistent motion, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * The LoRA weights to use for generation.
   */
  loras?: Array<LoRAWeight>;
  /**
   * The resolution of the video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16" | "auto";
  /**
   * The number of frames in the video. Default value: `89`
   */
  number_of_frames?: number;
  /**
   * The number of inference steps to use. Default value: `30`
   */
  number_of_steps?: number;
  /**
   * The frame rate of the video. Default value: `25`
   */
  frame_rate?: number;
  /**
   * The seed to use for generation.
   */
  seed?: number;
  /**
   * Whether to expand the prompt using the LLM.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The URL of the image to use as input.
   */
  image_url: string | Blob | File;
};
export type LtxVideoLoraImageToVideoOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video.
   */
  video: File;
};
export type LtxVideoLoraInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Default value: `"blurry, low quality, low resolution, inconsistent motion, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * The LoRA weights to use for generation.
   */
  loras?: Array<LoRAWeight>;
  /**
   * The resolution of the video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * The number of frames in the video. Default value: `89`
   */
  number_of_frames?: number;
  /**
   * The number of inference steps to use. Default value: `30`
   */
  number_of_steps?: number;
  /**
   * The frame rate of the video. Default value: `25`
   */
  frame_rate?: number;
  /**
   * The seed to use for generation.
   */
  seed?: number;
  /**
   * Whether to expand the prompt using the LLM.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type LtxVideoLoraMulticonditioningInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Default value: `"blurry, low quality, low resolution, inconsistent motion, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * The LoRA weights to use for generation.
   */
  loras?: Array<LoRAWeight>;
  /**
   * The resolution of the video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16" | "auto";
  /**
   * The number of frames in the video. Default value: `89`
   */
  number_of_frames?: number;
  /**
   * The number of inference steps to use. Default value: `30`
   */
  number_of_steps?: number;
  /**
   * The frame rate of the video. Default value: `25`
   */
  frame_rate?: number;
  /**
   * The seed to use for generation.
   */
  seed?: number;
  /**
   * Whether to expand the prompt using the LLM.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The image conditions to use for generation.
   */
  images?: Array<ImageCondition>;
  /**
   * The video conditions to use for generation.
   */
  videos?: Array<VideoCondition>;
};
export type LtxVideoLoraMulticonditioningOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video.
   */
  video: File;
};
export type LtxVideoLoraOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video.
   */
  video: File;
};
export type LtxVideoTrainerInput = {
  /**
   * URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
   *
   * **Supported video formats:** .mp4, .mov, .avi, .mkv
   * **Supported image formats:** .png, .jpg, .jpeg
   *
   * Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
   *
   * The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
   */
  training_data_url: string | Blob | File;
  /**
   * The rank of the LoRA. Default value: `"128"`
   */
  rank?: "8" | "16" | "32" | "64" | "128";
  /**
   * The number of steps to train for. Default value: `1000`
   */
  number_of_steps?: number;
  /**
   * The number of frames to use for training. This is the number of frames per second multiplied by the number of seconds. Default value: `81`
   */
  number_of_frames?: number;
  /**
   * The target frames per second for the video. Default value: `25`
   */
  frame_rate?: number;
  /**
   * The resolution to use for training. This is the resolution of the video. Default value: `"medium"`
   */
  resolution?: "low" | "medium" | "high";
  /**
   * The aspect ratio to use for training. This is the aspect ratio of the video. Default value: `"1:1"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`
   */
  learning_rate?: number;
  /**
   * The phrase that will trigger the model to generate an image. Default value: `""`
   */
  trigger_phrase?: string;
  /**
   * If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
   */
  auto_scale_input?: boolean;
  /**
   * If true, videos above a certain duration threshold will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned. This option has no effect on image datasets. Default value: `true`
   */
  split_input_into_scenes?: boolean;
  /**
   * The duration threshold in seconds. If a video is longer than this, it will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned. Default value: `30`
   */
  split_input_duration_threshold?: number;
  /**
   * A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.
   */
  validation?: Array<Validation>;
  /**
   * A negative prompt to use for validation. Default value: `"blurry, low quality, bad quality, out of focus"`
   */
  validation_negative_prompt?: string;
  /**
   * The number of frames to use for validation. Default value: `81`
   */
  validation_number_of_frames?: number;
  /**
   * The resolution to use for validation. Default value: `"high"`
   */
  validation_resolution?: "low" | "medium" | "high";
  /**
   * The aspect ratio to use for validation. Default value: `"1:1"`
   */
  validation_aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * If true, the validation videos will be reversed. This is useful for effects that are learned in reverse and then applied in reverse.
   */
  validation_reverse?: boolean;
};
export type LtxVideoTrainerOutput = {
  /**
   * The URL to the validations video.
   */
  video: File;
  /**
   * URL to the trained LoRA weights.
   */
  lora_file: File;
  /**
   * Configuration used for setting up the inference endpoints.
   */
  config_file: File;
};
export type LtxVideoV095ExtendInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Video to be extended.
   */
  video: VideoConditioningInput;
};
export type LtxVideoV095ExtendOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideoV095ImageToVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Image URL for Image-to-Video task
   */
  image_url: string | Blob | File;
};
export type LtxVideoV095ImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideoV095Input = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
};
export type LtxVideoV095MulticonditioningInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
};
export type LtxVideoV095MulticonditioningOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideoV095Output = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideoV097ExtendInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Video to be extended.
   */
  video: VideoConditioningInput;
};
export type LtxVideoV097ExtendOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideoV097ImageToVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Image URL for Image-to-Video task
   */
  image_url: string | Blob | File;
};
export type LtxVideoV097ImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideoV097Input = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
};
export type LtxVideoV097MulticonditioningInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
};
export type LtxVideoV097MulticonditioningOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type LtxVideoV097Output = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type lucidfluxInput = {
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The width of the output image. Default value: `1024`
   */
  target_width?: number;
  /**
   * The height of the output image. Default value: `1024`
   */
  target_height?: number;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The guidance to use for the diffusion process. Default value: `4`
   */
  guidance?: number;
  /**
   * Seed used for random number generation Default value: `42`
   */
  seed?: number;
};
export type lucidfluxOutput = {
  /**
   * Generated image
   */
  image: Image;
  /**
   * Seed used for random number generation
   */
  seed: number;
};
export type Lucy14bImageToVideoInput = {
  /**
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * Resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * If set to true, the function will wait for the image to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the image directly
   * in the response without going through the CDN. Default value: `true`
   */
  sync_mode?: boolean;
};
export type Lucy14bImageToVideoOutput = {
  /**
   * The generated MP4 video with H.264 encoding
   */
  video: File;
};
export type LucyEditDevInput = {
  /**
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * URL of the video to edit
   */
  video_url: string | Blob | File;
  /**
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN. Default value: `true`
   */
  sync_mode?: boolean;
  /**
   * Whether to enhance the prompt for better results. Default value: `true`
   */
  enhance_prompt?: boolean;
};
export type LucyEditDevOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type LucyEditFastInput = {
  /**
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * URL of the video to edit
   */
  video_url: string | Blob | File;
  /**
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * Whether to enhance the prompt for better results. Default value: `true`
   */
  enhance_prompt?: boolean;
};
export type LucyEditFastOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type LucyEditProInput = {
  /**
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * URL of the video to edit
   */
  video_url: string | Blob | File;
  /**
   * Resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p";
  /**
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN. Default value: `true`
   */
  sync_mode?: boolean;
  /**
   * Whether to enhance the prompt for better results. Default value: `true`
   */
  enhance_prompt?: boolean;
};
export type LucyEditProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type LucyRestyleInput = {
  /**
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * URL of the video to edit
   */
  video_url: string | Blob | File;
  /**
   * Seed for video generation
   */
  seed?: number;
  /**
   * Resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p";
  /**
   * Whether to enhance the prompt for better results. Default value: `true`
   */
  enhance_prompt?: boolean;
  /**
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   */
  sync_mode?: boolean;
};
export type LucyRestyleOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type LumaDreamMachineRay2FlashImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Initial image to start the video from. Can be used together with end_image_url.
   */
  image_url?: string | Blob | File;
  /**
   * Final image to end the video with. Can be used together with image_url.
   */
  end_image_url?: string | Blob | File;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Whether the video should loop (end of video is blended with the beginning)
   */
  loop?: boolean;
  /**
   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `"540p"`
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video Default value: `"5s"`
   */
  duration?: "5s" | "9s";
};
export type LumaDreamMachineRay2FlashImageToVideoOutput = {
  /**
   * URL of the generated video
   */
  video: File;
};
export type LumaDreamMachineRay2FlashInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Whether the video should loop (end of video is blended with the beginning)
   */
  loop?: boolean;
  /**
   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `"540p"`
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video (9s costs 2x more) Default value: `"5s"`
   */
  duration?: "5s" | "9s";
};
export type LumaDreamMachineRay2FlashModifyInput = {
  /**
   * URL of the input video to modify
   */
  video_url: string | Blob | File;
  /**
   * Optional URL of the first frame image for modification
   */
  image_url?: string | Blob | File;
  /**
   * Instruction for modifying the video
   */
  prompt?: string;
  /**
   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most Default value: `"flex_1"`
   */
  mode?:
    | "adhere_1"
    | "adhere_2"
    | "adhere_3"
    | "flex_1"
    | "flex_2"
    | "flex_3"
    | "reimagine_1"
    | "reimagine_2"
    | "reimagine_3";
};
export type LumaDreamMachineRay2FlashModifyOutput = {
  /**
   * URL of the modified video
   */
  video: File;
};
export type LumaDreamMachineRay2FlashOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type LumaDreamMachineRay2FlashReframeInput = {
  /**
   * URL of the input video to reframe
   */
  video_url: string | Blob | File;
  /**
   * The aspect ratio of the reframed video
   */
  aspect_ratio: "1:1" | "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Optional URL of the first frame image for reframing
   */
  image_url?: string | Blob | File;
  /**
   * X position of the grid for reframing
   */
  grid_position_x?: number;
  /**
   * Y position of the grid for reframing
   */
  grid_position_y?: number;
  /**
   * Optional prompt for reframing
   */
  prompt?: string;
  /**
   * End X coordinate for reframing
   */
  x_end?: number;
  /**
   * Start X coordinate for reframing
   */
  x_start?: number;
  /**
   * End Y coordinate for reframing
   */
  y_end?: number;
  /**
   * Start Y coordinate for reframing
   */
  y_start?: number;
};
export type LumaDreamMachineRay2FlashReframeOutput = {
  /**
   * URL of the reframed video
   */
  video: File;
};
export type LumaDreamMachineRay2ImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Initial image to start the video from. Can be used together with end_image_url.
   */
  image_url?: string | Blob | File;
  /**
   * Final image to end the video with. Can be used together with image_url.
   */
  end_image_url?: string | Blob | File;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Whether the video should loop (end of video is blended with the beginning)
   */
  loop?: boolean;
  /**
   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `"540p"`
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video Default value: `"5s"`
   */
  duration?: "5s" | "9s";
};
export type LumaDreamMachineRay2ImageToVideoOutput = {
  /**
   * URL of the generated video
   */
  video: File;
};
export type LumaDreamMachineRay2Input = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Whether the video should loop (end of video is blended with the beginning)
   */
  loop?: boolean;
  /**
   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `"540p"`
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video (9s costs 2x more) Default value: `"5s"`
   */
  duration?: "5s" | "9s";
};
export type LumaDreamMachineRay2ModifyInput = {
  /**
   * URL of the input video to modify
   */
  video_url: string | Blob | File;
  /**
   * Optional URL of the first frame image for modification
   */
  image_url?: string | Blob | File;
  /**
   * Instruction for modifying the video
   */
  prompt?: string;
  /**
   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most Default value: `"flex_1"`
   */
  mode?:
    | "adhere_1"
    | "adhere_2"
    | "adhere_3"
    | "flex_1"
    | "flex_2"
    | "flex_3"
    | "reimagine_1"
    | "reimagine_2"
    | "reimagine_3";
};
export type LumaDreamMachineRay2ModifyOutput = {
  /**
   * URL of the modified video
   */
  video: File;
};
export type LumaDreamMachineRay2Output = {
  /**
   * The generated video
   */
  video: File;
};
export type LumaDreamMachineRay2ReframeInput = {
  /**
   * URL of the input video to reframe
   */
  video_url: string | Blob | File;
  /**
   * The aspect ratio of the reframed video
   */
  aspect_ratio: "1:1" | "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Optional URL of the first frame image for reframing
   */
  image_url?: string | Blob | File;
  /**
   * X position of the grid for reframing
   */
  grid_position_x?: number;
  /**
   * Y position of the grid for reframing
   */
  grid_position_y?: number;
  /**
   * Optional prompt for reframing
   */
  prompt?: string;
  /**
   * End X coordinate for reframing
   */
  x_end?: number;
  /**
   * Start X coordinate for reframing
   */
  x_start?: number;
  /**
   * End Y coordinate for reframing
   */
  y_end?: number;
  /**
   * Start Y coordinate for reframing
   */
  y_start?: number;
};
export type LumaDreamMachineRay2ReframeOutput = {
  /**
   * URL of the reframed video
   */
  video: File;
};
export type LumaPhotonFlashModifyInput = {
  /**
   * Instruction for modifying the image
   */
  prompt?: string;
  /**
   * URL of the input image to reframe
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are corresponding to more influence of the initial image on the output.
   */
  strength: number;
  /**
   * The aspect ratio of the reframed image
   */
  aspect_ratio: "1:1" | "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
};
export type LumaPhotonFlashModifyOutput = {
  /**
   * The generated image
   */
  images: Array<File>;
};
export type LumaPhotonFlashReframeInput = {
  /**
   * URL of the input image to reframe
   */
  image_url: string | Blob | File;
  /**
   * The aspect ratio of the reframed image
   */
  aspect_ratio: "1:1" | "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Optional prompt for reframing
   */
  prompt?: string;
  /**
   * X position of the grid for reframing
   */
  grid_position_x?: number;
  /**
   * Y position of the grid for reframing
   */
  grid_position_y?: number;
  /**
   * End X coordinate for reframing
   */
  x_end?: number;
  /**
   * Start X coordinate for reframing
   */
  x_start?: number;
  /**
   * End Y coordinate for reframing
   */
  y_end?: number;
  /**
   * Start Y coordinate for reframing
   */
  y_start?: number;
};
export type LumaPhotonFlashReframeOutput = {
  /**
   * The generated image
   */
  images: Array<File>;
};
export type LumaPhotonModifyInput = {
  /**
   * Instruction for modifying the image
   */
  prompt?: string;
  /**
   * URL of the input image to reframe
   */
  image_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are corresponding to more influence of the initial image on the output.
   */
  strength: number;
  /**
   * The aspect ratio of the reframed image
   */
  aspect_ratio: "1:1" | "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
};
export type LumaPhotonModifyOutput = {
  /**
   * The generated image
   */
  images: Array<File>;
};
export type LumaPhotonReframeInput = {
  /**
   * URL of the input image to reframe
   */
  image_url: string | Blob | File;
  /**
   * The aspect ratio of the reframed image
   */
  aspect_ratio: "1:1" | "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Optional prompt for reframing
   */
  prompt?: string;
  /**
   * X position of the grid for reframing
   */
  grid_position_x?: number;
  /**
   * Y position of the grid for reframing
   */
  grid_position_y?: number;
  /**
   * End X coordinate for reframing
   */
  x_end?: number;
  /**
   * Start X coordinate for reframing
   */
  x_start?: number;
  /**
   * End Y coordinate for reframing
   */
  y_end?: number;
  /**
   * Start Y coordinate for reframing
   */
  y_start?: number;
};
export type LumaPhotonReframeOutput = {
  /**
   * The generated image
   */
  images: Array<File>;
};
export type LuminaImageV2Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The system prompt to use. Default value: `"You are an assistant designed to generate superior images with the superior degree of image-text alignment based on textual prompts or user prompts."`
   */
  system_prompt?: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to apply normalization-based guidance scale. Default value: `true`
   */
  cfg_normalization?: boolean;
  /**
   * The ratio of the timestep interval to apply normalization-based guidance scale. Default value: `1`
   */
  cfg_trunc_ratio?: number;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type LuminaImageV2Output = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type lynxInput = {
  /**
   * The URL of the subject image to be used for video generation
   */
  image_url: string | Blob | File;
  /**
   * Text prompt to guide video generation
   */
  prompt: string;
  /**
   * Negative prompt to guide what should not appear in the generated video Default value: `"Bright tones, overexposed, blurred background, static, subtitles, style, works, paintings, images, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p) Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9, 9:16, or 1:1) Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Identity preservation scale. Controls how closely the generated video preserves the subject's identity from the reference image. Default value: `1`
   */
  ip_scale?: number;
  /**
   * Reference image scale. Controls the influence of the reference image on the generated video. Default value: `1`
   */
  strength?: number;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Image guidance scale. Controls how closely the generated video follows the reference image. Higher values increase adherence to the reference image but may decrease quality. Default value: `2`
   */
  guidance_scale_2?: number;
  /**
   * Number of frames in the generated video. Must be between 9 to 100. Default value: `81`
   */
  num_frames?: number;
};
export type lynxOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type lyria2Input = {
  /**
   * The text prompt describing the music you want to generate
   */
  prompt: string;
  /**
   * A description of what to exclude from the generated audio Default value: `"low quality"`
   */
  negative_prompt?: string;
  /**
   * A seed for deterministic generation. If provided, the model will attempt to produce the same audio given the same prompt and other parameters.
   */
  seed?: number;
};
export type lyria2Output = {
  /**
   * The generated music
   */
  audio: File;
};
export type MagiDistilledExtendVideoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
   */
  video_url: string | Blob | File;
  /**
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`
   */
  num_frames?: number;
  /**
   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
   */
  start_frame?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `"16"`
   */
  num_inference_steps?: "4" | "8" | "16" | "32";
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
};
export type MagiDistilledExtendVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type MagiDistilledImageToVideoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * URL of the input image to represent the first frame of the video. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`
   */
  num_frames?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `"16"`
   */
  num_inference_steps?: "4" | "8" | "16" | "32";
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
};
export type MagiDistilledImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type MagiDistilledInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`
   */
  num_frames?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `"16"`
   */
  num_inference_steps?: "4" | "8" | "16" | "32";
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
};
export type MagiDistilledOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type MagiExtendVideoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
   */
  video_url: string | Blob | File;
  /**
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`
   */
  num_frames?: number;
  /**
   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
   */
  start_frame?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `"16"`
   */
  num_inference_steps?: "4" | "8" | "16" | "32" | "64";
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
};
export type MagiExtendVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type MagiImageToVideoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * URL of the input image to represent the first frame of the video. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`
   */
  num_frames?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `"16"`
   */
  num_inference_steps?: "4" | "8" | "16" | "32" | "64";
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
};
export type MagiImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type magiInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`
   */
  num_frames?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `"16"`
   */
  num_inference_steps?: "4" | "8" | "16" | "32" | "64";
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
};
export type magiOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type MakeupApplicationInput = {
  /**
   * Portrait image URL for makeup application
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"natural"`
   */
  makeup_style?:
    | "natural"
    | "glamorous"
    | "smoky_eyes"
    | "bold_lips"
    | "no_makeup"
    | "remove_makeup"
    | "dramatic"
    | "bridal"
    | "professional"
    | "korean_style"
    | "artistic";
  /**
   *  Default value: `"medium"`
   */
  intensity?: "light" | "medium" | "heavy" | "dramatic";
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type MakeupApplicationOutput = {
  /**
   * Portrait with applied makeup
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type MandarinOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type MareyI2vInput = {
  /**
   * The prompt to generate a video from
   */
  prompt: string;
  /**
   * The dimensions of the generated video in width x height format. Default value: `"1920x1080"`
   */
  dimensions?:
    | "1920x1080"
    | "1080x1920"
    | "1152x1152"
    | "1536x1152"
    | "1152x1536";
  /**
   * The duration of the generated video. Default value: `"5s"`
   */
  duration?: "5s" | "10s";
  /**
   * Negative prompt used to guide the model away from undesirable features. Default value: `"<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts"`
   */
  negative_prompt?: string;
  /**
   * Seed for random number generation. Use -1 for random seed each run. Default value: `-1`
   */
  seed?: number;
  /**
   * Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely.
   */
  guidance_scale?: number;
  /**
   * The URL of the image to use as the first frame of the video.
   */
  image_url: string | Blob | File;
};
export type MareyI2vOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type MareyMotionTransferInput = {
  /**
   * The prompt to generate a video from
   */
  prompt: string;
  /**
   * The URL of the video to use as the control video.
   */
  video_url: string | Blob | File;
  /**
   * Optional first frame image URL to use as the first frame of the generated video Default value: `"https://video-editor-files-prod.s3.us-east-2.amazonaws.com/users/1e4d46df-0702-4491-95ce-763592f33f34/uploaded-images/9b9dce1c-abd0-46c0-bac9-9454f8893b06/original"`
   */
  first_frame_image_url?: string | Blob | File;
  /**
   * Optional reference image URL to use for pose control or as a starting frame
   */
  reference_image_url?: string | Blob | File;
  /**
   * Negative prompt used to guide the model away from undesirable features. Default value: `"<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts"`
   */
  negative_prompt?: string;
  /**
   * Seed for random number generation. Use -1 for random seed each run. Default value: `-1`
   */
  seed?: number;
};
export type MareyMotionTransferOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type MareyPoseTransferInput = {
  /**
   * The prompt to generate a video from
   */
  prompt: string;
  /**
   * The URL of the video to use as the control video.
   */
  video_url: string | Blob | File;
  /**
   * Optional first frame image URL to use as the first frame of the generated video
   */
  first_frame_image_url?: string | Blob | File;
  /**
   * Optional reference image URL to use for pose control or as a starting frame
   */
  reference_image_url?: string | Blob | File;
  /**
   * Negative prompt used to guide the model away from undesirable features. Default value: `"<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts"`
   */
  negative_prompt?: string;
  /**
   * Seed for random number generation. Use -1 for random seed each run. Default value: `-1`
   */
  seed?: number;
};
export type MareyPoseTransferOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type MareyT2vInput = {
  /**
   * The prompt to generate a video from
   */
  prompt: string;
  /**
   * The dimensions of the generated video in width x height format. Default value: `"1920x1080"`
   */
  dimensions?: "1920x1080" | "1152x1152" | "1536x1152" | "1152x1536";
  /**
   * The duration of the generated video. Default value: `"5s"`
   */
  duration?: "5s" | "10s";
  /**
   * Negative prompt used to guide the model away from undesirable features. Default value: `"<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts"`
   */
  negative_prompt?: string;
  /**
   * Seed for random number generation. Use -1 for random seed each run. Default value: `-1`
   */
  seed?: number;
  /**
   * Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely.
   */
  guidance_scale?: number;
};
export type MareyT2vOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type MaskInput = {
  /**
   * The URL of the image to remove objects from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the mask image. White pixels (255) indicate areas to remove.
   */
  mask_url: string | Blob | File;
  /**
   *  Default value: `"best_quality"`
   */
  model?: "low_quality" | "medium_quality" | "high_quality" | "best_quality";
  /**
   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`
   */
  mask_expansion?: number;
};
export type MayaBatchInput = {
  /**
   * List of texts to synthesize into speech. You can embed emotion tags in each text using the format <emotion_name>.
   */
  texts: Array<string>;
  /**
   * List of voice descriptions for each text. Must match the length of texts list. Each describes the voice/character attributes.
   */
  prompts: Array<string>;
  /**
   * Sampling temperature for all generations. Default value: `0.4`
   */
  temperature?: number;
  /**
   * Nucleus sampling parameter for all generations. Default value: `0.9`
   */
  top_p?: number;
  /**
   * Maximum SNAC tokens per generation. Default value: `2000`
   */
  max_tokens?: number;
  /**
   * Repetition penalty for all generations. Default value: `1.1`
   */
  repetition_penalty?: number;
  /**
   * Output audio sample rate for all generations. 48 kHz provides higher quality, 24 kHz is faster. Default value: `"48 kHz"`
   */
  sample_rate?: "48 kHz" | "24 kHz";
  /**
   * Output audio format for all generated speech files Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type MayaBatchOutput = {
  /**
   * List of generated audio files
   */
  audios: Array<File>;
  /**
   * Duration of each generated audio in seconds
   */
  durations: Array<number>;
  /**
   * Sample rate of all generated audio files
   */
  sample_rate: string;
  /**
   * Total time taken to generate all audio files in seconds
   */
  total_generation_time: number;
  /**
   * Average real-time factor across all generations
   */
  average_rtf: number;
};
export type mayaInput = {
  /**
   * The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'
   */
  text: string;
  /**
   * Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format.
   */
  prompt: string;
  /**
   * Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation. Default value: `0.4`
   */
  temperature?: number;
  /**
   * Nucleus sampling parameter. Controls diversity of token selection. Default value: `0.9`
   */
  top_p?: number;
  /**
   * Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length. Default value: `2000`
   */
  max_tokens?: number;
  /**
   * Penalty for repeating tokens. Higher values reduce repetition artifacts. Default value: `1.1`
   */
  repetition_penalty?: number;
  /**
   * Output audio sample rate. 48 kHz provides higher quality audio, 24 kHz is faster. Default value: `"48 kHz"`
   */
  sample_rate?: "48 kHz" | "24 kHz";
  /**
   * Output audio format for the generated speech Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type mayaOutput = {
  /**
   * The generated audio file containing the speech (WAV or MP3 format, 24kHz or 48kHz mono depending on upsampler)
   */
  audio: File;
  /**
   * Duration of the generated audio in seconds
   */
  duration: number;
  /**
   * Sample rate of the generated audio
   */
  sample_rate: string;
  /**
   * Time taken to generate the audio in seconds
   */
  generation_time: number;
  /**
   * Real-time factor (generation_time / audio_duration). Lower is better.
   */
  rtf: number;
};
export type MayaStreamInput = {
  /**
   * The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'
   */
  text: string;
  /**
   * Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format.
   */
  prompt: string;
  /**
   * Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation. Default value: `0.4`
   */
  temperature?: number;
  /**
   * Nucleus sampling parameter. Controls diversity of token selection. Default value: `0.9`
   */
  top_p?: number;
  /**
   * Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length. Default value: `2000`
   */
  max_tokens?: number;
  /**
   * Penalty for repeating tokens. Higher values reduce repetition artifacts. Default value: `1.1`
   */
  repetition_penalty?: number;
  /**
   * Output audio sample rate. 48 kHz uses upsampling for higher quality audio, 24 kHz is native SNAC output (faster, lower latency). Default value: `"24 kHz"`
   */
  sample_rate?: "48 kHz" | "24 kHz";
  /**
   * Output audio format. 'mp3' for browser-playable audio, 'wav' for uncompressed audio, 'pcm' for raw PCM (lowest latency, requires client-side decoding). Default value: `"mp3"`
   */
  output_format?: "mp3" | "wav" | "pcm";
};
export type MayaVoiceBatchInput = {
  /**
   * List of texts to synthesize into speech. You can embed emotion tags in each text using the format <emotion_name>.
   */
  texts: Array<string>;
  /**
   * List of voice descriptions for each text. Must match the length of texts list. Each describes the voice/character attributes.
   */
  prompts: Array<string>;
  /**
   * Sampling temperature for all generations. Default value: `0.4`
   */
  temperature?: number;
  /**
   * Nucleus sampling parameter for all generations. Default value: `0.9`
   */
  top_p?: number;
  /**
   * Maximum SNAC tokens per generation. Default value: `2000`
   */
  max_tokens?: number;
  /**
   * Repetition penalty for all generations. Default value: `1.1`
   */
  repetition_penalty?: number;
  /**
   * Output audio sample rate for all generations. 48 kHz provides higher quality, 24 kHz is faster. Default value: `"48 kHz"`
   */
  sample_rate?: "48 kHz" | "24 kHz";
  /**
   * Output audio format for all generated speech files Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type MayaVoiceBatchOutput = {
  /**
   * List of generated audio files
   */
  audios: Array<File>;
  /**
   * Duration of each generated audio in seconds
   */
  durations: Array<number>;
  /**
   * Sample rate of all generated audio files
   */
  sample_rate: string;
  /**
   * Total time taken to generate all audio files in seconds
   */
  total_generation_time: number;
  /**
   * Average real-time factor across all generations
   */
  average_rtf: number;
};
export type MayaVoiceInput = {
  /**
   * The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'
   */
  text: string;
  /**
   * Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format.
   */
  prompt: string;
  /**
   * Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation. Default value: `0.4`
   */
  temperature?: number;
  /**
   * Nucleus sampling parameter. Controls diversity of token selection. Default value: `0.9`
   */
  top_p?: number;
  /**
   * Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length. Default value: `2000`
   */
  max_tokens?: number;
  /**
   * Penalty for repeating tokens. Higher values reduce repetition artifacts. Default value: `1.1`
   */
  repetition_penalty?: number;
  /**
   * Output audio sample rate. 48 kHz provides higher quality audio, 24 kHz is faster. Default value: `"48 kHz"`
   */
  sample_rate?: "48 kHz" | "24 kHz";
  /**
   * Output audio format for the generated speech Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type MayaVoiceOutput = {
  /**
   * The generated audio file containing the speech (WAV or MP3 format, 24kHz or 48kHz mono depending on upsampler)
   */
  audio: File;
  /**
   * Duration of the generated audio in seconds
   */
  duration: number;
  /**
   * Sample rate of the generated audio
   */
  sample_rate: string;
  /**
   * Time taken to generate the audio in seconds
   */
  generation_time: number;
  /**
   * Real-time factor (generation_time / audio_duration). Lower is better.
   */
  rtf: number;
};
export type MayaVoiceStreamingInput = {
  /**
   * The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'
   */
  text: string;
  /**
   * Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format.
   */
  prompt: string;
  /**
   * Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation. Default value: `0.4`
   */
  temperature?: number;
  /**
   * Nucleus sampling parameter. Controls diversity of token selection. Default value: `0.9`
   */
  top_p?: number;
  /**
   * Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length. Default value: `2000`
   */
  max_tokens?: number;
  /**
   * Penalty for repeating tokens. Higher values reduce repetition artifacts. Default value: `1.1`
   */
  repetition_penalty?: number;
  /**
   * Output audio sample rate. 48 kHz uses upsampling for higher quality audio, 24 kHz is native SNAC output (faster, lower latency). Default value: `"24 kHz"`
   */
  sample_rate?: "48 kHz" | "24 kHz";
  /**
   * Output audio format. 'mp3' for browser-playable audio, 'wav' for uncompressed audio, 'pcm' for raw PCM (lowest latency, requires client-side decoding). Default value: `"mp3"`
   */
  output_format?: "mp3" | "wav" | "pcm";
};
export type MergeAudioInput = {
  /**
   * List of audio file URLs to merge in order
   */
  audio_urls: Array<string>;
  /**
   * Optional list of gap durations in seconds between each audio. Length should be len(audio_urls)-1
   */
  gaps?: Array<number>;
};
export type MergeAudioOutput = {
  /**
   * The merged audio file
   */
  audio: AudioFile;
};
export type MergeAudiosInput = {
  /**
   * List of audio URLs to merge in order. The 0th stream of the audio will be considered as the merge candidate.
   */
  audio_urls: Array<string>;
  /**
   * Output format of the combined audio. If not used, will be determined automatically using FFMPEG. Formatted as codec_sample_rate_bitrate.
   */
  output_format?:
    | "mp3_22050_32"
    | "mp3_44100_32"
    | "mp3_44100_64"
    | "mp3_44100_96"
    | "mp3_44100_128"
    | "mp3_44100_192"
    | "pcm_8000"
    | "pcm_16000"
    | "pcm_22050"
    | "pcm_24000"
    | "pcm_44100"
    | "pcm_48000"
    | "ulaw_8000"
    | "alaw_8000"
    | "opus_48000_32"
    | "opus_48000_64"
    | "opus_48000_96"
    | "opus_48000_128"
    | "opus_48000_192";
};
export type MergeAudiosOutput = {
  /**
   * Merged audio file
   */
  audio: File;
};
export type MergeImagesInput = {
  /**
   * List of image URLs to merge into an array
   */
  image_urls: Array<string>;
  /**
   * Output format for processed images Default value: `"png"`
   */
  output_format?: "png" | "jpg" | "jpeg" | "webp";
};
export type MergeImagesOutput = {
  /**
   * Array of processed images
   */
  images: Array<Image>;
};
export type MergeTextInput = {
  /**
   * List of text strings to merge
   */
  texts: Array<string>;
  /**
   * Separator to join texts Default value: `"--"`
   */
  separator?: string;
};
export type MergeTextOutput = {
  /**
   * Merged text string
   */
  text: string;
};
export type MergeVideosInput = {
  /**
   * List of video URLs to merge in order
   */
  video_urls: Array<string>;
  /**
   * Target FPS for the output video. If not provided, uses the lowest FPS from input videos.
   */
  target_fps?: number;
  /**
   * Resolution of the final video. Width and height must be between 512 and 2048.
   */
  resolution?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
};
export type MergeVideosOutput = {
  /**
   * Merged video file
   */
  video: File;
  /**
   * Metadata about the merged video including original video info
   */
  metadata: unknown;
};
export type MeshyV5MultiImageTo3dInput = {
  /**
   * 1 to 4 images for 3D model creation. All images should depict the same object from different angles. Supports .jpg, .jpeg, .png formats, and AVIF/HEIF which will be automatically converted. If more than 4 images are provided, only the first 4 will be used.
   */
  image_urls: Array<string>;
  /**
   * Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry. Default value: `"triangle"`
   */
  topology?: "quad" | "triangle";
  /**
   * Target number of polygons in the generated model Default value: `30000`
   */
  target_polycount?: number;
  /**
   * Controls symmetry behavior during model generation. Default value: `"auto"`
   */
  symmetry_mode?: "off" | "auto" | "on";
  /**
   * Whether to enable the remesh phase. When false, returns triangular mesh ignoring topology and target_polycount. Default value: `true`
   */
  should_remesh?: boolean;
  /**
   * Whether to generate textures. False provides mesh without textures for 5 credits, True adds texture generation for additional 10 credits. Default value: `true`
   */
  should_texture?: boolean;
  /**
   * Generate PBR Maps (metallic, roughness, normal) in addition to base color. Requires should_texture to be true.
   */
  enable_pbr?: boolean;
  /**
   * Whether to generate the model in an A/T pose
   */
  is_a_t_pose?: boolean;
  /**
   * Text prompt to guide the texturing process. Requires should_texture to be true.
   */
  texture_prompt?: string;
  /**
   * 2D image to guide the texturing process. Requires should_texture to be true.
   */
  texture_image_url?: string | Blob | File;
  /**
   * If set to true, input data will be checked for safety before processing. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type MeshyV5MultiImageTo3dOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * Array of texture file objects
   */
  texture_urls?: Array<TextureFiles>;
  /**
   * The seed used for generation (if available)
   */
  seed?: number;
};
export type MeshyV5RemeshInput = {
  /**
   * URL or base64 data URI of a 3D model to remesh. Supports .glb, .gltf, .obj, .fbx, .stl formats. Can be a publicly accessible URL or data URI with MIME type application/octet-stream.
   */
  model_url: string | Blob | File;
  /**
   * List of target formats for the remeshed model.
   */
  target_formats?: Array<"glb" | "fbx" | "obj" | "usdz" | "blend" | "stl">;
  /**
   * Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry. Default value: `"triangle"`
   */
  topology?: "quad" | "triangle";
  /**
   * Target number of polygons in the generated model. Actual count may vary based on geometry complexity. Default value: `30000`
   */
  target_polycount?: number;
  /**
   * Resize the model to a certain height measured in meters. Set to 0 for no resizing.
   */
  resize_height?: number;
  /**
   * Position of the origin. None means no effect.
   */
  origin_at?: "bottom" | "center";
};
export type MeshyV5RemeshOutput = {
  /**
   * Remeshed 3D object in GLB format (if GLB was requested).
   */
  model_glb?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
};
export type MeshyV5RetextureInput = {
  /**
   * URL or base64 data URI of a 3D model to texture. Supports .glb, .gltf, .obj, .fbx, .stl formats. Can be a publicly accessible URL or data URI with MIME type application/octet-stream.
   */
  model_url: string | Blob | File;
  /**
   * Describe your desired texture style using text. Maximum 600 characters. Required if image_style_url is not provided.
   */
  text_style_prompt?: string;
  /**
   * 2D image to guide the texturing process. Supports .jpg, .jpeg, and .png formats. Required if text_style_prompt is not provided. If both are provided, image_style_url takes precedence.
   */
  image_style_url?: string | Blob | File;
  /**
   * Use the original UV mapping of the model instead of generating new UVs. If the model has no original UV, output quality may be reduced. Default value: `true`
   */
  enable_original_uv?: boolean;
  /**
   * Generate PBR Maps (metallic, roughness, normal) in addition to base color.
   */
  enable_pbr?: boolean;
  /**
   * If set to true, input data will be checked for safety before processing. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type MeshyV5RetextureOutput = {
  /**
   * Retextured 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the retextured model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * Array of texture file objects
   */
  texture_urls?: Array<TextureFiles>;
  /**
   * The text prompt used for texturing (if provided)
   */
  text_style_prompt?: string;
  /**
   * The image URL used for texturing (if provided)
   */
  image_style_url?: string | Blob | File;
};
export type MeshyV6PreviewImageTo3dInput = {
  /**
   * Image URL or base64 data URI for 3D model creation. Supports .jpg, .jpeg, and .png formats. Also supports AVIF and HEIF formats which will be automatically converted.
   */
  image_url: string | Blob | File;
  /**
   * Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry. Default value: `"triangle"`
   */
  topology?: "quad" | "triangle";
  /**
   * Target number of polygons in the generated model Default value: `30000`
   */
  target_polycount?: number;
  /**
   * Controls symmetry behavior during model generation. Off disables symmetry, Auto determines it automatically, On enforces symmetry. Default value: `"auto"`
   */
  symmetry_mode?: "off" | "auto" | "on";
  /**
   * Whether to enable the remesh phase Default value: `true`
   */
  should_remesh?: boolean;
  /**
   * Whether to generate textures Default value: `true`
   */
  should_texture?: boolean;
  /**
   * Generate PBR Maps (metallic, roughness, normal) in addition to base color
   */
  enable_pbr?: boolean;
  /**
   * Whether to generate the model in an A/T pose
   */
  is_a_t_pose?: boolean;
  /**
   * Text prompt to guide the texturing process
   */
  texture_prompt?: string;
  /**
   * 2D image to guide the texturing process
   */
  texture_image_url?: string | Blob | File;
  /**
   * If set to true, input data will be checked for safety before processing. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type MeshyV6PreviewImageTo3dOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * Array of texture file objects, matching Meshy API structure
   */
  texture_urls?: Array<TextureFiles>;
  /**
   * The seed used for generation (if available)
   */
  seed?: number;
};
export type MeshyV6PreviewTextTo3dInput = {
  /**
   * Describe what kind of object the 3D model is. Maximum 600 characters.
   */
  prompt: string;
  /**
   * Generation mode. 'preview' returns untextured geometry only, 'full' returns textured model (preview + refine). Default value: `"full"`
   */
  mode?: "preview" | "full";
  /**
   * Desired art style of the object. Note: enable_pbr should be false for sculpture style. Default value: `"realistic"`
   */
  art_style?: "realistic" | "sculpture";
  /**
   * Seed for reproducible results. Same prompt and seed usually generate the same result.
   */
  seed?: number;
  /**
   * Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry. Default value: `"triangle"`
   */
  topology?: "quad" | "triangle";
  /**
   * Target number of polygons in the generated model Default value: `30000`
   */
  target_polycount?: number;
  /**
   * Whether to enable the remesh phase. When false, returns unprocessed triangular mesh. Default value: `true`
   */
  should_remesh?: boolean;
  /**
   * Controls symmetry behavior during model generation. Default value: `"auto"`
   */
  symmetry_mode?: "off" | "auto" | "on";
  /**
   * Whether to generate the model in an A/T pose
   */
  is_a_t_pose?: boolean;
  /**
   * Generate PBR Maps (metallic, roughness, normal) in addition to base color. Should be false for sculpture style.
   */
  enable_pbr?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Additional text prompt to guide the texturing process (only used in 'full' mode)
   */
  texture_prompt?: string;
  /**
   * 2D image to guide the texturing process (only used in 'full' mode)
   */
  texture_image_url?: string | Blob | File;
  /**
   * If set to true, input data will be checked for safety before processing. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type MeshyV6PreviewTextTo3dOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * Array of texture file objects
   */
  texture_urls?: Array<TextureFiles>;
  /**
   * The seed used for generation
   */
  seed?: number;
  /**
   * The text prompt used for generation
   */
  prompt: string;
  /**
   * The actual prompt used if prompt expansion was enabled
   */
  actual_prompt?: string;
};
export type MetadataInput = {
  /**
   * URL of the media file (video or audio) to analyze
   */
  media_url: string | Blob | File;
  /**
   * Whether to extract the start and end frames for videos. Note that when true the request will be slower.
   */
  extract_frames?: boolean;
};
export type MetadataOutput = {
  /**
   * Metadata for the analyzed media file (either Video or Audio)
   */
  media: Video | Audio;
};
export type MinimaxHailuo02FastImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `"6"`
   */
  duration?: "6" | "10";
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type MinimaxHailuo02FastImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo02ProImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * Optional URL of the image to use as the last frame of the video
   */
  end_image_url?: string | Blob | File;
};
export type MinimaxHailuo02ProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo02ProTextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type MinimaxHailuo02ProTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo02StandardImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `"6"`
   */
  duration?: "6" | "10";
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * The resolution of the generated video. Default value: `"768P"`
   */
  resolution?: "512P" | "768P";
  /**
   * Optional URL of the image to use as the last frame of the video
   */
  end_image_url?: string | Blob | File;
};
export type MinimaxHailuo02StandardImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo02StandardTextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `"6"`
   */
  duration?: "6" | "10";
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type MinimaxHailuo02StandardTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo23FastProImageToVideoInput = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type MinimaxHailuo23FastProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo23FastStandardImageToVideoInput = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * The duration of the video in seconds. Default value: `"6"`
   */
  duration?: "6" | "10";
};
export type MinimaxHailuo23FastStandardImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo23ProImageToVideoInput = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type MinimaxHailuo23ProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo23ProTextToVideoInput = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type MinimaxHailuo23ProTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo23StandardImageToVideoInput = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * The duration of the video in seconds. Default value: `"6"`
   */
  duration?: "6" | "10";
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type MinimaxHailuo23StandardImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxHailuo23StandardTextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * The duration of the video in seconds. Default value: `"6"`
   */
  duration?: "6" | "10";
};
export type MinimaxHailuo23StandardTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxImage01Input = {
  /**
   * Text prompt for image generation (max 1500 characters)
   */
  prompt: string;
  /**
   * Aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "1:1"
    | "16:9"
    | "4:3"
    | "3:2"
    | "2:3"
    | "3:4"
    | "9:16"
    | "21:9";
  /**
   * Number of images to generate (1-9) Default value: `1`
   */
  num_images?: number;
  /**
   * Whether to enable automatic prompt optimization
   */
  prompt_optimizer?: boolean;
};
export type MinimaxImage01Output = {
  /**
   * Generated images
   */
  images: Array<File>;
};
export type MinimaxImage01SubjectReferenceInput = {
  /**
   * Text prompt for image generation (max 1500 characters)
   */
  prompt: string;
  /**
   * URL of the subject reference image to use for consistent character appearance
   */
  image_url: string | Blob | File;
  /**
   * Aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "1:1"
    | "16:9"
    | "4:3"
    | "3:2"
    | "2:3"
    | "3:4"
    | "9:16"
    | "21:9";
  /**
   * Number of images to generate (1-9) Default value: `1`
   */
  num_images?: number;
  /**
   * Whether to enable automatic prompt optimization
   */
  prompt_optimizer?: boolean;
};
export type MinimaxImage01SubjectReferenceOutput = {
  /**
   * Generated images
   */
  images: Array<File>;
};
export type MinimaxMusicV15Input = {
  /**
   * Lyrics, supports [intro][verse][chorus][bridge][outro] sections. 10-600 characters.
   */
  prompt: string;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
  /**
   * Control music generation. 10-3000 characters.
   */
  lyrics_prompt: string;
};
export type MinimaxMusicV15Output = {
  /**
   * The generated music
   */
  audio: File;
};
export type MinimaxMusicV2Input = {
  /**
   * A description of the music, specifying style, mood, and scenario. 10-300 characters.
   */
  prompt: string;
  /**
   * Lyrics of the song. Use n to separate lines. You may add structure tags like [Intro], [Verse], [Chorus], [Bridge], [Outro] to enhance the arrangement. 10-3000 characters.
   */
  lyrics_prompt: string;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
};
export type MinimaxMusicV2Output = {
  /**
   * The generated music
   */
  audio: File;
};
export type MinimaxPreviewSpeech25HdInput = {
  /**
   * Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)
   */
  text: string;
  /**
   * Voice configuration settings
   */
  voice_setting?: VoiceSetting;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
  /**
   * Enhance recognition of specified languages and dialects
   */
  language_boost?:
    | "Persian"
    | "Filipino"
    | "Tamil"
    | "Chinese"
    | "Chinese,Yue"
    | "English"
    | "Arabic"
    | "Russian"
    | "Spanish"
    | "French"
    | "Portuguese"
    | "German"
    | "Turkish"
    | "Dutch"
    | "Ukrainian"
    | "Vietnamese"
    | "Indonesian"
    | "Japanese"
    | "Italian"
    | "Korean"
    | "Thai"
    | "Polish"
    | "Romanian"
    | "Greek"
    | "Czech"
    | "Finnish"
    | "Hindi"
    | "Bulgarian"
    | "Danish"
    | "Hebrew"
    | "Malay"
    | "Slovak"
    | "Swedish"
    | "Croatian"
    | "Hungarian"
    | "Norwegian"
    | "Slovenian"
    | "Catalan"
    | "Nynorsk"
    | "Afrikaans"
    | "auto";
  /**
   * Format of the output content (non-streaming only) Default value: `"hex"`
   */
  output_format?: "url" | "hex";
  /**
   * Custom pronunciation dictionary for text replacement
   */
  pronunciation_dict?: PronunciationDict;
};
export type MinimaxPreviewSpeech25HdOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type MinimaxPreviewSpeech25TurboInput = {
  /**
   * Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)
   */
  text: string;
  /**
   * Voice configuration settings
   */
  voice_setting?: VoiceSetting;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
  /**
   * Enhance recognition of specified languages and dialects
   */
  language_boost?:
    | "Persian"
    | "Filipino"
    | "Tamil"
    | "Chinese"
    | "Chinese,Yue"
    | "English"
    | "Arabic"
    | "Russian"
    | "Spanish"
    | "French"
    | "Portuguese"
    | "German"
    | "Turkish"
    | "Dutch"
    | "Ukrainian"
    | "Vietnamese"
    | "Indonesian"
    | "Japanese"
    | "Italian"
    | "Korean"
    | "Thai"
    | "Polish"
    | "Romanian"
    | "Greek"
    | "Czech"
    | "Finnish"
    | "Hindi"
    | "Bulgarian"
    | "Danish"
    | "Hebrew"
    | "Malay"
    | "Slovak"
    | "Swedish"
    | "Croatian"
    | "Hungarian"
    | "Norwegian"
    | "Slovenian"
    | "Catalan"
    | "Nynorsk"
    | "Afrikaans"
    | "auto";
  /**
   * Format of the output content (non-streaming only) Default value: `"hex"`
   */
  output_format?: "url" | "hex";
  /**
   * Custom pronunciation dictionary for text replacement
   */
  pronunciation_dict?: PronunciationDict;
};
export type MinimaxPreviewSpeech25TurboOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type MinimaxSpeech02HdInput = {
  /**
   * Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)
   */
  text: string;
  /**
   * Voice configuration settings
   */
  voice_setting?: VoiceSetting;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
  /**
   * Enhance recognition of specified languages and dialects
   */
  language_boost?:
    | "Chinese"
    | "Chinese,Yue"
    | "English"
    | "Arabic"
    | "Russian"
    | "Spanish"
    | "French"
    | "Portuguese"
    | "German"
    | "Turkish"
    | "Dutch"
    | "Ukrainian"
    | "Vietnamese"
    | "Indonesian"
    | "Japanese"
    | "Italian"
    | "Korean"
    | "Thai"
    | "Polish"
    | "Romanian"
    | "Greek"
    | "Czech"
    | "Finnish"
    | "Hindi"
    | "Bulgarian"
    | "Danish"
    | "Hebrew"
    | "Malay"
    | "Slovak"
    | "Swedish"
    | "Croatian"
    | "Hungarian"
    | "Norwegian"
    | "Slovenian"
    | "Catalan"
    | "Nynorsk"
    | "Afrikaans"
    | "auto";
  /**
   * Format of the output content (non-streaming only) Default value: `"hex"`
   */
  output_format?: "url" | "hex";
  /**
   * Custom pronunciation dictionary for text replacement
   */
  pronunciation_dict?: PronunciationDict;
};
export type MinimaxSpeech02HdOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type MinimaxSpeech02TurboInput = {
  /**
   * Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)
   */
  text: string;
  /**
   * Voice configuration settings
   */
  voice_setting?: VoiceSetting;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
  /**
   * Enhance recognition of specified languages and dialects
   */
  language_boost?:
    | "Chinese"
    | "Chinese,Yue"
    | "English"
    | "Arabic"
    | "Russian"
    | "Spanish"
    | "French"
    | "Portuguese"
    | "German"
    | "Turkish"
    | "Dutch"
    | "Ukrainian"
    | "Vietnamese"
    | "Indonesian"
    | "Japanese"
    | "Italian"
    | "Korean"
    | "Thai"
    | "Polish"
    | "Romanian"
    | "Greek"
    | "Czech"
    | "Finnish"
    | "Hindi"
    | "Bulgarian"
    | "Danish"
    | "Hebrew"
    | "Malay"
    | "Slovak"
    | "Swedish"
    | "Croatian"
    | "Hungarian"
    | "Norwegian"
    | "Slovenian"
    | "Catalan"
    | "Nynorsk"
    | "Afrikaans"
    | "auto";
  /**
   * Format of the output content (non-streaming only) Default value: `"hex"`
   */
  output_format?: "url" | "hex";
  /**
   * Custom pronunciation dictionary for text replacement
   */
  pronunciation_dict?: PronunciationDict;
};
export type MinimaxSpeech02TurboOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type MinimaxSpeech26HdInput = {
  /**
   * Voice configuration settings
   */
  voice_setting?: VoiceSetting;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
  /**
   * Enhance recognition of specified languages and dialects
   */
  language_boost?:
    | "Chinese"
    | "Chinese,Yue"
    | "English"
    | "Arabic"
    | "Russian"
    | "Spanish"
    | "French"
    | "Portuguese"
    | "German"
    | "Turkish"
    | "Dutch"
    | "Ukrainian"
    | "Vietnamese"
    | "Indonesian"
    | "Japanese"
    | "Italian"
    | "Korean"
    | "Thai"
    | "Polish"
    | "Romanian"
    | "Greek"
    | "Czech"
    | "Finnish"
    | "Hindi"
    | "Bulgarian"
    | "Danish"
    | "Hebrew"
    | "Malay"
    | "Slovak"
    | "Swedish"
    | "Croatian"
    | "Hungarian"
    | "Norwegian"
    | "Slovenian"
    | "Catalan"
    | "Nynorsk"
    | "Afrikaans"
    | "auto";
  /**
   * Format of the output content (non-streaming only) Default value: `"hex"`
   */
  output_format?: "url" | "hex";
  /**
   * Custom pronunciation dictionary for text replacement
   */
  pronunciation_dict?: PronunciationDict;
  /**
   * Text to convert to speech. Paragraph breaks should be marked with newline characters. **NOTE**: You can customize speech pauses by adding markers in the form `<#x#>`, where `x` is the pause duration in seconds. Valid range: `[0.01, 99.99]`, up to two decimal places. Pause markers must be placed between speakable text segments and cannot be used consecutively.
   */
  prompt: string;
  /**
   * Loudness normalization settings for the audio
   */
  normalization_setting?: LoudnessNormalizationSetting;
};
export type MinimaxSpeech26HdOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type MinimaxSpeech26TurboInput = {
  /**
   * Voice configuration settings
   */
  voice_setting?: VoiceSetting;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
  /**
   * Enhance recognition of specified languages and dialects
   */
  language_boost?:
    | "Chinese"
    | "Chinese,Yue"
    | "English"
    | "Arabic"
    | "Russian"
    | "Spanish"
    | "French"
    | "Portuguese"
    | "German"
    | "Turkish"
    | "Dutch"
    | "Ukrainian"
    | "Vietnamese"
    | "Indonesian"
    | "Japanese"
    | "Italian"
    | "Korean"
    | "Thai"
    | "Polish"
    | "Romanian"
    | "Greek"
    | "Czech"
    | "Finnish"
    | "Hindi"
    | "Bulgarian"
    | "Danish"
    | "Hebrew"
    | "Malay"
    | "Slovak"
    | "Swedish"
    | "Croatian"
    | "Hungarian"
    | "Norwegian"
    | "Slovenian"
    | "Catalan"
    | "Nynorsk"
    | "Afrikaans"
    | "auto";
  /**
   * Format of the output content (non-streaming only) Default value: `"hex"`
   */
  output_format?: "url" | "hex";
  /**
   * Custom pronunciation dictionary for text replacement
   */
  pronunciation_dict?: PronunciationDict;
  /**
   * Text to convert to speech. Paragraph breaks should be marked with newline characters. **NOTE**: You can customize speech pauses by adding markers in the form `<#x#>`, where `x` is the pause duration in seconds. Valid range: `[0.01, 99.99]`, up to two decimal places. Pause markers must be placed between speakable text segments and cannot be used consecutively.
   */
  prompt: string;
  /**
   * Loudness normalization settings for the audio
   */
  normalization_setting?: LoudnessNormalizationSetting;
};
export type MinimaxSpeech26TurboOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type MinimaxSpeech28HdInput = {
  /**
   * Voice configuration settings
   */
  voice_setting?: VoiceSetting;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
  /**
   * Enhance recognition of specified languages and dialects
   */
  language_boost?:
    | "Chinese"
    | "Chinese,Yue"
    | "English"
    | "Arabic"
    | "Russian"
    | "Spanish"
    | "French"
    | "Portuguese"
    | "German"
    | "Turkish"
    | "Dutch"
    | "Ukrainian"
    | "Vietnamese"
    | "Indonesian"
    | "Japanese"
    | "Italian"
    | "Korean"
    | "Thai"
    | "Polish"
    | "Romanian"
    | "Greek"
    | "Czech"
    | "Finnish"
    | "Hindi"
    | "Bulgarian"
    | "Danish"
    | "Hebrew"
    | "Malay"
    | "Slovak"
    | "Swedish"
    | "Croatian"
    | "Hungarian"
    | "Norwegian"
    | "Slovenian"
    | "Catalan"
    | "Nynorsk"
    | "Afrikaans"
    | "auto";
  /**
   * Format of the output content (non-streaming only) Default value: `"hex"`
   */
  output_format?: "url" | "hex";
  /**
   * Custom pronunciation dictionary for text replacement
   */
  pronunciation_dict?: PronunciationDict;
  /**
   * Text to convert to speech. Use `<#x#>` for pauses (x = 0.01-99.99 seconds). Supports interjection tags: `(laughs)`, `(sighs)`, `(coughs)`, `(clears throat)`, `(gasps)`, `(sniffs)`, `(groans)`, `(yawns)`.
   */
  prompt: string;
  /**
   * Loudness normalization settings for the audio
   */
  normalization_setting?: LoudnessNormalizationSetting;
  /**
   * Voice modification settings to adjust pitch, intensity, and timbre.
   */
  voice_modify?: VoiceModify;
};
export type MinimaxSpeech28HdOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type MinimaxSpeech28TurboInput = {
  /**
   * Voice configuration settings
   */
  voice_setting?: VoiceSetting;
  /**
   * Audio configuration settings
   */
  audio_setting?: AudioSetting;
  /**
   * Enhance recognition of specified languages and dialects
   */
  language_boost?:
    | "Chinese"
    | "Chinese,Yue"
    | "English"
    | "Arabic"
    | "Russian"
    | "Spanish"
    | "French"
    | "Portuguese"
    | "German"
    | "Turkish"
    | "Dutch"
    | "Ukrainian"
    | "Vietnamese"
    | "Indonesian"
    | "Japanese"
    | "Italian"
    | "Korean"
    | "Thai"
    | "Polish"
    | "Romanian"
    | "Greek"
    | "Czech"
    | "Finnish"
    | "Hindi"
    | "Bulgarian"
    | "Danish"
    | "Hebrew"
    | "Malay"
    | "Slovak"
    | "Swedish"
    | "Croatian"
    | "Hungarian"
    | "Norwegian"
    | "Slovenian"
    | "Catalan"
    | "Nynorsk"
    | "Afrikaans"
    | "auto";
  /**
   * Format of the output content (non-streaming only) Default value: `"hex"`
   */
  output_format?: "url" | "hex";
  /**
   * Custom pronunciation dictionary for text replacement
   */
  pronunciation_dict?: PronunciationDict;
  /**
   * Text to convert to speech. Use `<#x#>` for pauses (x = 0.01-99.99 seconds). Supports interjection tags: `(laughs)`, `(sighs)`, `(coughs)`, `(clears throat)`, `(gasps)`, `(sniffs)`, `(groans)`, `(yawns)`.
   */
  prompt: string;
  /**
   * Loudness normalization settings for the audio
   */
  normalization_setting?: LoudnessNormalizationSetting;
  /**
   * Voice modification settings to adjust pitch, intensity, and timbre.
   */
  voice_modify?: VoiceModify;
};
export type MinimaxSpeech28TurboOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type MiniMaxTextToImageOutput = {
  /**
   * Generated images
   */
  images: Array<File>;
};
export type MiniMaxTextToImageWithReferenceOutput = {
  /**
   * Generated images
   */
  images: Array<File>;
};
export type MinimaxVideo01DirectorImageToVideoInput = {
  /**
   * Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type MinimaxVideo01DirectorImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxVideo01DirectorInput = {
  /**
   * Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type MinimaxVideo01DirectorOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxVideo01ImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type MinimaxVideo01ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxVideo01SubjectReferenceInput = {
  /**
   *
   */
  prompt: string;
  /**
   * URL of the subject reference image to use for consistent subject appearance
   */
  subject_reference_image_url: string | Blob | File;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type MinimaxVideo01SubjectReferenceOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MinimaxVoiceCloneInput = {
  /**
   * URL of the input audio file for voice cloning. Should be at least 10 seconds
   * long. To retain the voice permanently, use it with a TTS (text-to-speech)
   * endpoint at least once within 7 days. Otherwise, it will be
   * automatically deleted.
   */
  audio_url: string | Blob | File;
  /**
   * Enable noise reduction for the cloned voice
   */
  noise_reduction?: boolean;
  /**
   * Enable volume normalization for the cloned voice
   */
  need_volume_normalization?: boolean;
  /**
   * Text validation accuracy threshold (0-1)
   */
  accuracy?: number;
  /**
   * Text to generate a TTS preview with the cloned voice (optional) Default value: `"Hello, this is a preview of your cloned voice! I hope you like it!"`
   */
  text?: string;
  /**
   * TTS model to use for preview. Options: speech-02-hd, speech-02-turbo, speech-01-hd, speech-01-turbo Default value: `"speech-02-hd"`
   */
  model?:
    | "speech-02-hd"
    | "speech-02-turbo"
    | "speech-01-hd"
    | "speech-01-turbo";
};
export type MinimaxVoiceCloneOutput = {
  /**
   * The cloned voice ID for use with TTS
   */
  custom_voice_id: string;
  /**
   * Preview audio generated with the cloned voice (if requested)
   */
  audio?: File;
};
export type MinimaxVoiceDesignInput = {
  /**
   * Voice description prompt for generating a personalized voice
   */
  prompt: string;
  /**
   * Text for audio preview. Limited to 500 characters. A fee of $30 per 1M characters will be charged for the generation of the preview audio.
   */
  preview_text: string;
};
export type MinimaxVoiceDesignOutput = {
  /**
   * The voice_id of the generated voice
   */
  custom_voice_id: string;
  /**
   * The preview audio using the generated voice
   */
  audio: File;
};
export type MixDehazeNetInput = {
  /**
   * URL of image to be used for image enhancement
   */
  image_url: string | Blob | File;
  /**
   * Model to be used for dehazing Default value: `"indoor"`
   */
  model?: "indoor" | "outdoor";
  /**
   * seed to be used for generation
   */
  seed?: number;
};
export type MixDehazeNetOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
};
export type MmaudioV2Input = {
  /**
   * The URL of the video to generate the audio for.
   */
  video_url: string | Blob | File;
  /**
   * The prompt to generate the audio for.
   */
  prompt: string;
  /**
   * The negative prompt to generate the audio for. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * The number of steps to generate the audio for. Default value: `25`
   */
  num_steps?: number;
  /**
   * The duration of the audio to generate. Default value: `8`
   */
  duration?: number;
  /**
   * The strength of Classifier Free Guidance. Default value: `4.5`
   */
  cfg_strength?: number;
  /**
   * Whether to mask away the clip.
   */
  mask_away_clip?: boolean;
};
export type MmaudioV2Output = {
  /**
   * The generated video with the lip sync.
   */
  video: File;
};
export type ModifyOutput = {
  /**
   * URL of the modified video
   */
  video: File;
};
export type moondream2Input = {
  /**
   * URL of the image to be processed
   */
  image_url: string | Blob | File;
};
export type Moondream2ObjectDetectionInput = {
  /**
   * URL of the image to be processed
   */
  image_url: string | Blob | File;
  /**
   * Object to be detected in the image
   */
  object: string;
};
export type Moondream2ObjectDetectionOutput = {
  /**
   * Objects detected in the image
   */
  objects: Array<any>;
  /**
   * Image with detected objects
   */
  image: Image;
};
export type moondream2Output = {
  /**
   * Output for the given query
   */
  output: string;
};
export type Moondream2PointObjectDetectionInput = {
  /**
   * URL of the image to be processed
   */
  image_url: string | Blob | File;
  /**
   * Object to be detected in the image
   */
  object: string;
};
export type Moondream2PointObjectDetectionOutput = {
  /**
   * Objects detected in the image
   */
  objects: Array<any>;
  /**
   * Image with detected objects
   */
  image: Image;
};
export type Moondream2VisualQueryInput = {
  /**
   * URL of the image to be processed
   */
  image_url: string | Blob | File;
  /**
   * Query to be asked in the image
   */
  prompt: string;
};
export type Moondream2VisualQueryOutput = {
  /**
   * Output for the given query
   */
  output: string;
};
export type Moondream3PreviewCaptionInput = {
  /**
   * URL of the image to be processed
   *
   * Max width: 7000px, Max height: 7000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Length of the caption to generate Default value: `"normal"`
   */
  length?: "short" | "normal" | "long";
  /**
   * Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0.
   */
  temperature?: number;
  /**
   * Nucleus sampling probability mass to use, between 0 and 1.
   */
  top_p?: number;
};
export type Moondream3PreviewCaptionOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * Generated caption for the image
   */
  output: string;
};
export type Moondream3PreviewDetectInput = {
  /**
   * URL of the image to be processed
   *
   * Max width: 7000px, Max height: 7000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Object to be detected in the image
   */
  prompt: string;
  /**
   * Whether to preview the output
   */
  preview?: boolean;
};
export type Moondream3PreviewDetectOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * List of detected objects with their bounding boxes
   */
  objects: Array<Object>;
  /**
   * Image with bounding boxes drawn around detected objects
   */
  image?: ImageFile;
};
export type Moondream3PreviewPointInput = {
  /**
   * URL of the image to be processed
   *
   * Max width: 7000px, Max height: 7000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Object to be located in the image
   */
  prompt: string;
  /**
   * Whether to preview the output
   */
  preview?: boolean;
};
export type Moondream3PreviewPointOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * List of points marking the detected objects
   */
  points: Array<Point>;
  /**
   * Image with points drawn on detected objects
   */
  image?: ImageFile;
};
export type Moondream3PreviewQueryInput = {
  /**
   * URL of the image to be processed
   *
   * Max width: 7000px, Max height: 7000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Query to be asked in the image
   */
  prompt: string;
  /**
   * Whether to include detailed reasoning behind the answer Default value: `true`
   */
  reasoning?: boolean;
  /**
   * Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0.
   */
  temperature?: number;
  /**
   * Nucleus sampling probability mass to use, between 0 and 1.
   */
  top_p?: number;
};
export type Moondream3PreviewQueryOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * Answer to the query about the image
   */
  output: string;
  /**
   * Detailed reasoning behind the answer, if enabled
   */
  reasoning?: string;
};
export type Moondream3PreviewSegmentInput = {
  /**
   * URL of the image to be processed
   *
   * Max width: 7000px, Max height: 7000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Object to be segmented in the image
   */
  object: string;
  /**
   * Spatial references to guide the segmentation. By feeding in references you can help the segmentation process. Must be either list of Point object with x and y members, or list of arrays containing either 2 floats (x,y) or 4 floats (x1,y1,x2,y2).
   * **NOTE**: You can also use the [**point endpoint**](https://fal.ai/models/fal-ai/moondream3-preview/point) to get points for the objects, and pass them in here.
   */
  spatial_references?: Array<Point | Array<number>>;
  /**
   * Sampling settings for the segmentation model
   */
  settings?: SegmentSamplingSettings;
  /**
   * Whether to preview the output and return a binary mask of the image
   */
  preview?: boolean;
};
export type Moondream3PreviewSegmentOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * Segmentation mask image. If no object detected or preview not requested, will be null.
   */
  image?: ImageFile;
  /**
   * SVG path data representing the segmentation mask. If not detected, will be null.
   */
  path?: string;
  /**
   * Bounding box of the segmented object. If not detected, will be null.
   */
  bbox?: Object;
};
export type MoondreamCaptionInput = {
  /**
   * URL of the image to be processed
   *
   * Max width: 7000px, Max height: 7000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Length of the caption to generate Default value: `"normal"`
   */
  length?: "short" | "normal" | "long";
  /**
   * Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0.
   */
  temperature?: number;
  /**
   * Nucleus sampling probability mass to use, between 0 and 1.
   */
  top_p?: number;
};
export type MoondreamCaptionOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * Generated caption for the image
   */
  output: string;
};
export type MoondreamDetectInput = {
  /**
   * URL of the image to be processed
   *
   * Max width: 7000px, Max height: 7000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Object to be detected in the image
   */
  prompt: string;
  /**
   * Whether to preview the output
   */
  preview?: boolean;
};
export type MoondreamDetectOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * List of detected objects with their bounding boxes
   */
  objects: Array<Object>;
  /**
   * Image with bounding boxes drawn around detected objects
   */
  image?: ImageFile;
};
export type MoondreamInput = {
  /**
   * URL of the image to be processed
   */
  image_url: string | Blob | File;
};
export type MoondreamNextBatchInput = {
  /**
   * List of image URLs to be processed (maximum 32 images)
   */
  images_data_url: string | Blob | File;
  /**
   * Single prompt to apply to all images
   */
  prompt: string;
  /**
   * Maximum number of tokens to generate Default value: `64`
   */
  max_tokens?: number;
};
export type MoondreamNextBatchOutput = {
  /**
   * URL to the generated captions JSON file containing filename-caption pairs.
   */
  captions_file: File;
  /**
   * List of generated captions
   */
  outputs: Array<string>;
};
export type MoondreamObjectInput = {
  /**
   * URL of the image to be processed
   */
  image_url: string | Blob | File;
  /**
   * Object to be detected in the image
   */
  object: string;
};
export type MoondreamObjectOutput = {
  /**
   * Objects detected in the image
   */
  objects: Array<any>;
  /**
   * Image with detected objects
   */
  image: Image;
};
export type MoondreamOutput = {
  /**
   * Output for the given query
   */
  output: string;
};
export type MoonDreamOutput = {
  /**
   * Response from the model
   */
  output: string;
};
export type MoondreamPointInput = {
  /**
   * URL of the image to be processed
   *
   * Max width: 7000px, Max height: 7000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Object to be located in the image
   */
  prompt: string;
  /**
   * Whether to preview the output
   */
  preview?: boolean;
};
export type MoondreamPointOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * List of points marking the detected objects
   */
  points: Array<Point>;
  /**
   * Image with points drawn on detected objects
   */
  image?: ImageFile;
};
export type MoondreamQueryInput = {
  /**
   * URL of the image to be processed
   */
  image_url: string | Blob | File;
  /**
   * Query to be asked in the image
   */
  prompt: string;
};
export type MoondreamQueryOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * Answer to the query about the image
   */
  output: string;
  /**
   * Detailed reasoning behind the answer, if enabled
   */
  reasoning?: string;
};
export type MoondreamSegementationInput = {
  /**
   * URL of the image to be processed
   *
   * Max width: 7000px, Max height: 7000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * Object to be segmented in the image
   */
  object: string;
  /**
   * Spatial references to guide the segmentation. By feeding in references you can help the segmentation process. Must be either list of Point object with x and y members, or list of arrays containing either 2 floats (x,y) or 4 floats (x1,y1,x2,y2).
   * **NOTE**: You can also use the [**point endpoint**](https://fal.ai/models/fal-ai/moondream3-preview/point) to get points for the objects, and pass them in here.
   */
  spatial_references?: Array<Point | Array<number>>;
  /**
   * Sampling settings for the segmentation model
   */
  settings?: SegmentSamplingSettings;
  /**
   * Whether to preview the output and return a binary mask of the image
   */
  preview?: boolean;
};
export type MoondreamSegementationOutput = {
  /**
   * Reason for finishing the output generation
   */
  finish_reason: string;
  /**
   * Usage information for the request
   */
  usage_info: UsageInfo;
  /**
   * Segmentation mask image. If no object detected or preview not requested, will be null.
   */
  image?: ImageFile;
  /**
   * SVG path data representing the segmentation mask. If not detected, will be null.
   */
  path?: string;
  /**
   * Bounding box of the segmented object. If not detected, will be null.
   */
  bbox?: Object;
};
export type MotionControlOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type MulticonditioningVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Default value: `"blurry, low quality, low resolution, inconsistent motion, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * The LoRA weights to use for generation.
   */
  loras?: Array<LoRAWeight>;
  /**
   * The resolution of the video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * The aspect ratio of the video. Default value: `"auto"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16" | "auto";
  /**
   * The number of frames in the video. Default value: `89`
   */
  number_of_frames?: number;
  /**
   * The number of inference steps to use. Default value: `30`
   */
  number_of_steps?: number;
  /**
   * The frame rate of the video. Default value: `25`
   */
  frame_rate?: number;
  /**
   * The seed to use for generation.
   */
  seed?: number;
  /**
   * Whether to expand the prompt using the LLM.
   */
  expand_prompt?: boolean;
  /**
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The image conditions to use for generation.
   */
  images?: Array<ImageCondition>;
  /**
   * The video conditions to use for generation.
   */
  videos?: Array<VideoCondition>;
};
export type MultiConditioningVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
};
export type MulticonditioningVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type MultiConditioningVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type MultiFluxIDInput = {
  /**
   * Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years).
   */
  age_group:
    | "baby"
    | "toddler"
    | "preschool"
    | "gradeschooler"
    | "teen"
    | "adult"
    | "mid"
    | "senior";
  /**
   * Gender for the generated image. Choose from: 'male' or 'female'.
   */
  gender: "male" | "female";
  /**
   * Text prompt to guide the image generation Default value: `"a newborn baby, well dressed"`
   */
  prompt?: string;
  /**
   * The size of the generated image
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducibility. If None, a random seed will be used
   */
  seed?: number;
  /**
   * The format of the generated image. Choose from: 'jpeg' or 'png'. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * List of mother images for multi mode
   */
  mother_image_urls: Array<string>;
  /**
   * List of father images for multi mode
   */
  father_image_urls: Array<string>;
  /**
   * Weight of the father's influence in multi mode generation Default value: `0.5`
   */
  father_weight?: number;
};
export type MultiImageTo3DInput = {
  /**
   * 1 to 4 images for 3D model creation. All images should depict the same object from different angles. Supports .jpg, .jpeg, .png formats, and AVIF/HEIF which will be automatically converted. If more than 4 images are provided, only the first 4 will be used.
   */
  image_urls: Array<string>;
  /**
   * Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry. Default value: `"triangle"`
   */
  topology?: "quad" | "triangle";
  /**
   * Target number of polygons in the generated model Default value: `30000`
   */
  target_polycount?: number;
  /**
   * Controls symmetry behavior during model generation. Default value: `"auto"`
   */
  symmetry_mode?: "off" | "auto" | "on";
  /**
   * Whether to enable the remesh phase. When false, returns triangular mesh ignoring topology and target_polycount. Default value: `true`
   */
  should_remesh?: boolean;
  /**
   * Whether to generate textures. False provides mesh without textures for 5 credits, True adds texture generation for additional 10 credits. Default value: `true`
   */
  should_texture?: boolean;
  /**
   * Generate PBR Maps (metallic, roughness, normal) in addition to base color. Requires should_texture to be true.
   */
  enable_pbr?: boolean;
  /**
   * Whether to generate the model in an A/T pose
   */
  is_a_t_pose?: boolean;
  /**
   * Text prompt to guide the texturing process. Requires should_texture to be true.
   */
  texture_prompt?: string;
  /**
   * 2D image to guide the texturing process. Requires should_texture to be true.
   */
  texture_image_url?: string | Blob | File;
  /**
   * If set to true, input data will be checked for safety before processing. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type MultiImageTo3DOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * Array of texture file objects
   */
  texture_urls?: Array<TextureFiles>;
  /**
   * The seed used for generation (if available)
   */
  seed?: number;
};
export type MultipleAnglesInput = {
  /**
   * The URL of the image to adjust camera angle for.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Rotate camera left (positive) or right (negative) in degrees. Positive values rotate left, negative values rotate right.
   */
  rotate_right_left?: number;
  /**
   * Move camera forward (0=no movement, 10=close-up)
   */
  move_forward?: number;
  /**
   * Adjust vertical camera angle (-1=bird's-eye view/looking down, 0=neutral, 1=worm's-eye view/looking up)
   */
  vertical_angle?: number;
  /**
   * Enable wide-angle lens effect
   */
  wide_angle_lens?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the camera control effect. Default value: `1.25`
   */
  lora_scale?: number;
};
export type MultipleAnglesOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type MultiViewObjectOutput = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type MultiviewTo3dInput = {
  /**
   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.
   */
  seed?: number;
  /**
   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.
   */
  face_limit?: number;
  /**
   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.
   */
  pbr?: boolean;
  /**
   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `"standard"`
   */
  texture?: "no" | "standard" | "HD";
  /**
   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.
   */
  texture_seed?: number;
  /**
   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.
   */
  auto_size?: boolean;
  /**
   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.
   */
  quad?: boolean;
  /**
   * Determines the prioritization of texture alignment in the 3D model. The default value is original_image. Default value: `"original_image"`
   */
  texture_alignment?: "original_image" | "geometry";
  /**
   * Set orientation=align_image to automatically rotate the model to align the original image. The default value is default. Default value: `"default"`
   */
  orientation?: "default" | "align_image";
  /**
   * Front view image of the object.
   */
  front_image_url: string | Blob | File;
  /**
   * Left view image of the object.
   */
  left_image_url?: string | Blob | File;
  /**
   * Back view image of the object.
   */
  back_image_url?: string | Blob | File;
  /**
   * Right view image of the object.
   */
  right_image_url?: string | Blob | File;
};
export type MusicGenerationInput = {
  /**
   * Describe the music you want to generate
   */
  prompt: string;
  /**
   * Describe what you want to avoid in the music (instruments, styles, moods). Leave blank for none. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Length of the generated music in seconds Default value: `90`
   */
  duration?: number;
  /**
   * Refinement level - higher values may improve quality but take longer Default value: `100`
   */
  refinement?: number;
  /**
   * Creativity level - higher values allow more creative interpretation of the prompt Default value: `16`
   */
  creativity?: number;
  /**
   * Random seed for reproducible results - leave empty for random generation
   */
  seed?: number;
};
export type MusicGenerationOutput = {
  /**
   * Generated audio file in WAV format
   */
  audio: File;
  /**
   * The processed prompt used for generation
   */
  prompt: string;
  /**
   * Generation metadata including duration, sample rate, and parameters
   */
  metadata: unknown;
};
export type MusicGeneratorInput = {
  /**
   * The prompt to generate music from.
   */
  prompt: string;
  /**
   * The duration of the generated music in seconds.
   */
  duration: number;
};
export type MusicGeneratorOutput = {
  /**
   * The generated music
   */
  audio_file: File;
};
export type MusicOutput = {
  /**
   * The generated music audio file in MP3 format
   */
  audio: File;
};
export type NafnetDeblurInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * seed to be used for generation
   */
  seed?: number;
};
export type NafnetDeblurOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
};
export type NafnetDenoiseInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * seed to be used for generation
   */
  seed?: number;
};
export type NafnetDenoiseOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
};
export type NafnetInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * seed to be used for generation
   */
  seed?: number;
};
export type NafnetOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
};
export type NanoBananaEditInput = {
  /**
   * The prompt for image editing.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"auto"`
   */
  aspect_ratio?:
    | "auto"
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The URLs of the images to use for image-to-image generation or image editing.
   */
  image_urls: Array<string>;
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
};
export type NanoBananaEditOutput = {
  /**
   * The edited images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type NanoBananaImageToImageInput = {
  /**
   * The prompt for image editing.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"auto"`
   */
  aspect_ratio?:
    | "auto"
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The URLs of the images to use for image-to-image generation or image editing.
   */
  image_urls: Array<string>;
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
};
export type NanoBananaImageToImageOutput = {
  /**
   * The edited images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type NanoBananaInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
};
export type NanoBananaOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type NanoBananaProEditInput = {
  /**
   * The prompt for image editing.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"auto"`
   */
  aspect_ratio?:
    | "auto"
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict. Default value: `"4"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The URLs of the images to use for image-to-image generation or image editing.
   */
  image_urls: Array<string>;
  /**
   * The resolution of the image to generate. Default value: `"1K"`
   */
  resolution?: "1K" | "2K" | "4K";
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
  /**
   * Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image.
   */
  enable_web_search?: boolean;
};
export type NanoBananaProEditOutput = {
  /**
   * The edited images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type NanoBananaProInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * The aspect ratio of the generated image. Use "auto" to let the model decide based on the prompt. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "auto"
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict. Default value: `"4"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The resolution of the image to generate. Default value: `"1K"`
   */
  resolution?: "1K" | "2K" | "4K";
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
  /**
   * Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image.
   */
  enable_web_search?: boolean;
};
export type NanoBananaProOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type NanoBananaTextToImageInput = {
  /**
   * The text prompt to generate an image from.
   */
  prompt: string;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "3:2"
    | "4:3"
    | "5:4"
    | "1:1"
    | "4:5"
    | "3:4"
    | "2:3"
    | "9:16";
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.
   */
  limit_generations?: boolean;
};
export type NanoBananaTextToImageOutput = {
  /**
   * The generated images.
   */
  images: Array<ImageFile>;
  /**
   * The description of the generated images.
   */
  description: string;
};
export type NemotronAsrInput = {
  /**
   * URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * Controls the speed/accuracy trade-off. 'none' = best accuracy (1.12s chunks, ~7.16% WER), 'low' = balanced (0.56s chunks, ~7.22% WER), 'medium' = faster (0.16s chunks, ~7.84% WER), 'high' = fastest (0.08s chunks, ~8.53% WER). Default value: `"none"`
   */
  acceleration?: "none" | "low" | "medium" | "high";
};
export type NemotronAsrOutput = {
  /**
   * The transcribed text from the audio.
   */
  output: string;
  /**
   * True if this is an intermediate result during streaming.
   */
  partial?: boolean;
};
export type NemotronAsrStreamInput = {
  /**
   * URL of the audio file.
   */
  audio_url: string | Blob | File;
  /**
   * Controls the speed/accuracy trade-off. 'none' = best accuracy (1.12s chunks, ~7.16% WER), 'low' = balanced (0.56s chunks, ~7.22% WER), 'medium' = faster (0.16s chunks, ~7.84% WER), 'high' = fastest (0.08s chunks, ~8.53% WER). Default value: `"none"`
   */
  acceleration?: "none" | "low" | "medium" | "high";
};
export type NextSceneInput = {
  /**
   * The URL of the image to create the next scene from.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the camera movement, framing change, or scene transition. Start with 'Next Scene:' for best results. Examples: camera movements (dolly, push-in, pull-back), framing changes (wide to close-up), new elements entering frame. Default value: `"Next Scene: The camera moves forward revealing more of the scene"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type NextSceneOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type Nextstep1Input = {
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   */
  negative_prompt: string;
};
export type Nextstep1Output = {
  /**
   * Generated image
   */
  image: Image;
  /**
   * Seed used for random number generation
   */
  seed: number;
};
export type NovaSrInput = {
  /**
   * The URL of the audio file to enhance.
   */
  audio_url: string | Blob | File;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format for the output audio. Default value: `"mp3"`
   */
  audio_format?: "mp3" | "aac" | "m4a" | "ogg" | "opus" | "flac" | "wav";
  /**
   * The bitrate of the output audio. Default value: `"192k"`
   */
  bitrate?: string;
};
export type NovaSrOutput = {
  /**
   * The enhanced audio file.
   */
  audio: AudioFile;
  /**
   * Timings for each step in the pipeline.
   */
  timings: NovaSRTimings;
};
export type O3ProEditV2VOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type O3ProEditVideoV2VInput = {
  /**
   * Text prompt for video generation. Reference video as @Video1.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.
   */
  video_url: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Whether to keep the original audio from the reference video. Default value: `true`
   */
  keep_audio?: boolean;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ImageElementInput>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type O3ProImageR2VOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type O3ProImageToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * URL of the start frame image.
   */
  image_url: string | Blob | File;
  /**
   * URL of the end frame image (optional).
   */
  end_image_url?: string | Blob | File;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type O3ProImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type O3ProReferenceV2VOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type O3ProReferenceVideoI2VInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * Image to use as the first frame of the video.
   */
  start_image_url?: string | Blob | File;
  /**
   * Image to use as the last frame of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ComboElementInput>;
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * The aspect ratio of the generated video frame. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
};
export type O3ProReferenceVideoV2VInput = {
  /**
   * Text prompt for video generation. Reference video as @Video1.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.
   */
  video_url: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Whether to keep the original audio from the reference video. Default value: `true`
   */
  keep_audio?: boolean;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ImageElementInput>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * Aspect ratio. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Video duration in seconds (3-15s for reference video).
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
};
export type O3ProTextToVideoInput = {
  /**
   * Text prompt for video generation. Required unless multi_prompt is provided.
   */
  prompt?: string;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice
   */
  voice_ids?: Array<string>;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type O3ProTextToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type O3StandardEditV2VOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type O3StandardEditVideoV2VInput = {
  /**
   * Text prompt for video generation. Reference video as @Video1.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.
   */
  video_url: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Whether to keep the original audio from the reference video. Default value: `true`
   */
  keep_audio?: boolean;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ImageElementInput>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type O3StandardImageR2VOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type O3StandardImageToVideoInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * URL of the start frame image.
   */
  image_url: string | Blob | File;
  /**
   * URL of the end frame image (optional).
   */
  end_image_url?: string | Blob | File;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type O3StandardImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type O3StandardReferenceV2VOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type O3StandardReferenceVideoI2VInput = {
  /**
   * Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.
   */
  prompt?: string;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * Image to use as the first frame of the video.
   */
  start_image_url?: string | Blob | File;
  /**
   * Image to use as the last frame of the video.
   */
  end_image_url?: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ComboElementInput>;
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * The aspect ratio of the generated video frame. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
};
export type O3StandardReferenceVideoV2VInput = {
  /**
   * Text prompt for video generation. Reference video as @Video1.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.
   */
  video_url: string | Blob | File;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Whether to keep the original audio from the reference video. Default value: `true`
   */
  keep_audio?: boolean;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.
   */
  elements?: Array<KlingV3ImageElementInput>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
  /**
   * Aspect ratio. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Video duration in seconds (3-15s for reference video).
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
};
export type O3StandardTextToVideoInput = {
  /**
   * Text prompt for video generation. Required unless multi_prompt is provided.
   */
  prompt?: string;
  /**
   * Video duration in seconds (3-15s). Default value: `"5"`
   */
  duration?:
    | "3"
    | "4"
    | "5"
    | "6"
    | "7"
    | "8"
    | "9"
    | "10"
    | "11"
    | "12"
    | "13"
    | "14"
    | "15";
  /**
   * Aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Whether to generate native audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice
   */
  voice_ids?: Array<string>;
  /**
   * List of prompts for multi-shot video generation.
   */
  multi_prompt?: Array<KlingV3MultiPromptElement>;
  /**
   * The type of multi-shot video generation. Default value: `"customize"`
   */
  shot_type?: "customize";
};
export type O3StandardTextToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type ObjectOutput = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type ObjectRemovalBboxInput = {
  /**
   * The URL of the image to remove objects from.
   */
  image_url: string | Blob | File;
  /**
   * List of bounding box coordinates to erase (only one box prompt is supported)
   */
  box_prompts?: Array<BBoxPromptBase>;
  /**
   *  Default value: `"best_quality"`
   */
  model?: "low_quality" | "medium_quality" | "high_quality" | "best_quality";
  /**
   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`
   */
  mask_expansion?: number;
};
export type ObjectRemovalBboxOutput = {
  /**
   * The generated images with objects removed.
   */
  images: Array<Image>;
};
export type ObjectRemovalInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Specify which objects to remove from the image. Default value: `"background people"`
   */
  prompt?: string;
};
export type ObjectRemovalMaskInput = {
  /**
   * The URL of the image to remove objects from.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the mask image. White pixels (255) indicate areas to remove.
   */
  mask_url: string | Blob | File;
  /**
   *  Default value: `"best_quality"`
   */
  model?: "low_quality" | "medium_quality" | "high_quality" | "best_quality";
  /**
   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`
   */
  mask_expansion?: number;
};
export type ObjectRemovalMaskOutput = {
  /**
   * The generated images with objects removed.
   */
  images: Array<Image>;
};
export type ObjectRemovalOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type OmnigenV2Input = {
  /**
   * The prompt to generate or edit an image. Use specific language like 'Add the bird from image 1 to the desk in image 2' for better results.
   */
  prompt: string;
  /**
   * URLs of input images to use for image editing or multi-image generation. Support up to 3 images.
   */
  input_image_urls?: Array<string>;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The Text Guidance scale controls how closely the model follows the text prompt.
   * Higher values make the model stick more closely to the prompt. Default value: `5`
   */
  text_guidance_scale?: number;
  /**
   * The Image Guidance scale controls how closely the model follows the input images.
   * For image editing: 1.3-2.0, for in-context generation: 2.0-3.0 Default value: `2`
   */
  image_guidance_scale?: number;
  /**
   * Negative prompt to guide what should not be in the image. Default value: `"(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar"`
   */
  negative_prompt?: string;
  /**
   * CFG range start value.
   */
  cfg_range_start?: number;
  /**
   * CFG range end value. Default value: `1`
   */
  cfg_range_end?: number;
  /**
   * The scheduler to use for the diffusion process. Default value: `"euler"`
   */
  scheduler?: "euler" | "dpmsolver";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type OmnigenV2Output = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type OmniHumanInput = {
  /**
   * The URL of the image used to generate the video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file to generate the video. Audio must be under 30s long.
   */
  audio_url: string | Blob | File;
};
export type OmniHumanOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Duration of audio input/video output as used for billing.
   */
  duration: number;
};
export type OmniHumanv15Input = {
  /**
   * The text prompt used to guide the video generation.
   */
  prompt?: string;
  /**
   * The URL of the image used to generate the video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file to generate the video. Audio must be under 30s long for 1080p generation and under 60s long for 720p generation.
   */
  audio_url: string | Blob | File;
  /**
   * Generate a video at a faster rate with a slight quality trade-off.
   */
  turbo_mode?: boolean;
  /**
   * The resolution of the generated video. Defaults to 1080p. 720p generation is faster and higher in quality. 1080p generation is limited to 30s audio and 720p generation is limited to 60s audio. Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
};
export type OmniHumanv15Output = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Duration of audio input/video output as used for billing.
   */
  duration: number;
};
export type OmniImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type omnipartInput = {
  /**
   * URL of image to use while generating the 3D model.
   */
  input_image_url: string | Blob | File;
  /**
   * Minimum segment size (pixels) for the model. Default value: `2000`
   */
  minimum_segment_size?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time. Default value: `765464`
   */
  seed?: number;
  /**
   * Guidance scale for the model. Default value: `7.5`
   */
  guidance_scale?: number;
  /**
   * Specify which segments to merge (e.g., '0,1;3,4' merges segments 0&1 together and 3&4 together) Default value: `""`
   */
  parts?: string;
};
export type omnipartOutput = {
  /**
   * Generated 3D object file.
   */
  model_mesh: File;
  /**
   * Generated 3D object file.
   */
  full_model_mesh: File;
  /**
   * All outputs file.
   */
  output_zip: File;
  /**
   * Seed value used for generation.
   */
  seed: number;
};
export type OmniV2VEditInput = {
  /**
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
};
export type OmniV2VEditOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type OmniV2VReferenceInput = {
  /**
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
  /**
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Video duration in seconds. Default value: `"5"`
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
};
export type OmniV2VReferenceOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type OmniVideoElementInput = {
  /**
   * The frontal image of the element (main view).
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  frontal_image_url: string | Blob | File;
  /**
   * Additional reference images from different angles. 1-3 images supported. At least one image is required.
   */
  reference_image_urls?: Array<string>;
};
export type OmniVideoImageToVideoInput = {
  /**
   * Use @Image1 to reference the start frame, @Image2 to reference the end frame.
   */
  prompt: string;
  /**
   * Image to use as the first frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  start_image_url: string | Blob | File;
  /**
   * Image to use as the last frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  end_image_url?: string | Blob | File;
  /**
   * Video duration in seconds. Default value: `"5"`
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
};
export type OmniVideoImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type OmniVideoReferenceToVideoInput = {
  /**
   * Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image).
   */
  image_urls?: Array<string>;
  /**
   * Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image).
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * Video duration in seconds. Default value: `"5"`
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
  /**
   * The aspect ratio of the generated video frame. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
};
export type OmniVideoReferenceToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type OneToAllAnimation13bInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The negative prompt to generate the video from.
   */
  negative_prompt: string;
  /**
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string | Blob | File;
  /**
   * The resolution of the video to generate. Default value: `"480p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * The number of inference steps to use for the video generation. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The image guidance scale to use for the video generation. Default value: `2`
   */
  image_guidance_scale?: number;
  /**
   * The pose guidance scale to use for the video generation. Default value: `1.5`
   */
  pose_guidance_scale?: number;
};
export type OneToAllAnimation13bOutput = {
  /**
   * The generated video file.
   */
  video: File;
};
export type OneToAllAnimation14bInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The negative prompt to generate the video from.
   */
  negative_prompt: string;
  /**
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string | Blob | File;
  /**
   * The resolution of the video to generate. Default value: `"480p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * The number of inference steps to use for the video generation. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The image guidance scale to use for the video generation. Default value: `2`
   */
  image_guidance_scale?: number;
  /**
   * The pose guidance scale to use for the video generation. Default value: `1.5`
   */
  pose_guidance_scale?: number;
};
export type OneToAllAnimation14bOutput = {
  /**
   * The generated video file.
   */
  video: File;
};
export type OrpheusTtsInput = {
  /**
   * The text to be converted to speech. You can additionally add the following emotive tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>
   */
  text: string;
  /**
   * Voice ID for the desired voice. Default value: `"tara"`
   */
  voice?: "tara" | "leah" | "jess" | "leo" | "dan" | "mia" | "zac" | "zoe";
  /**
   * Temperature for generation (higher = more creative). Default value: `0.7`
   */
  temperature?: number;
  /**
   * Repetition penalty (>= 1.1 required for stable generations). Default value: `1.2`
   */
  repetition_penalty?: number;
};
export type OrpheusTtsOutput = {
  /**
   * The generated speech audio
   */
  audio: File;
};
export type OutpaintInput = {
  /**
   * Image URL to outpaint
   */
  image_url: string | Blob | File;
  /**
   * Number of pixels to add as black margin on the left side (0-700).
   */
  expand_left?: number;
  /**
   * Number of pixels to add as black margin on the right side (0-700).
   */
  expand_right?: number;
  /**
   * Number of pixels to add as black margin on the top side (0-700).
   */
  expand_top?: number;
  /**
   * Number of pixels to add as black margin on the bottom side (0-700). Default value: `400`
   */
  expand_bottom?: number;
  /**
   * Percentage to zoom out the image. If set, the image will be scaled down by this percentage and black margins will be added to maintain original size. Example: 50 means the image will be 50% of original size with black margins filling the rest. Default value: `20`
   */
  zoom_out_percentage?: number;
  /**
   * Optional prompt to guide the outpainting. If provided, it will be appended to the base outpaint instruction. Example: 'with a beautiful sunset in the background' Default value: `""`
   */
  prompt?: string;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If True, the function will wait for the image to be generated and uploaded before returning the response. If False, the function will return immediately and the image will be generated asynchronously.
   */
  sync_mode?: boolean;
  /**
   * The format of the output image. Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "jpg" | "webp";
};
export type OutpaintOutput = {
  /**
   * Outpainted image with extended scene
   */
  images: Array<Image>;
};
export type Output = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type OverlayImageInput = {
  /**
   * The URL of the background image
   *
   * Max file size: 9.5MB, Timeout: 20.0s
   */
  background_image_url: string | Blob | File;
  /**
   * The URL of the overlay image
   *
   * Max file size: 9.5MB, Timeout: 20.0s
   */
  overlay_image_url: string | Blob | File;
  /**
   * X position of overlay center as percentage of background width (0-100) Default value: `50`
   */
  x_percent?: number;
  /**
   * Y position of overlay center as percentage of background height (0-100) Default value: `50`
   */
  y_percent?: number;
  /**
   * Scale of overlay image as percentage (25-200) Default value: `100`
   */
  scale_percent?: number;
  /**
   * Opacity of overlay image (0.0-1.0) Default value: `1`
   */
  opacity?: number;
  /**
   * Width of stroke/border around overlay in pixels (0 for no stroke)
   */
  stroke_width?: number;
  /**
   * Color of stroke/border Default value: `"black"`
   */
  stroke_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Output format for the result image Default value: `"png"`
   */
  output_format?: "png" | "jpg" | "jpeg" | "webp";
};
export type OverlayImageOutput = {
  /**
   * Result image with overlay
   */
  image: Image;
};
export type OverlayVideoInput = {
  /**
   * URL of the main/background video
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  main_video_url: string | Blob | File;
  /**
   * URL of the overlay video to place on top
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  overlay_video_url: string | Blob | File;
  /**
   * X position of overlay center as percentage of background width (0-100) Default value: `14`
   */
  x_percent?: number;
  /**
   * Y position of overlay center as percentage of background height (0-100) Default value: `13`
   */
  y_percent?: number;
  /**
   * Scale of overlay video as percentage (10-200) Default value: `35`
   */
  scale_percent?: number;
  /**
   * Opacity of overlay video (0.0-1.0) Default value: `1`
   */
  opacity?: number;
  /**
   * End output when the shortest input ends Default value: `true`
   */
  shortest?: boolean;
};
export type OverlayVideoOutput = {
  /**
   * The output video with overlay
   */
  video: File;
};
export type OviImageToVideoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"jitter, bad hands, blur, distortion"`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Negative prompt for audio generation. Default value: `"robotic, muffled, echo, distorted"`
   */
  audio_negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * The image URL to guide video generation.
   */
  image_url: string | Blob | File;
};
export type OviImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video?: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type oviInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"jitter, bad hands, blur, distortion"`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Negative prompt for audio generation. Default value: `"robotic, muffled, echo, distorted"`
   */
  audio_negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video in W:H format. One of (512x992, 992x512, 960x512, 512x960, 720x720, or 448x1120). Default value: `"992x512"`
   */
  resolution?:
    | "512x992"
    | "992x512"
    | "960x512"
    | "512x960"
    | "720x720"
    | "448x1120"
    | "1120x448";
};
export type oviOutput = {
  /**
   * The generated video file.
   */
  video?: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type OvisImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type OvisImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ParabolizeInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Parabolize coefficient Default value: `1`
   */
  parabolize_coeff?: number;
  /**
   * Vertex X position Default value: `0.5`
   */
  vertex_x?: number;
  /**
   * Vertex Y position Default value: `0.5`
   */
  vertex_y?: number;
};
export type ParabolizeOutput = {
  /**
   * The processed images with parabolize effect
   */
  images: Array<Image>;
};
export type PartInput = {
  /**
   * URL of FBX file to split into parts. ONLY FBX format supported. Max size: 100MB, face count ≤30,000. Recommended: AIGC-generated models.
   */
  input_file_url: string | Blob | File;
};
export type PartOutput = {
  /**
   * List of generated part files in FBX format
   */
  result_files: Array<File>;
};
export type pasdInput = {
  /**
   * Input image to super-resolve
   */
  image_url: string | Blob | File;
  /**
   * Upscaling factor (1-4x) Default value: `2`
   */
  scale?: number;
  /**
   * Number of inference steps (10-50) Default value: `25`
   */
  steps?: number;
  /**
   * Guidance scale for diffusion (1.0-20.0) Default value: `7`
   */
  guidance_scale?: number;
  /**
   * ControlNet conditioning scale (0.1-1.0) Default value: `0.8`
   */
  conditioning_scale?: number;
  /**
   * Additional prompt to guide super-resolution Default value: `""`
   */
  prompt?: string;
  /**
   * Negative prompt to avoid unwanted artifacts Default value: `"blurry, dirty, messy, frames, deformed, dotted, noise, raster lines, unclear, lowres, over-smoothed, painting, ai generated"`
   */
  negative_prompt?: string;
};
export type pasdOutput = {
  /**
   * The generated super-resolved images
   */
  images: Array<Image>;
  /**
   * Timing information for different processing stages
   */
  timings?: any;
};
export type PerspectiveInput = {
  /**
   * Image URL for perspective change
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"front"`
   */
  target_perspective?:
    | "front"
    | "left_side"
    | "right_side"
    | "back"
    | "top_down"
    | "bottom_up"
    | "birds_eye"
    | "three_quarter_left"
    | "three_quarter_right";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type PerspectiveOutput = {
  /**
   * Image with changed perspective
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type PhotographyEffectsInput = {
  /**
   * Image URL for photography effects
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"film"`
   */
  effect_type?:
    | "film"
    | "vintage_film"
    | "portrait_photography"
    | "fashion_photography"
    | "street_photography"
    | "sepia_tone"
    | "film_grain"
    | "light_leaks"
    | "vignette_effect"
    | "instant_camera"
    | "golden_hour"
    | "dramatic_lighting"
    | "soft_focus"
    | "bokeh_effect"
    | "high_contrast"
    | "double_exposure";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type PhotographyEffectsOutput = {
  /**
   * Image with photography effects
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type PhotoLoraI2IInput = {
  /**
   * LoRA Scale of the photo lora model Default value: `0.75`
   */
  photo_lora_scale?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * URL of image to use for inpainting. or img2img
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
};
export type PhotoLoraInpaintInput = {
  /**
   * LoRA Scale of the photo lora model Default value: `0.75`
   */
  photo_lora_scale?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * URL of image to use for inpainting. or img2img
   */
  image_url: string | Blob | File;
  /**
   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`
   */
  strength?: number;
  /**
   * The mask to area to Inpaint in.
   */
  mask_url: string | Blob | File;
};
export type PhotoRestorationInput = {
  /**
   * URL of the old or damaged photo to restore.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type PhotoRestorationOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type piflowInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. You can choose between some presets or custom height and width
   * that **must be multiples of 8**. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducible generation. If set to None, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type piflowOutput = {
  /**
   * The URLs of the generated images.
   */
  images: Array<Image>;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type Pika22ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type Pika22KeyframesToVideoOutput = {
  /**
   * The generated video with transitions between keyframes
   */
  video: File;
};
export type Pika22PikascenesOutput = {
  /**
   * The generated video combining multiple images
   */
  video: File;
};
export type Pika22TextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PikadditionsOutput = {
  /**
   * The generated video with added objects/images
   */
  video: File;
};
export type PikaffectsOutput = {
  /**
   * The generated video with applied effect
   */
  video: File;
};
export type PikaswapsOutput = {
  /**
   * The generated video with swapped regions
   */
  video: File;
};
export type PikaV15PikaffectsInput = {
  /**
   * URL of the input image
   */
  image_url: string | Blob | File;
  /**
   * The Pikaffect to apply
   */
  pikaffect:
    | "Cake-ify"
    | "Crumble"
    | "Crush"
    | "Decapitate"
    | "Deflate"
    | "Dissolve"
    | "Explode"
    | "Eye-pop"
    | "Inflate"
    | "Levitate"
    | "Melt"
    | "Peel"
    | "Poke"
    | "Squish"
    | "Ta-da"
    | "Tear";
  /**
   * Text prompt to guide the effect
   */
  prompt?: string;
  /**
   * Negative prompt to guide the model
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
};
export type PikaV15PikaffectsOutput = {
  /**
   * The generated video with applied effect
   */
  video: File;
};
export type PikaV21ImageToVideoInput = {
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `5`
   */
  duration?: number;
};
export type PikaV21ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PikaV21TextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `5`
   */
  duration?: number;
};
export type PikaV21TextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PikaV22ImageToVideoInput = {
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
};
export type PikaV22ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PikaV22PikaframesInput = {
  /**
   * URLs of keyframe images (2-5 images) to create transitions between
   */
  image_urls: Array<string>;
  /**
   * Configuration for each transition. Length must be len(image_urls) - 1. Total duration of all transitions must not exceed 25 seconds. If not provided, uses default 5-second transitions with the global prompt.
   */
  transitions?: Array<KeyframeTransition>;
  /**
   * Default prompt for all transitions. Individual transition prompts override this.
   */
  prompt?: string;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
};
export type PikaV22PikaframesOutput = {
  /**
   * The generated video with transitions between keyframes
   */
  video: File;
};
export type PikaV22PikascenesInput = {
  /**
   * URLs of images to combine into a video
   */
  image_urls: Array<string>;
  /**
   * Text prompt describing the desired video
   */
  prompt: string;
  /**
   * A negative prompt to guide the model Default value: `"ugly, bad, terrible"`
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * The resolution of the generated video Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * Mode for integrating multiple images. Precise mode is more accurate, creative mode is more creative. Default value: `"precise"`
   */
  ingredients_mode?: "precise" | "creative";
};
export type PikaV22PikascenesOutput = {
  /**
   * The generated video combining multiple images
   */
  video: File;
};
export type PikaV22TextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `"ugly, bad, terrible"`
   */
  negative_prompt?: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "1080p" | "720p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "10";
};
export type PikaV22TextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PikaV2PikadditionsInput = {
  /**
   * URL of the input video
   */
  video_url: string | Blob | File;
  /**
   * URL of the image to add
   */
  image_url: string | Blob | File;
  /**
   * Text prompt describing what to add
   */
  prompt?: string;
  /**
   * Negative prompt to guide the model
   */
  negative_prompt?: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
};
export type PikaV2PikadditionsOutput = {
  /**
   * The generated video with added objects/images
   */
  video: File;
};
export type PikaV2TurboImageToVideoInput = {
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `5`
   */
  duration?: number;
};
export type PikaV2TurboImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PikaV2TurboTextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `5`
   */
  duration?: number;
};
export type PikaV2TurboTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseExtendFastInput = {
  /**
   * URL of the input video to extend
   */
  video_url: string | Blob | File;
  /**
   * Prompt describing how to extend the video
   */
  prompt: string;
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the extended video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The resolution of the generated video. Fast mode doesn't support 1080p Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * The model version to use for generation Default value: `"v4.5"`
   */
  model?: "v3.5" | "v4" | "v4.5" | "v5" | "v5.5" | "v5.6";
  /**
   * Random seed for generation
   */
  seed?: number;
};
export type PixverseExtendFastOutput = {
  /**
   * The extended video
   */
  video: File;
};
export type PixverseExtendInput = {
  /**
   * URL of the input video to extend
   */
  video_url: string | Blob | File;
  /**
   * Prompt describing how to extend the video
   */
  prompt: string;
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the extended video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 1080p videos are limited to 5 seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * The model version to use for generation Default value: `"v4.5"`
   */
  model?: "v3.5" | "v4" | "v4.5" | "v5" | "v5.5" | "v5.6";
  /**
   * Random seed for generation
   */
  seed?: number;
};
export type PixverseExtendOutput = {
  /**
   * The extended video
   */
  video: File;
};
export type PixverseLipsyncInput = {
  /**
   * URL of the input video
   */
  video_url: string | Blob | File;
  /**
   * URL of the input audio. If not provided, TTS will be used.
   */
  audio_url?: string | Blob | File;
  /**
   * Voice to use for TTS when audio_url is not provided Default value: `"Auto"`
   */
  voice_id?:
    | "Emily"
    | "James"
    | "Isabella"
    | "Liam"
    | "Chloe"
    | "Adrian"
    | "Harper"
    | "Ava"
    | "Sophia"
    | "Julia"
    | "Mason"
    | "Jack"
    | "Oliver"
    | "Ethan"
    | "Auto";
  /**
   * Text content for TTS when audio_url is not provided
   */
  text?: string;
};
export type PixverseLipsyncOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseSoundEffectsInput = {
  /**
   * URL of the input video to add sound effects to
   */
  video_url: string | Blob | File;
  /**
   * Whether to keep the original audio from the video
   */
  original_sound_switch?: boolean;
  /**
   * Description of the sound effect to generate. If empty, a random sound effect will be generated Default value: `""`
   */
  prompt?: string;
};
export type PixverseSoundEffectsOutput = {
  /**
   * The video with added sound effects
   */
  video: File;
};
export type PixverseSwapInput = {
  /**
   * URL of the external video to swap
   */
  video_url: string | Blob | File;
  /**
   * The swap mode to use Default value: `"person"`
   */
  mode?: "person" | "object" | "background";
  /**
   * The keyframe ID (from 1 to the last frame position) Default value: `1`
   */
  keyframe_id?: number;
  /**
   * URL of the target image for swapping
   */
  image_url: string | Blob | File;
  /**
   * The output resolution (1080p not supported) Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Whether to keep the original audio Default value: `true`
   */
  original_sound_switch?: boolean;
};
export type PixverseSwapOutput = {
  /**
   * The generated swapped video
   */
  video: File;
};
export type PixverseV35EffectsInput = {
  /**
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string | Blob | File;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
};
export type PixverseV35EffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV35ImageToVideoFastInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type PixverseV35ImageToVideoFastOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV35ImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type PixverseV35ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV35TextToVideoFastInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
};
export type PixverseV35TextToVideoFastOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV35TextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
};
export type PixverseV35TextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV35TransitionInput = {
  /**
   * The prompt for the transition
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  first_image_url: string | Blob | File;
  /**
   * URL of the image to use as the last frame
   */
  end_image_url?: string | Blob | File;
};
export type PixverseV35TransitionOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV45EffectsInput = {
  /**
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string | Blob | File;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
};
export type PixverseV45EffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV45ImageToVideoFastInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * The type of camera movement to apply to the video
   */
  camera_movement?:
    | "horizontal_left"
    | "horizontal_right"
    | "vertical_up"
    | "vertical_down"
    | "zoom_in"
    | "zoom_out"
    | "crane_up"
    | "quickly_zoom_in"
    | "quickly_zoom_out"
    | "smooth_zoom_in"
    | "camera_rotation"
    | "robo_arm"
    | "super_dolly_out"
    | "whip_pan"
    | "hitchcock"
    | "left_follow"
    | "right_follow"
    | "pan_left"
    | "pan_right"
    | "fix_bg";
};
export type PixverseV45ImageToVideoFastOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV45ImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * The type of camera movement to apply to the video
   */
  camera_movement?:
    | "horizontal_left"
    | "horizontal_right"
    | "vertical_up"
    | "vertical_down"
    | "zoom_in"
    | "zoom_out"
    | "crane_up"
    | "quickly_zoom_in"
    | "quickly_zoom_out"
    | "smooth_zoom_in"
    | "camera_rotation"
    | "robo_arm"
    | "super_dolly_out"
    | "whip_pan"
    | "hitchcock"
    | "left_follow"
    | "right_follow"
    | "pan_left"
    | "pan_right"
    | "fix_bg";
};
export type PixverseV45ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV45TextToVideoFastInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
};
export type PixverseV45TextToVideoFastOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV45TextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
};
export type PixverseV45TextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV45TransitionInput = {
  /**
   * The prompt for the transition
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  first_image_url: string | Blob | File;
  /**
   * URL of the image to use as the last frame
   */
  end_image_url?: string | Blob | File;
};
export type PixverseV45TransitionOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV4EffectsInput = {
  /**
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string | Blob | File;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
};
export type PixverseV4EffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV4ImageToVideoFastInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * The type of camera movement to apply to the video
   */
  camera_movement?:
    | "horizontal_left"
    | "horizontal_right"
    | "vertical_up"
    | "vertical_down"
    | "zoom_in"
    | "zoom_out"
    | "crane_up"
    | "quickly_zoom_in"
    | "quickly_zoom_out"
    | "smooth_zoom_in"
    | "camera_rotation"
    | "robo_arm"
    | "super_dolly_out"
    | "whip_pan"
    | "hitchcock"
    | "left_follow"
    | "right_follow"
    | "pan_left"
    | "pan_right"
    | "fix_bg";
};
export type PixverseV4ImageToVideoFastOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV4ImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * The type of camera movement to apply to the video
   */
  camera_movement?:
    | "horizontal_left"
    | "horizontal_right"
    | "vertical_up"
    | "vertical_down"
    | "zoom_in"
    | "zoom_out"
    | "crane_up"
    | "quickly_zoom_in"
    | "quickly_zoom_out"
    | "smooth_zoom_in"
    | "camera_rotation"
    | "robo_arm"
    | "super_dolly_out"
    | "whip_pan"
    | "hitchcock"
    | "left_follow"
    | "right_follow"
    | "pan_left"
    | "pan_right"
    | "fix_bg";
};
export type PixverseV4ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV4TextToVideoFastInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
};
export type PixverseV4TextToVideoFastOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV4TextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
};
export type PixverseV4TextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV55EffectsInput = {
  /**
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string | Blob | File;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "8" | "10";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
};
export type PixverseV55EffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV55ImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds Default value: `"5"`
   */
  duration?: "5" | "8" | "10";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Enable multi-clip generation with dynamic camera changes
   */
  generate_multi_clip_switch?: boolean;
  /**
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type PixverseV55ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV55TextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds Default value: `"5"`
   */
  duration?: "5" | "8" | "10";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Enable multi-clip generation with dynamic camera changes
   */
  generate_multi_clip_switch?: boolean;
  /**
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
};
export type PixverseV55TextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV55TransitionInput = {
  /**
   * The prompt for the transition
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds Default value: `"5"`
   */
  duration?: "5" | "8" | "10";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * URL of the image to use as the first frame
   */
  first_image_url: string | Blob | File;
  /**
   * URL of the image to use as the last frame
   */
  end_image_url?: string | Blob | File;
};
export type PixverseV55TransitionOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV56ImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds Default value: `"5"`
   */
  duration?: "5" | "8" | "10";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type PixverseV56ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV56TextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds Default value: `"5"`
   */
  duration?: "5" | "8" | "10";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
};
export type PixverseV56TextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV56TransitionInput = {
  /**
   * The prompt for the transition
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds Default value: `"5"`
   */
  duration?: "5" | "8" | "10";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * URL of the image to use as the first frame
   */
  first_image_url: string | Blob | File;
  /**
   * URL of the image to use as the last frame
   */
  end_image_url?: string | Blob | File;
};
export type PixverseV56TransitionOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV5EffectsInput = {
  /**
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string | Blob | File;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
};
export type PixverseV5EffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV5ImageToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type PixverseV5ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV5TextToVideoInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
};
export type PixverseV5TextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PixverseV5TransitionInput = {
  /**
   * The prompt for the transition
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `"5"`
   */
  duration?: "5" | "8";
  /**
   * Negative prompt to be used for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * URL of the image to use as the first frame
   */
  first_image_url: string | Blob | File;
  /**
   * URL of the image to use as the last frame
   */
  end_image_url?: string | Blob | File;
};
export type PixverseV5TransitionOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type PlushieStyleInput = {
  /**
   * URL of the image to convert to plushie style.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type PlushieStyleOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type plushifyInput = {
  /**
   * URL of the image to apply cartoon style to
   */
  image_url: string | Blob | File;
  /**
   * Prompt for the generation. Default is empty which is usually best, but sometimes it can help to add a description of the subject. Default value: `""`
   */
  prompt?: string;
  /**
   * Scale factor for the Cartoon effect Default value: `1`
   */
  scale?: number;
  /**
   * Guidance scale for the generation Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to use CFG zero
   */
  use_cfg_zero?: boolean;
  /**
   * The seed for image generation. Same seed with same parameters will generate same image.
   */
  seed?: number;
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
};
export type plushifyOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type PonyV7Input = {
  /**
   * The prompt to generate images from
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The seed to use for generating images
   */
  seed?: number;
  /**
   * Classifier free guidance scale Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to take Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * The source of the noise to use for generating images.
   * If set to 'gpu', the noise will be generated on the GPU.
   * If set to 'cpu', the noise will be generated on the CPU. Default value: `"gpu"`
   */
  noise_source?: "gpu" | "cpu";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type PonyV7Output = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type PortraitInput = {
  /**
   * Portrait image URL to enhance
   */
  image_url: string | Blob | File;
  /**
   * Aspect ratio for 4K output (default: 3:4 for portraits)
   */
  aspect_ratio?: AspectRatio;
};
export type PortraitOutput = {
  /**
   * Enhanced portrait
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type PostProcessingBlurInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Type of blur to apply Default value: `"gaussian"`
   */
  blur_type?: "gaussian" | "kuwahara";
  /**
   * Blur radius Default value: `3`
   */
  blur_radius?: number;
  /**
   * Sigma for Gaussian blur Default value: `1`
   */
  blur_sigma?: number;
};
export type PostProcessingBlurOutput = {
  /**
   * The processed images with blur effect
   */
  images: Array<Image>;
};
export type PostProcessingChromaticAberrationInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Red channel shift amount
   */
  red_shift?: number;
  /**
   * Red channel shift direction Default value: `"horizontal"`
   */
  red_direction?: "horizontal" | "vertical";
  /**
   * Green channel shift amount
   */
  green_shift?: number;
  /**
   * Green channel shift direction Default value: `"horizontal"`
   */
  green_direction?: "horizontal" | "vertical";
  /**
   * Blue channel shift amount
   */
  blue_shift?: number;
  /**
   * Blue channel shift direction Default value: `"horizontal"`
   */
  blue_direction?: "horizontal" | "vertical";
};
export type PostProcessingChromaticAberrationOutput = {
  /**
   * The processed images with chromatic aberration effect
   */
  images: Array<Image>;
};
export type PostProcessingColorCorrectionInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Color temperature adjustment
   */
  temperature?: number;
  /**
   * Brightness adjustment
   */
  brightness?: number;
  /**
   * Contrast adjustment
   */
  contrast?: number;
  /**
   * Saturation adjustment
   */
  saturation?: number;
  /**
   * Gamma adjustment Default value: `1`
   */
  gamma?: number;
};
export type PostProcessingColorCorrectionOutput = {
  /**
   * The processed images with color correction
   */
  images: Array<Image>;
};
export type PostProcessingColorTintInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Tint strength Default value: `1`
   */
  tint_strength?: number;
  /**
   * Tint color mode Default value: `"sepia"`
   */
  tint_mode?:
    | "sepia"
    | "red"
    | "green"
    | "blue"
    | "cyan"
    | "magenta"
    | "yellow"
    | "purple"
    | "orange"
    | "warm"
    | "cool"
    | "lime"
    | "navy"
    | "vintage"
    | "rose"
    | "teal"
    | "maroon"
    | "peach"
    | "lavender"
    | "olive";
};
export type PostProcessingColorTintOutput = {
  /**
   * The processed images with color tint effect
   */
  images: Array<Image>;
};
export type PostProcessingDesaturateInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Desaturation factor Default value: `1`
   */
  desaturate_factor?: number;
  /**
   * Desaturation method Default value: `"luminance (Rec.709)"`
   */
  desaturate_method?:
    | "luminance (Rec.709)"
    | "luminance (Rec.601)"
    | "average"
    | "lightness";
};
export type PostProcessingDesaturateOutput = {
  /**
   * The processed images with desaturation effect
   */
  images: Array<Image>;
};
export type PostProcessingDissolveInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * URL of second image for dissolve
   */
  dissolve_image_url: string | Blob | File;
  /**
   * Dissolve blend factor Default value: `0.5`
   */
  dissolve_factor?: number;
};
export type PostProcessingDissolveOutput = {
  /**
   * The processed images with dissolve effect
   */
  images: Array<Image>;
};
export type PostProcessingDodgeBurnInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Dodge and burn intensity Default value: `0.5`
   */
  dodge_burn_intensity?: number;
  /**
   * Dodge and burn mode Default value: `"dodge"`
   */
  dodge_burn_mode?:
    | "dodge"
    | "burn"
    | "dodge_and_burn"
    | "burn_and_dodge"
    | "color_dodge"
    | "color_burn"
    | "linear_dodge"
    | "linear_burn";
};
export type PostProcessingDodgeBurnOutput = {
  /**
   * The processed images with dodge and burn effect
   */
  images: Array<Image>;
};
export type PostProcessingGrainInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Film grain intensity Default value: `0.4`
   */
  grain_intensity?: number;
  /**
   * Film grain scale Default value: `10`
   */
  grain_scale?: number;
  /**
   * Style of film grain to apply Default value: `"modern"`
   */
  grain_style?:
    | "modern"
    | "analog"
    | "kodak"
    | "fuji"
    | "cinematic"
    | "newspaper";
};
export type PostProcessingGrainOutput = {
  /**
   * The processed images with grain effect
   */
  images: Array<Image>;
};
export type PostProcessingInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Enable film grain effect
   */
  enable_grain?: boolean;
  /**
   * Film grain intensity (when enabled) Default value: `0.4`
   */
  grain_intensity?: number;
  /**
   * Film grain scale (when enabled) Default value: `10`
   */
  grain_scale?: number;
  /**
   * Style of film grain to apply Default value: `"modern"`
   */
  grain_style?:
    | "modern"
    | "analog"
    | "kodak"
    | "fuji"
    | "cinematic"
    | "newspaper";
  /**
   * Enable color correction
   */
  enable_color_correction?: boolean;
  /**
   * Color temperature adjustment
   */
  temperature?: number;
  /**
   * Brightness adjustment
   */
  brightness?: number;
  /**
   * Contrast adjustment
   */
  contrast?: number;
  /**
   * Saturation adjustment
   */
  saturation?: number;
  /**
   * Gamma adjustment Default value: `1`
   */
  gamma?: number;
  /**
   * Enable chromatic aberration
   */
  enable_chromatic?: boolean;
  /**
   * Red channel shift amount
   */
  red_shift?: number;
  /**
   * Red channel shift direction Default value: `"horizontal"`
   */
  red_direction?: "horizontal" | "vertical";
  /**
   * Green channel shift amount
   */
  green_shift?: number;
  /**
   * Green channel shift direction Default value: `"horizontal"`
   */
  green_direction?: "horizontal" | "vertical";
  /**
   * Blue channel shift amount
   */
  blue_shift?: number;
  /**
   * Blue channel shift direction Default value: `"horizontal"`
   */
  blue_direction?: "horizontal" | "vertical";
  /**
   * Enable blur effect
   */
  enable_blur?: boolean;
  /**
   * Type of blur to apply Default value: `"gaussian"`
   */
  blur_type?: "gaussian" | "kuwahara";
  /**
   * Blur radius Default value: `3`
   */
  blur_radius?: number;
  /**
   * Sigma for Gaussian blur Default value: `1`
   */
  blur_sigma?: number;
  /**
   * Enable vignette effect
   */
  enable_vignette?: boolean;
  /**
   * Vignette strength (when enabled) Default value: `0.5`
   */
  vignette_strength?: number;
  /**
   * Enable parabolize effect
   */
  enable_parabolize?: boolean;
  /**
   * Parabolize coefficient Default value: `1`
   */
  parabolize_coeff?: number;
  /**
   * Vertex X position Default value: `0.5`
   */
  vertex_x?: number;
  /**
   * Vertex Y position Default value: `0.5`
   */
  vertex_y?: number;
  /**
   * Enable color tint effect
   */
  enable_tint?: boolean;
  /**
   * Tint strength Default value: `1`
   */
  tint_strength?: number;
  /**
   * Tint color mode Default value: `"sepia"`
   */
  tint_mode?:
    | "sepia"
    | "red"
    | "green"
    | "blue"
    | "cyan"
    | "magenta"
    | "yellow"
    | "purple"
    | "orange"
    | "warm"
    | "cool"
    | "lime"
    | "navy"
    | "vintage"
    | "rose"
    | "teal"
    | "maroon"
    | "peach"
    | "lavender"
    | "olive";
  /**
   * Enable dissolve effect
   */
  enable_dissolve?: boolean;
  /**
   * URL of second image for dissolve Default value: `""`
   */
  dissolve_image_url?: string | Blob | File;
  /**
   * Dissolve blend factor Default value: `0.5`
   */
  dissolve_factor?: number;
  /**
   * Enable dodge and burn effect
   */
  enable_dodge_burn?: boolean;
  /**
   * Dodge and burn intensity Default value: `0.5`
   */
  dodge_burn_intensity?: number;
  /**
   * Dodge and burn mode Default value: `"dodge"`
   */
  dodge_burn_mode?:
    | "dodge"
    | "burn"
    | "dodge_and_burn"
    | "burn_and_dodge"
    | "color_dodge"
    | "color_burn"
    | "linear_dodge"
    | "linear_burn";
  /**
   * Enable glow effect
   */
  enable_glow?: boolean;
  /**
   * Glow intensity Default value: `1`
   */
  glow_intensity?: number;
  /**
   * Glow blur radius Default value: `5`
   */
  glow_radius?: number;
  /**
   * Enable sharpen effect
   */
  enable_sharpen?: boolean;
  /**
   * Type of sharpening to apply Default value: `"basic"`
   */
  sharpen_mode?: "basic" | "smart" | "cas";
  /**
   * Sharpen radius (for basic mode) Default value: `1`
   */
  sharpen_radius?: number;
  /**
   * Sharpen strength (for basic mode) Default value: `1`
   */
  sharpen_alpha?: number;
  /**
   * Noise radius for smart sharpen Default value: `7`
   */
  noise_radius?: number;
  /**
   * Edge preservation factor Default value: `0.75`
   */
  preserve_edges?: number;
  /**
   * Smart sharpen strength Default value: `5`
   */
  smart_sharpen_strength?: number;
  /**
   * Smart sharpen blend ratio Default value: `0.5`
   */
  smart_sharpen_ratio?: number;
  /**
   * CAS sharpening amount Default value: `0.8`
   */
  cas_amount?: number;
  /**
   * Enable solarize effect
   */
  enable_solarize?: boolean;
  /**
   * Solarize threshold Default value: `0.5`
   */
  solarize_threshold?: number;
  /**
   * Enable desaturation effect
   */
  enable_desaturate?: boolean;
  /**
   * Desaturation factor Default value: `1`
   */
  desaturate_factor?: number;
  /**
   * Desaturation method Default value: `"luminance (Rec.709)"`
   */
  desaturate_method?:
    | "luminance (Rec.709)"
    | "luminance (Rec.601)"
    | "average"
    | "lightness";
};
export type PostProcessingOutput = {
  /**
   * The processed images
   */
  images: Array<Image>;
};
export type PostProcessingParabolizeInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Parabolize coefficient Default value: `1`
   */
  parabolize_coeff?: number;
  /**
   * Vertex X position Default value: `0.5`
   */
  vertex_x?: number;
  /**
   * Vertex Y position Default value: `0.5`
   */
  vertex_y?: number;
};
export type PostProcessingParabolizeOutput = {
  /**
   * The processed images with parabolize effect
   */
  images: Array<Image>;
};
export type PostProcessingSharpenInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Type of sharpening to apply Default value: `"basic"`
   */
  sharpen_mode?: "basic" | "smart" | "cas";
  /**
   * Sharpen radius (for basic mode) Default value: `1`
   */
  sharpen_radius?: number;
  /**
   * Sharpen strength (for basic mode) Default value: `1`
   */
  sharpen_alpha?: number;
  /**
   * Noise radius for smart sharpen Default value: `7`
   */
  noise_radius?: number;
  /**
   * Edge preservation factor Default value: `0.75`
   */
  preserve_edges?: number;
  /**
   * Smart sharpen strength Default value: `5`
   */
  smart_sharpen_strength?: number;
  /**
   * Smart sharpen blend ratio Default value: `0.5`
   */
  smart_sharpen_ratio?: number;
  /**
   * CAS sharpening amount Default value: `0.8`
   */
  cas_amount?: number;
};
export type PostProcessingSharpenOutput = {
  /**
   * The processed images with sharpen effect
   */
  images: Array<Image>;
};
export type PostProcessingSolarizeInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Solarize threshold Default value: `0.5`
   */
  solarize_threshold?: number;
};
export type PostProcessingSolarizeOutput = {
  /**
   * The processed images with solarize effect
   */
  images: Array<Image>;
};
export type PostProcessingVignetteInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Vignette strength Default value: `0.5`
   */
  vignette_strength?: number;
};
export type PostProcessingVignetteOutput = {
  /**
   * The processed images with vignette effect
   */
  images: Array<Image>;
};
export type ProcessedOutput = {
  /**
   * The processed images
   */
  images: Array<Image>;
};
export type ProductHoldingInput = {
  /**
   * Image URL of the person who will hold the product
   */
  person_image_url: string | Blob | File;
  /**
   * Image URL of the product to be held by the person
   */
  product_image_url: string | Blob | File;
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ProductHoldingOutput = {
  /**
   * Person holding the product naturally
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ProductPhotographyInput = {
  /**
   * Image URL of the product to create professional studio photography
   */
  product_image_url: string | Blob | File;
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type ProductPhotographyOutput = {
  /**
   * Professional studio product photography
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type ProductShotInput = {
  /**
   * The URL of the product shot to be placed in a lifestyle shot. If both image_url and image_file are provided, image_url will be used. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB.
   */
  image_url: string | Blob | File;
  /**
   * Text description of the new scene or background for the provided product shot. Bria currently supports prompts in English only, excluding special characters.
   */
  scene_description?: string;
  /**
   * The URL of the reference image to be used for generating the new scene or background for the product shot. Use "" to leave empty.Either ref_image_url or scene_description has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `""`
   */
  ref_image_url?: string | Blob | File;
  /**
   * Whether to optimize the scene description Default value: `true`
   */
  optimize_description?: boolean;
  /**
   * The number of lifestyle product shots you would like to generate. You will get num_results x 10 results when placement_type=automatic and according to the number of required placements x num_results if placement_type=manual_placement. Default value: `1`
   */
  num_results?: number;
  /**
   * Whether to use the fast model Default value: `true`
   */
  fast?: boolean;
  /**
   * This parameter allows you to control the positioning of the product in the image. Choosing 'original' will preserve the original position of the product in the image. Choosing 'automatic' will generate results with the 10 recommended positions for the product. Choosing 'manual_placement' will allow you to select predefined positions (using the parameter 'manual_placement_selection'). Selecting 'manual_padding' will allow you to control the position and size of the image by defining the desired padding in pixels around the product. Default value: `"manual_placement"`
   */
  placement_type?:
    | "original"
    | "automatic"
    | "manual_placement"
    | "manual_padding";
  /**
   * This flag is only relevant when placement_type=original. If true, the output image retains the original input image's size; otherwise, the image is scaled to 1 megapixel (1MP) while preserving its aspect ratio.
   */
  original_quality?: boolean;
  /**
   * The desired size of the final product shot. For optimal results, the total number of pixels should be around 1,000,000. This parameter is only relevant when placement_type=automatic or placement_type=manual_placement.
   */
  shot_size?: Array<number>;
  /**
   * If you've selected placement_type=manual_placement, you should use this parameter to specify which placements/positions you would like to use from the list. You can select more than one placement in one request. Default value: `"bottom_center"`
   */
  manual_placement_selection?:
    | "upper_left"
    | "upper_right"
    | "bottom_left"
    | "bottom_right"
    | "right_center"
    | "left_center"
    | "upper_center"
    | "bottom_center"
    | "center_vertical"
    | "center_horizontal";
  /**
   * The desired padding in pixels around the product, when using placement_type=manual_padding. The order of the values is [left, right, top, bottom]. For optimal results, the total number of pixels, including padding, should be around 1,000,000. It is recommended to first use the product cutout API, get the cutout and understand the size of the result, and then define the required padding and use the cutout as an input for this API.
   */
  padding_values?: Array<number>;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ProductShotOutput = {
  /**
   * The generated images
   */
  images: Array<Image>;
};
export type ProFastImageToVideoHailuo23Input = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type ProFastImageToVideoHailuo23Output = {
  /**
   * The generated video
   */
  video: File;
};
export type ProfessionalPhotoOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ProImageTo3DInput = {
  /**
   * Front view image URL. Resolution: 128-5000px, max 8MB, formats: JPG/PNG/WEBP. Tips: simple background, single object, object >50% of frame.
   */
  input_image_url: string | Blob | File;
  /**
   * Optional back/rear view image URL (JPG/PNG recommended).
   */
  back_image_url?: string | Blob | File;
  /**
   * Optional left side view image URL (JPG/PNG recommended).
   */
  left_image_url?: string | Blob | File;
  /**
   * Optional right side view image URL (JPG/PNG recommended).
   */
  right_image_url?: string | Blob | File;
  /**
   * Optional top view image URL (v3.1 exclusive, JPG/PNG recommended).
   */
  top_image_url?: string | Blob | File;
  /**
   * Optional bottom view image URL (v3.1 exclusive, JPG/PNG recommended).
   */
  bottom_image_url?: string | Blob | File;
  /**
   * Optional left-front 45 degree angle view image URL (v3.1 exclusive, JPG/PNG recommended).
   */
  left_front_image_url?: string | Blob | File;
  /**
   * Optional right-front 45 degree angle view image URL (v3.1 exclusive, JPG/PNG recommended).
   */
  right_front_image_url?: string | Blob | File;
  /**
   * Generation task type. Normal: textured model. Geometry: geometry-only white model (no textures). LowPoly/Sketch are not available in v3.1. Default value: `"Normal"`
   */
  generate_type?: "Normal" | "Geometry";
  /**
   * Enable PBR material generation (metallic, roughness, normal textures). Ignored when generate_type is Geometry.
   */
  enable_pbr?: boolean;
  /**
   * Target polygon face count. Range: 40,000-1,500,000. Default: 500,000. Default value: `500000`
   */
  face_count?: number;
};
export type ProImageTo3DOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * The seed used for generation
   */
  seed?: number;
};
export type ProImageToVideoHailuo02Input = {
  /**
   *
   */
  prompt: string;
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * Optional URL of the image to use as the last frame of the video
   */
  end_image_url?: string | Blob | File;
};
export type ProImageToVideoHailuo23Input = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type ProImageToVideoHailuo23Output = {
  /**
   * The generated video
   */
  video: File;
};
export type ProImageToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"auto"`
   */
  resolution?: "auto" | "720p" | "1080p";
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "9:16" | "16:9";
  /**
   * Duration of the generated video in seconds Default value: `"4"`
   */
  duration?: "4" | "8" | "12";
  /**
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted. Default value: `true`
   */
  delete_video?: boolean;
  /**
   * The URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type ProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: VideoFile;
  /**
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
};
export type PromptInput = {
  /**
   * The URL of the image to remove objects from.
   */
  image_url: string | Blob | File;
  /**
   * Text description of the object to remove.
   */
  prompt: string;
  /**
   *  Default value: `"best_quality"`
   */
  model?: "low_quality" | "medium_quality" | "high_quality" | "best_quality";
  /**
   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`
   */
  mask_expansion?: number;
};
export type ProSketchTo3DInput = {
  /**
   * Sketch or line art image URL. Resolution: 128-5000px, max 8MB, formats: JPG/PNG/WEBP.
   */
  input_image_url: string | Blob | File;
  /**
   * Text prompt describing 3D content attributes (color, category, material). Used together with sketch image. Max 1024 UTF-8 characters.
   */
  prompt: string;
  /**
   * Enable PBR material generation (metallic, roughness, normal textures).
   */
  enable_pbr?: boolean;
  /**
   * Target polygon face count. Range: 40,000-1,500,000. Default: 500,000. Default value: `500000`
   */
  face_count?: number;
};
export type ProSketchTo3DOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * The seed used for generation
   */
  seed?: number;
};
export type ProTextTo3DInput = {
  /**
   * Text description of the 3D content to generate. Max 1024 UTF-8 characters.
   */
  prompt: string;
  /**
   * Generation task type. Normal: textured model. Geometry: geometry-only white model (no textures). LowPoly/Sketch are not available in v3.1. Default value: `"Normal"`
   */
  generate_type?: "Normal" | "Geometry";
  /**
   * Enable PBR material generation (metallic, roughness, normal textures). Ignored when generate_type is Geometry.
   */
  enable_pbr?: boolean;
  /**
   * Target polygon face count. Range: 40,000-1,500,000. Default: 500,000. Default value: `500000`
   */
  face_count?: number;
};
export type ProTextTo3DOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * The seed used for generation
   */
  seed?: number;
};
export type ProTextToVideoHailuo02Input = {
  /**
   *
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type ProTextToVideoHailuo23Input = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type ProTextToVideoHailuo23Output = {
  /**
   * The generated video
   */
  video: File;
};
export type ProTextToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Duration of the generated video in seconds Default value: `"4"`
   */
  duration?: "4" | "8" | "12";
  /**
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted. Default value: `true`
   */
  delete_video?: boolean;
};
export type ProTextToVideoOutput = {
  /**
   * The generated video
   */
  video: VideoFile;
  /**
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
};
export type pshumanInput = {
  /**
   * A direct URL to the input image of a person.
   */
  image_url: string | Blob | File;
  /**
   * Guidance scale for the diffusion process. Controls how much the output adheres to the generated views. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * Seed for reproducibility. If None, a random seed will be used.
   */
  seed?: number;
};
export type pshumanOutput = {
  /**
   * The generated 3D model in OBJ format.
   */
  model_obj: File;
  /**
   * A preview image showing the input and the generated multi-view outputs.
   */
  preview_image: File;
};
export type Q1ImageToVideoOutput = {
  /**
   * The generated video using the Q1 model from a single image
   */
  video: File;
};
export type Q1ReferenceToVideoOutput = {
  /**
   * The generated video with consistent subjects from reference images using the Q1 model
   */
  video: File;
};
export type Q1StartEndToVideoOutput = {
  /**
   * The generated transition video between start and end frames using the Q1 model
   */
  video: File;
};
export type Q1TextToVideoOutput = {
  /**
   * The generated video using the Q1 model
   */
  video: File;
};
export type Q2ImageToVideoOutput = {
  /**
   * The generated video from image using the Q2 model
   */
  video: File;
};
export type Q2ProReferenceToVideoOutput = {
  /**
   * The generated video with video/image references using the Q2 Pro model
   */
  video: File;
};
export type Q2ReferenceToVideoOutput = {
  /**
   * The generated video with consistent subjects from reference images using the Q2 model
   */
  video: File;
};
export type Q2TextToVideoOutput = {
  /**
   * The generated video from text using the Q2 model
   */
  video: File;
};
export type Q2VideoExtensionOutput = {
  /**
   * The extended video using the Q2 model
   */
  video: File;
};
export type Q3ImageToVideoOutput = {
  /**
   * The generated video from image using the Q3 model
   */
  video: File;
};
export type Q3TextToVideoOutput = {
  /**
   * The generated video from text using the Q3 model
   */
  video: File;
};
export type QueryInput = {
  /**
   * Image URL to be processed
   */
  image_url: string | Blob | File;
  /**
   * Type of task to perform Default value: `"caption"`
   */
  task_type?: "caption" | "query";
  /**
   * Prompt for query task
   */
  prompt: string;
  /**
   * Maximum number of tokens to generate Default value: `64`
   */
  max_tokens?: number;
};
export type Qwen3CloneVoiceInput = {
  /**
   * URL to the reference audio file used for voice cloning.
   */
  audio_url: string | Blob | File;
  /**
   * Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice.
   */
  reference_text?: string;
};
export type Qwen3CloneVoiceOutput = {
  /**
   * The generated speaker embedding file in safetensors format.
   */
  speaker_embedding: File;
};
export type Qwen3DesignVoiceInput = {
  /**
   * The text to be converted to speech.
   */
  text: string;
  /**
   * The language of the voice to be designed. Default value: `"Auto"`
   */
  language?:
    | "Auto"
    | "English"
    | "Chinese"
    | "Spanish"
    | "French"
    | "German"
    | "Italian"
    | "Japanese"
    | "Korean"
    | "Portuguese"
    | "Russian";
  /**
   * Optional prompt to guide the style of the generated speech.
   */
  prompt: string;
  /**
   * Top-k sampling parameter. Default value: `50`
   */
  top_k?: number;
  /**
   * Top-p sampling parameter. Default value: `1`
   */
  top_p?: number;
  /**
   * Sampling temperature; higher => more random. Default value: `0.9`
   */
  temperature?: number;
  /**
   * Penalty to reduce repeated tokens/codes. Default value: `1.05`
   */
  repetition_penalty?: number;
  /**
   * Sampling switch for the sub-talker. Default value: `true`
   */
  subtalker_dosample?: boolean;
  /**
   * Top-k for sub-talker sampling. Default value: `50`
   */
  subtalker_top_k?: number;
  /**
   * Top-p for sub-talker sampling. Default value: `1`
   */
  subtalker_top_p?: number;
  /**
   * Temperature for sub-talker sampling. Default value: `0.9`
   */
  subtalker_temperature?: number;
  /**
   * Maximum number of new codec tokens to generate. Default value: `200`
   */
  max_new_tokens?: number;
};
export type Qwen3DesignVoiceOutput = {
  /**
   * The generated speech audio file.
   */
  audio: AudioFile;
};
export type Qwen3GuardInput = {
  /**
   * The input text to be classified
   */
  prompt: string;
};
export type Qwen3GuardOutput = {
  /**
   * The classification label
   */
  label: "Safe" | "Unsafe" | "Controversial";
  /**
   * The confidence score of the classification
   */
  categories: Array<
    | "Violent"
    | "Non-violent Illegal Acts"
    | "Sexual Content or Sexual Acts"
    | "PII"
    | "Suicide & Self-Harm"
    | "Unethical Acts"
    | "Politically Sensitive Topics"
    | "Copyright Violation"
    | "Jailbreak"
    | "None"
  >;
};
export type Qwen3TtsCloneVoice06bInput = {
  /**
   * URL to the reference audio file used for voice cloning.
   */
  audio_url: string | Blob | File;
  /**
   * Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice.
   */
  reference_text?: string;
};
export type Qwen3TtsCloneVoice06bOutput = {
  /**
   * The generated speaker embedding file in safetensors format.
   */
  speaker_embedding: File;
};
export type Qwen3TtsCloneVoice17bInput = {
  /**
   * URL to the reference audio file used for voice cloning.
   */
  audio_url: string | Blob | File;
  /**
   * Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice.
   */
  reference_text?: string;
};
export type Qwen3TtsCloneVoice17bOutput = {
  /**
   * The generated speaker embedding file in safetensors format.
   */
  speaker_embedding: File;
};
export type Qwen3TTSInput = {
  /**
   * The text to be converted to speech.
   */
  text: string;
  /**
   * Optional prompt to guide the style of the generated speech. This prompt will be ignored if a speaker embedding is provided.
   */
  prompt?: string;
  /**
   * The voice to be used for speech synthesis, will be ignored if a speaker embedding is provided. Check out the **[documentation](https://github.com/QwenLM/Qwen3-TTS/tree/main?tab=readme-ov-file#custom-voice-generate)** for each voice's details and which language they primarily support.
   */
  voice?:
    | "Vivian"
    | "Serena"
    | "Uncle_Fu"
    | "Dylan"
    | "Eric"
    | "Ryan"
    | "Aiden"
    | "Ono_Anna"
    | "Sohee";
  /**
   * The language of the voice. Default value: `"Auto"`
   */
  language?:
    | "Auto"
    | "English"
    | "Chinese"
    | "Spanish"
    | "French"
    | "German"
    | "Italian"
    | "Japanese"
    | "Korean"
    | "Portuguese"
    | "Russian";
  /**
   * URL to a speaker embedding file in safetensors format, from `fal-ai/qwen-3-tts/clone-voice` endpoint. If provided, the TTS model will use the cloned voice for synthesis instead of the predefined voices.
   */
  speaker_voice_embedding_file_url?: string | Blob | File;
  /**
   * Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice.
   */
  reference_text?: string;
  /**
   * Top-k sampling parameter. Default value: `50`
   */
  top_k?: number;
  /**
   * Top-p sampling parameter. Default value: `1`
   */
  top_p?: number;
  /**
   * Sampling temperature; higher => more random. Default value: `0.9`
   */
  temperature?: number;
  /**
   * Penalty to reduce repeated tokens/codes. Default value: `1.05`
   */
  repetition_penalty?: number;
  /**
   * Sampling switch for the sub-talker. Default value: `true`
   */
  subtalker_dosample?: boolean;
  /**
   * Top-k for sub-talker sampling. Default value: `50`
   */
  subtalker_top_k?: number;
  /**
   * Top-p for sub-talker sampling. Default value: `1`
   */
  subtalker_top_p?: number;
  /**
   * Temperature for sub-talker sampling. Default value: `0.9`
   */
  subtalker_temperature?: number;
  /**
   * Maximum number of new codec tokens to generate. Default value: `200`
   */
  max_new_tokens?: number;
};
export type Qwen3TTSOutput = {
  /**
   * The generated speech audio file.
   */
  audio: AudioFile;
};
export type Qwen3TtsTextToSpeech06bInput = {
  /**
   * The text to be converted to speech.
   */
  text: string;
  /**
   * Optional prompt to guide the style of the generated speech. This prompt will be ignored if a speaker embedding is provided.
   */
  prompt?: string;
  /**
   * The voice to be used for speech synthesis, will be ignored if a speaker embedding is provided. Check out the **[documentation](https://github.com/QwenLM/Qwen3-TTS/tree/main?tab=readme-ov-file#custom-voice-generate)** for each voice's details and which language they primarily support.
   */
  voice?:
    | "Vivian"
    | "Serena"
    | "Uncle_Fu"
    | "Dylan"
    | "Eric"
    | "Ryan"
    | "Aiden"
    | "Ono_Anna"
    | "Sohee";
  /**
   * The language of the voice. Default value: `"Auto"`
   */
  language?:
    | "Auto"
    | "English"
    | "Chinese"
    | "Spanish"
    | "French"
    | "German"
    | "Italian"
    | "Japanese"
    | "Korean"
    | "Portuguese"
    | "Russian";
  /**
   * URL to a speaker embedding file in safetensors format, from `fal-ai/qwen-3-tts/clone-voice/0.6b` endpoint. If provided, the TTS model will use the cloned voice for synthesis instead of the predefined voices.
   */
  speaker_voice_embedding_file_url?: string | Blob | File;
  /**
   * Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice.
   */
  reference_text?: string;
  /**
   * Top-k sampling parameter. Default value: `50`
   */
  top_k?: number;
  /**
   * Top-p sampling parameter. Default value: `1`
   */
  top_p?: number;
  /**
   * Sampling temperature; higher => more random. Default value: `0.9`
   */
  temperature?: number;
  /**
   * Penalty to reduce repeated tokens/codes. Default value: `1.05`
   */
  repetition_penalty?: number;
  /**
   * Sampling switch for the sub-talker. Default value: `true`
   */
  subtalker_dosample?: boolean;
  /**
   * Top-k for sub-talker sampling. Default value: `50`
   */
  subtalker_top_k?: number;
  /**
   * Top-p for sub-talker sampling. Default value: `1`
   */
  subtalker_top_p?: number;
  /**
   * Temperature for sub-talker sampling. Default value: `0.9`
   */
  subtalker_temperature?: number;
  /**
   * Maximum number of new codec tokens to generate. Default value: `200`
   */
  max_new_tokens?: number;
};
export type Qwen3TtsTextToSpeech06bOutput = {
  /**
   * The generated speech audio file.
   */
  audio: AudioFile;
};
export type Qwen3TtsTextToSpeech17bInput = {
  /**
   * The text to be converted to speech.
   */
  text: string;
  /**
   * Optional prompt to guide the style of the generated speech. This prompt will be ignored if a speaker embedding is provided.
   */
  prompt?: string;
  /**
   * The voice to be used for speech synthesis, will be ignored if a speaker embedding is provided. Check out the **[documentation](https://github.com/QwenLM/Qwen3-TTS/tree/main?tab=readme-ov-file#custom-voice-generate)** for each voice's details and which language they primarily support.
   */
  voice?:
    | "Vivian"
    | "Serena"
    | "Uncle_Fu"
    | "Dylan"
    | "Eric"
    | "Ryan"
    | "Aiden"
    | "Ono_Anna"
    | "Sohee";
  /**
   * The language of the voice. Default value: `"Auto"`
   */
  language?:
    | "Auto"
    | "English"
    | "Chinese"
    | "Spanish"
    | "French"
    | "German"
    | "Italian"
    | "Japanese"
    | "Korean"
    | "Portuguese"
    | "Russian";
  /**
   * URL to a speaker embedding file in safetensors format, from `fal-ai/qwen-3-tts/clone-voice` endpoint. If provided, the TTS model will use the cloned voice for synthesis instead of the predefined voices.
   */
  speaker_voice_embedding_file_url?: string | Blob | File;
  /**
   * Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice.
   */
  reference_text?: string;
  /**
   * Top-k sampling parameter. Default value: `50`
   */
  top_k?: number;
  /**
   * Top-p sampling parameter. Default value: `1`
   */
  top_p?: number;
  /**
   * Sampling temperature; higher => more random. Default value: `0.9`
   */
  temperature?: number;
  /**
   * Penalty to reduce repeated tokens/codes. Default value: `1.05`
   */
  repetition_penalty?: number;
  /**
   * Sampling switch for the sub-talker. Default value: `true`
   */
  subtalker_dosample?: boolean;
  /**
   * Top-k for sub-talker sampling. Default value: `50`
   */
  subtalker_top_k?: number;
  /**
   * Top-p for sub-talker sampling. Default value: `1`
   */
  subtalker_top_p?: number;
  /**
   * Temperature for sub-talker sampling. Default value: `0.9`
   */
  subtalker_temperature?: number;
  /**
   * Maximum number of new codec tokens to generate. Default value: `200`
   */
  max_new_tokens?: number;
};
export type Qwen3TtsTextToSpeech17bOutput = {
  /**
   * The generated speech audio file.
   */
  audio: AudioFile;
};
export type Qwen3TtsVoiceDesign17bInput = {
  /**
   * The text to be converted to speech.
   */
  text: string;
  /**
   * The language of the voice to be designed. Default value: `"Auto"`
   */
  language?:
    | "Auto"
    | "English"
    | "Chinese"
    | "Spanish"
    | "French"
    | "German"
    | "Italian"
    | "Japanese"
    | "Korean"
    | "Portuguese"
    | "Russian";
  /**
   * Optional prompt to guide the style of the generated speech.
   */
  prompt: string;
  /**
   * Top-k sampling parameter. Default value: `50`
   */
  top_k?: number;
  /**
   * Top-p sampling parameter. Default value: `1`
   */
  top_p?: number;
  /**
   * Sampling temperature; higher => more random. Default value: `0.9`
   */
  temperature?: number;
  /**
   * Penalty to reduce repeated tokens/codes. Default value: `1.05`
   */
  repetition_penalty?: number;
  /**
   * Sampling switch for the sub-talker. Default value: `true`
   */
  subtalker_dosample?: boolean;
  /**
   * Top-k for sub-talker sampling. Default value: `50`
   */
  subtalker_top_k?: number;
  /**
   * Top-p for sub-talker sampling. Default value: `1`
   */
  subtalker_top_p?: number;
  /**
   * Temperature for sub-talker sampling. Default value: `0.9`
   */
  subtalker_temperature?: number;
  /**
   * Maximum number of new codec tokens to generate. Default value: `200`
   */
  max_new_tokens?: number;
};
export type Qwen3TtsVoiceDesign17bOutput = {
  /**
   * The generated speech audio file.
   */
  audio: AudioFile;
};
export type QwenImage2512Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type QwenImage2512LoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
};
export type QwenImage2512LoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImage2512Output = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImage2512TrainerInput = {
  /**
   * URL to the input data zip archive for text-to-image training.
   *
   * The zip should contain images with their corresponding text captions:
   *
   * image.EXT and image.txt
   * For example:
   * photo.jpg and photo.txt
   *
   * The text file contains the caption/prompt describing the target image.
   *
   * If no text file is provided for an image, the default_caption will be used.
   *
   * If no default_caption is provided and a text file is missing, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Learning rate for LoRA parameters. Default value: `0.0005`
   */
  learning_rate?: number;
  /**
   * Number of steps to train for Default value: `1000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
};
export type QwenImage2512TrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type QwenImage2512TrainerV2Input = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images and corresponding captions.
   *
   * The images should be named: ROOT.EXT. For example: 001.jpg
   *
   * The corresponding captions should be named: ROOT.txt. For example: 001.txt
   *
   * If no text file is provided for an image, the default_caption will be used.
   */
  image_data_url: string | Blob | File;
  /**
   * Number of steps to train for Default value: `2000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Learning rate. Default value: `0.0005`
   */
  learning_rate?: number;
};
export type QwenImage2512TrainerV2Output = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type QwenImageEdit2509Input = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URLs of the images to edit.
   */
  image_urls: Array<string>;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
};
export type QwenImageEdit2509LoraGalleryAddBackgroundInput = {
  /**
   * The URLs of the images to edit. Provide an image with a white or clean background.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the background/scene you want to add behind the object. The model will remove the white background and add the specified environment. Default value: `"Remove white background and add a realistic scene behind the object"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEdit2509LoraGalleryAddBackgroundOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraGalleryFaceToFullPortraitInput = {
  /**
   * The URL of the cropped face image. Provide a close-up face photo.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the full portrait you want to generate from the face. Include clothing, setting, pose, and style details. Default value: `"Photography. A portrait of the person in professional attire with natural lighting"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEdit2509LoraGalleryFaceToFullPortraitOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraGalleryGroupPhotoInput = {
  /**
   * The URLs of the images to combine into a group photo. Provide 2 or more individual portrait images.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the group photo scene, setting, and style. The model will maintain character consistency and add vintage effects like grain, blur, and retro filters. Default value: `"Two people standing next to each other outside with a landscape background"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEdit2509LoraGalleryGroupPhotoOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraGalleryIntegrateProductInput = {
  /**
   * The URL of the image with product to integrate into background.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe how to blend and integrate the product/element into the background. The model will automatically correct perspective, lighting and shadows for natural integration. Default value: `"Blend and integrate the product into the background"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEdit2509LoraGalleryIntegrateProductOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraGalleryLightingRestorationInput = {
  /**
   * The URL of the image to restore lighting for.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
};
export type QwenImageEdit2509LoraGalleryLightingRestorationOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraGalleryMultipleAnglesInput = {
  /**
   * The URL of the image to adjust camera angle for.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Rotate camera left (positive) or right (negative) in degrees. Positive values rotate left, negative values rotate right.
   */
  rotate_right_left?: number;
  /**
   * Move camera forward (0=no movement, 10=close-up)
   */
  move_forward?: number;
  /**
   * Adjust vertical camera angle (-1=bird's-eye view/looking down, 0=neutral, 1=worm's-eye view/looking up)
   */
  vertical_angle?: number;
  /**
   * Enable wide-angle lens effect
   */
  wide_angle_lens?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the camera control effect. Default value: `1.25`
   */
  lora_scale?: number;
};
export type QwenImageEdit2509LoraGalleryMultipleAnglesOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraGalleryNextSceneInput = {
  /**
   * The URL of the image to create the next scene from.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the camera movement, framing change, or scene transition. Start with 'Next Scene:' for best results. Examples: camera movements (dolly, push-in, pull-back), framing changes (wide to close-up), new elements entering frame. Default value: `"Next Scene: The camera moves forward revealing more of the scene"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEdit2509LoraGalleryNextSceneOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraGalleryRemoveElementInput = {
  /**
   * The URL of the image containing elements to remove.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Specify what element(s) to remove from the image (objects, people, text, etc.). The model will cleanly remove the element while maintaining consistency of the rest of the image. Default value: `"Remove the specified element from the scene"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEdit2509LoraGalleryRemoveElementOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraGalleryRemoveLightingInput = {
  /**
   * The URL of the image with lighting/shadows to remove.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
};
export type QwenImageEdit2509LoraGalleryRemoveLightingOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraGalleryShirtDesignInput = {
  /**
   * The URLs of the images: first image is the person wearing a shirt, second image is the design/logo to put on the shirt.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe what design to put on the shirt. The model will apply the design from your input image onto the person's shirt. Default value: `"Put this design on their shirt"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEdit2509LoraGalleryShirtDesignOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEdit2509LoraInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used to calculate the size of the output image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URLs of the images to edit.
   */
  image_urls: Array<string>;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
};
export type QwenImageEdit2509LoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEdit2509Output = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEdit2509TrainerInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain more than one reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The Reference Image Count field should be set to the number of reference images.
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Learning rate for LoRA parameters. Default value: `0.0001`
   */
  learning_rate?: number;
  /**
   * Number of steps to train for Default value: `1000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
};
export type QwenImageEdit2509TrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type QwenImageEdit2511Input = {
  /**
   * The prompt to edit the image with.
   */
  prompt: string;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. If None, uses the input image dimensions.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The URLs of the images to edit.
   */
  image_urls: Array<string>;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type QwenImageEdit2511LoraInput = {
  /**
   * The prompt to edit the image with.
   */
  prompt: string;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. If None, uses the input image dimensions.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The URLs of the images to edit.
   */
  image_urls: Array<string>;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
};
export type QwenImageEdit2511LoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEdit2511MultipleAnglesInput = {
  /**
   * The URL of the image to adjust camera angle for.
   */
  image_urls: Array<string>;
  /**
   * Horizontal rotation angle around the object in degrees. 0°=front view, 90°=right side, 180°=back view, 270°=left side, 360°=front view again.
   */
  horizontal_angle?: number;
  /**
   * Vertical camera angle in degrees. -30°=low-angle shot (looking up), 0°=eye-level, 30°=elevated, 60°=high-angle, 90°=bird's-eye view (looking down).
   */
  vertical_angle?: number;
  /**
   * Camera zoom/distance. 0=wide shot (far away), 5=medium shot (normal), 10=close-up (very close). Default value: `5`
   */
  zoom?: number;
  /**
   * Additional text to append to the automatically generated prompt.
   */
  additional_prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the camera control effect. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The size of the generated image. If not provided, the size of the input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
};
export type QwenImageEdit2511MultipleAnglesOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The constructed prompt used for generation
   */
  prompt: string;
};
export type QwenImageEdit2511Output = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEdit2511TrainerInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain more than one reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The Reference Image Count field should be set to the number of reference images.
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Learning rate for LoRA parameters. Default value: `0.0001`
   */
  learning_rate?: number;
  /**
   * Number of steps to train for Default value: `1000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
};
export type QwenImageEdit2511TrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type QwenImageEditImageToImageInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Strength of the image-to-image transformation. Lower values preserve more of the original image. Default value: `0.94`
   */
  strength?: number;
};
export type QwenImageEditImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEditInpaintInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The URL of the mask for inpainting
   */
  mask_url: string | Blob | File;
  /**
   * Strength of noising process for inpainting Default value: `0.93`
   */
  strength?: number;
};
export type QwenImageEditInpaintOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEditInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type QwenImageEditLoraInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
};
export type QwenImageEditLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEditOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEditPlusInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URLs of the images to edit.
   */
  image_urls: Array<string>;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
};
export type QwenImageEditPlusLoraGalleryAddBackgroundInput = {
  /**
   * The URLs of the images to edit. Provide an image with a white or clean background.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the background/scene you want to add behind the object. The model will remove the white background and add the specified environment. Default value: `"Remove white background and add a realistic scene behind the object"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEditPlusLoraGalleryAddBackgroundOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraGalleryFaceToFullPortraitInput = {
  /**
   * The URL of the cropped face image. Provide a close-up face photo.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the full portrait you want to generate from the face. Include clothing, setting, pose, and style details. Default value: `"Photography. A portrait of the person in professional attire with natural lighting"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEditPlusLoraGalleryFaceToFullPortraitOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraGalleryGroupPhotoInput = {
  /**
   * The URLs of the images to combine into a group photo. Provide 2 or more individual portrait images.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the group photo scene, setting, and style. The model will maintain character consistency and add vintage effects like grain, blur, and retro filters. Default value: `"Two people standing next to each other outside with a landscape background"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEditPlusLoraGalleryGroupPhotoOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraGalleryIntegrateProductInput = {
  /**
   * The URL of the image with product to integrate into background.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe how to blend and integrate the product/element into the background. The model will automatically correct perspective, lighting and shadows for natural integration. Default value: `"Blend and integrate the product into the background"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEditPlusLoraGalleryIntegrateProductOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraGalleryLightingRestorationInput = {
  /**
   * The URL of the image to restore lighting for.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
};
export type QwenImageEditPlusLoraGalleryLightingRestorationOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraGalleryMultipleAnglesInput = {
  /**
   * The URL of the image to adjust camera angle for.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Rotate camera left (positive) or right (negative) in degrees. Positive values rotate left, negative values rotate right.
   */
  rotate_right_left?: number;
  /**
   * Move camera forward (0=no movement, 10=close-up)
   */
  move_forward?: number;
  /**
   * Adjust vertical camera angle (-1=bird's-eye view/looking down, 0=neutral, 1=worm's-eye view/looking up)
   */
  vertical_angle?: number;
  /**
   * Enable wide-angle lens effect
   */
  wide_angle_lens?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the camera control effect. Default value: `1.25`
   */
  lora_scale?: number;
};
export type QwenImageEditPlusLoraGalleryMultipleAnglesOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraGalleryNextSceneInput = {
  /**
   * The URL of the image to create the next scene from.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe the camera movement, framing change, or scene transition. Start with 'Next Scene:' for best results. Examples: camera movements (dolly, push-in, pull-back), framing changes (wide to close-up), new elements entering frame. Default value: `"Next Scene: The camera moves forward revealing more of the scene"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEditPlusLoraGalleryNextSceneOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraGalleryRemoveElementInput = {
  /**
   * The URL of the image containing elements to remove.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Specify what element(s) to remove from the image (objects, people, text, etc.). The model will cleanly remove the element while maintaining consistency of the rest of the image. Default value: `"Remove the specified element from the scene"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEditPlusLoraGalleryRemoveElementOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraGalleryRemoveLightingInput = {
  /**
   * The URL of the image with lighting/shadows to remove.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
};
export type QwenImageEditPlusLoraGalleryRemoveLightingOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraGalleryShirtDesignInput = {
  /**
   * The URLs of the images: first image is the person wearing a shirt, second image is the design/logo to put on the shirt.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe what design to put on the shirt. The model will apply the design from your input image onto the person's shirt. Default value: `"Put this design on their shirt"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type QwenImageEditPlusLoraGalleryShirtDesignOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageEditPlusLoraInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used to calculate the size of the output image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The URLs of the images to edit.
   */
  image_urls: Array<string>;
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
};
export type QwenImageEditPlusLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEditPlusOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageEditPlusTrainerInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain more than one reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The Reference Image Count field should be set to the number of reference images.
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Learning rate for LoRA parameters. Default value: `0.0001`
   */
  learning_rate?: number;
  /**
   * Number of steps to train for Default value: `1000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
};
export type QwenImageEditPlusTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type QwenImageEditTrainerInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Learning rate for LoRA parameters. Default value: `0.0001`
   */
  learning_rate?: number;
  /**
   * Number of steps to train for Default value: `1000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
};
export type QwenImageEditTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type QwenImageI2IInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image. By default, we will use the provided image for determining the image_size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2).
   */
  use_turbo?: boolean;
  /**
   * The reference image to guide the generation.
   */
  image_url: string | Blob | File;
  /**
   * Denoising strength. 1.0 = fully remake; 0.0 = preserve original. Default value: `0.6`
   */
  strength?: number;
};
export type QwenImageI2IOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageImageToImageInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image. By default, we will use the provided image for determining the image_size.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2).
   */
  use_turbo?: boolean;
  /**
   * The reference image to guide the generation.
   */
  image_url: string | Blob | File;
  /**
   * Denoising strength. 1.0 = fully remake; 0.0 = preserve original. Default value: `0.6`
   */
  strength?: number;
};
export type QwenImageImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageInpaintOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageInput = {
  /**
   * The prompt to generate the image with
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The LoRAs to use for the image generation. You can use up to 3 LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2).
   */
  use_turbo?: boolean;
};
export type QwenImageLayeredInput = {
  /**
   * A caption for the input image.
   */
  prompt?: string;
  /**
   * The URL of the input image.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of layers to generate. Default value: `4`
   */
  num_layers?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type QwenImageLayeredLoraInput = {
  /**
   * A caption for the input image.
   */
  prompt?: string;
  /**
   * The URL of the input image.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of layers to generate. Default value: `4`
   */
  num_layers?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type QwenImageLayeredLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used to generate the image.
   */
  prompt?: string;
};
export type QwenImageLayeredOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used to generate the image.
   */
  prompt?: string;
};
export type QwenImageLayeredTrainerInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain groups of images. The images should be named:
   *
   * ROOT_start.EXT, ROOT_end.EXT, ROOT_end2.EXT, ..., ROOT_endN.EXT
   * For example:
   * photo_start.png, photo_end.png, photo_end2.png, ..., photo_endN.png
   *
   * The start image is the base image that will be decomposed into layers.
   * The end images are the layers that will be added to the base image.  ROOT_end.EXT is the first layer, ROOT_end2.EXT is the second layer, and so on.
   * You can have up to 8 layers.
   * All image groups must have the same number of output layers.
   *
   * The end images can contain transparent regions. Only PNG and WebP images are supported since these are the only formats that support transparency.
   *
   * The zip can also contain a text file for each image group. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify a description of the base image.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Learning rate for LoRA parameters. Default value: `0.0001`
   */
  learning_rate?: number;
  /**
   * Number of steps to train for Default value: `1000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
};
export type QwenImageLayeredTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type QwenImageMaxEditInput = {
  /**
   * Text prompt describing the desired image. Supports Chinese and English. Max 800 characters.
   */
  prompt: string;
  /**
   * Content to avoid in the generated image. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable LLM prompt optimization for better results. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Random seed for reproducibility (0-2147483647).
   */
  seed?: number;
  /**
   * Enable content moderation for input and output. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * Reference images for editing (1-3 images required). Order matters: reference as 'image 1', 'image 2', 'image 3' in prompt. Resolution: 384-5000px each dimension. Max size: 10MB each. Formats: JPEG, JPG, PNG (no alpha), WEBP.
   */
  image_urls: Array<string>;
};
export type QwenImageMaxEditOutput = {
  /**
   * Generated images
   */
  images: Array<File>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageMaxTextToImageInput = {
  /**
   * Text prompt describing the desired image. Supports Chinese and English. Max 800 characters.
   */
  prompt: string;
  /**
   * Content to avoid in the generated image. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable LLM prompt optimization for better results. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Random seed for reproducibility (0-2147483647).
   */
  seed?: number;
  /**
   * Enable content moderation for input and output. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type QwenImageMaxTextToImageOutput = {
  /**
   * Generated images.
   */
  images: Array<File>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type QwenImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type QwenImageTrainerInput = {
  /**
   * URL to zip archive with images for training. The archive should contain images and corresponding text files with captions.
   * Each text file should have the same name as the image file it corresponds to (e.g., image1.jpg and image1.txt).
   * If text files are missing for some images, you can provide a trigger_phrase to automatically create them.
   * Supported image formats: PNG, JPG, JPEG, WEBP.
   * Try to use at least 10 images, although more is better.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps to perform. Default is 4000. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate for training. Default is 5e-4 Default value: `0.0005`
   */
  learning_rate?: number;
  /**
   * Default caption to use for images that don't have corresponding text files. If provided, missing .txt files will be created automatically. Default value: `""`
   */
  trigger_phrase?: string;
};
export type QwenImageTrainerOutput = {
  /**
   * URL to the trained LoRA weights file.
   */
  lora_file: File;
  /**
   * URL to the training configuration file.
   */
  config_file: File;
};
export type QwenImageTrainerV2Input = {
  /**
   * URL to the input data zip archive for text-to-image training.
   *
   * The zip should contain images with their corresponding text captions:
   *
   * image.EXT and image.txt
   * For example:
   * photo.jpg and photo.txt
   *
   * The text file contains the caption/prompt describing the target image.
   *
   * If no text file is provided for an image, the default_caption will be used.
   *
   * If no default_caption is provided and a text file is missing, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Learning rate for LoRA parameters. Default value: `0.0005`
   */
  learning_rate?: number;
  /**
   * Number of steps to train for Default value: `1000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
};
export type QwenImageTrainerV2Output = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type RapidImageTo3DInput = {
  /**
   * Front view image URL. Resolution: 128-5000px, max 8MB (recommended ≤6MB for base64 encoding), formats: JPG/PNG/WEBP. Tips: simple background, single object, object >50% of frame.
   */
  input_image_url: string | Blob | File;
  /**
   * Enable PBR material generation (metallic, roughness, normal textures). Does not take effect when enable_geometry is True.
   */
  enable_pbr?: boolean;
  /**
   * Generate geometry-only white model without textures. When enabled, enable_pbr is ignored and OBJ is not supported (default output is GLB).
   */
  enable_geometry?: boolean;
};
export type RapidImageTo3DOutput = {
  /**
   * Generated 3D model file. Contains GLB if available, otherwise OBJ.
   */
  model_glb?: File;
  /**
   * MTL material file for the OBJ model.
   */
  material_mtl?: File;
  /**
   * Texture image for the 3D model.
   */
  texture?: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats.
   */
  model_urls: ModelUrls;
};
export type RapidTextTo3DInput = {
  /**
   * Text description of the 3D content to generate. Max 200 UTF-8 characters.
   */
  prompt: string;
  /**
   * Enable PBR material generation (metallic, roughness, normal textures). Does not take effect when enable_geometry is True.
   */
  enable_pbr?: boolean;
  /**
   * Generate geometry-only white model without textures. When enabled, enable_pbr is ignored and OBJ is not supported (default output is GLB).
   */
  enable_geometry?: boolean;
};
export type RapidTextTo3DOutput = {
  /**
   * Generated 3D model in OBJ format.
   */
  model_obj?: File;
  /**
   * MTL material file for the OBJ model.
   */
  material_mtl?: File;
  /**
   * Texture image for the 3D model.
   */
  texture?: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats.
   */
  model_urls: ModelUrls;
};
export type Ray2I2VOutput = {
  /**
   * URL of the generated video
   */
  video: File;
};
export type Ray2T2VOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type React1Input = {
  /**
   * URL to the input video. Must be **15 seconds or shorter**.
   */
  video_url: string | Blob | File;
  /**
   * URL to the input audio. Must be **15 seconds or shorter**.
   */
  audio_url: string | Blob | File;
  /**
   * Emotion prompt for the generation. Currently supports single-word emotions only.
   */
  emotion: "happy" | "angry" | "sad" | "neutral" | "disgusted" | "surprised";
  /**
   * Controls the edit region and movement scope for the model. Available options:
   * - `lips`: Only lipsync using react-1 (minimal facial changes).
   * - `face`: Lipsync + facial expressions without head movements.
   * - `head`: Lipsync + facial expressions + natural talking head movements. Default value: `"face"`
   */
  model_mode?: "lips" | "face" | "head";
  /**
   * Lipsync mode when audio and video durations are out of sync. Default value: `"bounce"`
   */
  lipsync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
  /**
   * Controls the expresiveness of the lipsync. Default value: `0.5`
   */
  temperature?: number;
};
export type React1Output = {
  /**
   * The generated video with synchronized lip and facial movements.
   */
  video: VideoFile;
};
export type RealismInput = {
  /**
   * URL of the image to enhance with realism details.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `0.6`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type RealismOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type RealtimeEditInput = {
  /**
   * The prompt to guide image editing. Default value: `"Turn this into "Living oil painting, melting gold and sapphire""`
   */
  prompt?: string;
  /**
   * The size of the generated image. square=768x768, square_hd=1024x1024. Default value: `"square"`
   */
  image_size?: "square" | "square_hd";
  /**
   * Base64-encoded image data URI for editing. CDN URLs are not supported for realtime. For optimal performance, use 704x704 JPEG images with 50% quality. Other sizes will be resized automatically.
   */
  image_url: string | Blob | File;
  /**
   * Enable RIFE frame interpolation between consecutive frames (doubles output frames).
   */
  enable_interpolation?: boolean;
  /**
   * Schedule mu for time shift. 2.3=default, lower=more even denoising, 0.3=nearly linear. Default value: `2.3`
   */
  schedule_mu?: number;
  /**
   *  Default value: `3`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducibility. Default value: `35`
   */
  seed?: number;
  /**
   * Output feedback loop. 1.0 = pure noise (no feedback), 0.9 = 90% noise + 10% previous output latent. Default value: `1`
   */
  output_feedback_strength?: number;
};
export type RealtimeEditOutput = {
  /**
   * Generated images as raw bytes. When interpolation is enabled, returns [interpolated_frame, current_frame] in chronological order. Otherwise returns [current_frame].
   */
  images: Array<RawImage>;
  /**
   * Seed used for generation.
   */
  seed: number;
};
export type Recraft20BTextToImageInput = {
  /**
   *
   */
  prompt: string;
  /**
   *  Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The style of the generated images. Vector images cost 2X as much. Default value: `"realistic_image"`
   */
  style?:
    | "any"
    | "realistic_image"
    | "digital_illustration"
    | "vector_illustration"
    | "realistic_image/b_and_w"
    | "realistic_image/enterprise"
    | "realistic_image/hard_flash"
    | "realistic_image/hdr"
    | "realistic_image/motion_blur"
    | "realistic_image/natural_light"
    | "realistic_image/studio_portrait"
    | "digital_illustration/2d_art_poster"
    | "digital_illustration/2d_art_poster_2"
    | "digital_illustration/3d"
    | "digital_illustration/80s"
    | "digital_illustration/engraving_color"
    | "digital_illustration/glow"
    | "digital_illustration/grain"
    | "digital_illustration/hand_drawn"
    | "digital_illustration/hand_drawn_outline"
    | "digital_illustration/handmade_3d"
    | "digital_illustration/infantile_sketch"
    | "digital_illustration/kawaii"
    | "digital_illustration/pixel_art"
    | "digital_illustration/psychedelic"
    | "digital_illustration/seamless"
    | "digital_illustration/voxel"
    | "digital_illustration/watercolor"
    | "vector_illustration/cartoon"
    | "vector_illustration/doodle_line_art"
    | "vector_illustration/engraving"
    | "vector_illustration/flat_2"
    | "vector_illustration/kawaii"
    | "vector_illustration/line_art"
    | "vector_illustration/line_circuit"
    | "vector_illustration/linocut"
    | "vector_illustration/seamless";
  /**
   * An array of preferable colors
   */
  colors?: Array<RGBColor>;
  /**
   * The ID of the custom style reference (optional)
   */
  style_id?: string;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};
export type RecraftUpscaleCreativeInput = {
  /**
   * The URL of the image to be upscaled. Must be in PNG format.
   */
  image_url: string | Blob | File;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};
export type RecraftUpscaleCreativeOutput = {
  /**
   * The upscaled image.
   */
  image: File;
};
export type RecraftUpscaleCrispInput = {
  /**
   * The URL of the image to be upscaled. Must be in PNG format.
   */
  image_url: string | Blob | File;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};
export type RecraftUpscaleCrispOutput = {
  /**
   * The upscaled image.
   */
  image: File;
};
export type RecraftV3CreateStyleInput = {
  /**
   * URL to zip archive with images, use PNG format. Maximum 5 images are allowed.
   */
  images_data_url: string | Blob | File;
  /**
   * The base style of the generated images, this topic is covered above. Default value: `"digital_illustration"`
   */
  base_style?:
    | "any"
    | "realistic_image"
    | "digital_illustration"
    | "vector_illustration"
    | "realistic_image/b_and_w"
    | "realistic_image/hard_flash"
    | "realistic_image/hdr"
    | "realistic_image/natural_light"
    | "realistic_image/studio_portrait"
    | "realistic_image/enterprise"
    | "realistic_image/motion_blur"
    | "realistic_image/evening_light"
    | "realistic_image/faded_nostalgia"
    | "realistic_image/forest_life"
    | "realistic_image/mystic_naturalism"
    | "realistic_image/natural_tones"
    | "realistic_image/organic_calm"
    | "realistic_image/real_life_glow"
    | "realistic_image/retro_realism"
    | "realistic_image/retro_snapshot"
    | "realistic_image/urban_drama"
    | "realistic_image/village_realism"
    | "realistic_image/warm_folk"
    | "digital_illustration/pixel_art"
    | "digital_illustration/hand_drawn"
    | "digital_illustration/grain"
    | "digital_illustration/infantile_sketch"
    | "digital_illustration/2d_art_poster"
    | "digital_illustration/handmade_3d"
    | "digital_illustration/hand_drawn_outline"
    | "digital_illustration/engraving_color"
    | "digital_illustration/2d_art_poster_2"
    | "digital_illustration/antiquarian"
    | "digital_illustration/bold_fantasy"
    | "digital_illustration/child_book"
    | "digital_illustration/child_books"
    | "digital_illustration/cover"
    | "digital_illustration/crosshatch"
    | "digital_illustration/digital_engraving"
    | "digital_illustration/expressionism"
    | "digital_illustration/freehand_details"
    | "digital_illustration/grain_20"
    | "digital_illustration/graphic_intensity"
    | "digital_illustration/hard_comics"
    | "digital_illustration/long_shadow"
    | "digital_illustration/modern_folk"
    | "digital_illustration/multicolor"
    | "digital_illustration/neon_calm"
    | "digital_illustration/noir"
    | "digital_illustration/nostalgic_pastel"
    | "digital_illustration/outline_details"
    | "digital_illustration/pastel_gradient"
    | "digital_illustration/pastel_sketch"
    | "digital_illustration/pop_art"
    | "digital_illustration/pop_renaissance"
    | "digital_illustration/street_art"
    | "digital_illustration/tablet_sketch"
    | "digital_illustration/urban_glow"
    | "digital_illustration/urban_sketching"
    | "digital_illustration/vanilla_dreams"
    | "digital_illustration/young_adult_book"
    | "digital_illustration/young_adult_book_2"
    | "vector_illustration/bold_stroke"
    | "vector_illustration/chemistry"
    | "vector_illustration/colored_stencil"
    | "vector_illustration/contour_pop_art"
    | "vector_illustration/cosmics"
    | "vector_illustration/cutout"
    | "vector_illustration/depressive"
    | "vector_illustration/editorial"
    | "vector_illustration/emotional_flat"
    | "vector_illustration/infographical"
    | "vector_illustration/marker_outline"
    | "vector_illustration/mosaic"
    | "vector_illustration/naivector"
    | "vector_illustration/roundish_flat"
    | "vector_illustration/segmented_colors"
    | "vector_illustration/sharp_contrast"
    | "vector_illustration/thin"
    | "vector_illustration/vector_photo"
    | "vector_illustration/vivid_shapes"
    | "vector_illustration/engraving"
    | "vector_illustration/line_art"
    | "vector_illustration/line_circuit"
    | "vector_illustration/linocut";
};
export type RecraftV3CreateStyleOutput = {
  /**
   * The ID of the created style, this ID can be used to reference the style in the future.
   */
  style_id: string;
};
export type RecraftV3ImageToImageInput = {
  /**
   * A text description of areas to change.
   */
  prompt: string;
  /**
   * The URL of the image to modify. Must be less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels.
   */
  image_url: string | Blob | File;
  /**
   * Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity Default value: `0.5`
   */
  strength?: number;
  /**
   * The style of the generated images. Vector images cost 2X as much. Default value: `"realistic_image"`
   */
  style?:
    | "any"
    | "realistic_image"
    | "digital_illustration"
    | "vector_illustration"
    | "realistic_image/b_and_w"
    | "realistic_image/hard_flash"
    | "realistic_image/hdr"
    | "realistic_image/natural_light"
    | "realistic_image/studio_portrait"
    | "realistic_image/enterprise"
    | "realistic_image/motion_blur"
    | "realistic_image/evening_light"
    | "realistic_image/faded_nostalgia"
    | "realistic_image/forest_life"
    | "realistic_image/mystic_naturalism"
    | "realistic_image/natural_tones"
    | "realistic_image/organic_calm"
    | "realistic_image/real_life_glow"
    | "realistic_image/retro_realism"
    | "realistic_image/retro_snapshot"
    | "realistic_image/urban_drama"
    | "realistic_image/village_realism"
    | "realistic_image/warm_folk"
    | "digital_illustration/pixel_art"
    | "digital_illustration/hand_drawn"
    | "digital_illustration/grain"
    | "digital_illustration/infantile_sketch"
    | "digital_illustration/2d_art_poster"
    | "digital_illustration/handmade_3d"
    | "digital_illustration/hand_drawn_outline"
    | "digital_illustration/engraving_color"
    | "digital_illustration/2d_art_poster_2"
    | "digital_illustration/antiquarian"
    | "digital_illustration/bold_fantasy"
    | "digital_illustration/child_book"
    | "digital_illustration/child_books"
    | "digital_illustration/cover"
    | "digital_illustration/crosshatch"
    | "digital_illustration/digital_engraving"
    | "digital_illustration/expressionism"
    | "digital_illustration/freehand_details"
    | "digital_illustration/grain_20"
    | "digital_illustration/graphic_intensity"
    | "digital_illustration/hard_comics"
    | "digital_illustration/long_shadow"
    | "digital_illustration/modern_folk"
    | "digital_illustration/multicolor"
    | "digital_illustration/neon_calm"
    | "digital_illustration/noir"
    | "digital_illustration/nostalgic_pastel"
    | "digital_illustration/outline_details"
    | "digital_illustration/pastel_gradient"
    | "digital_illustration/pastel_sketch"
    | "digital_illustration/pop_art"
    | "digital_illustration/pop_renaissance"
    | "digital_illustration/street_art"
    | "digital_illustration/tablet_sketch"
    | "digital_illustration/urban_glow"
    | "digital_illustration/urban_sketching"
    | "digital_illustration/vanilla_dreams"
    | "digital_illustration/young_adult_book"
    | "digital_illustration/young_adult_book_2"
    | "vector_illustration/bold_stroke"
    | "vector_illustration/chemistry"
    | "vector_illustration/colored_stencil"
    | "vector_illustration/contour_pop_art"
    | "vector_illustration/cosmics"
    | "vector_illustration/cutout"
    | "vector_illustration/depressive"
    | "vector_illustration/editorial"
    | "vector_illustration/emotional_flat"
    | "vector_illustration/infographical"
    | "vector_illustration/marker_outline"
    | "vector_illustration/mosaic"
    | "vector_illustration/naivector"
    | "vector_illustration/roundish_flat"
    | "vector_illustration/segmented_colors"
    | "vector_illustration/sharp_contrast"
    | "vector_illustration/thin"
    | "vector_illustration/vector_photo"
    | "vector_illustration/vivid_shapes"
    | "vector_illustration/engraving"
    | "vector_illustration/line_art"
    | "vector_illustration/line_circuit"
    | "vector_illustration/linocut";
  /**
   * An array of preferable colors
   */
  colors?: Array<RGBColor>;
  /**
   * The ID of the custom style reference (optional)
   */
  style_id?: string;
  /**
   * A text description of undesired elements on an image
   */
  negative_prompt?: string;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type RecraftV3ImageToImageOutput = {
  /**
   * The generated images
   */
  images: Array<File>;
};
export type RecraftV3TextToImageInput = {
  /**
   *
   */
  prompt: string;
  /**
   *  Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The style of the generated images. Vector images cost 2X as much. Default value: `"realistic_image"`
   */
  style?:
    | "any"
    | "realistic_image"
    | "digital_illustration"
    | "vector_illustration"
    | "realistic_image/b_and_w"
    | "realistic_image/hard_flash"
    | "realistic_image/hdr"
    | "realistic_image/natural_light"
    | "realistic_image/studio_portrait"
    | "realistic_image/enterprise"
    | "realistic_image/motion_blur"
    | "realistic_image/evening_light"
    | "realistic_image/faded_nostalgia"
    | "realistic_image/forest_life"
    | "realistic_image/mystic_naturalism"
    | "realistic_image/natural_tones"
    | "realistic_image/organic_calm"
    | "realistic_image/real_life_glow"
    | "realistic_image/retro_realism"
    | "realistic_image/retro_snapshot"
    | "realistic_image/urban_drama"
    | "realistic_image/village_realism"
    | "realistic_image/warm_folk"
    | "digital_illustration/pixel_art"
    | "digital_illustration/hand_drawn"
    | "digital_illustration/grain"
    | "digital_illustration/infantile_sketch"
    | "digital_illustration/2d_art_poster"
    | "digital_illustration/handmade_3d"
    | "digital_illustration/hand_drawn_outline"
    | "digital_illustration/engraving_color"
    | "digital_illustration/2d_art_poster_2"
    | "digital_illustration/antiquarian"
    | "digital_illustration/bold_fantasy"
    | "digital_illustration/child_book"
    | "digital_illustration/child_books"
    | "digital_illustration/cover"
    | "digital_illustration/crosshatch"
    | "digital_illustration/digital_engraving"
    | "digital_illustration/expressionism"
    | "digital_illustration/freehand_details"
    | "digital_illustration/grain_20"
    | "digital_illustration/graphic_intensity"
    | "digital_illustration/hard_comics"
    | "digital_illustration/long_shadow"
    | "digital_illustration/modern_folk"
    | "digital_illustration/multicolor"
    | "digital_illustration/neon_calm"
    | "digital_illustration/noir"
    | "digital_illustration/nostalgic_pastel"
    | "digital_illustration/outline_details"
    | "digital_illustration/pastel_gradient"
    | "digital_illustration/pastel_sketch"
    | "digital_illustration/pop_art"
    | "digital_illustration/pop_renaissance"
    | "digital_illustration/street_art"
    | "digital_illustration/tablet_sketch"
    | "digital_illustration/urban_glow"
    | "digital_illustration/urban_sketching"
    | "digital_illustration/vanilla_dreams"
    | "digital_illustration/young_adult_book"
    | "digital_illustration/young_adult_book_2"
    | "vector_illustration/bold_stroke"
    | "vector_illustration/chemistry"
    | "vector_illustration/colored_stencil"
    | "vector_illustration/contour_pop_art"
    | "vector_illustration/cosmics"
    | "vector_illustration/cutout"
    | "vector_illustration/depressive"
    | "vector_illustration/editorial"
    | "vector_illustration/emotional_flat"
    | "vector_illustration/infographical"
    | "vector_illustration/marker_outline"
    | "vector_illustration/mosaic"
    | "vector_illustration/naivector"
    | "vector_illustration/roundish_flat"
    | "vector_illustration/segmented_colors"
    | "vector_illustration/sharp_contrast"
    | "vector_illustration/thin"
    | "vector_illustration/vector_photo"
    | "vector_illustration/vivid_shapes"
    | "vector_illustration/engraving"
    | "vector_illustration/line_art"
    | "vector_illustration/line_circuit"
    | "vector_illustration/linocut";
  /**
   * An array of preferable colors
   */
  colors?: Array<RGBColor>;
  /**
   * The ID of the custom style reference (optional)
   */
  style_id?: string;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};
export type RecraftV3TextToImageOutput = {
  /**
   *
   */
  images: Array<File>;
};
export type RecraftVectorizeInput = {
  /**
   * The URL of the image to be vectorized. Must be in PNG, JPG or WEBP format, less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels, min dimension more than 256 pixels.
   */
  image_url: string | Blob | File;
};
export type RecraftVectorizeOutput = {
  /**
   * The vectorized image.
   */
  image: File;
};
export type ReferenceImageInput = {
  /**
   * The image to use for the measurement.
   */
  reference: string;
  /**
   * The hypothesis image to use for the measurement.
   */
  hypothesis: string;
};
export type ReferenceToImageOutput = {
  /**
   * The edited image
   */
  image: Image;
};
export type ReferenceToVideoFlashInput = {
  /**
   * Use Character1, Character2, etc. to reference subjects from your reference files. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 1500 characters. Reference order: video_urls first, then image_urls.
   */
  prompt: string;
  /**
   * Reference videos for subject consistency (0-3 videos). Videos' FPS must be at least 16 FPS. Combined with image_urls, total references cannot exceed 5. Reference order: video_urls are numbered first (Character1, Character2...), then image_urls continue the sequence.
   */
  video_urls?: Array<string>;
  /**
   * Reference images for subject consistency (0-5 images). Combined with video_urls, total references cannot exceed 5. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP. Resolution: 240-5000px. Max 10MB each. Reference order: image_urls continue numbering after video_urls.
   */
  image_urls?: Array<string>;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:3" | "3:4";
  /**
   * Video resolution tier. R2V Flash only supports 720p and 1080p. Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration of the generated video in seconds. R2V Flash supports only 5 or 10 seconds. Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * Negative prompt to describe content to avoid. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Whether to enable prompt rewriting using LLM. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True. Default value: `true`
   */
  multi_shots?: boolean;
  /**
   * Whether to generate a video with audio. Set to false for silent video generation. Silent videos are faster and cost 25% of the audio version price. Default value: `true`
   */
  enable_audio?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type ReferenceToVideoInput = {
  /**
   * Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters.
   */
  prompt: string;
  /**
   * Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects.
   */
  video_urls: Array<string>;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:3" | "3:4";
  /**
   * Video resolution tier. R2V only supports 720p and 1080p (no 480p). Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s). Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * Negative prompt to describe content to avoid. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Whether to enable prompt rewriting using LLM. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True. Default value: `true`
   */
  multi_shots?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type ReferenceToVideoOutput = {
  /**
   * The generated video with consistent subjects from reference images
   */
  video: File;
};
export type ReframeInput = {
  /**
   * URL of the old or damaged photo to restore.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The desired aspect ratio for the reframed image. Default value: `"16:9"`
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ReframeOutput = {
  /**
   * URL of the reframed video
   */
  video: File;
};
export type Reimagine32Input = {
  /**
   * Prompt for image generation.
   */
  prompt: string;
  /**
   * Number of inference steps. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducibility. Default value: `5555`
   */
  seed?: number;
  /**
   * Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9 Default value: `"1:1"`
   */
  aspect_ratio?:
    | "1:1"
    | "2:3"
    | "3:2"
    | "3:4"
    | "4:3"
    | "4:5"
    | "5:4"
    | "9:16"
    | "16:9";
  /**
   * Negative prompt for image generation. Default value: `"Logo,Watermark,Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra limbs,Gross proportions,Missing arms,Mutated hands,Long neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad anatomy,Cloned face,Malformed limbs,Missing legs,Too many fingers"`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for text. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Whether to truncate the prompt. Default value: `true`
   */
  truncate_prompt?: boolean;
  /**
   * Whether to improve the prompt. Default value: `true`
   */
  prompt_enhancer?: boolean;
  /**
   * If true, returns the image directly in the response (increases latency).
   */
  sync_mode?: boolean;
  /**
   * Depth control image (file or URL). Default value: `""`
   */
  depth_image_url?: string | Blob | File;
  /**
   * Depth control strength (0.0 to 1.0). Default value: `0.5`
   */
  depth_scale?: number;
  /**
   * Depth image preprocess. Default value: `true`
   */
  depth_preprocess?: boolean;
  /**
   * Canny edge control image (file or URL). Default value: `""`
   */
  canny_image_url?: string | Blob | File;
  /**
   * Canny image preprocess. Default value: `true`
   */
  canny_preprocess?: boolean;
  /**
   * Canny edge control strength (0.0 to 1.0). Default value: `0.5`
   */
  canny_scale?: number;
};
export type Reimagine32Output = {
  /**
   * Generated image.
   */
  image: Image;
};
export type RelightingInput = {
  /**
   * Image URL for relighting
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"natural"`
   */
  lighting_style?:
    | "natural"
    | "studio"
    | "golden_hour"
    | "blue_hour"
    | "dramatic"
    | "soft"
    | "hard"
    | "backlight"
    | "side_light"
    | "front_light"
    | "rim_light"
    | "sunset"
    | "sunrise"
    | "neon"
    | "candlelight"
    | "moonlight"
    | "spotlight"
    | "ambient";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type RelightingOutput = {
  /**
   * Image with new lighting
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type RelightInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * Where the light comes from.
   */
  light_direction: "front" | "side" | "bottom" | "top-down";
  /**
   * The quality/style/time of day.
   */
  light_type:
    | "midday"
    | "blue hour light"
    | "low-angle sunlight"
    | "sunrise light"
    | "spotlight on subject"
    | "overcast light"
    | "soft overcast daylight lighting"
    | "cloud-filtered lighting"
    | "fog-diffused lighting"
    | "moonlight lighting"
    | "starlight nighttime"
    | "soft bokeh lighting"
    | "harsh studio lighting";
};
export type RembgEnhanceInput = {
  /**
   * URL of the input image
   */
  image_url: string | Blob | File;
};
export type RembgEnhanceOutput = {
  /**
   * The segmented output image
   */
  image: File;
};
export type RemeshInput = {
  /**
   * URL or base64 data URI of a 3D model to remesh. Supports .glb, .gltf, .obj, .fbx, .stl formats. Can be a publicly accessible URL or data URI with MIME type application/octet-stream.
   */
  model_url: string | Blob | File;
  /**
   * List of target formats for the remeshed model.
   */
  target_formats?: Array<"glb" | "fbx" | "obj" | "usdz" | "blend" | "stl">;
  /**
   * Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry. Default value: `"triangle"`
   */
  topology?: "quad" | "triangle";
  /**
   * Target number of polygons in the generated model. Actual count may vary based on geometry complexity. Default value: `30000`
   */
  target_polycount?: number;
  /**
   * Resize the model to a certain height measured in meters. Set to 0 for no resizing.
   */
  resize_height?: number;
  /**
   * Position of the origin. None means no effect.
   */
  origin_at?: "bottom" | "center";
};
export type RemeshOutput = {
  /**
   * Remeshed 3D object in GLB format (if GLB was requested).
   */
  model_glb?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
};
export type RemixImageInput = {
  /**
   * The prompt to remix the image with
   */
  prompt: string;
  /**
   * The image URL to remix
   */
  image_url: string | Blob | File;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "10:16"
    | "16:10"
    | "9:16"
    | "16:9"
    | "4:3"
    | "3:4"
    | "1:1"
    | "1:3"
    | "3:1"
    | "3:2"
    | "2:3";
  /**
   * Strength of the input image in the remix Default value: `0.8`
   */
  strength?: number;
  /**
   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The style of the generated image Default value: `"auto"`
   */
  style?: "auto" | "general" | "realistic" | "design" | "render_3D" | "anime";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type RemixInput = {
  /**
   * The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos.
   */
  video_id: string;
  /**
   * Updated text prompt that directs the remix generation
   */
  prompt: string;
  /**
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted. Default value: `true`
   */
  delete_video?: boolean;
};
export type RemixOutput = {
  /**
   * The generated video
   */
  video: VideoFile;
  /**
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
};
export type RemoveElementInput = {
  /**
   * The URL of the image containing elements to remove.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Specify what element(s) to remove from the image (objects, people, text, etc.). The model will cleanly remove the element while maintaining consistency of the rest of the image. Default value: `"Remove the specified element from the scene"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type RemoveElementOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type RemoveLightingInput = {
  /**
   * The URL of the image with lighting/shadows to remove.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
};
export type RemoveLightingOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type ReplaceBackgroundInput = {
  /**
   * Prompt for background replacement.
   */
  prompt?: string;
  /**
   * Reference image (file or URL). Default value: `"https://v3b.fal.media/files/b/0a8bea8c/Mztgx0NG3HPdby-4iPqwH_a_coffee_machine_standing_in_the_kitchen.png"`
   */
  image_url?: string | Blob | File;
  /**
   * Random seed for reproducibility. Default value: `4925634`
   */
  seed?: number;
  /**
   * Number of inference steps. Default value: `30`
   */
  steps_num?: number;
  /**
   * Negative prompt for background replacement. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * If true, returns the image directly in the response (increases latency).
   */
  sync_mode?: boolean;
};
export type ReplaceBackgroundOutput = {
  /**
   * Generated image.
   */
  image: Image;
  /**
   * Generated images.
   */
  images?: Array<any>;
};
export type ReplaceObjectInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The full natural language command describing what to replace.
   */
  instruction: string;
};
export type ReseasonInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The desired season.
   */
  season: "spring" | "summer" | "autumn" | "winter";
};
export type RestoreInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
};
export type RestyletInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * Select the desired artistic style for the output image.
   */
  style:
    | "3D Render"
    | "Cubism"
    | "Oil Painting"
    | "Anime"
    | "Cartoon"
    | "Coloring Book"
    | "Retro Ad"
    | "Pop Art Halftone"
    | "Vector Art"
    | "Story Board"
    | "Art Nouveau"
    | "Cross Etching"
    | "Wood Cut";
};
export type RetextureInput = {
  /**
   * URL or base64 data URI of a 3D model to texture. Supports .glb, .gltf, .obj, .fbx, .stl formats. Can be a publicly accessible URL or data URI with MIME type application/octet-stream.
   */
  model_url: string | Blob | File;
  /**
   * Describe your desired texture style using text. Maximum 600 characters. Required if image_style_url is not provided.
   */
  text_style_prompt?: string;
  /**
   * 2D image to guide the texturing process. Supports .jpg, .jpeg, and .png formats. Required if text_style_prompt is not provided. If both are provided, image_style_url takes precedence.
   */
  image_style_url?: string | Blob | File;
  /**
   * Use the original UV mapping of the model instead of generating new UVs. If the model has no original UV, output quality may be reduced. Default value: `true`
   */
  enable_original_uv?: boolean;
  /**
   * Generate PBR Maps (metallic, roughness, normal) in addition to base color.
   */
  enable_pbr?: boolean;
  /**
   * If set to true, input data will be checked for safety before processing. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type RetextureOutput = {
  /**
   * Retextured 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the retextured model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * Array of texture file objects
   */
  texture_urls?: Array<TextureFiles>;
  /**
   * The text prompt used for texturing (if provided)
   */
  text_style_prompt?: string;
  /**
   * The image URL used for texturing (if provided)
   */
  image_style_url?: string | Blob | File;
};
export type RetouchInput = {
  /**
   * URL of the image to retouch.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type RetouchOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type ReveCreateInput = {
  /**
   * The text description of the desired image.
   */
  prompt: string;
  /**
   * The desired aspect ratio of the generated image. Default value: `"3:2"`
   */
  aspect_ratio?: "16:9" | "9:16" | "3:2" | "2:3" | "4:3" | "3:4" | "1:1";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the generated image. Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ReveCreateOutput = {
  /**
   * The generated images
   */
  images: Array<Image>;
};
export type ReveEditInput = {
  /**
   * The text description of how to edit the provided image.
   */
  prompt: string;
  /**
   * URL of the reference image to edit. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
   */
  image_url: string | Blob | File;
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the generated image. Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ReveEditOutput = {
  /**
   * The edited images
   */
  images: Array<Image>;
};
export type ReveFastEditInput = {
  /**
   * The text description of how to edit the provided image.
   */
  prompt: string;
  /**
   * URL of the reference image to edit. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
   */
  image_url: string | Blob | File;
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the generated image. Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ReveFastEditOutput = {
  /**
   * The edited images
   */
  images: Array<Image>;
};
export type ReveFastRemixInput = {
  /**
   * The text description of the desired image. May include XML img tags like <img>0</img> to refer to specific images by their index in the image_urls list.
   */
  prompt: string;
  /**
   * List of URLs of reference images. Must provide between 1 and 6 images (inclusive). Each image must be less than 10 MB. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
   */
  image_urls: Array<string>;
  /**
   * The desired aspect ratio of the generated image. If not provided, will be smartly chosen by the model.
   */
  aspect_ratio?: "16:9" | "9:16" | "3:2" | "2:3" | "4:3" | "3:4" | "1:1";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the generated image. Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ReveFastRemixOutput = {
  /**
   * The remixed images
   */
  images: Array<Image>;
};
export type ReveRemixInput = {
  /**
   * The text description of the desired image. May include XML img tags like <img>0</img> to refer to specific images by their index in the image_urls list.
   */
  prompt: string;
  /**
   * List of URLs of reference images. Must provide between 1 and 6 images (inclusive). Each image must be less than 10 MB. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
   */
  image_urls: Array<string>;
  /**
   * The desired aspect ratio of the generated image. If not provided, will be smartly chosen by the model.
   */
  aspect_ratio?: "16:9" | "9:16" | "3:2" | "2:3" | "4:3" | "3:4" | "1:1";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the generated image. Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ReveRemixOutput = {
  /**
   * The remixed images
   */
  images: Array<Image>;
};
export type ReveTextToImageInput = {
  /**
   * The text description of the desired image.
   */
  prompt: string;
  /**
   * The desired aspect ratio of the generated image. Default value: `"3:2"`
   */
  aspect_ratio?: "16:9" | "9:16" | "3:2" | "2:3" | "4:3" | "3:4" | "1:1";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Output format for the generated image. Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type ReveTextToImageOutput = {
  /**
   * The generated images
   */
  images: Array<Image>;
};
export type RewriteTextInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
  /**
   * The new text string to appear in the image.
   */
  new_text: string;
};
export type RFInversionInput = {
  /**
   * The prompt to edit the image with
   */
  prompt: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  control_loras?: Array<ControlLoraWeight>;
  /**
   * The controlnets to use for the image generation. Only one controlnet is supported at the moment.
   */
  controlnets?: Array<ControlNet>;
  /**
   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.
   */
  controlnet_unions?: Array<ControlNetUnion>;
  /**
   * EasyControl Inputs to use for image generation.
   */
  easycontrols?: Array<EasyControlWeight>;
  /**
   * Use an image input to influence the generation. Can be used to fill images in masked areas.
   */
  fill_image?: ImageFillInput;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.
   */
  use_cfg_zero?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * URL of Image for Reference-Only
   */
  reference_image_url?: string | Blob | File;
  /**
   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`
   */
  reference_strength?: number;
  /**
   * The percentage of the total timesteps when the reference guidance is to bestarted.
   */
  reference_start?: number;
  /**
   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`
   */
  reference_end?: number;
  /**
   * Base shift for the scheduled timesteps Default value: `0.5`
   */
  base_shift?: number;
  /**
   * Max shift for the scheduled timesteps Default value: `1.15`
   */
  max_shift?: number;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * Specifies whether beta sigmas ought to be used.
   */
  use_beta_schedule?: boolean;
  /**
   * Sigmas schedule for the denoising process.
   */
  sigma_schedule?: "sgm_uniform";
  /**
   * Scheduler for the denoising process. Default value: `"euler"`
   */
  scheduler?: "euler" | "dpmpp_2m";
  /**
   * Negative prompt to steer the image generation away from unwanted features.
   * By default, we will be using NAG for processing the negative prompt. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The scale for NAG. Higher values will result in a image that is more distant
   * to the negative prompt. Default value: `3`
   */
  nag_scale?: number;
  /**
   * The tau for NAG. Controls the normalization of the hidden state.
   * Higher values will result in a less aggressive normalization,
   * but may also lead to unexpected changes with respect to the original image.
   * Not recommended to change this value. Default value: `2.5`
   */
  nag_tau?: number;
  /**
   * The alpha value for NAG. This value is used as a final weighting
   * factor for steering the normalized guidance (positive and negative prompts)
   * in the direction of the positive prompt. Higher values will result in less
   * steering on the normalized guidance where lower values will result in
   * considering the positive prompt guidance more. Default value: `0.25`
   */
  nag_alpha?: number;
  /**
   * The proportion of steps to apply NAG. After the specified proportion
   * of steps has been iterated, the remaining steps will use original
   * attention processors in FLUX. Default value: `0.25`
   */
  nag_end?: number;
  /**
   * URL of image to be edited
   */
  image_url: string | Blob | File;
  /**
   * The controller guidance (gamma) used in the creation of structured noise. Default value: `0.6`
   */
  controller_guidance_forward?: number;
  /**
   * The controller guidance (eta) used in the denoising process.Using values closer to 1 will result in an image closer to input. Default value: `0.75`
   */
  controller_guidance_reverse?: number;
  /**
   * Timestep to start guidance during reverse process.
   */
  reverse_guidance_start?: number;
  /**
   * Timestep to stop guidance during reverse process. Default value: `8`
   */
  reverse_guidance_end?: number;
  /**
   * Scheduler for applying reverse guidance. Default value: `"constant"`
   */
  reverse_guidance_schedule?:
    | "constant"
    | "linear_increase"
    | "linear_decrease";
};
export type RIFEImageInput = {
  /**
   * The URL of the first image to use as the starting point for interpolation.
   */
  start_image_url: string | Blob | File;
  /**
   * The URL of the second image to use as the ending point for interpolation.
   */
  end_image_url: string | Blob | File;
  /**
   * The type of output to generate; either individual images or a video. Default value: `"images"`
   */
  output_type?: "images" | "video";
  /**
   * The format of the output images. Only applicable if output_type is 'images'. Default value: `"jpeg"`
   */
  output_format?: "png" | "jpeg";
  /**
   * The number of frames to generate between the input images. Default value: `1`
   */
  num_frames?: number;
  /**
   * Whether to include the start image in the output.
   */
  include_start?: boolean;
  /**
   * Whether to include the end image in the output.
   */
  include_end?: boolean;
  /**
   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `8`
   */
  fps?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type RIFEImageOutput = {
  /**
   * The generated frames as individual images.
   */
  images?: Array<Image>;
  /**
   * The generated video file, if output_type is 'video'.
   */
  video?: File;
};
export type rifeInput = {
  /**
   * The URL of the first image to use as the starting point for interpolation.
   */
  start_image_url: string | Blob | File;
  /**
   * The URL of the second image to use as the ending point for interpolation.
   */
  end_image_url: string | Blob | File;
  /**
   * The type of output to generate; either individual images or a video. Default value: `"images"`
   */
  output_type?: "images" | "video";
  /**
   * The format of the output images. Only applicable if output_type is 'images'. Default value: `"jpeg"`
   */
  output_format?: "png" | "jpeg";
  /**
   * The number of frames to generate between the input images. Default value: `1`
   */
  num_frames?: number;
  /**
   * Whether to include the start image in the output.
   */
  include_start?: boolean;
  /**
   * Whether to include the end image in the output.
   */
  include_end?: boolean;
  /**
   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `8`
   */
  fps?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type rifeOutput = {
  /**
   * The generated frames as individual images.
   */
  images?: Array<Image>;
  /**
   * The generated video file, if output_type is 'video'.
   */
  video?: File;
};
export type RifeVideoInput = {
  /**
   * The URL of the video to use for interpolation.
   */
  video_url: string | Blob | File;
  /**
   * The number of frames to generate between the input video frames. Default value: `1`
   */
  num_frames?: number;
  /**
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean;
  /**
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used. Default value: `true`
   */
  use_calculated_fps?: boolean;
  /**
   * Frames per second for the output video. Only applicable if use_calculated_fps is False. Default value: `8`
   */
  fps?: number;
  /**
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean;
};
export type RIFEVideoInput = {
  /**
   * The URL of the video to use for interpolation.
   */
  video_url: string | Blob | File;
  /**
   * The number of frames to generate between the input video frames. Default value: `1`
   */
  num_frames?: number;
  /**
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean;
  /**
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used. Default value: `true`
   */
  use_calculated_fps?: boolean;
  /**
   * Frames per second for the output video. Only applicable if use_calculated_fps is False. Default value: `8`
   */
  fps?: number;
  /**
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean;
};
export type RifeVideoOutput = {
  /**
   * The generated video file with interpolated frames.
   */
  video: File;
};
export type RIFEVideoOutput = {
  /**
   * The generated video file with interpolated frames.
   */
  video: File;
};
export type Rodin3DInput = {
  /**
   * A textual prompt to guide model generation. Required for Text-to-3D mode. Optional for Image-to-3D mode. Default value: `""`
   */
  prompt?: string;
  /**
   * URL of images to use while generating the 3D model. Required for Image-to-3D mode. Optional for Text-to-3D mode.
   */
  input_image_urls?: Array<string>;
  /**
   * For fuse mode, One or more images are required.It will generate a model by extracting and fusing features of objects from multiple images.For concat mode, need to upload multiple multi-view images of the same object and generate the model. (You can upload multi-view images in any order, regardless of the order of view.) Default value: `"concat"`
   */
  condition_mode?: "fuse" | "concat";
  /**
   * Seed value for randomization, ranging from 0 to 65535. Optional.
   */
  seed?: number;
  /**
   * Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb. Default value: `"glb"`
   */
  geometry_file_format?: "glb" | "usdz" | "fbx" | "obj" | "stl";
  /**
   * Material type. Possible values: PBR, Shaded. Default is PBR. Default value: `"PBR"`
   */
  material?: "PBR" | "Shaded";
  /**
   * Generation quality. Possible values: high, medium, low, extra-low. Default is medium. Default value: `"medium"`
   */
  quality?: "high" | "medium" | "low" | "extra-low";
  /**
   * Whether to export the model using hyper mode. Default is false.
   */
  use_hyper?: boolean;
  /**
   * Tier of generation. For Rodin Sketch, set to Sketch. For Rodin Regular, set to Regular. Default value: `"Regular"`
   */
  tier?: "Regular" | "Sketch";
  /**
   * When generating the human-like model, this parameter control the generation result to T/A Pose.
   */
  TAPose?: boolean;
  /**
   * An array that specifies the dimensions and scaling factor of the bounding box. Typically, this array contains 3 elements, Length(X-axis), Width(Y-axis) and Height(Z-axis).
   */
  bbox_condition?: Array<number>;
  /**
   * Generation add-on features. Default is []. Possible values are HighPack. The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost triple the billable units.
   */
  addons?: "HighPack";
};
export type RodinGen2TextTo3DInput = {
  /**
   * A textual prompt to guide model generation. Required for Text-to-3D mode.
   */
  prompt: string;
  /**
   * Seed value for randomization, ranging from 0 to 65535. Optional.
   */
  seed?: number;
  /**
   * Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb. Default value: `"glb"`
   */
  geometry_file_format?: "glb" | "usdz" | "fbx" | "obj" | "stl";
  /**
   * Material type. PBR: Physically-based materials with realistic lighting. Shaded: Simple materials with baked lighting. All: Both types included. Default value: `"All"`
   */
  material?: "PBR" | "Shaded" | "All";
  /**
   * Combined quality and mesh type selection. Quad = smooth surfaces, Triangle = detailed geometry. Default value: `"18K Quad"`
   */
  quality_mesh_option?:
    | "4K Quad"
    | "8K Quad"
    | "18K Quad"
    | "50K Quad"
    | "2K Triangle"
    | "20K Triangle"
    | "150K Triangle"
    | "500K Triangle";
  /**
   * Generate characters in T-pose or A-pose format, making them easier to rig and animate in 3D software.
   */
  TAPose?: boolean;
  /**
   * An array that specifies the bounding box dimensions [width, height, length].
   */
  bbox_condition?: Array<number>;
  /**
   * The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost **triple the billable units**.
   */
  addons?: "HighPack";
};
export type RouterAudioInput = {
  /**
   * URL or data URI of the audio file to process. Supported formats: wav, mp3, aiff, aac, ogg, flac, m4a.
   */
  audio_url: string | Blob | File;
  /**
   * Prompt to be used for the audio processing
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type RouterAudioOutput = {
  /**
   * Generated output from audio processing
   */
  output: string;
  /**
   * Token usage information
   */
  usage: UsageInfo;
};
export type routerInput = {
  /**
   * Prompt to be used for the chat completion
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type routerOutput = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Generated reasoning for the final answer
   */
  reasoning?: string;
  /**
   * Whether the output is partial
   */
  partial?: boolean;
  /**
   * Error message if an error occurred
   */
  error?: string;
  /**
   * Token usage information
   */
  usage?: UsageInfo;
};
export type RouterVideoEnterpriseInput = {
  /**
   * List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
   */
  video_urls?: Array<string>;
  /**
   * Prompt to be used for the video processing
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type RouterVideoEnterpriseOutput = {
  /**
   * Generated output from video processing
   */
  output: string;
  /**
   * Token usage information
   */
  usage: UsageInfo;
};
export type RouterVideoInput = {
  /**
   * List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
   */
  video_urls?: Array<string>;
  /**
   * Prompt to be used for the video processing
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type RouterVideoOutput = {
  /**
   * Generated output from video processing
   */
  output: string;
  /**
   * Token usage information
   */
  usage: UsageInfo;
};
export type RouterVisionInput = {
  /**
   * List of image URLs to be processed
   */
  image_urls: Array<string>;
  /**
   * Prompt to be used for the image
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type RouterVisionOutput = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Token usage information
   */
  usage: UsageInfo;
};
export type RundiffusionPhotoFluxInput = {
  /**
   * LoRA Scale of the photo lora model Default value: `0.75`
   */
  photo_lora_scale?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type RundiffusionPhotoFluxOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type Sa2va4bImageInput = {
  /**
   * Prompt to be used for the chat completion
   */
  prompt: string;
  /**
   * Url for the Input image.
   */
  image_url: string | Blob | File;
};
export type Sa2va4bImageOutput = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Dictionary of label: mask image
   */
  masks: Array<Image>;
};
export type Sa2va4bVideoInput = {
  /**
   * Prompt to be used for the chat completion
   */
  prompt: string;
  /**
   * The URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * Number of frames to sample from the video. If not provided, all frames are sampled.
   */
  num_frames_to_sample?: number;
};
export type Sa2va4bVideoOutput = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Dictionary of label: mask video
   */
  masks: Array<File>;
};
export type Sa2va8bVideoInput = {
  /**
   * Prompt to be used for the chat completion
   */
  prompt: string;
  /**
   * The URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * Number of frames to sample from the video. If not provided, all frames are sampled.
   */
  num_frames_to_sample?: number;
};
export type Sa2va8bVideoOutput = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Dictionary of label: mask video
   */
  masks: Array<File>;
};
export type Sam2AutoSegmentInput = {
  /**
   * URL of the image to be automatically segmented
   */
  image_url: string | Blob | File;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * Number of points to sample along each side of the image. Default value: `32`
   */
  points_per_side?: number;
  /**
   * Threshold for predicted IOU score. Default value: `0.88`
   */
  pred_iou_thresh?: number;
  /**
   * Threshold for stability score. Default value: `0.95`
   */
  stability_score_thresh?: number;
  /**
   * Minimum area of a mask region. Default value: `100`
   */
  min_mask_region_area?: number;
};
export type Sam2AutoSegmentOutput = {
  /**
   * Combined segmentation mask.
   */
  combined_mask: Image;
  /**
   * Individual segmentation masks.
   */
  individual_masks: Array<Image>;
};
export type SAM2EmbeddingOutput = {
  /**
   * Embedding of the image
   */
  embedding_b64: string;
};
export type SAM2ImageInput = {
  /**
   * URL of the image to be segmented
   */
  image_url: string | Blob | File;
  /**
   * List of prompts to segment the image
   */
  prompts?: Array<PointPrompt>;
  /**
   * Coordinates for boxes
   */
  box_prompts?: Array<BoxPrompt>;
  /**
   * Apply the mask on the image.
   */
  apply_mask?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type SAM2ImageOutput = {
  /**
   * Segmented image.
   */
  image: Image;
};
export type SAM2RLEFileOutput = {
  /**
   * Run Length Encoding of the mask.
   */
  rle: File;
  /**
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
};
export type SAM2RLEOutput = {
  /**
   * Run Length Encoding of the mask.
   */
  rle: string | Array<string>;
  /**
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
};
export type SAM2VideoOutput = {
  /**
   * The segmented video.
   */
  video: File;
  /**
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
};
export type SAM2VideoRLEInput = {
  /**
   * The URL of the video to be segmented.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the mask to be applied initially.
   */
  mask_url?: string | Blob | File;
  /**
   * List of prompts to segment the video
   */
  prompts?: Array<PointPrompt>;
  /**
   * Coordinates for boxes
   */
  box_prompts?: Array<BoxPrompt>;
  /**
   * Apply the mask on the video.
   */
  apply_mask?: boolean;
  /**
   * Return per-frame bounding box overlays as a zip archive.
   */
  boundingbox_zip?: boolean;
};
export type Sam33dAlignInput = {
  /**
   * URL of the original image used for MoGe depth estimation
   */
  image_url: string | Blob | File;
  /**
   * URL of the SAM-3D Body mesh file (.ply or .glb) to align
   */
  body_mesh_url: string | Blob | File;
  /**
   * URL of the human mask image. If not provided, uses full image.
   */
  body_mask_url?: string | Blob | File;
  /**
   * Optional URL of SAM-3D Object mesh (.glb) to create combined scene
   */
  object_mesh_url?: string | Blob | File;
  /**
   * Focal length from SAM-3D Body metadata. If not provided, estimated from MoGe.
   */
  focal_length?: number;
};
export type Sam33dAlignOutput = {
  /**
   * Aligned body mesh in PLY format
   */
  body_mesh_ply: File;
  /**
   * Aligned body mesh in GLB format (for 3D preview)
   */
  model_glb: File;
  /**
   * Visualization of aligned mesh overlaid on input image
   */
  visualization: File;
  /**
   * Alignment info (scale, translation, etc.)
   */
  metadata: SAM3DBodyAlignmentInfo;
  /**
   * Combined scene with body + object meshes in GLB format (only when object_mesh_url provided)
   */
  scene_glb?: File;
};
export type Sam33dBodyInput = {
  /**
   * URL of the image containing humans
   */
  image_url: string | Blob | File;
  /**
   * Optional URL of a binary mask image (white=person, black=background). When provided, skips auto human detection and uses this mask instead. Bbox is auto-computed from the mask.
   */
  mask_url?: string | Blob | File;
  /**
   * Export individual mesh files (.ply) per person Default value: `true`
   */
  export_meshes?: boolean;
  /**
   * Include 3D keypoint markers (spheres) in the GLB mesh for visualization Default value: `true`
   */
  include_3d_keypoints?: boolean;
};
export type Sam33dBodyOutput = {
  /**
   * 3D body mesh in GLB format with optional 3D keypoint markers
   */
  model_glb: File;
  /**
   * Combined visualization image (original + keypoints + mesh + side view)
   */
  visualization: File;
  /**
   * Individual mesh files (.ply), one per detected person (when export_meshes=True)
   */
  meshes?: Array<File>;
  /**
   * Structured metadata including keypoints and camera parameters
   */
  metadata: SAM3DBodyMetadata;
};
export type Sam33dObjectsInput = {
  /**
   * URL of the image to reconstruct in 3D
   */
  image_url: string | Blob | File;
  /**
   * Optional list of mask URLs (one per object). If not provided, use prompt/point_prompts/box_prompts to auto-segment, or entire image will be used.
   */
  mask_urls?: Array<string>;
  /**
   * Text prompt for auto-segmentation when no masks provided (e.g., 'chair', 'lamp') Default value: `"car"`
   */
  prompt?: string;
  /**
   * Point prompts for auto-segmentation when no masks provided
   */
  point_prompts?: Array<PointPromptBase>;
  /**
   * Box prompts for auto-segmentation when no masks provided. Multiple boxes supported - each produces a separate object mask for 3D reconstruction.
   */
  box_prompts?: Array<BoxPromptBase>;
  /**
   * Random seed for reproducibility
   */
  seed?: number;
  /**
   * Optional URL to external pointmap/depth data (NPY or NPZ format) for improved 3D reconstruction depth estimation
   */
  pointmap_url?: string | Blob | File;
  /**
   * Detection confidence threshold (0.1-1.0). Lower = more detections but less precise. If not set, uses the model's default.
   */
  detection_threshold?: number;
  /**
   * If True, exports GLB with baked texture and UVs instead of vertex colors.
   */
  export_textured_glb?: boolean;
};
export type Sam33dObjectsOutput = {
  /**
   * Gaussian splat file (.ply) - combined scene splat for multi-object, single splat otherwise
   */
  gaussian_splat: File;
  /**
   * 3D mesh in GLB format - combined scene for multi-object, single mesh otherwise Default value: `https://v3b.fal.media/files/b/0a8439e7/mqHMt17hzqDaqVMF7q0dB_combined_scene.glb`
   */
  model_glb?: File;
  /**
   * Per-object metadata (rotation/translation/scale)
   */
  metadata: Array<SAM3DObjectMetadata>;
  /**
   * Individual Gaussian splat files per object (only for multi-object scenes)
   */
  individual_splats?: Array<File>;
  /**
   * Individual GLB mesh files per object (only for multi-object scenes)
   */
  individual_glbs?: Array<File>;
  /**
   * Zip bundle containing all artifacts and metadata
   */
  artifacts_zip?: File;
};
export type SAM3DAlignmentInput = {
  /**
   * URL of the original image used for MoGe depth estimation
   */
  image_url: string | Blob | File;
  /**
   * URL of the SAM-3D Body mesh file (.ply or .glb) to align
   */
  body_mesh_url: string | Blob | File;
  /**
   * URL of the human mask image. If not provided, uses full image.
   */
  body_mask_url?: string | Blob | File;
  /**
   * Optional URL of SAM-3D Object mesh (.glb) to create combined scene
   */
  object_mesh_url?: string | Blob | File;
  /**
   * Focal length from SAM-3D Body metadata. If not provided, estimated from MoGe.
   */
  focal_length?: number;
};
export type SAM3DAlignmentOutput = {
  /**
   * Aligned body mesh in PLY format
   */
  body_mesh_ply: File;
  /**
   * Aligned body mesh in GLB format (for 3D preview)
   */
  model_glb: File;
  /**
   * Visualization of aligned mesh overlaid on input image
   */
  visualization: File;
  /**
   * Alignment info (scale, translation, etc.)
   */
  metadata: SAM3DBodyAlignmentInfo;
  /**
   * Combined scene with body + object meshes in GLB format (only when object_mesh_url provided)
   */
  scene_glb?: File;
};
export type SAM3DBodyInput = {
  /**
   * URL of the image containing humans
   */
  image_url: string | Blob | File;
  /**
   * Optional URL of a binary mask image (white=person, black=background). When provided, skips auto human detection and uses this mask instead. Bbox is auto-computed from the mask.
   */
  mask_url?: string | Blob | File;
  /**
   * Export individual mesh files (.ply) per person Default value: `true`
   */
  export_meshes?: boolean;
  /**
   * Include 3D keypoint markers (spheres) in the GLB mesh for visualization Default value: `true`
   */
  include_3d_keypoints?: boolean;
};
export type SAM3DBodyOutput = {
  /**
   * 3D body mesh in GLB format with optional 3D keypoint markers
   */
  model_glb: File;
  /**
   * Combined visualization image (original + keypoints + mesh + side view)
   */
  visualization: File;
  /**
   * Individual mesh files (.ply), one per detected person (when export_meshes=True)
   */
  meshes?: Array<File>;
  /**
   * Structured metadata including keypoints and camera parameters
   */
  metadata: SAM3DBodyMetadata;
};
export type SAM3DObjectInput = {
  /**
   * URL of the image to reconstruct in 3D
   */
  image_url: string | Blob | File;
  /**
   * Optional list of mask URLs (one per object). If not provided, use prompt/point_prompts/box_prompts to auto-segment, or entire image will be used.
   */
  mask_urls?: Array<string>;
  /**
   * Text prompt for auto-segmentation when no masks provided (e.g., 'chair', 'lamp') Default value: `"car"`
   */
  prompt?: string;
  /**
   * Point prompts for auto-segmentation when no masks provided
   */
  point_prompts?: Array<PointPromptBase>;
  /**
   * Box prompts for auto-segmentation when no masks provided. Multiple boxes supported - each produces a separate object mask for 3D reconstruction.
   */
  box_prompts?: Array<BoxPromptBase>;
  /**
   * Random seed for reproducibility
   */
  seed?: number;
  /**
   * Optional URL to external pointmap/depth data (NPY or NPZ format) for improved 3D reconstruction depth estimation
   */
  pointmap_url?: string | Blob | File;
  /**
   * Detection confidence threshold (0.1-1.0). Lower = more detections but less precise. If not set, uses the model's default.
   */
  detection_threshold?: number;
  /**
   * If True, exports GLB with baked texture and UVs instead of vertex colors.
   */
  export_textured_glb?: boolean;
};
export type SAM3DObjectOutput = {
  /**
   * Gaussian splat file (.ply) - combined scene splat for multi-object, single splat otherwise
   */
  gaussian_splat: File;
  /**
   * 3D mesh in GLB format - combined scene for multi-object, single mesh otherwise Default value: `https://v3b.fal.media/files/b/0a8439e7/mqHMt17hzqDaqVMF7q0dB_combined_scene.glb`
   */
  model_glb?: File;
  /**
   * Per-object metadata (rotation/translation/scale)
   */
  metadata: Array<SAM3DObjectMetadata>;
  /**
   * Individual Gaussian splat files per object (only for multi-object scenes)
   */
  individual_splats?: Array<File>;
  /**
   * Individual GLB mesh files per object (only for multi-object scenes)
   */
  individual_glbs?: Array<File>;
  /**
   * Zip bundle containing all artifacts and metadata
   */
  artifacts_zip?: File;
};
export type SAM3EmbeddingInput = {
  /**
   * URL of the image to embed.
   */
  image_url: string | Blob | File;
};
export type SAM3EmbeddingOutput = {
  /**
   * Embedding of the image
   */
  embedding_b64: string;
};
export type Sam3ImageEmbedInput = {
  /**
   * URL of the image to embed.
   */
  image_url: string | Blob | File;
};
export type Sam3ImageEmbedOutput = {
  /**
   * Embedding of the image
   */
  embedding_b64: string;
};
export type Sam3ImageInput = {
  /**
   * URL of the image to be segmented
   */
  image_url: string | Blob | File;
  /**
   * Text prompt for segmentation Default value: `"wheel"`
   */
  prompt?: string;
  /**
   * List of point prompts
   */
  point_prompts?: Array<PointPrompt>;
  /**
   * Box prompt coordinates (x_min, y_min, x_max, y_max). Multiple boxes supported - use object_id to group boxes for the same object or leave empty for separate objects.
   */
  box_prompts?: Array<BoxPrompt>;
  /**
   * Apply the mask on the image. Default value: `true`
   */
  apply_mask?: boolean;
  /**
   * If True, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If True, upload and return multiple generated masks as defined by `max_masks`.
   */
  return_multiple_masks?: boolean;
  /**
   * Maximum number of masks to return when `return_multiple_masks` is enabled. Default value: `3`
   */
  max_masks?: number;
  /**
   * Whether to include mask confidence scores.
   */
  include_scores?: boolean;
  /**
   * Whether to include bounding boxes for each mask (when available).
   */
  include_boxes?: boolean;
};
export type SAM3ImageInput = {
  /**
   * URL of the image to be segmented
   */
  image_url: string | Blob | File;
  /**
   * Text prompt for segmentation Default value: `"wheel"`
   */
  prompt?: string;
  /**
   * List of point prompts
   */
  point_prompts?: Array<PointPrompt>;
  /**
   * Box prompt coordinates (x_min, y_min, x_max, y_max). Multiple boxes supported - use object_id to group boxes for the same object or leave empty for separate objects.
   */
  box_prompts?: Array<BoxPrompt>;
  /**
   * Apply the mask on the image. Default value: `true`
   */
  apply_mask?: boolean;
  /**
   * If True, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If True, upload and return multiple generated masks as defined by `max_masks`.
   */
  return_multiple_masks?: boolean;
  /**
   * Maximum number of masks to return when `return_multiple_masks` is enabled. Default value: `3`
   */
  max_masks?: number;
  /**
   * Whether to include mask confidence scores.
   */
  include_scores?: boolean;
  /**
   * Whether to include bounding boxes for each mask (when available).
   */
  include_boxes?: boolean;
};
export type Sam3ImageOutput = {
  /**
   * Primary segmented mask preview.
   */
  image?: Image;
  /**
   * Segmented mask images.
   */
  masks: Array<Image>;
  /**
   * Per-mask metadata including scores and boxes.
   */
  metadata?: Array<MaskMetadata>;
  /**
   * Per-mask confidence scores when requested.
   */
  scores?: Array<number>;
  /**
   * Per-mask normalized bounding boxes [cx, cy, w, h] when requested.
   */
  boxes?: Array<Array<number>>;
};
export type SAM3ImageOutput = {
  /**
   * Primary segmented mask preview.
   */
  image?: Image;
  /**
   * Segmented mask images.
   */
  masks: Array<Image>;
  /**
   * Per-mask metadata including scores and boxes.
   */
  metadata?: Array<MaskMetadata>;
  /**
   * Per-mask confidence scores when requested.
   */
  scores?: Array<number>;
  /**
   * Per-mask normalized bounding boxes [cx, cy, w, h] when requested.
   */
  boxes?: Array<Array<number>>;
};
export type Sam3ImageRleInput = {
  /**
   * URL of the image to be segmented
   */
  image_url: string | Blob | File;
  /**
   * Text prompt for segmentation Default value: `"wheel"`
   */
  prompt?: string;
  /**
   * List of point prompts
   */
  point_prompts?: Array<PointPrompt>;
  /**
   * Box prompt coordinates (x_min, y_min, x_max, y_max). Multiple boxes supported - use object_id to group boxes for the same object or leave empty for separate objects.
   */
  box_prompts?: Array<BoxPrompt>;
  /**
   * Apply the mask on the image. Default value: `true`
   */
  apply_mask?: boolean;
  /**
   * If True, the media will be returned as a data URI.
   */
  sync_mode?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If True, upload and return multiple generated masks as defined by `max_masks`.
   */
  return_multiple_masks?: boolean;
  /**
   * Maximum number of masks to return when `return_multiple_masks` is enabled. Default value: `3`
   */
  max_masks?: number;
  /**
   * Whether to include mask confidence scores.
   */
  include_scores?: boolean;
  /**
   * Whether to include bounding boxes for each mask (when available).
   */
  include_boxes?: boolean;
};
export type Sam3ImageRleOutput = {
  /**
   * Run Length Encoding of the mask.
   */
  rle: string | Array<string>;
  /**
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
  /**
   * Per-mask metadata when multiple RLEs are returned.
   */
  metadata?: Array<MaskMetadata>;
  /**
   * Per-mask confidence scores when requested.
   */
  scores?: Array<number>;
  /**
   * Per-mask normalized bounding boxes [cx, cy, w, h] when requested.
   */
  boxes?: Array<Array<number>>;
};
export type SAM3RLEFileOutput = {
  /**
   * Run Length Encoding of the mask.
   */
  rle: File;
  /**
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
  /**
   * Per-mask metadata when multiple RLEs are returned.
   */
  metadata?: Array<MaskMetadata>;
  /**
   * Per-mask confidence scores when requested.
   */
  scores?: Array<number>;
  /**
   * Per-mask normalized bounding boxes [cx, cy, w, h] when requested.
   */
  boxes?: Array<Array<number>>;
};
export type SAM3RLEOutput = {
  /**
   * Run Length Encoding of the mask.
   */
  rle: string | Array<string>;
  /**
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
  /**
   * Per-mask metadata when multiple RLEs are returned.
   */
  metadata?: Array<MaskMetadata>;
  /**
   * Per-mask confidence scores when requested.
   */
  scores?: Array<number>;
  /**
   * Per-mask normalized bounding boxes [cx, cy, w, h] when requested.
   */
  boxes?: Array<Array<number>>;
};
export type Sam3VideoInput = {
  /**
   * The URL of the video to be segmented.
   */
  video_url: string | Blob | File;
  /**
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth'). Default value: `""`
   */
  prompt?: string;
  /**
   * List of point prompts
   */
  point_prompts?: Array<PointPromptBase>;
  /**
   * List of box prompt coordinates (x_min, y_min, x_max, y_max).
   */
  box_prompts?: Array<BoxPromptBase>;
  /**
   * Apply the mask on the video. Default value: `true`
   */
  apply_mask?: boolean;
  /**
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Default value: `0.5`
   */
  detection_threshold?: number;
};
export type SAM3VideoInput = {
  /**
   * The URL of the video to be segmented.
   */
  video_url: string | Blob | File;
  /**
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth'). Default value: `""`
   */
  prompt?: string;
  /**
   * List of point prompts
   */
  point_prompts?: Array<PointPromptBase>;
  /**
   * List of box prompt coordinates (x_min, y_min, x_max, y_max).
   */
  box_prompts?: Array<BoxPromptBase>;
  /**
   * Apply the mask on the video. Default value: `true`
   */
  apply_mask?: boolean;
  /**
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Default value: `0.5`
   */
  detection_threshold?: number;
};
export type Sam3VideoOutput = {
  /**
   * The segmented video.
   */
  video: File;
  /**
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
};
export type SAM3VideoOutput = {
  /**
   * The segmented video.
   */
  video: File;
  /**
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
};
export type Sam3VideoRleInput = {
  /**
   * The URL of the video to be segmented.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the mask to be applied initially.
   */
  mask_url?: string | Blob | File;
  /**
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth'). Default value: `""`
   */
  prompt?: string;
  /**
   * List of point prompts with frame indices.
   */
  point_prompts?: Array<PointPrompt>;
  /**
   * List of box prompts with optional frame_index.
   */
  box_prompts?: Array<BoxPrompt>;
  /**
   * Apply the mask on the video.
   */
  apply_mask?: boolean;
  /**
   * Return per-frame bounding box overlays as a zip archive.
   */
  boundingbox_zip?: boolean;
  /**
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail. Default value: `0.5`
   */
  detection_threshold?: number;
  /**
   * Frame index used for initial interaction when mask_url is provided.
   */
  frame_index?: number;
};
export type SAM3VideoRLEInput = {
  /**
   * The URL of the video to be segmented.
   */
  video_url: string | Blob | File;
  /**
   * The URL of the mask to be applied initially.
   */
  mask_url?: string | Blob | File;
  /**
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth'). Default value: `""`
   */
  prompt?: string;
  /**
   * List of point prompts with frame indices.
   */
  point_prompts?: Array<PointPrompt>;
  /**
   * List of box prompts with optional frame_index.
   */
  box_prompts?: Array<BoxPrompt>;
  /**
   * Apply the mask on the video.
   */
  apply_mask?: boolean;
  /**
   * Return per-frame bounding box overlays as a zip archive.
   */
  boundingbox_zip?: boolean;
  /**
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail. Default value: `0.5`
   */
  detection_threshold?: number;
  /**
   * Frame index used for initial interaction when mask_url is provided.
   */
  frame_index?: number;
};
export type Sam3VideoRleOutput = {
  /**
   * The segmented video.
   */
  video: File;
  /**
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
};
export type SAMAudioInput = {
  /**
   * URL of the audio file to process (WAV, MP3, FLAC supported)
   */
  audio_url: string | Blob | File;
  /**
   * Text prompt describing the sound to isolate.
   */
  prompt: string;
  /**
   * Automatically predict temporal spans where the target sound occurs.
   */
  predict_spans?: boolean;
  /**
   * Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Default value: `1`
   */
  reranking_candidates?: number;
  /**
   * The acceleration level to use. Default value: `"balanced"`
   */
  acceleration?: "fast" | "balanced" | "quality";
  /**
   * Output audio format. Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type SAMAudioJudgeInput = {
  /**
   * URL of the original audio file.
   */
  original_audio_url: string | Blob | File;
  /**
   * URL of the separated audio file to evaluate.
   */
  separated_audio_url: string | Blob | File;
  /**
   * Text prompt of the target sound that was separated.
   */
  prompt: string;
};
export type SAMAudioJudgeOutput = {
  /**
   * Overall separation quality score (1-5 scale)
   */
  overall: number;
  /**
   * Precision score - how much of the output is the target sound (1-5 scale)
   */
  precision: number;
  /**
   * Recall score - how much of the target sound was recovered (1-5 scale)
   */
  recall: number;
  /**
   * Faithfulness score - overall separation quality (1-5 scale)
   */
  faithfulness: number;
};
export type SamAudioSeparateInput = {
  /**
   * URL of the audio file to process (WAV, MP3, FLAC supported)
   */
  audio_url: string | Blob | File;
  /**
   * Text prompt describing the sound to isolate.
   */
  prompt: string;
  /**
   * Automatically predict temporal spans where the target sound occurs.
   */
  predict_spans?: boolean;
  /**
   * Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Default value: `1`
   */
  reranking_candidates?: number;
  /**
   * The acceleration level to use. Default value: `"balanced"`
   */
  acceleration?: "fast" | "balanced" | "quality";
  /**
   * Output audio format. Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type SamAudioSeparateOutput = {
  /**
   * The isolated target sound.
   */
  target: File;
  /**
   * Everything else in the audio.
   */
  residual: File;
  /**
   * Duration of the output audio in seconds.
   */
  duration: number;
  /**
   * Sample rate of the output audio in Hz. Default value: `48000`
   */
  sample_rate?: number;
};
export type SAMAudioSeparateOutput = {
  /**
   * The isolated target sound.
   */
  target: File;
  /**
   * Everything else in the audio.
   */
  residual: File;
  /**
   * Duration of the output audio in seconds.
   */
  duration: number;
  /**
   * Sample rate of the output audio in Hz. Default value: `48000`
   */
  sample_rate?: number;
};
export type SAMAudioSpanInput = {
  /**
   * URL of the audio file to process.
   */
  audio_url: string | Blob | File;
  /**
   * Text prompt describing the sound to isolate. Optional but recommended - helps the model identify what type of sound to extract from the span.
   */
  prompt?: string;
  /**
   * Time spans where the target sound occurs which should be isolated.
   */
  spans: Array<AudioTimeSpan>;
  /**
   * Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Requires text prompt; ignored for span-only separation. Default value: `1`
   */
  reranking_candidates?: number;
  /**
   * The acceleration level to use. Default value: `"balanced"`
   */
  acceleration?: "fast" | "balanced" | "quality";
  /**
   * Trim output audio to only include the specified span time range. If False, returns the full audio length with the target sound isolated throughout.
   */
  trim_to_span?: boolean;
  /**
   * Output audio format. Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type SamAudioSpanSeparateInput = {
  /**
   * URL of the audio file to process.
   */
  audio_url: string | Blob | File;
  /**
   * Text prompt describing the sound to isolate. Optional but recommended - helps the model identify what type of sound to extract from the span.
   */
  prompt?: string;
  /**
   * Time spans where the target sound occurs which should be isolated.
   */
  spans: Array<AudioTimeSpan>;
  /**
   * Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Requires text prompt; ignored for span-only separation. Default value: `1`
   */
  reranking_candidates?: number;
  /**
   * The acceleration level to use. Default value: `"balanced"`
   */
  acceleration?: "fast" | "balanced" | "quality";
  /**
   * Trim output audio to only include the specified span time range. If False, returns the full audio length with the target sound isolated throughout.
   */
  trim_to_span?: boolean;
  /**
   * Output audio format. Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type SamAudioSpanSeparateOutput = {
  /**
   * The isolated target sound.
   */
  target: File;
  /**
   * Everything else in the audio.
   */
  residual: File;
  /**
   * Duration of the output audio in seconds.
   */
  duration: number;
  /**
   * Sample rate of the output audio in Hz. Default value: `48000`
   */
  sample_rate?: number;
};
export type SAMAudioSpanSeparateOutput = {
  /**
   * The isolated target sound.
   */
  target: File;
  /**
   * Everything else in the audio.
   */
  residual: File;
  /**
   * Duration of the output audio in seconds.
   */
  duration: number;
  /**
   * Sample rate of the output audio in Hz. Default value: `48000`
   */
  sample_rate?: number;
};
export type SAMAudioVisualInput = {
  /**
   * URL of the video file to process (MP4, MOV, etc.)
   */
  video_url: string | Blob | File;
  /**
   * URL of the mask video (binary mask indicating target object). Black=target, White=background.
   */
  mask_video_url?: string | Blob | File;
  /**
   * Text prompt to assist with separation. Use natural language to describe the target sound. Default value: `""`
   */
  prompt?: string;
  /**
   * Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Default value: `1`
   */
  reranking_candidates?: number;
  /**
   * The acceleration level to use. Default value: `"balanced"`
   */
  acceleration?: "fast" | "balanced" | "quality";
  /**
   * Output audio format. Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type SamAudioVisualSeparateInput = {
  /**
   * URL of the video file to process (MP4, MOV, etc.)
   */
  video_url: string | Blob | File;
  /**
   * URL of the mask video (binary mask indicating target object). Black=target, White=background.
   */
  mask_video_url?: string | Blob | File;
  /**
   * Text prompt to assist with separation. Use natural language to describe the target sound. Default value: `""`
   */
  prompt?: string;
  /**
   * Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Default value: `1`
   */
  reranking_candidates?: number;
  /**
   * The acceleration level to use. Default value: `"balanced"`
   */
  acceleration?: "fast" | "balanced" | "quality";
  /**
   * Output audio format. Default value: `"wav"`
   */
  output_format?: "wav" | "mp3";
};
export type SamAudioVisualSeparateOutput = {
  /**
   * The isolated target sound.
   */
  target: File;
  /**
   * Everything else in the audio.
   */
  residual: File;
  /**
   * Duration of the output audio in seconds.
   */
  duration: number;
  /**
   * Sample rate of the output audio in Hz. Default value: `48000`
   */
  sample_rate?: number;
};
export type SAMAudioVisualSeparateOutput = {
  /**
   * The isolated target sound.
   */
  target: File;
  /**
   * Everything else in the audio.
   */
  residual: File;
  /**
   * Duration of the output audio in seconds.
   */
  duration: number;
  /**
   * Sample rate of the output audio in Hz. Default value: `48000`
   */
  sample_rate?: number;
};
export type SanaI2VVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * The random seed used for the generation process
   */
  seed: number;
};
export type SanaImageToVideoInput = {
  /**
   * The input image URL to animate
   */
  image_url: string | Blob | File;
  /**
   * The text prompt describing how the image should be animated
   */
  prompt: string;
  /**
   * The negative prompt describing what to avoid in the generation Default value: `"A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience."`
   */
  negative_prompt?: string;
  /**
   * The resolution of the output video Default value: `"480p"`
   */
  resolution?: "480p";
  /**
   * Number of frames to generate Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second for the output video Default value: `16`
   */
  fps?: number;
  /**
   * Motion intensity score (higher = more motion) Default value: `30`
   */
  motion_score?: number;
  /**
   * Guidance scale for generation (higher = more prompt adherence) Default value: `6`
   */
  guidance_scale?: number;
  /**
   * Number of denoising steps Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducible generation. If not provided, a random seed will be used.
   */
  seed?: number;
};
export type SanaSprintInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `2`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The style to generate the image in. Default value: `"(No style)"`
   */
  style_name?:
    | "(No style)"
    | "Cinematic"
    | "Photographic"
    | "Anime"
    | "Manga"
    | "Digital Art"
    | "Pixel art"
    | "Fantasy art"
    | "Neonpunk"
    | "3D Model";
};
export type SanaSprintOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type SanaV1516bInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `18`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The style to generate the image in. Default value: `"(No style)"`
   */
  style_name?:
    | "(No style)"
    | "Cinematic"
    | "Photographic"
    | "Anime"
    | "Manga"
    | "Digital Art"
    | "Pixel art"
    | "Fantasy art"
    | "Neonpunk"
    | "3D Model";
};
export type SanaV1516bOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type SanaV1548bInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `18`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The style to generate the image in. Default value: `"(No style)"`
   */
  style_name?:
    | "(No style)"
    | "Cinematic"
    | "Photographic"
    | "Anime"
    | "Manga"
    | "Digital Art"
    | "Pixel art"
    | "Fantasy art"
    | "Neonpunk"
    | "3D Model";
};
export type SanaV1548bOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type SanaVideoInput = {
  /**
   * The text prompt describing the video to generate
   */
  prompt: string;
  /**
   * The negative prompt describing what to avoid in the generation Default value: `"A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience."`
   */
  negative_prompt?: string;
  /**
   * The resolution of the output video Default value: `"480p"`
   */
  resolution?: "480p";
  /**
   * Number of frames to generate Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second for the output video Default value: `16`
   */
  fps?: number;
  /**
   * Motion intensity score (higher = more motion) Default value: `30`
   */
  motion_score?: number;
  /**
   * Guidance scale for generation (higher = more prompt adherence) Default value: `6`
   */
  guidance_scale?: number;
  /**
   * Number of denoising steps Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducible generation. If not provided, a random seed will be used.
   */
  seed?: number;
};
export type SanaVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * The random seed used for the generation process
   */
  seed: number;
};
export type SatelliteViewStyleInput = {
  /**
   * The prompt to generate a satellite/aerial view style image.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the satellite view style effect. Default value: `1`
   */
  lora_scale?: number;
};
export type SatelliteViewStyleOutput = {
  /**
   * The generated satellite view style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type scailInput = {
  /**
   * The prompt to guide video generation.
   */
  prompt: string;
  /**
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string | Blob | File;
  /**
   * Enable multi-character mode. Use when driving video has multiple people.
   */
  multi_character?: boolean;
  /**
   * Output resolution. Outputs 896x512 (landscape) or 512x896 (portrait) based on the input image aspect ratio. Default value: `"512p"`
   */
  resolution?: "512p";
  /**
   * The number of inference steps to use for the video generation. Default value: `28`
   */
  num_inference_steps?: number;
};
export type scailOutput = {
  /**
   * The generated video file.
   */
  video: File;
};
export type SceneCompositionInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Describe the scene where you want to place the subject. Default value: `"enchanted forest"`
   */
  prompt?: string;
};
export type SceneCompositionOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type SchnellFlux1ReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type SchnellFlux1TextToImageInput = {
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type SchnellReduxInput = {
  /**
   * The URL of the image to generate an image from.
   */
  image_url: string | Blob | File;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type SchnellTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type Seed3DImageTo3DInput = {
  /**
   * URL of the image for the 3D asset generation.
   */
  image_url: string | Blob | File;
};
export type Seed3DImageTo3DOutput = {
  /**
   * The generated 3D model files
   */
  model: File;
  /**
   * The number of tokens used for the 3D model generation
   */
  usage_tokens: number;
};
export type SeedanceFastI2VVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedanceFastT2VVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedanceImageToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The URL of the image used to generate video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image the video ends with. Defaults to None.
   */
  end_image_url?: string | Blob | File;
};
export type SeedanceProFastImageToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The URL of the image used to generate video
   */
  image_url: string | Blob | File;
};
export type SeedanceProFastTextToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type SeedanceProI2VVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedanceProImageToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The URL of the image used to generate video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image the video ends with. Defaults to None.
   */
  end_image_url?: string | Blob | File;
};
export type SeedanceProT2VVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedanceProTextToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type SeedanceProv15I2VVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedanceProv15ImageToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to generate audio for the video Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The URL of the image used to generate video
   */
  image_url: string | Blob | File;
  /**
   * The URL of the image the video ends with. Defaults to None.
   */
  end_image_url?: string | Blob | File;
};
export type SeedanceProv15T2VVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedanceProv15TextToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to generate audio for the video Default value: `true`
   */
  generate_audio?: boolean;
};
export type SeedanceReferenceToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Reference images to generate the video with.
   */
  reference_image_urls: Array<string>;
};
export type SeedanceReferenceToVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedanceTextToVideoInput = {
  /**
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "9:21";
  /**
   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `"720p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the video in seconds Default value: `"5"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type SeedanceVideoOutput = {
  /**
   * Generated video file
   */
  video: File;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedDream45EditInput = {
  /**
   * The text prompt used to edit the image
   */
  prompt: string;
  /**
   * The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto_2K"
    | "auto_4K";
  /**
   * Number of separate model generations to be run with the prompt. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15 Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used.
   */
  image_urls: Array<string>;
};
export type SeedDream45EditOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type SeedDream45T2IInput = {
  /**
   * The text prompt used to generate the image
   */
  prompt: string;
  /**
   * The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto_2K"
    | "auto_4K";
  /**
   * Number of separate model generations to be run with the prompt. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type SeedDream45T2IOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedDream4EditInput = {
  /**
   * The text prompt used to edit the image
   */
  prompt: string;
  /**
   * The size of the generated image. The minimum total image area is 921600 pixels. Failing this, the image size will be adjusted to by scaling it up, while maintaining the aspect ratio.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto"
    | "auto_2K"
    | "auto_4K";
  /**
   * Number of separate model generations to be run with the prompt. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15 Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate. Default value: `"standard"`
   */
  enhance_prompt_mode?: "standard" | "fast";
  /**
   * List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used.
   */
  image_urls: Array<string>;
};
export type SeedDream4EditOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedDream4T2IInput = {
  /**
   * The text prompt used to generate the image
   */
  prompt: string;
  /**
   * The size of the generated image. Total pixels must be between 960x960 and 4096x4096.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto"
    | "auto_2K"
    | "auto_4K";
  /**
   * Number of separate model generations to be run with the prompt. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate. Default value: `"standard"`
   */
  enhance_prompt_mode?: "standard" | "fast";
};
export type SeedDream4T2IOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedDreamInput = {
  /**
   * The text prompt used to generate the image
   */
  prompt: string;
  /**
   * Use for finer control over the output image size. Will be used over aspect_ratio, if both are provided. Width and height must be between 512 and 2048.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type SeedDreamOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedEditInput = {
  /**
   * The text prompt used to edit the image
   */
  prompt: string;
  /**
   * URL of the image to be edited.
   */
  image_url: string | Blob | File;
  /**
   * Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation. Default value: `0.5`
   */
  guidance_scale?: number;
  /**
   * Random seed to control the stochasticity of image generation.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type SeedEditOutput = {
  /**
   * Generated image
   */
  image: Image;
  /**
   * Seed used for generation
   */
  seed: number;
};
export type SeedVRImageInput = {
  /**
   * The input image to be processed
   */
  image_url: string | Blob | File;
  /**
   * The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly. Default value: `"factor"`
   */
  upscale_mode?: "target" | "factor";
  /**
   * Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`. Default value: `2`
   */
  upscale_factor?: number;
  /**
   * The target resolution to upscale to when `upscale_mode` is `target`. Default value: `"1080p"`
   */
  target_resolution?: "720p" | "1080p" | "1440p" | "2160p";
  /**
   * The random seed used for the generation process.
   */
  seed?: number;
  /**
   * The noise scale to use for the generation process. Default value: `0.1`
   */
  noise_scale?: number;
  /**
   * The format of the output image. Default value: `"jpg"`
   */
  output_format?: "png" | "jpg" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type SeedVRImageOutput = {
  /**
   * Upscaled image file after processing
   */
  image: ImageFile;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
};
export type SeedvrUpscaleImageInput = {
  /**
   * The input image to be processed
   */
  image_url: string | Blob | File;
  /**
   * The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly. Default value: `"factor"`
   */
  upscale_mode?: "target" | "factor";
  /**
   * Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`. Default value: `2`
   */
  upscale_factor?: number;
  /**
   * The target resolution to upscale to when `upscale_mode` is `target`. Default value: `"1080p"`
   */
  target_resolution?: "720p" | "1080p" | "1440p" | "2160p";
  /**
   * The random seed used for the generation process.
   */
  seed?: number;
  /**
   * The noise scale to use for the generation process. Default value: `0.1`
   */
  noise_scale?: number;
  /**
   * The format of the output image. Default value: `"jpg"`
   */
  output_format?: "png" | "jpg" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type SeedvrUpscaleImageOutput = {
  /**
   * Upscaled image file after processing
   */
  image: ImageFile;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
};
export type SeedvrUpscaleVideoInput = {
  /**
   * The input video to be processed
   */
  video_url: string | Blob | File;
  /**
   * The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly. Default value: `"factor"`
   */
  upscale_mode?: "target" | "factor";
  /**
   * Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`. Default value: `2`
   */
  upscale_factor?: number;
  /**
   * The target resolution to upscale to when `upscale_mode` is `target`. Default value: `"1080p"`
   */
  target_resolution?: "720p" | "1080p" | "1440p" | "2160p";
  /**
   * The random seed used for the generation process.
   */
  seed?: number;
  /**
   * The noise scale to use for the generation process. Default value: `0.1`
   */
  noise_scale?: number;
  /**
   * The format of the output video. Default value: `"X264 (.mp4)"`
   */
  output_format?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the output video. Default value: `"high"`
   */
  output_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Default value: `"balanced"`
   */
  output_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type SeedvrUpscaleVideoOutput = {
  /**
   * Upscaled video file after processing
   */
  video: File;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
};
export type SeedVRVideoInput = {
  /**
   * The input video to be processed
   */
  video_url: string | Blob | File;
  /**
   * The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly. Default value: `"factor"`
   */
  upscale_mode?: "target" | "factor";
  /**
   * Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`. Default value: `2`
   */
  upscale_factor?: number;
  /**
   * The target resolution to upscale to when `upscale_mode` is `target`. Default value: `"1080p"`
   */
  target_resolution?: "720p" | "1080p" | "1440p" | "2160p";
  /**
   * The random seed used for the generation process.
   */
  seed?: number;
  /**
   * The noise scale to use for the generation process. Default value: `0.1`
   */
  noise_scale?: number;
  /**
   * The format of the output video. Default value: `"X264 (.mp4)"`
   */
  output_format?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the output video. Default value: `"high"`
   */
  output_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Default value: `"balanced"`
   */
  output_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type SeedVRVideoOutput = {
  /**
   * Upscaled video file after processing
   */
  video: File;
  /**
   * The random seed used for the generation process.
   */
  seed: number;
};
export type SemanticImageInput = {
  /**
   * The hypothesis image to use for the measurement.
   */
  hypothesis: string;
  /**
   * The text reference to use for the measurement.
   */
  reference: string;
};
export type SemanticImageMeasurementInput = {
  /**
   * The measurements to use for the measurement.
   */
  measurements: Array<"clip_score">;
  /**
   * The inputs to use for the measurement.
   */
  inputs: Array<SemanticImageInput>;
};
export type SepiaVintageInput = {
  /**
   * The prompt to generate a sepia vintage photography style image.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the sepia vintage photography effect. Default value: `1`
   */
  lora_scale?: number;
};
export type SepiaVintageOutput = {
  /**
   * The generated sepia vintage photography style images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type SetptsVideoInput = {
  /**
   * URL of the video file to change speed
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Speed multiplier (0.25-4.0). Values > 1.0 speed up, < 1.0 slow down. E.g., 2.0 = 2x faster, 0.5 = half speed Default value: `4`
   */
  speed_factor?: number;
};
export type SetptsVideoOutput = {
  /**
   * The output video with adjusted speed
   */
  video: File;
};
export type SfxV15VideoToAudioInput = {
  /**
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string | Blob | File;
  /**
   * Additional description to guide the model
   */
  text_prompt?: string;
  /**
   * The number of samples to generate from the model Default value: `2`
   */
  num_samples?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used Default value: `8069`
   */
  seed?: number;
  /**
   * The duration of the generated audio in seconds Default value: `10`
   */
  duration?: number;
  /**
   * The start offset in seconds to start the audio generation from
   */
  start_offset?: number;
};
export type SfxV15VideoToAudioOutput = {
  /**
   * The generated sound effects audio
   */
  audio: Array<AudioOutput>;
};
export type SfxV15VideoToVideoInput = {
  /**
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string | Blob | File;
  /**
   * Additional description to guide the model
   */
  text_prompt?: string;
  /**
   * The number of samples to generate from the model Default value: `2`
   */
  num_samples?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used Default value: `8069`
   */
  seed?: number;
  /**
   * The duration of the generated audio in seconds Default value: `10`
   */
  duration?: number;
  /**
   * The start offset in seconds to start the audio generation from
   */
  start_offset?: number;
};
export type SfxV15VideoToVideoOutput = {
  /**
   * The processed video with sound effects
   */
  video: Array<VideoOutput>;
};
export type SfxV1VideoToAudioInput = {
  /**
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string | Blob | File;
  /**
   * Additional description to guide the model
   */
  text_prompt?: string;
  /**
   * The number of samples to generate from the model Default value: `2`
   */
  num_samples?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used Default value: `2105`
   */
  seed?: number;
  /**
   * The duration of the generated audio in seconds Default value: `10`
   */
  duration?: number;
};
export type SfxV1VideoToAudioOutput = {
  /**
   * The generated sound effects audio
   */
  audio: Array<Audio>;
};
export type SfxV1VideoToVideoInput = {
  /**
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string | Blob | File;
  /**
   * Additional description to guide the model
   */
  text_prompt?: string;
  /**
   * The number of samples to generate from the model Default value: `2`
   */
  num_samples?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used Default value: `2105`
   */
  seed?: number;
  /**
   * The duration of the generated audio in seconds Default value: `10`
   */
  duration?: number;
};
export type SfxV1VideoToVideoOutput = {
  /**
   * The processed video with sound effects
   */
  video: Array<Video>;
};
export type SharpenInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Type of sharpening to apply Default value: `"basic"`
   */
  sharpen_mode?: "basic" | "smart" | "cas";
  /**
   * Sharpen radius (for basic mode) Default value: `1`
   */
  sharpen_radius?: number;
  /**
   * Sharpen strength (for basic mode) Default value: `1`
   */
  sharpen_alpha?: number;
  /**
   * Noise radius for smart sharpen Default value: `7`
   */
  noise_radius?: number;
  /**
   * Edge preservation factor Default value: `0.75`
   */
  preserve_edges?: number;
  /**
   * Smart sharpen strength Default value: `5`
   */
  smart_sharpen_strength?: number;
  /**
   * Smart sharpen blend ratio Default value: `0.5`
   */
  smart_sharpen_ratio?: number;
  /**
   * CAS sharpening amount Default value: `0.8`
   */
  cas_amount?: number;
};
export type SharpenOutput = {
  /**
   * The processed images with sharpen effect
   */
  images: Array<Image>;
};
export type ShirtDesignInput = {
  /**
   * The URLs of the images: first image is the person wearing a shirt, second image is the design/logo to put on the shirt.
   */
  image_urls: Array<string>;
  /**
   * The size of the generated image. If not provided, the size of the final input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The negative prompt for the generation Default value: `" "`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Describe what design to put on the shirt. The model will apply the design from your input image onto the person's shirt. Default value: `"Put this design on their shirt"`
   */
  prompt?: string;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
};
export type ShirtDesignOutput = {
  /**
   * The generated/edited images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type SileroVadInput = {
  /**
   * The URL of the audio to get speech timestamps from.
   */
  audio_url: string | Blob | File;
};
export type SileroVadOutput = {
  /**
   * Whether the audio has speech.
   */
  has_speech: boolean;
  /**
   * The speech timestamps.
   */
  timestamps: Array<SpeechTimestamp>;
};
export type SimaVideoUpscalerLiteInput = {
  /**
   * URL of the video to upscale
   */
  video_url: string | Blob | File;
  /**
   * CRF quality (lower = better quality, 0-51) Default value: `18`
   */
  crf?: number;
};
export type SimaVideoUpscalerLiteOutput = {
  /**
   * The upscaled video
   */
  video: VideoOutput;
  /**
   * Original video dimensions (width, height)
   */
  original_size: number[];
  /**
   * Upscaled video dimensions (width, height)
   */
  upscaled_size: number[];
  /**
   * Video frame rate
   */
  frame_rate: string;
};
export type SingleFluxIDInput = {
  /**
   * Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years).
   */
  age_group:
    | "baby"
    | "toddler"
    | "preschool"
    | "gradeschooler"
    | "teen"
    | "adult"
    | "mid"
    | "senior";
  /**
   * Gender for the generated image. Choose from: 'male' or 'female'.
   */
  gender: "male" | "female";
  /**
   * Text prompt to guide the image generation Default value: `"a newborn baby, well dressed"`
   */
  prompt?: string;
  /**
   * The size of the generated image
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducibility. If None, a random seed will be used
   */
  seed?: number;
  /**
   * The format of the generated image. Choose from: 'jpeg' or 'png'. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * List of ID images for single mode (or general reference images)
   */
  id_image_urls: Array<string>;
};
export type SketchColoredImageInput = {
  /**
   * The source image.
   */
  image_url: string | Blob | File;
};
export type SketchTo3DInput = {
  /**
   * URL of sketch or line art image to transform into a 3D model. Image resolution must be between 128x128 and 5000x5000 pixels.
   */
  input_image_url: string | Blob | File;
  /**
   * Text prompt describing the 3D content attributes such as color, category, and material.
   */
  prompt: string;
  /**
   * Whether to enable PBR material generation.
   */
  enable_pbr?: boolean;
  /**
   * Target face count. Range: 40000-1500000 Default value: `500000`
   */
  face_count?: number;
};
export type SketchTo3DOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * The seed used for generation
   */
  seed?: number;
};
export type SkyRaccoonInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If true, the video will be generated faster with no noticeable degradation in the visual quality.
   */
  turbo_mode?: boolean;
};
export type SkyRaccoonOutput = {
  /**
   * The generated image file.
   */
  image: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type SkyreelsI2vInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * URL of the image input.
   */
  image_url: string | Blob | File;
  /**
   * Random seed for generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * Guidance scale for generation (between 1.0 and 20.0) Default value: `6`
   */
  guidance_scale?: number;
  /**
   * Number of denoising steps (between 1 and 50). Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Negative prompt to guide generation away from certain attributes.
   */
  negative_prompt?: string;
  /**
   * Aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
};
export type SkyreelsI2vOutput = {
  /**
   *
   */
  video: File;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type SmartTopologyInput = {
  /**
   * URL of GLB or OBJ file to optimize topology. Max size: 200MB. Default value: `"https://v3b.fal.media/files/b/0a8c09c0/VYDiCTcDGK55qY2-idGbX_model.glb"`
   */
  input_file_url?: string | Blob | File;
  /**
   * Input 3D file format. Default value: `"glb"`
   */
  input_file_type?: "glb" | "obj";
  /**
   * Output polygon type. triangle: triangular faces only. quadrilateral: mixed quad and triangle faces. Default value: `"triangle"`
   */
  polygon_type?: "triangle" | "quadrilateral";
  /**
   * Target polygon density. high: more detail/polygons, medium: balanced, low: fewer polygons. Default value: `"medium"`
   */
  face_level?: "high" | "medium" | "low";
};
export type SmartTopologyOutput = {
  /**
   * Processed 3D model with optimized topology (primary file).
   */
  model_glb: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
};
export type SmartTurnInput = {
  /**
   * The URL of the audio file to be processed.
   */
  audio_url: string | Blob | File;
};
export type SmartTurnOutput = {
  /**
   * The predicted turn type. 1 for Complete, 0 for Incomplete.
   */
  prediction: number;
  /**
   * The probability of the predicted turn type.
   */
  probability: number;
  /**
   * The metrics of the inference.
   */
  metrics: any;
};
export type SolarizeInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Solarize threshold Default value: `0.5`
   */
  solarize_threshold?: number;
};
export type SolarizeOutput = {
  /**
   * The processed images with solarize effect
   */
  images: Array<Image>;
};
export type Sora2ImageToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"auto"`
   */
  resolution?: "auto" | "720p";
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "9:16" | "16:9";
  /**
   * Duration of the generated video in seconds Default value: `"4"`
   */
  duration?: "4" | "8" | "12";
  /**
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted. Default value: `true`
   */
  delete_video?: boolean;
  /**
   * The model to use for the generation. When the default model is selected, the latest snapshot of the model will be used - otherwise, select a specific snapshot of the model. Default value: `"sora-2"`
   */
  model?: "sora-2" | "sora-2-2025-12-08" | "sora-2-2025-10-06";
  /**
   * The URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type Sora2ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: VideoFile;
  /**
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
};
export type Sora2ImageToVideoProInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"auto"`
   */
  resolution?: "auto" | "720p" | "1080p";
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "9:16" | "16:9";
  /**
   * Duration of the generated video in seconds Default value: `"4"`
   */
  duration?: "4" | "8" | "12";
  /**
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted. Default value: `true`
   */
  delete_video?: boolean;
  /**
   * The URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type Sora2ImageToVideoProOutput = {
  /**
   * The generated video
   */
  video: VideoFile;
  /**
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
};
export type Sora2TextToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p";
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Duration of the generated video in seconds Default value: `"4"`
   */
  duration?: "4" | "8" | "12";
  /**
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted. Default value: `true`
   */
  delete_video?: boolean;
  /**
   * The model to use for the generation. When the default model is selected, the latest snapshot of the model will be used - otherwise, select a specific snapshot of the model. Default value: `"sora-2"`
   */
  model?: "sora-2" | "sora-2-2025-12-08" | "sora-2-2025-10-06";
};
export type Sora2TextToVideoOutput = {
  /**
   * The generated video
   */
  video: VideoFile;
  /**
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
};
export type Sora2TextToVideoProInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The resolution of the generated video Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Duration of the generated video in seconds Default value: `"4"`
   */
  duration?: "4" | "8" | "12";
  /**
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted. Default value: `true`
   */
  delete_video?: boolean;
};
export type Sora2TextToVideoProOutput = {
  /**
   * The generated video
   */
  video: VideoFile;
  /**
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
};
export type Sora2VideoToVideoRemixInput = {
  /**
   * The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos.
   */
  video_id: string;
  /**
   * Updated text prompt that directs the remix generation
   */
  prompt: string;
  /**
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted. Default value: `true`
   */
  delete_video?: boolean;
};
export type Sora2VideoToVideoRemixOutput = {
  /**
   * The generated video
   */
  video: VideoFile;
  /**
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
};
export type SoundEffectGenerationInput = {
  /**
   * Describe the sound effect you want to generate
   */
  prompt: string;
  /**
   * Describe the types of sounds you don't want to generate in the output, avoid double-negatives, compare with positive prompts Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Length of the generated sound effect in seconds Default value: `5`
   */
  duration?: number;
  /**
   * Refinement level - Higher values may improve quality but take longer Default value: `40`
   */
  refinement?: number;
  /**
   * Creativity level - higher values allow more creative interpretation of the prompt Default value: `16`
   */
  creativity?: number;
  /**
   * Random seed for reproducible results - leave empty for random generation
   */
  seed?: number;
};
export type SoundEffectGenerationOutput = {
  /**
   * Generated audio file in WAV format
   */
  audio: File;
  /**
   * The processed prompt used for generation
   */
  prompt: string;
  /**
   * Generation metadata including duration, sample rate, and parameters
   */
  metadata: unknown;
};
export type SoundEffectOutput = {
  /**
   * The video with added sound effects
   */
  video: File;
};
export type SoundEffectsGeneratorInput = {
  /**
   * The prompt to generate SFX.
   */
  prompt: string;
  /**
   * The duration of the generated SFX in seconds.
   */
  duration: number;
};
export type SoundEffectsGeneratorOutput = {
  /**
   * The generated SFX
   */
  audio_file: File;
};
export type SpanishOutput = {
  /**
   * The generated music
   */
  audio: File;
};
export type SpeechOutput = {
  /**
   * The partial or final transcription output from Canary
   */
  output: string;
  /**
   * Indicates if this is a partial (in-progress) transcript
   */
  partial?: boolean;
};
export type SpeechToTextInput = {
  /**
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string | Blob | File;
  /**
   * Whether to use Canary's built-in punctuation & capitalization Default value: `true`
   */
  use_pnc?: boolean;
};
export type SpeechToTextOutput = {
  /**
   * The partial or final transcription output from Canary
   */
  output: string;
  /**
   * Indicates if this is a partial (in-progress) transcript
   */
  partial?: boolean;
};
export type SpeechToTextStreamInput = {
  /**
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string | Blob | File;
  /**
   * Whether to use Canary's built-in punctuation & capitalization Default value: `true`
   */
  use_pnc?: boolean;
};
export type SpeechToTextTurboInput = {
  /**
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string | Blob | File;
  /**
   * Whether to use Canary's built-in punctuation & capitalization Default value: `true`
   */
  use_pnc?: boolean;
};
export type SpeechToTextTurboOutput = {
  /**
   * The partial or final transcription output from Canary
   */
  output: string;
  /**
   * Indicates if this is a partial (in-progress) transcript
   */
  partial?: boolean;
};
export type SpeechToTextTurboStreamInput = {
  /**
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string | Blob | File;
  /**
   * Whether to use Canary's built-in punctuation & capitalization Default value: `true`
   */
  use_pnc?: boolean;
};
export type SplitAudioInput = {
  /**
   * URL of the audio file to split
   */
  audio_url: string | Blob | File;
  /**
   * List of timestamps in seconds where to split the audio
   */
  split_points: Array<number>;
};
export type SplitAudioOutput = {
  /**
   * List of split audio segments
   */
  audio: Array<AudioFile>;
};
export type SplitImagesInput = {
  /**
   * List of image URLs to split/extract
   */
  image_urls: Array<string>;
  /**
   * Output format for processed images Default value: `"png"`
   */
  output_format?: "png" | "jpg" | "jpeg" | "webp";
};
export type SplitImagesOutput = {
  /**
   * Array of processed images
   */
  images: Array<Image>;
};
export type SplitTextInput = {
  /**
   * Text to split into parts
   */
  text: string;
  /**
   * Separator to use for splitting the text Default value: `"|"`
   */
  separator?: string;
  /**
   * Maximum number of parts to return (max 6) Default value: `6`
   */
  max_parts?: number;
};
export type SplitTextOutput = {
  /**
   * First text part
   */
  text1?: string;
  /**
   * Second text part
   */
  text2?: string;
  /**
   * Third text part
   */
  text3?: string;
  /**
   * Fourth text part
   */
  text4?: string;
  /**
   * Fifth text part
   */
  text5?: string;
  /**
   * Sixth text part
   */
  text6?: string;
};
export type SprintInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `2`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The style to generate the image in. Default value: `"(No style)"`
   */
  style_name?:
    | "(No style)"
    | "Cinematic"
    | "Photographic"
    | "Anime"
    | "Manga"
    | "Digital Art"
    | "Pixel art"
    | "Fantasy art"
    | "Neonpunk"
    | "3D Model";
};
export type SRPOOutput = {
  /**
   * The generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type StableAudio25AudioToAudioInput = {
  /**
   * The prompt to guide the audio generation
   */
  prompt: string;
  /**
   * The audio clip to transform
   */
  audio_url: string | Blob | File;
  /**
   * Sometimes referred to as denoising, this parameter controls how much influence the `audio_url` parameter has on the generated audio. A value of 0 would yield audio that is identical to the input. A value of 1 would be as if you passed in no audio at all. Default value: `0.8`
   */
  strength?: number;
  /**
   * The number of steps to denoise the audio for Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio.
   */
  total_seconds?: number;
  /**
   * How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt). Default value: `1`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   *
   */
  seed?: number;
};
export type StableAudio25AudioToAudioOutput = {
  /**
   * The generated audio clip
   */
  audio: File;
  /**
   * The random seed used for generation
   */
  seed: number;
};
export type StableAudio25InpaintInput = {
  /**
   * The prompt to guide the audio generation
   */
  prompt: string;
  /**
   * The audio clip to inpaint
   */
  audio_url: string | Blob | File;
  /**
   * The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio. Default value: `190`
   */
  seconds_total?: number;
  /**
   * How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt). Default value: `1`
   */
  guidance_scale?: number;
  /**
   * The start point of the audio mask Default value: `30`
   */
  mask_start?: number;
  /**
   * The end point of the audio mask Default value: `190`
   */
  mask_end?: number;
  /**
   * The number of steps to denoise the audio for Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   *
   */
  seed?: number;
};
export type StableAudio25InpaintOutput = {
  /**
   * The generated audio clip
   */
  audio: File;
  /**
   * The random seed used for generation
   */
  seed: number;
};
export type StableAudio25TextToAudioInput = {
  /**
   * The prompt to generate audio from
   */
  prompt: string;
  /**
   * The duration of the audio clip to generate Default value: `190`
   */
  seconds_total?: number;
  /**
   * The number of steps to denoise the audio for Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt). Default value: `1`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   *
   */
  seed?: number;
};
export type StableAudio25TextToAudioOutput = {
  /**
   * The generated audio clip
   */
  audio: File;
  /**
   * The random seed used for generation
   */
  seed: number;
};
export type StableAvatarInput = {
  /**
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio to use as a reference for the video generation.
   */
  audio_url: string | Blob | File;
  /**
   * The prompt to use for the video generation.
   */
  prompt: string;
  /**
   * The aspect ratio of the video to generate. If 'auto', the aspect ratio will be determined by the reference image. Default value: `"auto"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16" | "auto";
  /**
   * The guidance scale to use for the video generation. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The audio guidance scale to use for the video generation. Default value: `4`
   */
  audio_guidance_scale?: number;
  /**
   * The number of inference steps to use for the video generation. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The seed to use for the video generation.
   */
  seed?: number;
  /**
   * The amount of perturbation to use for the video generation. 0.0 means no perturbation, 1.0 means full perturbation. Default value: `0.1`
   */
  perturbation?: number;
};
export type StableAvatarOutput = {
  /**
   * The generated video file.
   */
  video: File;
};
export type StableDiffusionV35LargeInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * ControlNet for inference.
   */
  controlnet?: ControlNet;
  /**
   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * IP-Adapter to use during inference.
   */
  ip_adapter?: IPAdapter;
};
export type StableDiffusionV35LargeOutput = {
  /**
   * The generated image files info.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type StandardFastImageToVideoHailuo23Input = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * The duration of the video in seconds. Default value: `"6"`
   */
  duration?: "6" | "10";
};
export type StandardFastImageToVideoHailuo23Output = {
  /**
   * The generated video
   */
  video: File;
};
export type StandardImageToVideoHailuo02Input = {
  /**
   *
   */
  prompt: string;
  /**
   *
   */
  image_url: string | Blob | File;
  /**
   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `"6"`
   */
  duration?: "6" | "10";
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * The resolution of the generated video. Default value: `"768P"`
   */
  resolution?: "512P" | "768P";
  /**
   * Optional URL of the image to use as the last frame of the video
   */
  end_image_url?: string | Blob | File;
};
export type StandardImageToVideoHailuo23Input = {
  /**
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * The duration of the video in seconds. Default value: `"6"`
   */
  duration?: "6" | "10";
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
};
export type StandardImageToVideoHailuo23Output = {
  /**
   * The generated video
   */
  video: File;
};
export type StandardTextToVideoHailuo02Input = {
  /**
   *
   */
  prompt: string;
  /**
   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `"6"`
   */
  duration?: "6" | "10";
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
};
export type StandardTextToVideoHailuo23Input = {
  /**
   *
   */
  prompt: string;
  /**
   * Whether to use the model's prompt optimizer Default value: `true`
   */
  prompt_optimizer?: boolean;
  /**
   * The duration of the video in seconds. Default value: `"6"`
   */
  duration?: "6" | "10";
};
export type StandardTextToVideoHailuo23Output = {
  /**
   * The generated video
   */
  video: File;
};
export type StartEndToVideoOutput = {
  /**
   * The generated transition video between start and end frames
   */
  video: File;
};
export type StarVectorInput = {
  /**
   * URL of image to be used for relighting
   */
  image_url: string | Blob | File;
  /**
   * seed to be used for generation
   */
  seed?: number;
};
export type StarVectorOutput = {
  /**
   * The generated image file info.
   */
  image: File;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
};
export type SteadyDancerInput = {
  /**
   * URL of the reference image to animate. This is the person/character whose appearance will be preserved. Default value: `"https://v3b.fal.media/files/b/0a85edaa/GDUCMPrdvOMcI5JpEcU7f.png"`
   */
  image_url?: string | Blob | File;
  /**
   * URL of the driving pose video. The motion from this video will be transferred to the reference image. Default value: `"https://v3b.fal.media/files/b/0a84de68/jXDWywjhagRfR-GuZjoRs_video.mp4"`
   */
  video_url?: string | Blob | File;
  /**
   * Text prompt describing the desired animation. Default value: `"A person dancing with smooth and natural movements."`
   */
  prompt?: string;
  /**
   * Negative prompt for video generation. Default value: `"blurred, distorted face, bad anatomy, extra limbs, poorly drawn hands, poorly drawn feet, disfigured, out of frame, duplicate, watermark, signature, text"`
   */
  negative_prompt?: string;
  /**
   * Frames per second of the generated video. Must be between 5 to 24. If not specified, uses the FPS from the input video.
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. 576p is default, 720p for higher quality. 480p is lower quality. Default value: `"576p"`
   */
  resolution?: "480p" | "576p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `6`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale for prompt adherence. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Pose guidance scale for pose control strength. Default value: `1`
   */
  pose_guidance_scale?: number;
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * Start ratio for pose guidance. Controls when pose guidance begins. Default value: `0.1`
   */
  pose_guidance_start?: number;
  /**
   * End ratio for pose guidance. Controls when pose guidance ends. Default value: `0.4`
   */
  pose_guidance_end?: number;
  /**
   * Acceleration levels. Default value: `"aggressive"`
   */
  acceleration?: "light" | "moderate" | "aggressive";
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Aspect ratio of the generated video. If 'auto', will be determined from the reference image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Number of frames to generate. If not specified, uses the frame count from the input video (capped at 241). Will be adjusted to nearest valid value (must satisfy 4k+1 pattern).
   */
  num_frames?: number;
  /**
   * If enabled, copies audio from the input driving video to the output video. Default value: `true`
   */
  preserve_audio?: boolean;
  /**
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized (num_inference_steps=6, guidance_scale=1.0) and uses the LightX2V distillation LoRA.
   */
  use_turbo?: boolean;
};
export type SteadyDancerOutput = {
  /**
   * The generated dance animation video.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The actual number of frames generated (aligned to 4k+1 pattern).
   */
  num_frames: number;
};
export type Step1xEditInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The image URL to generate an image from. Needs to match the dimensions of the mask.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type Step1xEditOutput = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type StepxEdit2Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The image URL to generate an image from. Needs to match the dimensions of the mask.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The true CFG scale. Controls how closely the model follows the prompt. Default value: `6`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Recommended: 50. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Enable thinking mode. Uses multimodal language model knowledge to interpret abstract editing instructions. Default value: `true`
   */
  enable_thinking_mode?: boolean;
  /**
   * Enable reflection mode. Reviews outputs, corrects unintended changes, and determines when editing is complete. Default value: `true`
   */
  enable_reflection_mode?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type StepxEdit2Output = {
  /**
   * The generated images
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
  /**
   * The model's interpretation of your instruction (only available when thinking mode is enabled).
   */
  reformat_prompt?: string;
  /**
   * Reasoning process details (only available when thinking mode is enabled).
   */
  think_info?: Array<string>;
  /**
   * Reflection analysis (only available when reflection mode is enabled).
   */
  best_info?: Array<any>;
};
export type StreamingDevTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type StreamingFastTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `16`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you.
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type StreamingFlux1Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type StreamingFlux2EditImageInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used.
   */
  image_urls: Array<string>;
};
export type StreamingFlux2EditImageLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The URsL of the images for editing. A maximum of 3 images are allowed, if more are provided, only the first 3 will be used.
   */
  image_urls: Array<string>;
  /**
   * List of LoRA weights to apply (maximum 3). Each LoRA can be a URL, HuggingFace repo ID, or local path.
   */
  loras?: Array<LoRAInput>;
};
export type StreamingFlux2TextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type StreamingFlux2TextToImageLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. The width and height must be between 512 and 2048 pixels. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The acceleration level to use for the image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * List of LoRA weights to apply (maximum 3). Each LoRA can be a URL, HuggingFace repo ID, or local path.
   */
  loras?: Array<LoRAInput>;
};
export type StreamingFullTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The size of the generated image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name.
   */
  loras?: Array<LoraWeight>;
};
export type StreamingInput = {
  /**
   * The description of the target image after your edits have been made. Leave this blank to allow the model to use its own imagination.
   */
  target_image_description?: string;
  /**
   * The prompt to generate an image from.
   */
  prompt?: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `"low resolution, blur"`
   */
  negative_prompt?: string;
  /**
   * URL of an input image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your initial image when looking for a related image to show you. Default value: `2`
   */
  image_guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type StreamingKleinBaseInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Negative prompt for classifier-free guidance. Describes what to avoid in the image. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for classifier-free guidance. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The seed to use for the generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The size of the image to generate. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The acceleration level to use for image generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * If `True`, the media will be returned as a data URI. Output is not stored when this is True.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
};
export type StreamingKontextEditInput = {
  /**
   * The prompt to edit the image.
   */
  prompt: string;
  /**
   * The URL of the image to edit.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output format Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Determines how the output resolution is set for image editing.
   * - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.
   * - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).
   * Apart from these, a few aspect ratios are also supported. Default value: `"match_input"`
   */
  resolution_mode?:
    | "auto"
    | "match_input"
    | "1:1"
    | "16:9"
    | "21:9"
    | "3:2"
    | "2:3"
    | "4:5"
    | "5:4"
    | "3:4"
    | "4:3"
    | "9:16"
    | "9:21";
};
export type StreamingKontextImg2ImgInput = {
  /**
   * The prompt for the image to image task.
   */
  prompt: string;
  /**
   * The URL of the image for image-to-image.
   */
  image_url: string | Blob | File;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`
   */
  strength?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output format Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type StreamingKontextInpaintInput = {
  /**
   * The URL of the image to be inpainted.
   */
  image_url: string | Blob | File;
  /**
   * The prompt for the image to image task.
   */
  prompt: string;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   */
  loras?: Array<LoraWeight>;
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The URL of the reference image for inpainting.
   */
  reference_image_url: string | Blob | File;
  /**
   * The URL of the mask for inpainting.
   */
  mask_url: string | Blob | File;
  /**
   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`
   */
  strength?: number;
};
export type StreamingKontextInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output format Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type StreamingKreaFlux1Input = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type StreamingKreaInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type StreamingSRPOInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The speed of the generation. The higher the speed, the faster the generation. Default value: `"none"`
   */
  acceleration?: "none" | "regular" | "high";
};
export type StreamingTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The system prompt to use. Default value: `"You are an assistant designed to generate superior images with the superior degree of image-text alignment based on textual prompts or user prompts."`
   */
  system_prompt?: string;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to apply normalization-based guidance scale. Default value: `true`
   */
  cfg_normalization?: boolean;
  /**
   * The ratio of the timestep interval to apply normalization-based guidance scale. Default value: `1`
   */
  cfg_trunc_ratio?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
};
export type STSInput = {
  /**
   * URL to the source audio file to be voice-converted.
   */
  source_audio_url: string | Blob | File;
  /**
   * The voice to use for the speech-to-speech request. If neither target_voice nor target_voice_audio_url are provided, a random target voice will be used.
   */
  target_voice?:
    | "Aurora"
    | "Blade"
    | "Britney"
    | "Carl"
    | "Cliff"
    | "Richard"
    | "Rico"
    | "Siobhan"
    | "Vicky";
  /**
   * URL to the audio file which represents the voice of the output audio. If provided, this will override the target_voice setting. If neither target_voice nor target_voice_audio_url are provided, the default target voice will be used.
   */
  target_voice_audio_url?: string | Blob | File;
  /**
   * If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz.
   */
  high_quality_audio?: boolean;
};
export type STSOutput = {
  /**
   * The generated voice-converted audio file.
   */
  audio: Audio;
};
export type StyleReferenceInput = {
  /**
   * URL to zip archive with images, use PNG format. Maximum 5 images are allowed.
   */
  images_data_url: string | Blob | File;
  /**
   * The base style of the generated images, this topic is covered above. Default value: `"digital_illustration"`
   */
  base_style?:
    | "any"
    | "realistic_image"
    | "digital_illustration"
    | "vector_illustration"
    | "realistic_image/b_and_w"
    | "realistic_image/hard_flash"
    | "realistic_image/hdr"
    | "realistic_image/natural_light"
    | "realistic_image/studio_portrait"
    | "realistic_image/enterprise"
    | "realistic_image/motion_blur"
    | "realistic_image/evening_light"
    | "realistic_image/faded_nostalgia"
    | "realistic_image/forest_life"
    | "realistic_image/mystic_naturalism"
    | "realistic_image/natural_tones"
    | "realistic_image/organic_calm"
    | "realistic_image/real_life_glow"
    | "realistic_image/retro_realism"
    | "realistic_image/retro_snapshot"
    | "realistic_image/urban_drama"
    | "realistic_image/village_realism"
    | "realistic_image/warm_folk"
    | "digital_illustration/pixel_art"
    | "digital_illustration/hand_drawn"
    | "digital_illustration/grain"
    | "digital_illustration/infantile_sketch"
    | "digital_illustration/2d_art_poster"
    | "digital_illustration/handmade_3d"
    | "digital_illustration/hand_drawn_outline"
    | "digital_illustration/engraving_color"
    | "digital_illustration/2d_art_poster_2"
    | "digital_illustration/antiquarian"
    | "digital_illustration/bold_fantasy"
    | "digital_illustration/child_book"
    | "digital_illustration/child_books"
    | "digital_illustration/cover"
    | "digital_illustration/crosshatch"
    | "digital_illustration/digital_engraving"
    | "digital_illustration/expressionism"
    | "digital_illustration/freehand_details"
    | "digital_illustration/grain_20"
    | "digital_illustration/graphic_intensity"
    | "digital_illustration/hard_comics"
    | "digital_illustration/long_shadow"
    | "digital_illustration/modern_folk"
    | "digital_illustration/multicolor"
    | "digital_illustration/neon_calm"
    | "digital_illustration/noir"
    | "digital_illustration/nostalgic_pastel"
    | "digital_illustration/outline_details"
    | "digital_illustration/pastel_gradient"
    | "digital_illustration/pastel_sketch"
    | "digital_illustration/pop_art"
    | "digital_illustration/pop_renaissance"
    | "digital_illustration/street_art"
    | "digital_illustration/tablet_sketch"
    | "digital_illustration/urban_glow"
    | "digital_illustration/urban_sketching"
    | "digital_illustration/vanilla_dreams"
    | "digital_illustration/young_adult_book"
    | "digital_illustration/young_adult_book_2"
    | "vector_illustration/bold_stroke"
    | "vector_illustration/chemistry"
    | "vector_illustration/colored_stencil"
    | "vector_illustration/contour_pop_art"
    | "vector_illustration/cosmics"
    | "vector_illustration/cutout"
    | "vector_illustration/depressive"
    | "vector_illustration/editorial"
    | "vector_illustration/emotional_flat"
    | "vector_illustration/infographical"
    | "vector_illustration/marker_outline"
    | "vector_illustration/mosaic"
    | "vector_illustration/naivector"
    | "vector_illustration/roundish_flat"
    | "vector_illustration/segmented_colors"
    | "vector_illustration/sharp_contrast"
    | "vector_illustration/thin"
    | "vector_illustration/vector_photo"
    | "vector_illustration/vivid_shapes"
    | "vector_illustration/engraving"
    | "vector_illustration/line_art"
    | "vector_illustration/line_circuit"
    | "vector_illustration/linocut";
};
export type StyleReferenceOutput = {
  /**
   * The ID of the created style, this ID can be used to reference the style in the future.
   */
  style_id: string;
};
export type StyleTransferInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The artistic style to apply. Default value: `"Van Gogh's Starry Night"`
   */
  prompt?: string;
};
export type StyleTransferOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type StylizeInput = {
  /**
   * The style for your character in the video. Please use a short description.
   */
  style: string;
  /**
   * URL of the image to make the stylized video from.
   */
  image_url: string | Blob | File;
};
export type SubjectCustomizeInput = {
  /**
   * The text prompt describing what you want to see, using [1] to reference the subject
   */
  prompt: string;
  /**
   * Type of subject in the reference images
   */
  subject_type: "product" | "animal";
  /**
   * 1-4 reference images of the subject to customize
   */
  reference_images: Array<ReferenceImage>;
  /**
   * Optional description of the subject in the reference images
   */
  subject_description?: string;
  /**
   * A description of what to discourage in the generated images Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Number of images to generate (1-4) Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducible generation
   */
  seed?: number;
};
export type SubjectReferenceOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type SwapOutput = {
  /**
   * The generated swapped video
   */
  video: File;
};
export type swin2srInput = {
  /**
   * URL of image to be used for image enhancement
   */
  image_url: string | Blob | File;
  /**
   * seed to be used for generation
   */
  seed?: number;
  /**
   * Task to perform Default value: `"classical_sr"`
   */
  task?: "classical_sr" | "compressed_sr" | "real_sr";
};
export type swin2srOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
};
export type SyncLipsyncInput = {
  /**
   * The model to use for lipsyncing Default value: `"lipsync-1.9.0-beta"`
   */
  model?: "lipsync-1.8.0" | "lipsync-1.7.1" | "lipsync-1.9.0-beta";
  /**
   * URL of the input video
   */
  video_url: string | Blob | File;
  /**
   * URL of the input audio
   */
  audio_url: string | Blob | File;
  /**
   * Lipsync mode when audio and video durations are out of sync. Default value: `"cut_off"`
   */
  sync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
};
export type SyncLipsyncOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type SyncLipsyncReact1Input = {
  /**
   * URL to the input video. Must be **15 seconds or shorter**.
   */
  video_url: string | Blob | File;
  /**
   * URL to the input audio. Must be **15 seconds or shorter**.
   */
  audio_url: string | Blob | File;
  /**
   * Emotion prompt for the generation. Currently supports single-word emotions only.
   */
  emotion: "happy" | "angry" | "sad" | "neutral" | "disgusted" | "surprised";
  /**
   * Controls the edit region and movement scope for the model. Available options:
   * - `lips`: Only lipsync using react-1 (minimal facial changes).
   * - `face`: Lipsync + facial expressions without head movements.
   * - `head`: Lipsync + facial expressions + natural talking head movements. Default value: `"face"`
   */
  model_mode?: "lips" | "face" | "head";
  /**
   * Lipsync mode when audio and video durations are out of sync. Default value: `"bounce"`
   */
  lipsync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
  /**
   * Controls the expresiveness of the lipsync. Default value: `0.5`
   */
  temperature?: number;
};
export type SyncLipsyncReact1Output = {
  /**
   * The generated video with synchronized lip and facial movements.
   */
  video: VideoFile;
};
export type SyncLipsyncV2Input = {
  /**
   * The model to use for lipsyncing. `lipsync-2-pro` will cost roughly 1.67 times as much as `lipsync-2` for the same duration. Default value: `"lipsync-2"`
   */
  model?: "lipsync-2" | "lipsync-2-pro";
  /**
   * URL of the input video
   */
  video_url: string | Blob | File;
  /**
   * URL of the input audio
   */
  audio_url: string | Blob | File;
  /**
   * Lipsync mode when audio and video durations are out of sync. Default value: `"cut_off"`
   */
  sync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
};
export type SyncLipsyncV2Output = {
  /**
   * The generated video
   */
  video: File;
};
export type SyncLipsyncV2ProInput = {
  /**
   * URL of the input video
   */
  video_url: string | Blob | File;
  /**
   * URL of the input audio
   */
  audio_url: string | Blob | File;
  /**
   * Lipsync mode when audio and video durations are out of sync. Default value: `"cut_off"`
   */
  sync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
};
export type SyncLipsyncV2ProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type T2IO3ImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type T2IV3ImageOutput = {
  /**
   * Generated images
   */
  images: Array<Image>;
};
export type T2VDirectorOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type T2VLiveOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type T2VOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type TemplateToVideoOutput = {
  /**
   * The generated video using a predefined template
   */
  video: File;
};
export type TestClientSelectionInput = {
  /**
   * Optional user ID to simulate. Affects routing for favored users.
   */
  user_id?: string;
  /**
   * Whether to simulate a retry (passes a fake exception list to get_client_for_request).
   */
  is_retry?: boolean;
  /**
   * Number of times to call the client selection algorithm. Default value: `100`
   */
  num_trials?: number;
  /**
   * Resolution to simulate (e.g. '1K', '2K', '4K'). Default value: `"1K"`
   */
  resolution?: "1K" | "2K" | "4K";
  /**
   * Whether to simulate a request that requires web search.
   */
  enable_web_search?: boolean;
  /**
   * Seed value to simulate (non-None excludes Poe/OpenRouter).
   */
  seed?: number;
  /**
   * Safety tolerance to simulate (1-6). Values <= 3 exclude Poe/OpenRouter. Default value: `"4"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
};
export type TestClientSelectionOutput = {
  /**
   *
   */
  total_trials: number;
  /**
   * Map of client description to selection count.
   */
  counts: unknown;
  /**
   * Map of client description to selection percentage.
   */
  percentages: unknown;
  /**
   * Current balancer state summary.
   */
  balancer_state: string;
};
export type Text2VideoInput = {
  /**
   * The avatar to use for the video
   */
  avatar_id:
    | "emily_vertical_primary"
    | "emily_vertical_secondary"
    | "marcus_vertical_primary"
    | "marcus_vertical_secondary"
    | "mira_vertical_primary"
    | "mira_vertical_secondary"
    | "jasmine_vertical_primary"
    | "jasmine_vertical_secondary"
    | "jasmine_vertical_walking"
    | "aisha_vertical_walking"
    | "elena_vertical_primary"
    | "elena_vertical_secondary"
    | "any_male_vertical_primary"
    | "any_female_vertical_primary"
    | "any_male_vertical_secondary"
    | "any_female_vertical_secondary"
    | "any_female_vertical_walking"
    | "emily_primary"
    | "emily_side"
    | "marcus_primary"
    | "marcus_side"
    | "aisha_walking"
    | "elena_primary"
    | "elena_side"
    | "any_male_primary"
    | "any_female_primary"
    | "any_male_side"
    | "any_female_side";
  /**
   *
   */
  text: string;
};
export type TextDetectionInput = {
  /**
   * Text content to analyze for AI generation.
   */
  text: string;
};
export type TextOutput = {
  /**
   * The answer to the query.
   */
  text: string;
  /**
   * The seed used for the generation.
   */
  seed: number;
  /**
   * The query used for the generation.
   */
  prompt: string;
  /**
   * The timings of the generation.
   */
  timings: any;
};
export type TextReferenceInput = {
  /**
   * The reference text to use for the measurement.
   */
  reference: string;
  /**
   * The hypothesis text to use for the measurement.
   */
  hypothesis: string;
};
export type TextReferenceMeasurementInput = {
  /**
   * The measurements to use for the measurement.
   */
  measurements: Array<"wer">;
  /**
   * The inputs to use for the measurement.
   */
  inputs: Array<TextReferenceInput>;
};
export type TextRemovalInput = {
  /**
   * URL of the image containing text to be removed.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type TextRemovalOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type TextTo3dInput = {
  /**
   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.
   */
  seed?: number;
  /**
   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.
   */
  face_limit?: number;
  /**
   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.
   */
  pbr?: boolean;
  /**
   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `"standard"`
   */
  texture?: "no" | "standard" | "HD";
  /**
   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.
   */
  texture_seed?: number;
  /**
   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.
   */
  auto_size?: boolean;
  /**
   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.
   */
  quad?: boolean;
  /**
   * Text input that directs the model generation. The maximum prompt length is 1024 characters, equivalent to approximately 100 words. The API supports multiple languages. However, emojis and certain special Unicode characters are not supported.
   */
  prompt: string;
  /**
   * Unlike prompt, it provides a reverse direction to assist in generating content contrasting with the original prompt. The maximum length is 255 characters.
   */
  negative_prompt?: string;
  /**
   * This is the random seed used for the process based on the prompt. This parameter is an integer and is randomly chosen if not set.
   */
  image_seed?: number;
};
export type TextTo3DInput = {
  /**
   * Describe what kind of object the 3D model is. Maximum 600 characters.
   */
  prompt: string;
  /**
   * Generation mode. 'preview' returns untextured geometry only, 'full' returns textured model (preview + refine). Default value: `"full"`
   */
  mode?: "preview" | "full";
  /**
   * Desired art style of the object. Note: enable_pbr should be false for sculpture style. Default value: `"realistic"`
   */
  art_style?: "realistic" | "sculpture";
  /**
   * Seed for reproducible results. Same prompt and seed usually generate the same result.
   */
  seed?: number;
  /**
   * Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry. Default value: `"triangle"`
   */
  topology?: "quad" | "triangle";
  /**
   * Target number of polygons in the generated model Default value: `30000`
   */
  target_polycount?: number;
  /**
   * Whether to enable the remesh phase. When false, returns unprocessed triangular mesh. Default value: `true`
   */
  should_remesh?: boolean;
  /**
   * Controls symmetry behavior during model generation. Default value: `"auto"`
   */
  symmetry_mode?: "off" | "auto" | "on";
  /**
   * Whether to generate the model in an A/T pose
   */
  is_a_t_pose?: boolean;
  /**
   * Generate PBR Maps (metallic, roughness, normal) in addition to base color. Should be false for sculpture style.
   */
  enable_pbr?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Additional text prompt to guide the texturing process (only used in 'full' mode)
   */
  texture_prompt?: string;
  /**
   * 2D image to guide the texturing process (only used in 'full' mode)
   */
  texture_image_url?: string | Blob | File;
  /**
   * If set to true, input data will be checked for safety before processing. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type TextTo3DOutput = {
  /**
   * Generated 3D object in GLB format.
   */
  model_glb: File;
  /**
   * Preview thumbnail of the generated model
   */
  thumbnail?: File;
  /**
   * URLs for different 3D model formats
   */
  model_urls: ModelUrls;
  /**
   * Array of texture file objects
   */
  texture_urls?: Array<TextureFiles>;
  /**
   * The seed used for generation
   */
  seed?: number;
  /**
   * The text prompt used for generation
   */
  prompt: string;
  /**
   * The actual prompt used if prompt expansion was enabled
   */
  actual_prompt?: string;
};
export type TextToAudioInput = {
  /**
   * The prompt to generate audio from
   */
  prompt: string;
  /**
   * The duration of the audio clip to generate Default value: `190`
   */
  seconds_total?: number;
  /**
   * The number of steps to denoise the audio for Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt). Default value: `1`
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   *
   */
  seed?: number;
};
export type TextToAudioOutput = {
  /**
   * The generated audio clip
   */
  audio: File;
  /**
   * The random seed used for generation
   */
  seed: number;
};
export type TextToDialogueOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Random seed for reproducibility.
   */
  seed: number;
};
export type TextToImage32Input = {
  /**
   * Prompt for image generation.
   */
  prompt: string;
  /**
   * Number of inference steps. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Random seed for reproducibility. Default value: `5555`
   */
  seed?: number;
  /**
   * Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9 Default value: `"1:1"`
   */
  aspect_ratio?:
    | "1:1"
    | "2:3"
    | "3:2"
    | "3:4"
    | "4:3"
    | "4:5"
    | "5:4"
    | "9:16"
    | "16:9";
  /**
   * Negative prompt for image generation. Default value: `"Logo,Watermark,Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra limbs,Gross proportions,Missing arms,Mutated hands,Long neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad anatomy,Cloned face,Malformed limbs,Missing legs,Too many fingers"`
   */
  negative_prompt?: string;
  /**
   * Guidance scale for text. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Whether to truncate the prompt. Default value: `true`
   */
  truncate_prompt?: boolean;
  /**
   * Whether to improve the prompt. Default value: `true`
   */
  prompt_enhancer?: boolean;
  /**
   * If true, returns the image directly in the response (increases latency).
   */
  sync_mode?: boolean;
};
export type TextToImage32Output = {
  /**
   * Generated image.
   */
  image: Image;
};
export type TextToImageInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The aspect ratio of the generated image Default value: `"1:1"`
   */
  aspect_ratio?:
    | "10:16"
    | "16:10"
    | "9:16"
    | "16:9"
    | "4:3"
    | "3:4"
    | "1:1"
    | "1:3"
    | "3:1"
    | "3:2"
    | "2:3";
  /**
   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The style of the generated image Default value: `"auto"`
   */
  style?: "auto" | "general" | "realistic" | "design" | "render_3D" | "anime";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * A negative prompt to avoid in the generated image Default value: `""`
   */
  negative_prompt?: string;
};
export type TextToImageLoRAInput = {
  /**
   * A caption for the input image.
   */
  prompt?: string;
  /**
   * The URL of the input image.
   */
  image_url: string | Blob | File;
  /**
   * The negative prompt to generate an image from. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale to use for the image generation. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of layers to generate. Default value: `4`
   */
  num_layers?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type TextToImageOutput = {
  /**
   * The edited image
   */
  image: Image;
};
export type TextToImageTurboInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution). Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to perform. Default value: `4`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you.
   */
  guidance_scale?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
};
export type TextToImageWanInput = {
  /**
   * Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters.
   */
  prompt: string;
  /**
   * Optional reference image (0 or 1). When provided, can be used for style guidance. Resolution: 384-5000px each dimension. Max size: 10MB. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP.
   */
  image_url?: string | Blob | File;
  /**
   * Content to avoid in the generated image. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Output image size. If not set: matches input image size (up to 1280*1280). Use presets like 'square_hd', 'landscape_16_9', or specify exact dimensions.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Maximum number of images to generate (1-5). Actual count may be less depending on model inference. Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed for reproducibility (0-2147483647).
   */
  seed?: number;
  /**
   * Enable content moderation for input and output. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type TextToImageWanOutput = {
  /**
   * Generated images in PNG format
   */
  images: Array<File>;
  /**
   * Generated text content (in mixed text-and-image mode). May be None if only images were generated.
   */
  generated_text?: string;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type TextToSpeechHD26Output = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type TextToSpeechHD28Output = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type TextToSpeechOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type TextToSpeechTurbo26Output = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type TextToSpeechTurbo28Output = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Duration of the audio in milliseconds
   */
  duration_ms: number;
};
export type TextToVideoHailuo02Output = {
  /**
   * The generated video
   */
  video: File;
};
export type TextToVideoInput = {
  /**
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Negative prompt for generation Default value: `"worst quality, inconsistent motion, blurry, jittery, distorted"`
   */
  negative_prompt?: string;
  /**
   * Resolution of the generated video (480p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * Number of inference steps Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Whether to expand the prompt using the model's own capabilities. Default value: `true`
   */
  expand_prompt?: boolean;
};
export type TextToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type TextToVideoTurboInput = {
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `5`
   */
  duration?: number;
};
export type TextToVideov21Input = {
  /**
   *
   */
  prompt: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * A negative prompt to guide the model Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * The resolution of the generated video Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The duration of the generated video in seconds Default value: `5`
   */
  duration?: number;
};
export type TextToVideoV21MasterOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type TextToVideoV21Output = {
  /**
   * The generated video
   */
  video: File;
};
export type TextToVideoV25ProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type TextToVideoV26ProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type TextToVideoV2MasterOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type TextToVideoV3ProOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type TextToVideoV3StandardOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type TextToVoiceCreateOutput = {
  /**
   * The ID of the voice.
   */
  voice_id: string;
  /**
   * Preview of the created voice.
   */
  audio: File;
};
export type TextToVoiceDesignOutput = {
  /**
   * The text voiced by the created voice.
   */
  text: string;
  /**
   * A list of audio previews generated with the created voice.
   */
  previews: Array<VoicePreview>;
  /**
   * Random seed for reproducibility.
   */
  seed: number;
};
export type TextToVoiceRemixOutput = {
  /**
   * The text voiced by the created voice.
   */
  text: string;
  /**
   * A list of audio previews generated with the created voice.
   */
  previews: Array<VoicePreview>;
  /**
   * Random seed for reproducibility.
   */
  seed: number;
};
export type TextureTransformInput = {
  /**
   * Image URL for texture transformation
   */
  image_url: string | Blob | File;
  /**
   *  Default value: `"marble"`
   */
  target_texture?:
    | "cotton"
    | "denim"
    | "wool"
    | "felt"
    | "wood"
    | "leather"
    | "velvet"
    | "stone"
    | "marble"
    | "ceramic"
    | "concrete"
    | "brick"
    | "clay"
    | "foam"
    | "glass"
    | "metal"
    | "silk"
    | "fabric"
    | "crystal"
    | "rubber"
    | "plastic"
    | "lace";
  /**
   * Aspect ratio for 4K output
   */
  aspect_ratio?: AspectRatio;
};
export type TextureTransformOutput = {
  /**
   * Image with transformed texture
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type theraInput = {
  /**
   * URL of image to be used for upscaling
   */
  image_url: string | Blob | File;
  /**
   * The upscaling factor for the image. Default value: `2`
   */
  upscale_factor?: number;
  /**
   * Backbone to use for upscaling
   */
  backbone: "edsr" | "rdn";
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
};
export type theraOutput = {
  /**
   * The generated image file info.
   */
  image: Image;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
};
export type ThinksoundAudioInput = {
  /**
   * The URL of the video to generate the audio for.
   */
  video_url: string | Blob | File;
  /**
   * A prompt to guide the audio generation. If not provided, it will be extracted from the video. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * The number of inference steps for audio generation. Default value: `24`
   */
  num_inference_steps?: number;
  /**
   * The classifier-free guidance scale for audio generation. Default value: `5`
   */
  cfg_scale?: number;
};
export type ThinksoundAudioOutput = {
  /**
   * The generated audio file.
   */
  audio: File;
  /**
   * The prompt used to generate the audio.
   */
  prompt: string;
};
export type thinksoundInput = {
  /**
   * The URL of the video to generate the audio for.
   */
  video_url: string | Blob | File;
  /**
   * A prompt to guide the audio generation. If not provided, it will be extracted from the video. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * The number of inference steps for audio generation. Default value: `24`
   */
  num_inference_steps?: number;
  /**
   * The classifier-free guidance scale for audio generation. Default value: `5`
   */
  cfg_scale?: number;
};
export type thinksoundOutput = {
  /**
   * The generated video with audio.
   */
  video: File;
  /**
   * The prompt used to generate the audio.
   */
  prompt: string;
};
export type TimeOfDayInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The time of day to transform the scene to. Default value: `"golden hour"`
   */
  prompt?: string;
};
export type TimeOfDayOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type TopazUpscaleImageInput = {
  /**
   * Model to use for image enhancement. Default value: `"Standard V2"`
   */
  model?:
    | "Low Resolution V2"
    | "Standard V2"
    | "CGI"
    | "High Fidelity V2"
    | "Text Refine"
    | "Recovery"
    | "Redefine"
    | "Recovery V2";
  /**
   * Factor to upscale the video by (e.g. 2.0 doubles width and height) Default value: `2`
   */
  upscale_factor?: number;
  /**
   *
   */
  crop_to_fill?: boolean;
  /**
   * Url of the image to be upscaled
   */
  image_url: string | Blob | File;
  /**
   * Output format of the upscaled image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * Subject detection mode for the image enhancement. Default value: `"All"`
   */
  subject_detection?: "All" | "Foreground" | "Background";
  /**
   * Whether to apply face enhancement to the image. Default value: `true`
   */
  face_enhancement?: boolean;
  /**
   * Creativity level for face enhancement. 0.0 means no creativity, 1.0 means maximum creativity. Ignored if face ehnancement is disabled.
   */
  face_enhancement_creativity?: number;
  /**
   * Strength of the face enhancement. 0.0 means no enhancement, 1.0 means maximum enhancement. Ignored if face ehnancement is disabled. Default value: `0.8`
   */
  face_enhancement_strength?: number;
};
export type TopazUpscaleImageOutput = {
  /**
   * The upscaled image.
   */
  image: File;
};
export type TopazUpscaleVideoInput = {
  /**
   * URL of the video to upscale
   */
  video_url: string | Blob | File;
  /**
   * Factor to upscale the video by (e.g. 2.0 doubles width and height) Default value: `2`
   */
  upscale_factor?: number;
  /**
   * Target FPS for frame interpolation. If set, frame interpolation will be enabled.
   */
  target_fps?: number;
  /**
   * Whether to use H264 codec for output video. Default is H265.
   */
  H264_output?: boolean;
};
export type TopazUpscaleVideoOutput = {
  /**
   * The upscaled video file
   */
  video: File;
};
export type TranscriptionOutput = {
  /**
   * The full transcribed text
   */
  text: string;
  /**
   * Detected or specified language code
   */
  language_code: string;
  /**
   * Confidence in language detection
   */
  language_probability: number;
  /**
   * Word-level transcription details
   */
  words: Array<TranscriptionWord>;
};
export type TransitionOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type transpixarInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The negative prompt to generate video from Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps to perform. Default value: `24`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`
   */
  guidance_scale?: number;
  /**
   * The target FPS of the video Default value: `8`
   */
  export_fps?: number;
};
export type transpixarOutput = {
  /**
   * The URL to the generated video
   */
  videos: Array<File>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated video. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * The prompt used for generating the video.
   */
  prompt: string;
};
export type Trellis2Input = {
  /**
   * Random seed for reproducibility
   */
  seed?: number;
  /**
   * Output resolution; higher is slower but more detailed Default value: `"1024"`
   */
  resolution?: "512" | "1024" | "1536";
  /**
   *  Default value: `7.5`
   */
  ss_guidance_strength?: number;
  /**
   *  Default value: `0.7`
   */
  ss_guidance_rescale?: number;
  /**
   *  Default value: `12`
   */
  ss_sampling_steps?: number;
  /**
   *  Default value: `5`
   */
  ss_rescale_t?: number;
  /**
   *  Default value: `7.5`
   */
  shape_slat_guidance_strength?: number;
  /**
   *  Default value: `0.5`
   */
  shape_slat_guidance_rescale?: number;
  /**
   *  Default value: `12`
   */
  shape_slat_sampling_steps?: number;
  /**
   *  Default value: `3`
   */
  shape_slat_rescale_t?: number;
  /**
   *  Default value: `1`
   */
  tex_slat_guidance_strength?: number;
  /**
   *
   */
  tex_slat_guidance_rescale?: number;
  /**
   *  Default value: `12`
   */
  tex_slat_sampling_steps?: number;
  /**
   *  Default value: `3`
   */
  tex_slat_rescale_t?: number;
  /**
   * Target vertex count for mesh simplification during export Default value: `500000`
   */
  decimation_target?: number;
  /**
   * Texture resolution Default value: `"2048"`
   */
  texture_size?: "1024" | "2048" | "4096";
  /**
   * Run remeshing (slower; often improves topology) Default value: `true`
   */
  remesh?: boolean;
  /**
   *  Default value: `1`
   */
  remesh_band?: number;
  /**
   *
   */
  remesh_project?: number;
  /**
   * URL of the input image to convert to 3D
   */
  image_url: string | Blob | File;
};
export type Trellis2Output = {
  /**
   * Generated 3D GLB file
   */
  model_glb: File;
};
export type TrellisMultiInput = {
  /**
   * List of URLs of input images to convert to 3D
   */
  image_urls: Array<string>;
  /**
   * Random seed for reproducibility
   */
  seed?: number;
  /**
   * Guidance strength for sparse structure generation Default value: `7.5`
   */
  ss_guidance_strength?: number;
  /**
   * Sampling steps for sparse structure generation Default value: `12`
   */
  ss_sampling_steps?: number;
  /**
   * Guidance strength for structured latent generation Default value: `3`
   */
  slat_guidance_strength?: number;
  /**
   * Sampling steps for structured latent generation Default value: `12`
   */
  slat_sampling_steps?: number;
  /**
   * Mesh simplification factor Default value: `0.95`
   */
  mesh_simplify?: number;
  /**
   * Texture resolution Default value: `"1024"`
   */
  texture_size?: "512" | "1024" | "2048";
  /**
   * Algorithm for multi-image generation Default value: `"stochastic"`
   */
  multiimage_algo?: "stochastic" | "multidiffusion";
};
export type TrellisMultiOutput = {
  /**
   * Generated 3D mesh file
   */
  model_mesh: File;
  /**
   * Processing timings
   */
  timings: any;
};
export type TripoV25ImageTo3dInput = {
  /**
   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.
   */
  seed?: number;
  /**
   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.
   */
  face_limit?: number;
  /**
   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.
   */
  pbr?: boolean;
  /**
   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `"standard"`
   */
  texture?: "no" | "standard" | "HD";
  /**
   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.
   */
  texture_seed?: number;
  /**
   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.
   */
  auto_size?: boolean;
  /**
   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.
   */
  quad?: boolean;
  /**
   * Determines the prioritization of texture alignment in the 3D model. The default value is original_image. Default value: `"original_image"`
   */
  texture_alignment?: "original_image" | "geometry";
  /**
   * Set orientation=align_image to automatically rotate the model to align the original image. The default value is default. Default value: `"default"`
   */
  orientation?: "default" | "align_image";
  /**
   * URL of the image to use for model generation.
   */
  image_url: string | Blob | File;
};
export type TripoV25ImageTo3dOutput = {
  /**
   * The task id of the 3D model generation.
   */
  task_id: string;
  /**
   * Model
   */
  model_mesh?: File;
  /**
   * Base model
   */
  base_model?: File;
  /**
   * Pbr model
   */
  pbr_model?: File;
  /**
   * A preview image of the model
   */
  rendered_image?: File;
};
export type TripoV25MultiviewTo3dInput = {
  /**
   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.
   */
  seed?: number;
  /**
   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.
   */
  face_limit?: number;
  /**
   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.
   */
  pbr?: boolean;
  /**
   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `"standard"`
   */
  texture?: "no" | "standard" | "HD";
  /**
   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.
   */
  texture_seed?: number;
  /**
   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.
   */
  auto_size?: boolean;
  /**
   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.
   */
  quad?: boolean;
  /**
   * Determines the prioritization of texture alignment in the 3D model. The default value is original_image. Default value: `"original_image"`
   */
  texture_alignment?: "original_image" | "geometry";
  /**
   * Set orientation=align_image to automatically rotate the model to align the original image. The default value is default. Default value: `"default"`
   */
  orientation?: "default" | "align_image";
  /**
   * Front view image of the object.
   */
  front_image_url: string | Blob | File;
  /**
   * Left view image of the object.
   */
  left_image_url?: string | Blob | File;
  /**
   * Back view image of the object.
   */
  back_image_url?: string | Blob | File;
  /**
   * Right view image of the object.
   */
  right_image_url?: string | Blob | File;
};
export type TripoV25MultiviewTo3dOutput = {
  /**
   * The task id of the 3D model generation.
   */
  task_id: string;
  /**
   * Model
   */
  model_mesh?: File;
  /**
   * Base model
   */
  base_model?: File;
  /**
   * Pbr model
   */
  pbr_model?: File;
  /**
   * A preview image of the model
   */
  rendered_image?: File;
};
export type TTSInput = {
  /**
   * The text to be converted to speech
   */
  text: string;
  /**
   * The voice ID to use for speech synthesis Default value: `"genshin_vindi2"`
   */
  voice_id?:
    | "genshin_vindi2"
    | "zhinen_xuesheng"
    | "AOT"
    | "ai_shatang"
    | "genshin_klee2"
    | "genshin_kirara"
    | "ai_kaiya"
    | "oversea_male1"
    | "ai_chenjiahao_712"
    | "girlfriend_4_speech02"
    | "chat1_female_new-3"
    | "chat_0407_5-1"
    | "cartoon-boy-07"
    | "uk_boy1"
    | "cartoon-girl-01"
    | "PeppaPig_platform"
    | "ai_huangzhong_712"
    | "ai_huangyaoshi_712"
    | "ai_laoguowang_712"
    | "chengshu_jiejie"
    | "you_pingjing"
    | "calm_story1"
    | "uk_man2"
    | "laopopo_speech02"
    | "heainainai_speech02"
    | "reader_en_m-v1"
    | "commercial_lady_en_f-v1"
    | "tiyuxi_xuedi"
    | "tiexin_nanyou"
    | "girlfriend_1_speech02"
    | "girlfriend_2_speech02"
    | "zhuxi_speech02"
    | "uk_oldman3"
    | "dongbeilaotie_speech02"
    | "chongqingxiaohuo_speech02"
    | "chuanmeizi_speech02"
    | "chaoshandashu_speech02"
    | "ai_taiwan_man2_speech02"
    | "xianzhanggui_speech02"
    | "tianjinjiejie_speech02"
    | "diyinnansang_DB_CN_M_04-v2"
    | "yizhipiannan-v1"
    | "guanxiaofang-v2"
    | "tianmeixuemei-v1"
    | "daopianyansang-v1"
    | "mengwa-v1";
  /**
   * Rate of speech Default value: `1`
   */
  voice_speed?: number;
};
export type TTSOutput = {
  /**
   * The generated audio
   */
  audio: File;
};
export type TurboFluxTrainerInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   */
  images_data_url: string | Blob | File;
  /**
   * Trigger phrase to be used in the captions. If None, a trigger word will not be used.
   * If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions. Default value: `"ohwx"`
   */
  trigger_phrase?: string;
  /**
   * Number of steps to train the LoRA on. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate for the training. Default value: `0.00115`
   */
  learning_rate?: number;
  /**
   * Training style to use. Default value: `"subject"`
   */
  training_style?: "subject" | "style";
  /**
   * Whether to try to detect the face and crop the images to the face. Default value: `true`
   */
  face_crop?: boolean;
};
export type TurboFluxTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the trained diffusers config file.
   */
  config_file: File;
};
export type TurboImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type TurboTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ultrashapeInput = {
  /**
   * URL of the reference image for mesh refinement.
   */
  image_url: string | Blob | File;
  /**
   * URL of the coarse mesh (.glb or .obj) to refine.
   */
  model_url: string | Blob | File;
  /**
   * Diffusion steps. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * Marching cubes resolution. Default value: `1024`
   */
  octree_resolution?: number;
  /**
   * Random seed. Default value: `42`
   */
  seed?: number;
  /**
   * Remove image background. Default value: `true`
   */
  remove_background?: boolean;
};
export type ultrashapeOutput = {
  /**
   * Generated 3D object.
   */
  model_glb: File;
};
export type unoInput = {
  /**
   * URL of images to use while generating the image.
   */
  input_image_urls: Array<string>;
  /**
   * The size of the generated image. You can choose between some presets or custom height and width
   * that **must be multiples of 8**. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Random seed for reproducible generation. If set none, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, the function will wait for the image to be generated and uploaded
   * before returning the response. This will increase the latency of the function but
   * it allows you to get the image directly in the response without going through the CDN.
   */
  sync_mode?: boolean;
};
export type unoOutput = {
  /**
   * The URLs of the generated images.
   */
  images: Array<Image>;
  /**
   *
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used to generate the image.
   */
  prompt: string;
};
export type UpscaleImageInput = {
  /**
   * The image URL to upscale
   */
  image_url: string | Blob | File;
  /**
   * The prompt to upscale the image with Default value: `""`
   */
  prompt?: string;
  /**
   * The resemblance of the upscaled image to the original image Default value: `50`
   */
  resemblance?: number;
  /**
   * The detail of the upscaled image Default value: `50`
   */
  detail?: number;
  /**
   * Whether to expand the prompt with MagicPrompt functionality.
   */
  expand_prompt?: boolean;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type UpscaleInput = {
  /**
   * The URL of the image to be upscaled. Must be in PNG format.
   */
  image_url: string | Blob | File;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};
export type UpscaleOutput = {
  /**
   *
   */
  images: Array<File>;
  /**
   * Seed used for the random number generator
   */
  seed: number;
};
export type usoInput = {
  /**
   * Text prompt for generation. Can be empty for pure style transfer. Default value: `""`
   */
  prompt?: string;
  /**
   * List of image URLs in order: [content_image, style_image, extra_style_image].
   */
  input_image_urls: Array<string>;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * What you don't want in the image. Use it to exclude unwanted elements, styles, or artifacts. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Number of denoising steps. More steps can improve quality but increase generation time. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * How closely to follow the prompt. Higher values stick closer to the prompt. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * Preserve the layout and dimensions of the input content image. Useful for style transfer.
   */
  keep_size?: boolean;
  /**
   * Number of images to generate in parallel. Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducible generation. Use same seed for consistent results.
   */
  seed?: number;
  /**
   * If true, wait for generation and upload before returning. Increases latency but provides immediate access to images.
   */
  sync_mode?: boolean;
  /**
   * Enable NSFW content detection and filtering. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Output image format. PNG preserves transparency, JPEG is smaller. Default value: `"png"`
   */
  output_format?: "jpeg" | "png";
};
export type usoOutput = {
  /**
   * The generated images with applied style and/or subject customization
   */
  images: Array<Image>;
  /**
   * Seed used for generation
   */
  seed: number;
  /**
   * NSFW detection results for each generated image
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generation
   */
  prompt: string;
  /**
   * Performance timings for different stages
   */
  timings: any;
};
export type V16Input = {
  /**
   * URL or base64 of the model image
   */
  model_image: string;
  /**
   * URL or base64 of the garment image
   */
  garment_image: string;
  /**
   * Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment. Default value: `"auto"`
   */
  category?: "tops" | "bottoms" | "one-pieces" | "auto";
  /**
   * Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results. Default value: `"balanced"`
   */
  mode?: "performance" | "balanced" | "quality";
  /**
   * Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type. Default value: `"auto"`
   */
  garment_photo_type?: "auto" | "model" | "flat-lay";
  /**
   * Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear. Default value: `"permissive"`
   */
  moderation_level?: "none" | "permissive" | "conservative";
  /**
   * Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results.
   */
  seed?: number;
  /**
   * Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result. Default value: `1`
   */
  num_samples?: number;
  /**
   * Disables human parsing on the model image. Default value: `true`
   */
  segmentation_free?: boolean;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster Default value: `"png"`
   */
  output_format?: "png" | "jpeg";
};
export type V16Output = {
  /**
   *
   */
  images: Array<File>;
};
export type V26ImageToImageInput = {
  /**
   * Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters. Example: 'Generate an image using the style of image 1 and background of image 2'.
   */
  prompt: string;
  /**
   * Reference images for editing (1-3 images required). Order matters: reference as 'image 1', 'image 2', 'image 3' in prompt. Resolution: 384-5000px each dimension. Max size: 10MB each. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP.
   */
  image_urls: Array<string>;
  /**
   * Content to avoid in the generated image. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Output image size. Use presets like 'square_hd', 'landscape_16_9', 'portrait_9_16', or specify exact dimensions with ImageSize(width=1280, height=720). Total pixels must be between 768*768 and 1280*1280. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Number of images to generate (1-4). Directly affects billing cost. Default value: `1`
   */
  num_images?: number;
  /**
   * Enable LLM prompt optimization. Significantly improves results for simple prompts but adds 3-4 seconds processing time. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Random seed for reproducibility (0-2147483647). Same seed produces more consistent results.
   */
  seed?: number;
  /**
   * Enable content moderation for input and output. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type V26ImageToImageOutput = {
  /**
   * Generated images in PNG format
   */
  images: Array<File>;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type V26ImageToVideoFlashInput = {
  /**
   * The text prompt describing the desired video motion. Max 800 characters.
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680.
   */
  image_url: string | Blob | File;
  /**
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds),
   * the audio is truncated to the first N seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   */
  audio_url?: string | Blob | File;
  /**
   * Video resolution. Valid values: 720p, 1080p Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds. Default value: `"5"`
   */
  duration?: "5" | "10" | "15";
  /**
   * Negative prompt to describe content to avoid. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Whether to enable prompt rewriting using LLM. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.
   */
  multi_shots?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type V26ImageToVideoFlashOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
};
export type V26ImageToVideoInput = {
  /**
   * The text prompt describing the desired video motion. Max 800 characters.
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680.
   */
  image_url: string | Blob | File;
  /**
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds),
   * the audio is truncated to the first N seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   */
  audio_url?: string | Blob | File;
  /**
   * Video resolution. Valid values: 720p, 1080p Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds. Default value: `"5"`
   */
  duration?: "5" | "10" | "15";
  /**
   * Negative prompt to describe content to avoid. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Whether to enable prompt rewriting using LLM. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.
   */
  multi_shots?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type V26ImageToVideoOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
};
export type V26ReferenceToVideoFlashInput = {
  /**
   * Use Character1, Character2, etc. to reference subjects from your reference files. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 1500 characters. Reference order: video_urls first, then image_urls.
   */
  prompt: string;
  /**
   * Reference videos for subject consistency (0-3 videos). Videos' FPS must be at least 16 FPS. Combined with image_urls, total references cannot exceed 5. Reference order: video_urls are numbered first (Character1, Character2...), then image_urls continue the sequence.
   */
  video_urls?: Array<string>;
  /**
   * Reference images for subject consistency (0-5 images). Combined with video_urls, total references cannot exceed 5. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP. Resolution: 240-5000px. Max 10MB each. Reference order: image_urls continue numbering after video_urls.
   */
  image_urls?: Array<string>;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:3" | "3:4";
  /**
   * Video resolution tier. R2V Flash only supports 720p and 1080p. Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration of the generated video in seconds. R2V Flash supports only 5 or 10 seconds. Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * Negative prompt to describe content to avoid. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Whether to enable prompt rewriting using LLM. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True. Default value: `true`
   */
  multi_shots?: boolean;
  /**
   * Whether to generate a video with audio. Set to false for silent video generation. Silent videos are faster and cost 25% of the audio version price. Default value: `true`
   */
  enable_audio?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type V26ReferenceToVideoFlashOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
};
export type V26ReferenceToVideoInput = {
  /**
   * Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters.
   */
  prompt: string;
  /**
   * Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects.
   */
  video_urls: Array<string>;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:3" | "3:4";
  /**
   * Video resolution tier. R2V only supports 720p and 1080p (no 480p). Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s). Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * Negative prompt to describe content to avoid. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Whether to enable prompt rewriting using LLM. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True. Default value: `true`
   */
  multi_shots?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type V26ReferenceToVideoOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
};
export type V26TextToImageInput = {
  /**
   * Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters.
   */
  prompt: string;
  /**
   * Optional reference image (0 or 1). When provided, can be used for style guidance. Resolution: 384-5000px each dimension. Max size: 10MB. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP.
   */
  image_url?: string | Blob | File;
  /**
   * Content to avoid in the generated image. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Output image size. If not set: matches input image size (up to 1280*1280). Use presets like 'square_hd', 'landscape_16_9', or specify exact dimensions.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Maximum number of images to generate (1-5). Actual count may be less depending on model inference. Default value: `1`
   */
  max_images?: number;
  /**
   * Random seed for reproducibility (0-2147483647).
   */
  seed?: number;
  /**
   * Enable content moderation for input and output. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type V26TextToImageOutput = {
  /**
   * Generated images in PNG format
   */
  images: Array<File>;
  /**
   * Generated text content (in mixed text-and-image mode). May be None if only images were generated.
   */
  generated_text?: string;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type V26TextToVideoInput = {
  /**
   * The text prompt for video generation. Supports Chinese and English, max 800 characters. For multi-shot videos, use format: 'Overall description. First shot [0-3s] content. Second shot [3-5s] content.'
   */
  prompt: string;
  /**
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds),
   * the audio is truncated to the first N seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   */
  audio_url?: string | Blob | File;
  /**
   * The aspect ratio of the generated video. Wan 2.6 supports additional ratios. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:3" | "3:4";
  /**
   * Video resolution tier. Wan 2.6 T2V only supports 720p and 1080p (no 480p). Default value: `"1080p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration of the generated video in seconds. Choose between 5, 10, or 15 seconds. Default value: `"5"`
   */
  duration?: "5" | "10" | "15";
  /**
   * Negative prompt to describe content to avoid. Max 500 characters. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * When true, enables intelligent multi-shot segmentation for coherent narrative videos. Only active when enable_prompt_expansion is True. Set to false for single-shot generation. Default value: `true`
   */
  multi_shots?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type V26TextToVideoOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
};
export type V2ExtendInput = {
  /**
   * A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics.
   */
  prompt?: string;
  /**
   * Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.
   */
  tags?: Array<string>;
  /**
   * The lyrics sung in the generated song. An empty string will generate an instrumental track.
   */
  lyrics_prompt?: string;
  /**
   * The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.
   */
  seed?: number;
  /**
   * Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.) Default value: `1.8`
   */
  prompt_strength?: number;
  /**
   * Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7. Default value: `0.7`
   */
  balance_strength?: number;
  /**
   * Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed. Default value: `1`
   */
  num_songs?: number;
  /**
   *  Default value: `"wav"`
   */
  output_format?: "flac" | "mp3" | "wav" | "ogg" | "m4a";
  /**
   * The bit rate to use for mp3 and m4a formats. Not available for other formats.
   */
  output_bit_rate?: "128" | "192" | "256" | "320";
  /**
   * The URL of the audio file to alter. Must be a valid publicly accessible URL.
   */
  audio_url: string | Blob | File;
  /**
   * Add more to the beginning (left) or end (right) of the song
   */
  side: "left" | "right";
  /**
   * Duration in seconds to extend the song. If not provided, will attempt to automatically determine.
   */
  extend_duration?: number;
  /**
   * Duration in seconds to crop from the selected side before extending from that side.
   */
  crop_duration?: number;
};
export type V2ExtendOutput = {
  /**
   * The seed used for generation. This can be used to generate an identical song by passing the same parameters with this seed in a future request.
   */
  seed: number;
  /**
   * The style tags used for generation.
   */
  tags?: Array<string>;
  /**
   * The lyrics used for generation.
   */
  lyrics?: string;
  /**
   * The generated audio files.
   */
  audio: Array<File>;
  /**
   * The duration in seconds that the song was extended by.
   */
  extend_duration: number;
};
export type V2InpaintInput = {
  /**
   * Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.
   */
  tags?: Array<string>;
  /**
   * The lyrics sung in the generated song. An empty string will generate an instrumental track.
   */
  lyrics_prompt: string;
  /**
   * The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.
   */
  seed?: number;
  /**
   * Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.) Default value: `2`
   */
  prompt_strength?: number;
  /**
   * Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7. Default value: `0.7`
   */
  balance_strength?: number;
  /**
   * Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed. Default value: `1`
   */
  num_songs?: number;
  /**
   *  Default value: `"wav"`
   */
  output_format?: "flac" | "mp3" | "wav" | "ogg" | "m4a";
  /**
   * The bit rate to use for mp3 and m4a formats. Not available for other formats.
   */
  output_bit_rate?: "128" | "192" | "256" | "320";
  /**
   * The URL of the audio file to alter. Must be a valid publicly accessible URL.
   */
  audio_url: string | Blob | File;
  /**
   * List of sections to inpaint. Currently, only one section is supported so the list length must be 1.
   */
  sections: Array<InpaintSection>;
  /**
   * Crop to the selected region
   */
  selection_crop?: boolean;
};
export type V2InpaintOutput = {
  /**
   * The generated audio files.
   */
  audio: Array<File>;
  /**
   * The seed used for generation. This can be used to generate an identical song by passing the same parameters with this seed in a future request.
   */
  seed: number;
};
export type V2TextToMusicInput = {
  /**
   * A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics.
   */
  prompt?: string;
  /**
   * Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.
   */
  tags?: Array<string>;
  /**
   * The lyrics sung in the generated song. An empty string will generate an instrumental track.
   */
  lyrics_prompt?: string;
  /**
   * The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.
   */
  seed?: number;
  /**
   * Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.) Default value: `2`
   */
  prompt_strength?: number;
  /**
   * Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7. Default value: `0.7`
   */
  balance_strength?: number;
  /**
   * Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed. Default value: `1`
   */
  num_songs?: number;
  /**
   *  Default value: `"wav"`
   */
  output_format?: "flac" | "mp3" | "wav" | "ogg" | "m4a";
  /**
   * The bit rate to use for mp3 and m4a formats. Not available for other formats.
   */
  output_bit_rate?: "128" | "192" | "256" | "320";
  /**
   * The beats per minute of the song. This can be set to an integer or the literal string "auto" to pick a suitable bpm based on the tags. Set bpm to null to not condition the model on bpm information. Default value: `auto`
   */
  bpm?: number | string;
};
export type V2TextToMusicOutput = {
  /**
   * The seed used for generation. This can be used to generate an identical song by passing the same parameters with this seed in a future request.
   */
  seed: number;
  /**
   * The style tags used for generation.
   */
  tags?: Array<string>;
  /**
   * The lyrics used for generation.
   */
  lyrics?: string;
  /**
   * The generated audio files.
   */
  audio: Array<File>;
};
export type VectorizeInput = {
  /**
   * The URL of the image to be vectorized. Must be in PNG, JPG or WEBP format, less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels, min dimension more than 256 pixels.
   */
  image_url: string | Blob | File;
};
export type VectorizeOutput = {
  /**
   * The vectorized image.
   */
  image: File;
};
export type Veo2ImageToVideoInput = {
  /**
   * The text prompt describing how the image should be animated
   */
  prompt: string;
  /**
   * URL of the input image to animate. Should be 720p or higher resolution.
   */
  image_url: string | Blob | File;
  /**
   * The aspect ratio of the generated video Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "auto_prefer_portrait" | "16:9" | "9:16";
  /**
   * The duration of the generated video in seconds Default value: `"5s"`
   */
  duration?: "5s" | "6s" | "7s" | "8s";
};
export type Veo2ImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type veo2Input = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The duration of the generated video in seconds Default value: `"5s"`
   */
  duration?: "5s" | "6s" | "7s" | "8s";
  /**
   * A negative prompt to guide the video generation
   */
  negative_prompt?: string;
  /**
   * Whether to enhance the video generation Default value: `true`
   */
  enhance_prompt?: boolean;
  /**
   * A seed to use for the video generation
   */
  seed?: number;
};
export type veo2Output = {
  /**
   * The generated video
   */
  video: File;
};
export type Veo31ExtendVideoInput = {
  /**
   * The text prompt describing how the video should be extended
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"7s"`
   */
  duration?: "7s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
   */
  video_url: string | Blob | File;
};
export type Veo31ExtendVideoOutput = {
  /**
   * The extended video.
   */
  video: File;
};
export type Veo31FastExtendVideoInput = {
  /**
   * The text prompt describing how the video should be extended
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"7s"`
   */
  duration?: "7s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
   */
  video_url: string | Blob | File;
};
export type Veo31FastExtendVideoOutput = {
  /**
   * The extended video.
   */
  video: File;
};
export type Veo31FastFirstLastFrameToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URL of the first frame of the video
   */
  first_frame_url: string | Blob | File;
  /**
   * URL of the last frame of the video
   */
  last_frame_url: string | Blob | File;
};
export type Veo31FastFirstLastFrameToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo31FastImageToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Only 16:9 and 9:16 are supported. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
   */
  image_url: string | Blob | File;
};
export type Veo31FastImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo31FastInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them. Default value: `true`
   */
  auto_fix?: boolean;
};
export type Veo31FastOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo31FirstLastFrameToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URL of the first frame of the video
   */
  first_frame_url: string | Blob | File;
  /**
   * URL of the last frame of the video
   */
  last_frame_url: string | Blob | File;
};
export type Veo31FirstLastFrameToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo31ImageToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Only 16:9 and 9:16 are supported. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
   */
  image_url: string | Blob | File;
};
export type Veo31ImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo31Input = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them. Default value: `true`
   */
  auto_fix?: boolean;
};
export type Veo31Output = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo31ReferenceToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "8s";
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URLs of the reference images to use for consistent subject appearance
   */
  image_urls: Array<string>;
};
export type Veo31ReferenceToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo31TextToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them. Default value: `true`
   */
  auto_fix?: boolean;
};
export type Veo31TextToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo31VideoToVideoInput = {
  /**
   * The text prompt describing how the video should be extended
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"7s"`
   */
  duration?: "7s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
   */
  video_url: string | Blob | File;
};
export type Veo31VideoToVideoOutput = {
  /**
   * The extended video.
   */
  video: File;
};
export type Veo3FastImageToVideoInput = {
  /**
   * The text prompt describing how the image should be animated
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
   */
  image_url: string | Blob | File;
};
export type Veo3FastImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo3FastInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them. Default value: `true`
   */
  auto_fix?: boolean;
};
export type Veo3FastOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo3ImageToVideoInput = {
  /**
   * The text prompt describing how the image should be animated
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
   */
  image_url: string | Blob | File;
};
export type Veo3ImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type veo3Input = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them. Default value: `true`
   */
  auto_fix?: boolean;
};
export type veo3Output = {
  /**
   * The generated video.
   */
  video: File;
};
export type Veo3TextToVideoInput = {
  /**
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * The duration of the generated video. Default value: `"8s"`
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
  /**
   * The resolution of the generated video. Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * Whether to generate audio for the video. Default value: `true`
   */
  generate_audio?: boolean;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them. Default value: `true`
   */
  auto_fix?: boolean;
};
export type Veo3TextToVideoOutput = {
  /**
   * The generated video.
   */
  video: File;
};
export type Vibevoice05bInput = {
  /**
   * The script to convert to speech.
   */
  script: string;
  /**
   * Voice to use for speaking.
   */
  speaker: "Frank" | "Wayne" | "Carter" | "Emma" | "Grace" | "Mike";
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text. Default value: `1.3`
   */
  cfg_scale?: number;
};
export type Vibevoice05bOutput = {
  /**
   * The generated audio file containing the speech
   */
  audio: File;
  /**
   * Duration of the generated audio in seconds
   */
  duration: number;
  /**
   * Sample rate of the generated audio
   */
  sample_rate: number;
  /**
   * Time taken to generate the audio in seconds
   */
  generation_time: number;
  /**
   * Real-time factor (generation_time / audio_duration). Lower is better.
   */
  rtf: number;
};
export type Vibevoice7bInput = {
  /**
   * The script to convert to speech. Can be formatted with 'Speaker X:' prefixes for multi-speaker dialogues.
   */
  script: string;
  /**
   * List of speakers to use for the script. If not provided, will be inferred from the script or voice samples.
   */
  speakers: Array<VibeVoiceSpeaker>;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text. Default value: `1.3`
   */
  cfg_scale?: number;
};
export type VibeVoice7bInput = {
  /**
   * The script to convert to speech. Can be formatted with 'Speaker X:' prefixes for multi-speaker dialogues.
   */
  script: string;
  /**
   * List of speakers to use for the script. If not provided, will be inferred from the script or voice samples.
   */
  speakers: Array<VibeVoiceSpeaker>;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text. Default value: `1.3`
   */
  cfg_scale?: number;
};
export type Vibevoice7bOutput = {
  /**
   * The generated audio file containing the speech
   */
  audio: File;
  /**
   * Duration of the generated audio in seconds
   */
  duration: number;
  /**
   * Sample rate of the generated audio
   */
  sample_rate: number;
  /**
   * Time taken to generate the audio in seconds
   */
  generation_time: number;
  /**
   * Real-time factor (generation_time / audio_duration). Lower is better.
   */
  rtf: number;
};
export type vibevoiceInput = {
  /**
   * The script to convert to speech. Can be formatted with 'Speaker X:' prefixes for multi-speaker dialogues.
   */
  script: string;
  /**
   * List of speakers to use for the script. If not provided, will be inferred from the script or voice samples.
   */
  speakers: Array<VibeVoiceSpeaker>;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text. Default value: `1.3`
   */
  cfg_scale?: number;
};
export type VibeVoiceInput = {
  /**
   * The script to convert to speech. Can be formatted with 'Speaker X:' prefixes for multi-speaker dialogues.
   */
  script: string;
  /**
   * List of speakers to use for the script. If not provided, will be inferred from the script or voice samples.
   */
  speakers: Array<VibeVoiceSpeaker>;
  /**
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text. Default value: `1.3`
   */
  cfg_scale?: number;
};
export type vibevoiceOutput = {
  /**
   * The generated audio file containing the speech
   */
  audio: File;
  /**
   * Duration of the generated audio in seconds
   */
  duration: number;
  /**
   * Sample rate of the generated audio
   */
  sample_rate: number;
  /**
   * Time taken to generate the audio in seconds
   */
  generation_time: number;
  /**
   * Real-time factor (generation_time / audio_duration). Lower is better.
   */
  rtf: number;
};
export type VibeVoiceOutput = {
  /**
   * The generated audio file containing the speech
   */
  audio: File;
  /**
   * Duration of the generated audio in seconds
   */
  duration: number;
  /**
   * Sample rate of the generated audio
   */
  sample_rate: number;
  /**
   * Time taken to generate the audio in seconds
   */
  generation_time: number;
  /**
   * Real-time factor (generation_time / audio_duration). Lower is better.
   */
  rtf: number;
};
export type Video2PixelInput = {
  /**
   * The video URL to process into improved pixel art
   */
  video_url: string | Blob | File;
  /**
   * Maximum number of colors in the output palette. Set None to disable limit. Default value: `32`
   */
  max_colors?: number;
  /**
   * Enable automatic detection of optimal number of colors.
   */
  auto_color_detect?: boolean;
  /**
   * Optional fixed color palette as hex strings (e.g., ['#000000', '#ffffff']).
   */
  fixed_palette?: Array<string>;
  /**
   * Scale detection method to use. Default value: `"auto"`
   */
  detect_method?: "auto" | "runs" | "edge";
  /**
   * Force a specific pixel scale. If None, auto-detect.
   */
  scale?: number;
  /**
   * Downscaling method to produce the pixel-art output. Default value: `"dominant"`
   */
  downscale_method?:
    | "dominant"
    | "median"
    | "mode"
    | "mean"
    | "content-adaptive";
  /**
   * Trim borders of the image.
   */
  trim_borders?: boolean;
  /**
   * Remove background of the image. This will check for contiguous color regions from the edges after correction and make them transparent.
   */
  transparent_background?: boolean;
  /**
   * Apply morphological operations to remove noise.
   */
  cleanup_morph?: boolean;
  /**
   * Remove isolated diagonal pixels (jaggy edge cleanup).
   */
  cleanup_jaggy?: boolean;
  /**
   * Align output to the pixel grid. Default value: `true`
   */
  snap_grid?: boolean;
  /**
   * Alpha binarization threshold (0-255). Default value: `128`
   */
  alpha_threshold?: number;
  /**
   * Dominant color threshold (0.0-1.0). Default value: `0.05`
   */
  dominant_color_threshold?: number;
  /**
   * Background tolerance (0-255).
   */
  background_tolerance?: number;
  /**
   * Controls where to flood-fill from when removing the background. Default value: `"corners"`
   */
  background_mode?: "edges" | "corners" | "midpoints";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Resample the input video to a different FPS.
   */
  resample_fps?: boolean;
  /**
   * The target FPS to resample the input video to. Only relevant if `resample_fps` is True. Default value: `24`
   */
  target_fps?: number;
  /**
   * The type of output video to generate. Default value: `"X264 (.mp4)"`
   */
  video_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the output video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Whether to loop the output video. Only applicable if output_type is 'gif'.
   */
  video_loop?: boolean;
};
export type Video2PixelOutput = {
  /**
   * The detected pixel scale of the input.
   */
  pixel_scale: number;
  /**
   * The palette of the processed media.
   */
  palette: Array<string>;
  /**
   * The number of colors in the processed media.
   */
  num_colors: number;
  /**
   * The processed pixel-art videos.
   */
  videos: Array<VideoFile>;
};
export type VideoAsPromptInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * reference video to generate effect video from.
   */
  video_url: string | Blob | File;
  /**
   * Input image to generate the effect video for.
   */
  image_url: string | Blob | File;
  /**
   * Random seed for reproducible generation. If set none, a random seed will be used.
   */
  seed?: number;
  /**
   * The number of frames to generate. Default value: `49`
   */
  num_frames?: number;
  /**
   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `16`
   */
  fps?: number;
  /**
   * A brief description of the input video content.
   */
  video_description: string;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Aspect ratio of the generated video. Default value: `"9:16"`
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution of the generated video. Default value: `"480p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Guidance scale for generation. Default value: `5`
   */
  guidance_scale?: number;
};
export type VideoAsPromptOutput = {
  /**
   * The URLs of the generated video.
   */
  video: File;
};
export type VideoBackgroundRemovalFastInput = {
  /**
   *
   */
  video_url: string | Blob | File;
  /**
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality. Default value: `"vp9"`
   */
  output_codec?: "vp9" | "h264";
  /**
   * Improves the quality of the extracted object's edges. Default value: `true`
   */
  refine_foreground_edges?: boolean;
  /**
   * Set to False if the subject is not a person. Default value: `true`
   */
  subject_is_person?: boolean;
};
export type VideoBackgroundRemovalFastOutput = {
  /**
   *
   */
  video: Array<File>;
};
export type VideoBackgroundRemovalGreenScreenInput = {
  /**
   *
   */
  video_url: string | Blob | File;
  /**
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality. Default value: `"vp9"`
   */
  output_codec?: "vp9" | "h264";
  /**
   * Increase the value if green spots remain in the video, decrease if color changes are noticed on the extracted subject. Default value: `0.8`
   */
  spill_suppression_strength?: number;
};
export type VideoBackgroundRemovalGreenScreenOutput = {
  /**
   *
   */
  video: Array<File>;
};
export type VideoBackgroundRemovalInput = {
  /**
   *
   */
  video_url: string | Blob | File;
  /**
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality. Default value: `"vp9"`
   */
  output_codec?: "vp9" | "h264";
  /**
   * Improves the quality of the extracted object's edges. Default value: `true`
   */
  refine_foreground_edges?: boolean;
  /**
   * Set to False if the subject is not a person. Default value: `true`
   */
  subject_is_person?: boolean;
};
export type VideoBackgroundRemovalOutput = {
  /**
   *
   */
  video: Array<File>;
};
export type VideoChatOutput = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Dictionary of label: mask video
   */
  masks: Array<File>;
};
export type VideoConditioningInput = {
  /**
   * URL of video to be extended
   */
  video_url: string | Blob | File;
  /**
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num: number;
};
export type VideoEffectInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * reference video to generate effect video from.
   */
  video_url: string | Blob | File;
  /**
   * Input image to generate the effect video for.
   */
  image_url: string | Blob | File;
  /**
   * Random seed for reproducible generation. If set none, a random seed will be used.
   */
  seed?: number;
  /**
   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `16`
   */
  fps?: number;
  /**
   * A brief description of the input video content.
   */
  video_description: string;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type VideoEffectsOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type VideoEnterpriseInput = {
  /**
   * List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
   */
  video_urls?: Array<string>;
  /**
   * Prompt to be used for the video processing
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type VideoEraseKeypointsInput = {
  /**
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4. Default value: `"mp4_h264"`
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * auto trim the video, to working duration ( 5s ) Default value: `true`
   */
  auto_trim?: boolean;
  /**
   * If true, audio will be preserved in the output video. Default value: `true`
   */
  preserve_audio?: boolean;
  /**
   * Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}
   */
  keypoints: Array<string>;
  /**
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string | Blob | File;
};
export type VideoEraseKeypointsOutput = {
  /**
   * Final video.
   */
  video: Video | File;
};
export type VideoEraseMaskInput = {
  /**
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4. Default value: `"mp4_h264"`
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * auto trim the video, to working duration ( 5s ) Default value: `true`
   */
  auto_trim?: boolean;
  /**
   * If true, audio will be preserved in the output video. Default value: `true`
   */
  preserve_audio?: boolean;
  /**
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string | Blob | File;
  /**
   * Input video to mask erase object from. duration must be less than 5s.
   */
  mask_video_url: string | Blob | File;
};
export type VideoEraseMaskOutput = {
  /**
   * Final video.
   */
  video: Video | File;
};
export type VideoErasePromptInput = {
  /**
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4. Default value: `"mp4_h264"`
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * auto trim the video, to working duration ( 5s ) Default value: `true`
   */
  auto_trim?: boolean;
  /**
   * If true, audio will be preserved in the output video. Default value: `true`
   */
  preserve_audio?: boolean;
  /**
   * Input prompt to detect object to erase
   */
  prompt: string;
  /**
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string | Blob | File;
};
export type VideoErasePromptOutput = {
  /**
   * Final video.
   */
  video: Video | File;
};
export type VideoIncreaseResolutionInput = {
  /**
   * Input video to increase resolution. Size should be less than 14142x14142 and duration less than 30s.
   */
  video_url: string | Blob | File;
  /**
   * desired_increase factor. Options: 2x, 4x. Default value: `"2"`
   */
  desired_increase?: "2" | "4";
  /**
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif. Default value: `"webm_vp9"`
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h265"
    | "mkv_h264"
    | "mkv_vp9"
    | "gif";
};
export type VideoIncreaseResolutionOutput = {
  /**
   * Video with removed background and audio.
   */
  video: Video | File;
};
export type VideoInput = {
  /**
   * Prompt to be used for the chat completion
   */
  prompt: string;
  /**
   * The URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * Number of frames to sample from the video. If not provided, all frames are sampled.
   */
  num_frames_to_sample?: number;
};
export type VideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type VideoPromptGeneratorInput = {
  /**
   * Core concept or thematic input for the video prompt
   */
  input_concept: string;
  /**
   * Style of the video prompt Default value: `"Simple"`
   */
  style?:
    | "Minimalist"
    | "Simple"
    | "Detailed"
    | "Descriptive"
    | "Dynamic"
    | "Cinematic"
    | "Documentary"
    | "Animation"
    | "Action"
    | "Experimental";
  /**
   * Camera movement style Default value: `"None"`
   */
  camera_style?:
    | "None"
    | "Steadicam flow"
    | "Drone aerials"
    | "Handheld urgency"
    | "Crane elegance"
    | "Dolly precision"
    | "VR 360"
    | "Multi-angle rig"
    | "Static tripod"
    | "Gimbal smoothness"
    | "Slider motion"
    | "Jib sweep"
    | "POV immersion"
    | "Time-slice array"
    | "Macro extreme"
    | "Tilt-shift miniature"
    | "Snorricam character"
    | "Whip pan dynamics"
    | "Dutch angle tension"
    | "Underwater housing"
    | "Periscope lens";
  /**
   * Camera direction Default value: `"None"`
   */
  camera_direction?:
    | "None"
    | "Zoom in"
    | "Zoom out"
    | "Pan left"
    | "Pan right"
    | "Tilt up"
    | "Tilt down"
    | "Orbital rotation"
    | "Push in"
    | "Pull out"
    | "Track forward"
    | "Track backward"
    | "Spiral in"
    | "Spiral out"
    | "Arc movement"
    | "Diagonal traverse"
    | "Vertical rise"
    | "Vertical descent";
  /**
   * Pacing rhythm Default value: `"None"`
   */
  pacing?:
    | "None"
    | "Slow burn"
    | "Rhythmic pulse"
    | "Frantic energy"
    | "Ebb and flow"
    | "Hypnotic drift"
    | "Time-lapse rush"
    | "Stop-motion staccato"
    | "Gradual build"
    | "Quick cut rhythm"
    | "Long take meditation"
    | "Jump cut energy"
    | "Match cut flow"
    | "Cross-dissolve dreamscape"
    | "Parallel action"
    | "Slow motion impact"
    | "Ramping dynamics"
    | "Montage tempo"
    | "Continuous flow"
    | "Episodic breaks";
  /**
   * Special effects approach Default value: `"None"`
   */
  special_effects?:
    | "None"
    | "Practical effects"
    | "CGI enhancement"
    | "Analog glitches"
    | "Light painting"
    | "Projection mapping"
    | "Nanosecond exposures"
    | "Double exposure"
    | "Smoke diffusion"
    | "Lens flare artistry"
    | "Particle systems"
    | "Holographic overlay"
    | "Chromatic aberration"
    | "Digital distortion"
    | "Wire removal"
    | "Motion capture"
    | "Miniature integration"
    | "Weather simulation"
    | "Color grading"
    | "Mixed media composite"
    | "Neural style transfer";
  /**
   * Custom technical elements (optional) Default value: `""`
   */
  custom_elements?: string;
  /**
   * URL of an image to analyze and incorporate into the video prompt (optional)
   */
  image_url?: string | Blob | File;
  /**
   * Model to use Default value: `"google/gemini-2.0-flash-001"`
   */
  model?:
    | "anthropic/claude-3.5-sonnet"
    | "anthropic/claude-3-5-haiku"
    | "anthropic/claude-3-haiku"
    | "google/gemini-2.5-flash-lite"
    | "google/gemini-2.0-flash-001"
    | "meta-llama/llama-3.2-1b-instruct"
    | "meta-llama/llama-3.2-3b-instruct"
    | "meta-llama/llama-3.1-8b-instruct"
    | "meta-llama/llama-3.1-70b-instruct"
    | "openai/gpt-4o-mini"
    | "openai/gpt-4o"
    | "deepseek/deepseek-r1";
  /**
   * Length of the prompt Default value: `"Medium"`
   */
  prompt_length?: "Short" | "Medium" | "Long";
};
export type VideoPromptGeneratorOutput = {
  /**
   * Generated video prompt
   */
  prompt: string;
};
export type VideoSoundEffectsGeneratorInput = {
  /**
   * A video file to analyze & re-sound with generated SFX.
   */
  video_url: Video;
};
export type VideoSoundEffectsGeneratorOutput = {
  /**
   * The final video with the newly generated SFX track.
   */
  video: File;
};
export type VideoToAudioInput = {
  /**
   * The video URL to extract audio from. Only .mp4/.mov formats are supported. File size does not exceed 100MB. Video duration between 3.0s and 20.0s.
   */
  video_url: string | Blob | File;
  /**
   * Sound effect prompt. Cannot exceed 200 characters. Default value: `"Car tires screech as they accelerate in a drag race"`
   */
  sound_effect_prompt?: string;
  /**
   * Background music prompt. Cannot exceed 200 characters. Default value: `"intense car race"`
   */
  background_music_prompt?: string;
  /**
   * Enable ASMR mode. This mode enhances detailed sound effects and is suitable for highly immersive content scenarios.
   */
  asmr_mode?: boolean;
};
export type VideoToAudioOutput = {
  /**
   * The original video with dubbed audio applied
   */
  video: File;
  /**
   * The extracted/generated audio from the video in MP3 format
   */
  audio: File;
};
export type VideoToGifInput = {
  /**
   * URL of the video file to convert to GIF
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Start time in seconds
   */
  start_time?: number;
  /**
   * Duration in seconds (max 15s to avoid memory issues, if not set uses 10s from start_time)
   */
  duration?: number;
  /**
   * Output width in pixels (height auto-calculated, if not set defaults to max 800px to prevent large file sizes)
   */
  width?: number;
  /**
   * Frames per second for the GIF Default value: `10`
   */
  fps?: number;
  /**
   * Quality level (affects color palette) Default value: `"medium"`
   */
  quality?: "low" | "medium" | "high";
};
export type VideoToGifOutput = {
  /**
   * The generated GIF file
   */
  image: File;
};
export type VideoToVideoInput = {
  /**
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The negative prompt to generate video from Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The LoRAs to use for the image generation. We currently support one lora.
   */
  loras?: Array<LoraWeight>;
  /**
   * The number of inference steps to perform. Default value: `50`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   */
  seed?: number;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`
   */
  guidance_scale?: number;
  /**
   * Use RIFE for video interpolation Default value: `true`
   */
  use_rife?: boolean;
  /**
   * The target FPS of the video Default value: `16`
   */
  export_fps?: number;
  /**
   * The video to generate the video from.
   */
  video_url: string | Blob | File;
  /**
   * The strength to use for Video to Video.  1.0 completely remakes the video while 0.0 preserves the original. Default value: `0.8`
   */
  strength?: number;
};
export type VideoUnderstandingInput = {
  /**
   * URL of the video to analyze
   */
  video_url: string | Blob | File;
  /**
   * The question or prompt about the video content.
   */
  prompt: string;
  /**
   * Whether to request a more detailed analysis of the video
   */
  detailed_analysis?: boolean;
};
export type VideoUnderstandingOutput = {
  /**
   * The analysis of the video content based on the prompt
   */
  output: string;
};
export type VideoUpscaleOutput = {
  /**
   * The upscaled video file
   */
  video: File;
};
export type ViduImageToVideoInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};
export type ViduImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type ViduQ1ImageToVideoInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame
   */
  image_url: string | Blob | File;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};
export type ViduQ1ImageToVideoOutput = {
  /**
   * The generated video using the Q1 model from a single image
   */
  video: File;
};
export type ViduQ1ReferenceToVideoInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * URLs of the reference images to use for consistent subject appearance. Q1 model supports up to 7 reference images.
   */
  reference_image_urls: Array<string>;
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * Whether to add background music to the generated video
   */
  bgm?: boolean;
};
export type ViduQ1ReferenceToVideoOutput = {
  /**
   * The generated video with consistent subjects from reference images using the Q1 model
   */
  video: File;
};
export type ViduQ1StartEndToVideoInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame
   */
  start_image_url: string | Blob | File;
  /**
   * URL of the image to use as the last frame
   */
  end_image_url: string | Blob | File;
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};
export type ViduQ1StartEndToVideoOutput = {
  /**
   * The generated transition video between start and end frames using the Q1 model
   */
  video: File;
};
export type ViduQ1TextToVideoInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * The style of output video Default value: `"general"`
   */
  style?: "general" | "anime";
  /**
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * The aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};
export type ViduQ1TextToVideoOutput = {
  /**
   * The generated video using the Q1 model
   */
  video: File;
};
export type ViduQ2ImageToVideoProInput = {
  /**
   * Text prompt for video generation, max 3000 characters
   */
  prompt: string;
  /**
   * URL of the image to use as the starting frame
   */
  image_url: string | Blob | File;
  /**
   * URL of the image to use as the ending frame. When provided, generates a transition video between start and end frames.
   */
  end_image_url?: string | Blob | File;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Duration of the video in seconds Default value: `"4"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8";
  /**
   * Output video resolution Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * Whether to add background music to the video (only for 4-second videos)
   */
  bgm?: boolean;
};
export type ViduQ2ImageToVideoProOutput = {
  /**
   * The generated video from image using the Q2 model
   */
  video: File;
};
export type ViduQ2ImageToVideoTurboInput = {
  /**
   * Text prompt for video generation, max 3000 characters
   */
  prompt: string;
  /**
   * URL of the image to use as the starting frame
   */
  image_url: string | Blob | File;
  /**
   * URL of the image to use as the ending frame. When provided, generates a transition video between start and end frames.
   */
  end_image_url?: string | Blob | File;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Duration of the video in seconds Default value: `"4"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8";
  /**
   * Output video resolution Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * Whether to add background music to the video (only for 4-second videos)
   */
  bgm?: boolean;
};
export type ViduQ2ImageToVideoTurboOutput = {
  /**
   * The generated video from image using the Q2 model
   */
  video: File;
};
export type ViduQ2ReferenceToImageInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * URLs of the reference images to use for consistent subject appearance
   */
  reference_image_urls: Array<string>;
  /**
   * The aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Random seed for generation
   */
  seed?: number;
};
export type ViduQ2ReferenceToImageOutput = {
  /**
   * The edited image
   */
  image: Image;
};
export type ViduQ2ReferenceToVideoProInput = {
  /**
   * Text prompt for video generation, max 2000 characters
   */
  prompt: string;
  /**
   * URLs of the reference images for subject appearance. If videos are provided, up to 4 images are allowed; otherwise up to 7 images.
   */
  reference_image_urls?: Array<string>;
  /**
   * URLs of the reference videos for video editing or motion reference. Supports up to 2 videos.
   */
  reference_video_urls?: Array<string>;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Duration of the video in seconds (0 for automatic duration) Default value: `4`
   */
  duration?: number;
  /**
   * Output video resolution Default value: `"720p"`
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * Aspect ratio of the output video (e.g., auto, 16:9, 9:16, 1:1, or any W:H) Default value: `"16:9"`
   */
  aspect_ratio?: string;
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * Whether to add background music to the generated video
   */
  bgm?: boolean;
};
export type ViduQ2ReferenceToVideoProOutput = {
  /**
   * The generated video with video/image references using the Q2 Pro model
   */
  video: File;
};
export type ViduQ2TextToImageInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * The aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Random seed for generation
   */
  seed?: number;
};
export type ViduQ2TextToImageOutput = {
  /**
   * The edited image
   */
  image: Image;
};
export type ViduQ2TextToVideoInput = {
  /**
   * Text prompt for video generation, max 3000 characters
   */
  prompt: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Duration of the video in seconds Default value: `"4"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8";
  /**
   * Output video resolution Default value: `"720p"`
   */
  resolution?: "360p" | "520p" | "720p" | "1080p";
  /**
   * The aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * Whether to add background music to the video (only for 4-second videos)
   */
  bgm?: boolean;
};
export type ViduQ2TextToVideoOutput = {
  /**
   * The generated video from text using the Q2 model
   */
  video: File;
};
export type ViduQ2VideoExtensionProInput = {
  /**
   * URL of the video to extend
   */
  video_url: string | Blob | File;
  /**
   * text prompt to guide the video extension
   */
  prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Duration of the extension in seconds Default value: `"4"`
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7";
  /**
   * Output video resolution Default value: `"720p"`
   */
  resolution?: "720p" | "1080p";
};
export type ViduQ2VideoExtensionProOutput = {
  /**
   * The extended video using the Q2 model
   */
  video: File;
};
export type ViduQ3ImageToVideoInput = {
  /**
   * Text prompt for video generation, max 2000 characters
   */
  prompt: string;
  /**
   * URL or base64 image to use as the starting frame
   */
  image_url: string | Blob | File;
  /**
   * Duration of the video in seconds Default value: `5`
   */
  duration?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Output video resolution Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Whether to use direct audio-video generation. When true, outputs video with sound. Default value: `true`
   */
  audio?: boolean;
};
export type ViduQ3ImageToVideoOutput = {
  /**
   * The generated video from image using the Q3 model
   */
  video: File;
};
export type ViduQ3TextToVideoInput = {
  /**
   * Text prompt for video generation, max 2000 characters
   */
  prompt: string;
  /**
   * Duration of the video in seconds Default value: `5`
   */
  duration?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * The aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "1:1";
  /**
   * Output video resolution Default value: `"720p"`
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Whether to use direct audio-video generation. When true, outputs video with sound. Default value: `true`
   */
  audio?: boolean;
};
export type ViduQ3TextToVideoOutput = {
  /**
   * The generated video from text using the Q3 model
   */
  video: File;
};
export type ViduReferenceToImageInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * URLs of the reference images to use for consistent subject appearance
   */
  reference_image_urls: Array<string>;
  /**
   * The aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Random seed for generation
   */
  seed?: number;
};
export type ViduReferenceToImageOutput = {
  /**
   * The edited image
   */
  image: Image;
};
export type ViduReferenceToVideoInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * URLs of the reference images to use for consistent subject appearance
   */
  reference_image_urls: Array<string>;
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};
export type ViduReferenceToVideoOutput = {
  /**
   * The generated video with consistent subjects from reference images
   */
  video: File;
};
export type ViduStartEndToVideoInput = {
  /**
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame
   */
  start_image_url: string | Blob | File;
  /**
   * URL of the image to use as the last frame
   */
  end_image_url: string | Blob | File;
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The movement amplitude of objects in the frame Default value: `"auto"`
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};
export type ViduStartEndToVideoOutput = {
  /**
   * The generated transition video between start and end frames
   */
  video: File;
};
export type ViduTemplateToVideoInput = {
  /**
   * AI video template to use. Pricing varies by template: Standard templates (hug, kiss, love_pose, etc.) cost 4 credits ($0.20), Premium templates (lunar_newyear, dynasty_dress, dreamy_wedding, etc.) cost 6 credits ($0.30), and Advanced templates (live_photo) cost 10 credits ($0.50). Default value: `"hug"`
   */
  template?:
    | "dreamy_wedding"
    | "romantic_lift"
    | "sweet_proposal"
    | "couple_arrival"
    | "cupid_arrow"
    | "pet_lovers"
    | "lunar_newyear"
    | "hug"
    | "kiss"
    | "dynasty_dress"
    | "wish_sender"
    | "love_pose"
    | "hair_swap"
    | "youth_rewind"
    | "morphlab"
    | "live_photo"
    | "emotionlab"
    | "live_memory"
    | "interaction"
    | "christmas"
    | "pet_finger"
    | "eat_mushrooms"
    | "beast_chase_library"
    | "beast_chase_supermarket"
    | "petal_scattered"
    | "emoji_figure"
    | "hair_color_change"
    | "multiple_people_kissing"
    | "beast_chase_amazon"
    | "beast_chase_mountain"
    | "balloonman_explodes_pro"
    | "get_thinner"
    | "jump2pool"
    | "bodyshake"
    | "jiggle_up"
    | "shake_it_dance"
    | "subject_3"
    | "pubg_winner_hit"
    | "shake_it_down"
    | "blueprint_supreme"
    | "hip_twist"
    | "motor_dance"
    | "rat_dance"
    | "kwok_dance"
    | "leg_sweep_dance"
    | "heeseung_march"
    | "shake_to_max"
    | "dame_un_grrr"
    | "i_know"
    | "lit_bounce"
    | "wave_dance"
    | "chill_dance"
    | "hip_flicking"
    | "sakura_season"
    | "zongzi_wrap"
    | "zongzi_drop"
    | "dragonboat_shot"
    | "rain_kiss"
    | "child_memory"
    | "couple_drop"
    | "couple_walk"
    | "flower_receive"
    | "love_drop"
    | "cheek_kiss"
    | "carry_me"
    | "blow_kiss"
    | "love_fall"
    | "french_kiss_8s"
    | "workday_feels"
    | "love_story"
    | "bloom_magic"
    | "ghibli"
    | "minecraft"
    | "box_me"
    | "claw_me"
    | "clayshot"
    | "manga_meme"
    | "quad_meme"
    | "pixel_me"
    | "clayshot_duo"
    | "irasutoya"
    | "american_comic"
    | "simpsons_comic"
    | "yayoi_kusama_style"
    | "pop_art"
    | "jojo_style"
    | "slice_therapy"
    | "balloon_flyaway"
    | "flying"
    | "paperman"
    | "pinch"
    | "bloom_doorobear"
    | "gender_swap"
    | "nap_me"
    | "sexy_me"
    | "spin360"
    | "smooth_shift"
    | "paper_fall"
    | "jump_to_cloud"
    | "pilot"
    | "sweet_dreams"
    | "soul_depart"
    | "punch_hit"
    | "watermelon_hit"
    | "split_stance_pet"
    | "make_face"
    | "break_glass"
    | "split_stance_human"
    | "covered_liquid_metal"
    | "fluffy_plunge"
    | "pet_belly_dance"
    | "water_float"
    | "relax_cut"
    | "head_to_balloon"
    | "cloning"
    | "across_the_universe_jungle"
    | "clothes_spinning_remnant"
    | "across_the_universe_jurassic"
    | "across_the_universe_moon"
    | "fisheye_pet"
    | "hitchcock_zoom"
    | "cute_bangs"
    | "earth_zoom_out"
    | "fisheye_human"
    | "drive_yacht"
    | "virtual_singer"
    | "earth_zoom_in"
    | "aliens_coming"
    | "drive_ferrari"
    | "bjd_style"
    | "virtual_fitting"
    | "orbit"
    | "zoom_in"
    | "ai_outfit"
    | "spin180"
    | "orbit_dolly"
    | "orbit_dolly_fast"
    | "auto_spin"
    | "walk_forward"
    | "outfit_show"
    | "zoom_in_fast"
    | "zoom_out_image"
    | "zoom_out_startend"
    | "muscling"
    | "captain_america"
    | "hulk"
    | "cap_walk"
    | "hulk_dive"
    | "exotic_princess"
    | "beast_companion"
    | "cartoon_doll"
    | "golden_epoch"
    | "oscar_gala"
    | "fashion_stride"
    | "star_carpet"
    | "flame_carpet"
    | "frost_carpet"
    | "mecha_x"
    | "style_me"
    | "tap_me"
    | "saber_warrior"
    | "pet2human"
    | "graduation"
    | "fishermen"
    | "happy_birthday"
    | "fairy_me"
    | "ladudu_me"
    | "ladudu_me_random"
    | "squid_game"
    | "superman"
    | "grow_wings"
    | "clevage"
    | "fly_with_doraemon"
    | "creatice_product_down"
    | "pole_dance"
    | "hug_from_behind"
    | "creatice_product_up_cybercity"
    | "creatice_product_up_bluecircuit"
    | "creatice_product_up"
    | "run_fast"
    | "background_explosion";
  /**
   * URLs of the images to use with the template. Number of images required varies by template: 'dynasty_dress' and 'shop_frame' accept 1-2 images, 'wish_sender' requires exactly 3 images, all other templates accept only 1 image.
   */
  input_image_urls: Array<string>;
  /**
   * Random seed for generation
   */
  seed?: number;
  /**
   * The aspect ratio of the output video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16";
};
export type ViduTemplateToVideoOutput = {
  /**
   * The generated video using a predefined template
   */
  video: File;
};
export type VignetteInput = {
  /**
   * URL of image to process
   */
  image_url: string | Blob | File;
  /**
   * Vignette strength Default value: `0.5`
   */
  vignette_strength?: number;
};
export type VignetteOutput = {
  /**
   * The processed images with vignette effect
   */
  images: Array<Image>;
};
export type VirtualTryonInput = {
  /**
   * The URLs of the images for virtual try-on. Provide person image and clothing image.
   */
  image_urls: Array<string>;
  /**
   * The prompt to generate a virtual try-on image.
   */
  prompt: string;
  /**
   * The size of the generated image. If not provided, the size of the input image will be used.
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt. Default value: `2.5`
   */
  guidance_scale?: number;
  /**
   * The number of inference steps to perform. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Acceleration level for image generation. 'regular' balances speed and quality. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Random seed for reproducibility. Same seed with same prompt will produce same result.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and won't be saved in history.
   */
  sync_mode?: boolean;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the output image Default value: `"png"`
   */
  output_format?: "png" | "jpeg" | "webp";
  /**
   * Number of images to generate Default value: `1`
   */
  num_images?: number;
  /**
   * The strength of the virtual try-on effect. Default value: `1`
   */
  lora_scale?: number;
};
export type VirtualTryOnInput = {
  /**
   * Person photo URL
   */
  person_image_url: string | Blob | File;
  /**
   * Clothing photo URL
   */
  clothing_image_url: string | Blob | File;
  /**
   *  Default value: `true`
   */
  preserve_pose?: boolean;
  /**
   * Aspect ratio for 4K output (default: 3:4 for fashion)
   */
  aspect_ratio?: AspectRatio;
};
export type VirtualTryonOutput = {
  /**
   * The generated virtual try-on images
   */
  images: Array<Image>;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The prompt used for generation
   */
  prompt: string;
};
export type VirtualTryOnOutput = {
  /**
   * Person wearing the virtual clothing
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type VisionEnterpriseInput = {
  /**
   * List of image URLs to be processed
   */
  image_urls: Array<string>;
  /**
   * Prompt to be used for the image
   */
  prompt: string;
  /**
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string;
  /**
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string;
  /**
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean;
  /**
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input. Default value: `1`
   */
  temperature?: number;
  /**
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number;
};
export type VisionInput = {
  /**
   * Image URL to be processed
   */
  image_url: string | Blob | File;
  /**
   * Prompt to be used for the image
   */
  prompt: string;
  /**
   * Response style to be used for the image.
   *
   * - text: Model will output text. Good for descriptions and captioning.
   * - box: Model will output a combination of text and bounding boxes. Good for
   * localization.
   * - point: Model will output a combination of text and points. Good for counting many
   * objects.
   * - polygon: Model will output a combination of text and polygons. Good for granular
   * segmentation. Default value: `"text"`
   */
  response_style?: "text" | "box" | "point" | "polygon";
};
export type VisionOutput = {
  /**
   * Generated output
   */
  output: string;
  /**
   * Token usage information
   */
  usage: UsageInfo;
};
export type VoiceChangerOutput = {
  /**
   * The generated audio file
   */
  audio: File;
  /**
   * Random seed for reproducibility.
   */
  seed: number;
};
export type VoiceCloneOutput = {
  /**
   * The cloned voice ID for use with TTS
   */
  custom_voice_id: string;
  /**
   * Preview audio generated with the cloned voice (if requested)
   */
  audio?: File;
};
export type VoiceCloningOutput = {
  /**
   * The id of the cloned voice
   */
  voice_id: string;
};
export type VoiceDeleteOutput = {
  /**
   * The voice_id of the voice that was deleted
   */
  voice_id: string;
};
export type VoiceDesignOutput = {
  /**
   * The voice_id of the generated voice
   */
  custom_voice_id: string;
  /**
   * The preview audio using the generated voice
   */
  audio: File;
};
export type Wan22ImageTrainerInput = {
  /**
   * URL to the training data.
   */
  training_data_url: string | Blob | File;
  /**
   * Trigger phrase for the model.
   */
  trigger_phrase: string;
  /**
   * Whether to include synthetic captions.
   */
  include_synthetic_captions?: boolean;
  /**
   * Whether to use face detection for the training data. When enabled, images will use the center of the face as the center of the image when resizing. Default value: `true`
   */
  use_face_detection?: boolean;
  /**
   * Whether to use face cropping for the training data. When enabled, images will be cropped to the face before resizing.
   */
  use_face_cropping?: boolean;
  /**
   * Whether to use masks for the training data. Default value: `true`
   */
  use_masks?: boolean;
  /**
   * Number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate for training. Default value: `0.0007`
   */
  learning_rate?: number;
  /**
   * Whether the training data is style data. If true, face specific options like masking and face detection will be disabled.
   */
  is_style?: boolean;
};
export type Wan22ImageTrainerOutput = {
  /**
   * Low noise LoRA file.
   */
  diffusers_lora_file: File;
  /**
   * High noise LoRA file.
   */
  high_noise_lora: File;
  /**
   * Config file helping inference endpoints after training.
   */
  config_file: File;
};
export type Wan22VaceFunA14bDepthInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. Required for depth task.
   */
  video_url: string | Blob | File;
  /**
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
};
export type Wan22VaceFunA14bDepthOutput = {
  /**
   * The generated depth video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type Wan22VaceFunA14bInpaintingInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. Required for inpainting.
   */
  video_url: string | Blob | File;
  /**
   * URL to the source mask file. Required for inpainting.
   */
  mask_video_url: string | Blob | File;
  /**
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.
   */
  mask_image_url?: string | Blob | File;
  /**
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
};
export type Wan22VaceFunA14bInpaintingOutput = {
  /**
   * The generated inpainting video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type Wan22VaceFunA14bOutpaintingInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. Required for outpainting.
   */
  video_url: string | Blob | File;
  /**
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Whether to expand the video to the left.
   */
  expand_left?: boolean;
  /**
   * Whether to expand the video to the right.
   */
  expand_right?: boolean;
  /**
   * Whether to expand the video to the top.
   */
  expand_top?: boolean;
  /**
   * Whether to expand the video to the bottom.
   */
  expand_bottom?: boolean;
  /**
   * Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides. Default value: `0.25`
   */
  expand_ratio?: number;
};
export type Wan22VaceFunA14bOutpaintingOutput = {
  /**
   * The generated outpainting video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type Wan22VaceFunA14bPoseInput = {
  /**
   * The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. Required for pose task.
   */
  video_url: string | Blob | File;
  /**
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
};
export type Wan22VaceFunA14bPoseOutput = {
  /**
   * The generated pose video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type Wan22VaceFunA14bReframeInput = {
  /**
   * The text prompt to guide video generation. Optional for reframing. Default value: `""`
   */
  prompt?: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter. Default value: `true`
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter. Default value: `true`
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string | Blob | File;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number;
  /**
   * Whether to trim borders from the video. Default value: `true`
   */
  trim_borders?: boolean;
};
export type Wan22VaceFunA14bReframeOutput = {
  /**
   * The generated reframe video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type Wan25PreviewImageToImageInput = {
  /**
   * The text prompt describing how to edit the image. Max 2000 characters.
   */
  prompt: string;
  /**
   * URLs of images to edit. For single-image editing, provide 1 URL. For multi-reference generation, provide up to 2 URLs. If more than 2 URLs are provided, only the first 2 will be used.
   */
  image_urls: Array<string>;
  /**
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * The size of the generated image. Width and height must be between 384 and 1440 pixels. Default value: `square`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Number of images to generate. Values from 1 to 4. Default value: `1`
   */
  num_images?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type Wan25PreviewImageToImageOutput = {
  /**
   * The edited images
   */
  images: Array<ImageFile>;
  /**
   * The seeds used for each generated image
   */
  seeds: Array<number>;
  /**
   * The original prompt (prompt expansion is not available for image editing)
   */
  actual_prompt?: string;
};
export type Wan25PreviewImageToVideoInput = {
  /**
   * The text prompt describing the desired video motion. Max 800 characters.
   */
  prompt: string;
  /**
   * URL of the image to use as the first frame. Must be publicly accessible or base64 data URI.
   *
   * Max file size: 25.0MB, Min width: 360px, Min height: 360px, Max width: 2000px, Max height: 2000px, Timeout: 20.0s
   */
  image_url: string | Blob | File;
  /**
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds),
   * the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   */
  audio_url?: string | Blob | File;
  /**
   * Video resolution. Valid values: 480p, 720p, 1080p Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the generated video in seconds. Choose between 5 or 10 seconds. Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * Whether to enable prompt rewriting using LLM. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type Wan25PreviewImageToVideoOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
};
export type Wan25PreviewTextToImageInput = {
  /**
   * The prompt for image generation. Supports Chinese and English, max 2000 characters.
   */
  prompt: string;
  /**
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * Number of images to generate. Values from 1 to 4. Default value: `1`
   */
  num_images?: number;
  /**
   * The size of the generated image. Can use preset names like 'square', 'landscape_16_9', etc., or specific dimensions. Total pixels must be between 768×768 and 1440×1440, with aspect ratio between [1:4, 4:1]. Default value: `square`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type Wan25PreviewTextToImageOutput = {
  /**
   * The generated images
   */
  images: Array<ImageFile>;
  /**
   * The seeds used for each generated image
   */
  seeds: Array<number>;
  /**
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
};
export type Wan25PreviewTextToVideoInput = {
  /**
   * The text prompt for video generation. Supports Chinese and English, max 800 characters.
   */
  prompt: string;
  /**
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds),
   * the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   */
  audio_url?: string | Blob | File;
  /**
   * The aspect ratio of the generated video Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Video resolution tier Default value: `"1080p"`
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration of the generated video in seconds. Choose between 5 or 10 seconds. Default value: `"5"`
   */
  duration?: "5" | "10";
  /**
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time. Default value: `true`
   */
  enable_prompt_expansion?: boolean;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type Wan25PreviewTextToVideoOutput = {
  /**
   * The generated video file
   */
  video: VideoFile;
  /**
   * The seed used for generation
   */
  seed: number;
  /**
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
};
export type WanAlphaInput = {
  /**
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * The number of frames to generate. Default value: `81`
   */
  num_frames?: number;
  /**
   * The frame rate of the generated video. Default value: `16`
   */
  fps?: number;
  /**
   * The number of inference steps to use. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * The sampler to use. Default value: `"euler"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * The shift of the generated video. Default value: `10.5`
   */
  shift?: number;
  /**
   * The resolution of the generated video. Default value: `"480p"`
   */
  resolution?: "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * The aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to enable safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The lower bound of the mask clamping. Default value: `0.1`
   */
  mask_clamp_lower?: number;
  /**
   * The upper bound of the mask clamping. Default value: `0.75`
   */
  mask_clamp_upper?: number;
  /**
   * Whether to binarize the mask.
   */
  binarize_mask?: boolean;
  /**
   * The threshold for mask binarization. When binarize_mask is True, this threshold will be used to binarize the mask. This will also be used for transparency when the output type is `.webm`. Default value: `0.8`
   */
  mask_binarization_threshold?: number;
  /**
   * The output type of the generated video. Default value: `"VP9 (.webm)"`
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type WanAlphaOutput = {
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video?: VideoFile;
  /**
   * The generated image file.
   */
  image?: VideoFile;
  /**
   * The generated mask file.
   */
  mask?: VideoFile;
};
export type WanAtiInput = {
  /**
   * URL of the input image.
   */
  image_url: string | Blob | File;
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Motion tracks to guide video generation. Each track is a sequence of points defining a motion trajectory. Multiple tracks can control different elements or objects in the video. Expected format: array of tracks, where each track is an array of points with 'x' and 'y' coordinates (up to 121 points per track). Points will be automatically padded to 121 if fewer are provided. Coordinates should be within the image dimensions.
   */
  track: Array<Array<TrackPoint>>;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Resolution of the generated video (480p, 580p, 720p). Default value: `"480p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};
export type WanAtiOutput = {
  /**
   * The generated video file.
   */
  video: File;
};
export type WanEffectsInput = {
  /**
   * The subject to insert into the predefined prompt template for the selected effect.
   */
  subject: string;
  /**
   * URL of the input image.
   */
  image_url: string | Blob | File;
  /**
   * The type of effect to apply to the video. Default value: `"cakeify"`
   */
  effect_type?:
    | "squish"
    | "muscle"
    | "inflate"
    | "crush"
    | "rotate"
    | "gun-shooting"
    | "deflate"
    | "cakeify"
    | "hulk"
    | "baby"
    | "bride"
    | "classy"
    | "puppy"
    | "snow-white"
    | "disney-princess"
    | "mona-lisa"
    | "painting"
    | "pirate-captain"
    | "princess"
    | "jungle"
    | "samurai"
    | "vip"
    | "warrior"
    | "zen"
    | "assassin"
    | "timelapse"
    | "tsunami"
    | "fire"
    | "zoom-call"
    | "doom-fps"
    | "fus-ro-dah"
    | "hug-jesus"
    | "robot-face-reveal"
    | "super-saiyan"
    | "jumpscare"
    | "laughing"
    | "cartoon-jaw-drop"
    | "crying"
    | "kissing"
    | "angry-face"
    | "selfie-younger-self"
    | "animeify"
    | "blast";
  /**
   * Number of frames to generate. Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Aspect ratio of the output video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The scale of the LoRA weight. Used to adjust effect intensity. Default value: `1`
   */
  lora_scale?: number;
  /**
   * Whether to use turbo mode. If True, the video will be generated faster but with lower quality.
   */
  turbo_mode?: boolean;
};
export type WanEffectsOutput = {
  /**
   * The generated video
   */
  video: File;
  /**
   *
   */
  seed: number;
};
export type WanFlf2vInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * URL of the starting image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  start_image_url: string | Blob | File;
  /**
   * URL of the ending image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  end_image_url: string | Blob | File;
  /**
   * Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `5`
   */
  guide_scale?: number;
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
};
export type WanFlf2vOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanFunControlInput = {
  /**
   * The prompt to generate the video.
   */
  prompt: string;
  /**
   * The negative prompt to generate the video. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * The number of inference steps. Default value: `27`
   */
  num_inference_steps?: number;
  /**
   * The guidance scale. Default value: `6`
   */
  guidance_scale?: number;
  /**
   * The shift for the scheduler. Default value: `5`
   */
  shift?: number;
  /**
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Whether to match the number of frames in the input video. Default value: `true`
   */
  match_input_num_frames?: boolean;
  /**
   * The number of frames to generate. Only used when match_input_num_frames is False. Default value: `81`
   */
  num_frames?: number;
  /**
   * Whether to match the fps in the input video. Default value: `true`
   */
  match_input_fps?: boolean;
  /**
   * The fps to generate. Only used when match_input_fps is False. Default value: `16`
   */
  fps?: number;
  /**
   * The URL of the control video to use as a reference for the video generation.
   */
  control_video_url: string | Blob | File;
  /**
   * Whether to preprocess the video. If True, the video will be preprocessed to depth or pose.
   */
  preprocess_video?: boolean;
  /**
   * The type of preprocess to apply to the video. Only used when preprocess_video is True. Default value: `"depth"`
   */
  preprocess_type?: "depth" | "pose";
  /**
   * The URL of the reference image to use as a reference for the video generation.
   */
  reference_image_url?: string | Blob | File;
};
export type WanFunControlOutput = {
  /**
   * The video generated by the model.
   */
  video: File;
};
export type WanI2vInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `5`
   */
  guide_scale?: number;
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
};
export type WanI2vLoraInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `5`
   */
  guide_scale?: number;
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Aspect ratio of the output video. Default value: `"16:9"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * LoRA weights to be used in the inference.
   */
  loras?: Array<LoraWeight>;
  /**
   * If true, the video will be reversed.
   */
  reverse_video?: boolean;
  /**
   * If true, the video will be generated faster with no noticeable degradation in the visual quality. Default value: `true`
   */
  turbo_mode?: boolean;
};
export type WanI2vLoraOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanI2vOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanMoveInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * Text prompt to guide the video generation.
   */
  prompt: string;
  /**
   * A list of trajectories. Each trajectory list means the movement of one object.
   */
  trajectories: Array<Array<TrajectoryPoint>>;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Negative prompt to guide the video generation. Default value: `"色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走"`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};
export type WanMoveOutput = {
  /**
   * Generated Video File
   */
  video: VideoFile;
  /**
   * Random seed used for generation.
   */
  seed: number;
};
export type WanProImageToVideoInput = {
  /**
   * The prompt to generate the video
   */
  prompt: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Whether to enable the safety checker Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The URL of the image to generate the video from
   */
  image_url: string | Blob | File;
};
export type WanProImageToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type WanProTextToVideoInput = {
  /**
   * The prompt to generate the video
   */
  prompt: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Whether to enable the safety checker Default value: `true`
   */
  enable_safety_checker?: boolean;
};
export type WanProTextToVideoOutput = {
  /**
   * The generated video
   */
  video: File;
};
export type WanT2vInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If true, the video will be generated faster with no noticeable degradation in the visual quality.
   */
  turbo_mode?: boolean;
};
export type WanT2vLoraInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p,580p, or 720p). Default value: `"480p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * If true, the video will be generated faster with no noticeable degradation in the visual quality. Default value: `true`
   */
  turbo_mode?: boolean;
  /**
   * LoRA weights to be used in the inference.
   */
  loras?: Array<LoraWeight>;
  /**
   * If true, the video will be reversed.
   */
  reverse_video?: boolean;
};
export type WanT2vLoraOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanT2vOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanTrainerFlf2v720pInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string | Blob | File;
  /**
   * The number of steps to train for. Default value: `400`
   */
  number_of_steps?: number;
  /**
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`
   */
  learning_rate?: number;
  /**
   * The phrase that will trigger the model to generate an image. Default value: `""`
   */
  trigger_phrase?: string;
  /**
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};
export type WanTrainerFlf2v720pOutput = {
  /**
   * URL to the trained LoRA weights.
   */
  lora_file: File;
  /**
   * Configuration used for setting up the inference endpoints.
   */
  config_file: File;
};
export type WanTrainerI2v720pInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string | Blob | File;
  /**
   * The number of steps to train for. Default value: `400`
   */
  number_of_steps?: number;
  /**
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`
   */
  learning_rate?: number;
  /**
   * The phrase that will trigger the model to generate an image. Default value: `""`
   */
  trigger_phrase?: string;
  /**
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};
export type WanTrainerI2v720pOutput = {
  /**
   * URL to the trained LoRA weights.
   */
  lora_file: File;
  /**
   * Configuration used for setting up the inference endpoints.
   */
  config_file: File;
};
export type WanTrainerInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string | Blob | File;
  /**
   * The number of steps to train for. Default value: `400`
   */
  number_of_steps?: number;
  /**
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`
   */
  learning_rate?: number;
  /**
   * The phrase that will trigger the model to generate an image. Default value: `""`
   */
  trigger_phrase?: string;
  /**
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};
export type WanTrainerOutput = {
  /**
   * URL to the trained LoRA weights.
   */
  lora_file: File;
  /**
   * Configuration used for setting up the inference endpoints.
   */
  config_file: File;
};
export type WanTrainerT2v14bInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string | Blob | File;
  /**
   * The number of steps to train for. Default value: `400`
   */
  number_of_steps?: number;
  /**
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`
   */
  learning_rate?: number;
  /**
   * The phrase that will trigger the model to generate an image. Default value: `""`
   */
  trigger_phrase?: string;
  /**
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};
export type WanTrainerT2v14bOutput = {
  /**
   * URL to the trained LoRA weights.
   */
  lora_file: File;
  /**
   * Configuration used for setting up the inference endpoints.
   */
  config_file: File;
};
export type WanTrainerT2vInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string | Blob | File;
  /**
   * The number of steps to train for. Default value: `400`
   */
  number_of_steps?: number;
  /**
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`
   */
  learning_rate?: number;
  /**
   * The phrase that will trigger the model to generate an image. Default value: `""`
   */
  trigger_phrase?: string;
  /**
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};
export type WanTrainerT2vOutput = {
  /**
   * URL to the trained LoRA weights.
   */
  lora_file: File;
  /**
   * Configuration used for setting up the inference endpoints.
   */
  config_file: File;
};
export type WanV2214bAnimateMoveInput = {
  /**
   * URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"480p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `20`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).
   */
  return_frames_zip?: boolean;
  /**
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.
   */
  use_turbo?: boolean;
};
export type WanV2214bAnimateMoveOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * ZIP archive of generated frames (if requested).
   */
  frames_zip?: File;
  /**
   * The prompt used for generation (auto-generated by the model)
   */
  prompt: string;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type WanV2214bAnimateReplaceInput = {
  /**
   * URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"480p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `20`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).
   */
  return_frames_zip?: boolean;
  /**
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.
   */
  use_turbo?: boolean;
};
export type WanV2214bAnimateReplaceOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * ZIP archive of generated frames (if requested).
   */
  frames_zip?: File;
  /**
   * The prompt used for generation (auto-generated by the model)
   */
  prompt: string;
  /**
   * The seed used for generation
   */
  seed: number;
};
export type WanV2214bSpeechToVideoInput = {
  /**
   * The text prompt used for video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Number of frames to generate. Must be between 40 to 120, (must be multiple of 4). Default value: `80`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"480p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The URL of the audio file.
   */
  audio_url: string | Blob | File;
};
export type WanV2214bSpeechToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
};
export type WanV225bImageToVideoInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 17 to 161 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `24`
   */
  frames_per_second?: number;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (580p or 720p). Default value: `"720p"`
   */
  resolution?: "580p" | "720p";
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `"film"`
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`
   */
  adjust_fps_for_interpolation?: boolean;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
};
export type WanV225bImageToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV225bTextToImageInput = {
  /**
   * The text prompt to guide image generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Shift value for the image. Must be between 1.0 and 10.0. Default value: `2`
   */
  shift?: number;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The format of the output image. Default value: `"jpeg"`
   */
  image_format?: "png" | "jpeg";
};
export type WanV225bTextToImageOutput = {
  /**
   * The generated image file.
   */
  image: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV225bTextToVideoDistillInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 17 to 161 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `24`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (580p or 720p). Default value: `"720p"`
   */
  resolution?: "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `1`
   */
  guidance_scale?: number;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `"film"`
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`
   */
  adjust_fps_for_interpolation?: boolean;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
};
export type WanV225bTextToVideoDistillOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV225bTextToVideoFastWanInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Number of frames to generate. Must be between 17 to 161 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `24`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (580p or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `"film"`
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`
   */
  adjust_fps_for_interpolation?: boolean;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
};
export type WanV225bTextToVideoFastWanOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV225bTextToVideoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Number of frames to generate. Must be between 17 to 161 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `24`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (580p or 720p). Default value: `"720p"`
   */
  resolution?: "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `"film"`
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`
   */
  adjust_fps_for_interpolation?: boolean;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
};
export type WanV225bTextToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV22A14bImageToImageInput = {
  /**
   * URL of the input image.
   */
  image_url: string | Blob | File;
  /**
   * The text prompt to guide image generation.
   */
  prompt: string;
  /**
   * Denoising strength. 1.0 = fully remake; 0.0 = preserve original. Default value: `0.5`
   */
  strength?: number;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Aspect ratio of the generated image. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Classifier-free guidance scale. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`
   */
  guidance_scale_2?: number;
  /**
   *  Default value: `2`
   */
  shift?: number;
  /**
   *
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The format of the output image. Default value: `"jpeg"`
   */
  image_format?: "png" | "jpeg";
};
export type WanV22A14bImageToImageOutput = {
  /**
   * The generated image file.
   */
  image: File;
  /**
   * The text prompt used for image generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV22A14bImageToVideoLoraInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of frames to generate. Must be between 17 to 161 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`
   */
  guidance_scale_2?: number;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `"film"`
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4. Default value: `1`
   */
  num_interpolated_frames?: number;
  /**
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`
   */
  adjust_fps_for_interpolation?: boolean;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * LoRA weights to be used in the inference.
   */
  loras?: Array<LoRAWeight>;
  /**
   * If true, the video will be reversed.
   */
  reverse_video?: boolean;
  /**
   * URL of the end image.
   */
  end_image_url?: string | Blob | File;
};
export type WanV22A14bImageToVideoLoraOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV22A14bImageToVideoTurboInput = {
  /**
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string | Blob | File;
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * URL of the end image.
   */
  end_image_url?: string | Blob | File;
};
export type WanV22A14bImageToVideoTurboOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV22A14bTextToImageInput = {
  /**
   * The text prompt to guide image generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`
   */
  guidance_scale_2?: number;
  /**
   * Shift value for the image. Must be between 1.0 and 10.0. Default value: `2`
   */
  shift?: number;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
};
export type WanV22A14bTextToImageLoraInput = {
  /**
   * The text prompt to guide image generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`
   */
  guidance_scale_2?: number;
  /**
   * Shift value for the image. Must be between 1.0 and 10.0. Default value: `2`
   */
  shift?: number;
  /**
   * LoRA weights to be used in the inference.
   */
  loras?: Array<LoRAWeight>;
  /**
   * If true, the video will be reversed.
   */
  reverse_video?: boolean;
  /**
   * The size of the generated image. Default value: `square_hd`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The format of the output image. Default value: `"jpeg"`
   */
  image_format?: "png" | "jpeg";
};
export type WanV22A14bTextToImageLoraOutput = {
  /**
   * The generated image file.
   */
  image: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV22A14bTextToImageOutput = {
  /**
   * The generated image file.
   */
  image: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV22A14bTextToVideoInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Number of frames to generate. Must be between 17 to 161 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`
   */
  guidance_scale_2?: number;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `"film"`
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4. Default value: `1`
   */
  num_interpolated_frames?: number;
  /**
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`
   */
  adjust_fps_for_interpolation?: boolean;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
};
export type WanV22A14bTextToVideoLoraInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Number of frames to generate. Must be between 17 to 161 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`
   */
  guidance_scale_2?: number;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `"film"`
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4. Default value: `1`
   */
  num_interpolated_frames?: number;
  /**
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`
   */
  adjust_fps_for_interpolation?: boolean;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * LoRA weights to be used in the inference.
   */
  loras?: Array<LoRAWeight>;
  /**
   * If true, the video will be reversed.
   */
  reverse_video?: boolean;
};
export type WanV22A14bTextToVideoLoraOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV22A14bTextToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV22A14bTextToVideoTurboInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
};
export type WanV22A14bTextToVideoTurboOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanV22A14bVideoToVideoInput = {
  /**
   * URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Strength of the video transformation. A value of 1.0 means the output will be completely based on the prompt, while a value of 0.0 means the output will be identical to the input video. Default value: `0.9`
   */
  strength?: number;
  /**
   * Number of frames to generate. Must be between 17 to 161 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Negative prompt for video generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p, 580p, or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`
   */
  num_inference_steps?: number;
  /**
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `"regular"`
   */
  acceleration?: "none" | "regular";
  /**
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`
   */
  guidance_scale_2?: number;
  /**
   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`
   */
  shift?: number;
  /**
   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `"film"`
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4. Default value: `1`
   */
  num_interpolated_frames?: number;
  /**
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`
   */
  adjust_fps_for_interpolation?: boolean;
  /**
   * The quality of the output video. Higher quality means better visual quality but larger file size. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * If true, the video will be resampled to the passed frames per second. If false, the video will not be resampled.
   */
  resample_fps?: boolean;
};
export type WanV22A14bVideoToVideoOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The text prompt used for video generation. Default value: `""`
   */
  prompt?: string;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanVace13bInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Task type for the model. Default value: `"depth"`
   */
  task?: "depth" | "inpainting" | "pose";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p,580p, or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "auto" | "9:16" | "16:9";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string | Blob | File;
  /**
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string | Blob | File;
  /**
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string | Blob | File;
  /**
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
};
export type WanVace13bOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanVace14bDepthInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. Required for depth task.
   */
  video_url: string | Blob | File;
  /**
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
};
export type WanVace14bDepthOutput = {
  /**
   * The generated depth video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type WanVace14bInpaintingInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. Required for inpainting.
   */
  video_url: string | Blob | File;
  /**
   * URL to the source mask file. Required for inpainting.
   */
  mask_video_url: string | Blob | File;
  /**
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.
   */
  mask_image_url?: string | Blob | File;
  /**
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
};
export type WanVace14bInpaintingOutput = {
  /**
   * The generated inpainting video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type WanVace14bInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Task type for the model. Default value: `"depth"`
   */
  task?: "depth" | "pose" | "inpainting" | "outpainting" | "reframe";
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string | Blob | File;
  /**
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string | Blob | File;
  /**
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string | Blob | File;
  /**
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
};
export type WanVace14bOutpaintingInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. Required for outpainting.
   */
  video_url: string | Blob | File;
  /**
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Whether to expand the video to the left.
   */
  expand_left?: boolean;
  /**
   * Whether to expand the video to the right.
   */
  expand_right?: boolean;
  /**
   * Whether to expand the video to the top.
   */
  expand_top?: boolean;
  /**
   * Whether to expand the video to the bottom.
   */
  expand_bottom?: boolean;
  /**
   * Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides. Default value: `0.25`
   */
  expand_ratio?: number;
};
export type WanVace14bOutpaintingOutput = {
  /**
   * The generated outpainting video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type WanVace14bOutput = {
  /**
   * The generated video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type WanVace14bPoseInput = {
  /**
   * The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. Required for pose task.
   */
  video_url: string | Blob | File;
  /**
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
};
export type WanVace14bPoseOutput = {
  /**
   * The generated pose video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type WanVace14bReframeInput = {
  /**
   * The text prompt to guide video generation. Optional for reframing. Default value: `""`
   */
  prompt?: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter. Default value: `true`
   */
  match_input_num_frames?: boolean;
  /**
   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`
   */
  num_frames?: number;
  /**
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter. Default value: `true`
   */
  match_input_frames_per_second?: boolean;
  /**
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string | Blob | File;
  /**
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | Blob | File;
  /**
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `regular`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number;
  /**
   * Whether to trim borders from the video. Default value: `true`
   */
  trim_borders?: boolean;
};
export type WanVace14bReframeOutput = {
  /**
   * The generated reframe video file.
   */
  video: VideoFile;
  /**
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: File;
};
export type WanVaceAppsLongReframeInput = {
  /**
   * The text prompt to guide video generation. Optional for reframing. Default value: `""`
   */
  prompt?: string;
  /**
   * Negative prompt for video generation. Default value: `"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`
   */
  guidance_scale?: number;
  /**
   * Sampler to use for video generation. Default value: `"unipc"`
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string | Blob | File;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `"regular"`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * The quality of the generated video. Default value: `"high"`
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * The write mode of the generated video. Default value: `"balanced"`
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Whether to enable auto downsample. Default value: `true`
   */
  enable_auto_downsample?: boolean;
  /**
   * Minimum FPS for auto downsample. Default value: `6`
   */
  auto_downsample_min_fps?: number;
  /**
   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `"film"`
   */
  interpolator_model?: "rife" | "film";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled. Default value: `"content_aware"`
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number;
  /**
   * Whether to trim borders from the video. Default value: `true`
   */
  trim_borders?: boolean;
  /**
   * Threshold for scene detection sensitivity (0-100). Lower values detect more scenes. Default value: `30`
   */
  scene_threshold?: number;
  /**
   * Whether to paste back the reframed scene to the original video. Default value: `true`
   */
  paste_back?: boolean;
};
export type WanVaceAppsLongReframeOutput = {
  /**
   * The output video file.
   */
  video: VideoFile;
};
export type WanVaceAppsVideoEditInput = {
  /**
   * Prompt to edit the video.
   */
  prompt: string;
  /**
   * URL of the input video.
   */
  video_url: string | Blob | File;
  /**
   * The type of video you're editing. Use 'general' for most videos, and 'human' for videos emphasizing human subjects and motions. The default value 'auto' means the model will guess based on the first frame of the video. Default value: `"auto"`
   */
  video_type?: "auto" | "general" | "human";
  /**
   * URLs of the input images to use as a reference for the generation.
   */
  image_urls?: Array<string>;
  /**
   * Resolution of the edited video. Default value: `"auto"`
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster. Default value: `"regular"`
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * Whether to enable automatic downsampling. If your video has a high frame rate or is long, enabling longer sequences to be generated. The video will be interpolated back to the original frame rate after generation. Default value: `true`
   */
  enable_auto_downsample?: boolean;
  /**
   * Aspect ratio of the edited video. Default value: `"auto"`
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * The minimum frames per second to downsample the video to. Default value: `15`
   */
  auto_downsample_min_fps?: number;
  /**
   * Whether to enable the safety checker. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to include a ZIP archive containing all generated frames.
   */
  return_frames_zip?: boolean;
};
export type WanVaceAppsVideoEditOutput = {
  /**
   * The edited video.
   */
  video: VideoFile;
  /**
   * ZIP archive of generated frames if requested.
   */
  frames_zip?: File;
};
export type WanVaceInput = {
  /**
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Negative prompt for video generation. Default value: `"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames Default value: `81`
   */
  num_frames?: number;
  /**
   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`
   */
  frames_per_second?: number;
  /**
   * Task type for the model. Default value: `"depth"`
   */
  task?: "depth" | "inpainting";
  /**
   * Shift parameter for video generation. Default value: `5`
   */
  shift?: number;
  /**
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Resolution of the generated video (480p,580p, or 720p). Default value: `"720p"`
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `"16:9"`
   */
  aspect_ratio?: "auto" | "9:16" | "16:9";
  /**
   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string | Blob | File;
  /**
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string | Blob | File;
  /**
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string | Blob | File;
  /**
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
};
export type WanVaceOutput = {
  /**
   * The generated video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
};
export type WanVisionEnhancerInput = {
  /**
   * The URL of the video to enhance with Wan Video. Maximum 200MB file size. Videos longer than 500 frames will have only the first 500 frames processed (~8-21 seconds depending on fps).
   */
  video_url: string | Blob | File;
  /**
   * Target output resolution for the enhanced video. 720p (native, fast) or 1080p (upscaled, slower). Processing is always done at 720p, then upscaled if 1080p selected. Default value: `"720p"`
   */
  target_resolution?: "720p" | "1080p";
  /**
   * Controls how much the model enhances/changes the video. 0 = Minimal change (preserves original), 1 = Subtle enhancement (default), 2 = Medium enhancement, 3 = Strong enhancement, 4 = Maximum enhancement. Default value: `1`
   */
  creativity?: number;
  /**
   * Optional prompt to prepend to the VLM-generated description. Leave empty to use only the auto-generated description from the video.
   */
  prompt?: string;
  /**
   * Negative prompt to avoid unwanted features. Default value: `"oversaturated, overexposed, static, blurry details, subtitles, stylized, artwork, painting, still frame, overall gray, worst quality, low quality, JPEG artifacts, ugly, mutated, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fused fingers, static motion, cluttered background, three legs, crowded background, walking backwards"`
   */
  negative_prompt?: string;
  /**
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number;
};
export type WanVisionEnhancerOutput = {
  /**
   * The enhanced video file.
   */
  video: File;
  /**
   * The seed used for generation.
   */
  seed: number;
  /**
   * The timings of the different steps in the workflow.
   */
  timings: any;
};
export type WaveformInput = {
  /**
   * URL of the audio file to analyze
   */
  media_url: string | Blob | File;
  /**
   * Controls how many points are sampled per second of audio. Lower values (e.g. 1-2) create a coarser waveform, higher values (e.g. 4-10) create a more detailed one. Default value: `4`
   */
  points_per_second?: number;
  /**
   * Number of decimal places for the waveform values. Higher values provide more precision but increase payload size. Default value: `2`
   */
  precision?: number;
  /**
   * Size of the smoothing window. Higher values create a smoother waveform. Must be an odd number. Default value: `3`
   */
  smoothing_window?: number;
};
export type WaveformOutput = {
  /**
   * Normalized waveform data as an array of values between -1 and 1. The number of points is determined by audio duration × points_per_second.
   */
  waveform: Array<number>;
  /**
   * Duration of the audio in seconds
   */
  duration: number;
  /**
   * Number of points in the waveform data
   */
  points: number;
  /**
   * Number of decimal places used in the waveform values
   */
  precision: number;
};
export type WeatherEffectInput = {
  /**
   * Image prompt for the omni model.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `"2"`
   */
  safety_tolerance?: "1" | "2" | "3" | "4" | "5" | "6";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png";
  /**
   * The aspect ratio of the generated image.
   */
  aspect_ratio?:
    | "21:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:21";
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The weather effect to apply. Default value: `"heavy snowfall"`
   */
  prompt?: string;
};
export type WeatherEffectOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type WojakStyleInput = {
  /**
   * URL of the image to convert to wojak style.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type WojakStyleOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type WorkflowUtilitiesAudioCompressorInput = {
  /**
   * URL of the audio file to compress
   */
  audio_url: string | Blob | File;
  /**
   * Threshold level in dB above which compression is applied (-60 to 0) Default value: `-18`
   */
  threshold?: number;
  /**
   * Compression ratio (1 = no compression, higher = more compression) Default value: `3`
   */
  ratio?: number;
  /**
   * Attack time in milliseconds (how fast compression starts) Default value: `5`
   */
  attack?: number;
  /**
   * Release time in milliseconds (how fast compression stops) Default value: `50`
   */
  release?: number;
  /**
   * Makeup gain in dB to compensate for volume reduction Default value: `8`
   */
  makeup?: number;
  /**
   * Knee width in dB for soft knee compression (0 = hard knee) Default value: `2.83`
   */
  knee?: number;
  /**
   * Output audio bitrate Default value: `"192k"`
   */
  output_bitrate?: "128k" | "192k" | "256k" | "320k";
};
export type WorkflowUtilitiesAudioCompressorOutput = {
  /**
   * The compressed audio file
   */
  audio: AudioFile;
};
export type WorkflowUtilitiesAutoSubtitleInput = {
  /**
   * URL of the video file to add automatic subtitles to
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra') Default value: `"en"`
   */
  language?: string;
  /**
   * Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty') Default value: `"Montserrat"`
   */
  font_name?: string;
  /**
   * Font size for subtitles (TikTok style uses larger text) Default value: `100`
   */
  font_size?: number;
  /**
   * Font weight (TikTok style typically uses bold or black) Default value: `"bold"`
   */
  font_weight?: "normal" | "bold" | "black";
  /**
   * Subtitle text color for non-active words Default value: `"white"`
   */
  font_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Color for the currently speaking word (karaoke-style highlight) Default value: `"purple"`
   */
  highlight_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Text stroke/outline width in pixels (0 for no stroke) Default value: `3`
   */
  stroke_width?: number;
  /**
   * Text stroke/outline color Default value: `"black"`
   */
  stroke_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Background color behind text ('none' or 'transparent' for no background) Default value: `"none"`
   */
  background_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta"
    | "none"
    | "transparent";
  /**
   * Background opacity (0.0 = fully transparent, 1.0 = fully opaque)
   */
  background_opacity?: number;
  /**
   * Vertical position of subtitles Default value: `"bottom"`
   */
  position?: "top" | "center" | "bottom";
  /**
   * Vertical offset in pixels (positive = move down, negative = move up) Default value: `75`
   */
  y_offset?: number;
  /**
   * Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences. Default value: `3`
   */
  words_per_subtitle?: number;
  /**
   * Enable animation effects for subtitles (bounce style entrance) Default value: `true`
   */
  enable_animation?: boolean;
};
export type WorkflowUtilitiesAutoSubtitleOutput = {
  /**
   * The video with automatic subtitles
   */
  video: File;
  /**
   * Full transcription text
   */
  transcription: string;
  /**
   * Number of subtitle segments generated
   */
  subtitle_count: number;
  /**
   * Word-level timing information from transcription service
   */
  words?: Array<any>;
  /**
   * Additional transcription metadata from ElevenLabs (language, segments, etc.)
   */
  transcription_metadata?: any;
};
export type WorkflowUtilitiesBlendVideoInput = {
  /**
   * URL of the top layer video
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  top_video_url: string | Blob | File;
  /**
   * URL of the bottom layer video
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  bottom_video_url: string | Blob | File;
  /**
   * Blend mode to use for combining the videos Default value: `"overlay"`
   */
  blend_mode?:
    | "addition"
    | "average"
    | "burn"
    | "darken"
    | "difference"
    | "divide"
    | "dodge"
    | "exclusion"
    | "grainextract"
    | "grainmerge"
    | "hardlight"
    | "lighten"
    | "multiply"
    | "negation"
    | "normal"
    | "overlay"
    | "phoenix"
    | "pinlight"
    | "reflect"
    | "screen"
    | "softlight"
    | "subtract"
    | "vividlight";
  /**
   * Opacity of the top layer (0.0-1.0) Default value: `1`
   */
  opacity?: number;
  /**
   * End output when the shortest input ends Default value: `true`
   */
  shortest?: boolean;
};
export type WorkflowUtilitiesBlendVideoOutput = {
  /**
   * The blended video output
   */
  video: File;
};
export type WorkflowUtilitiesExtractNthFrameInput = {
  /**
   * URL of the video file to extract frames from
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string | Blob | File;
  /**
   * Extract every Nth frame (e.g., 3 = every 3rd frame, 12 = every 12th frame) Default value: `12`
   */
  frame_interval?: number;
  /**
   * Output format for extracted frames Default value: `"png"`
   */
  output_format?: "png" | "jpg" | "jpeg" | "webp";
  /**
   * Maximum number of frames to extract Default value: `100`
   */
  max_frames?: number;
  /**
   * Quality for jpg/webp output (1-100) Default value: `95`
   */
  quality?: number;
};
export type WorkflowUtilitiesExtractNthFrameOutput = {
  /**
   * Array of extracted frame images
   */
  images: Array<Image>;
  /**
   * Total number of frames extracted
   */
  frame_count: number;
};
export type WorkflowUtilitiesImpulseResponseInput = {
  /**
   * URL of the main audio file to process
   */
  audio_url: string | Blob | File;
  /**
   * URL of the impulse response WAV file (reverb/effect profile)
   */
  impulse_response_url: string | Blob | File;
  /**
   * Level of the original (dry) signal in the mix (0.0-1.0) Default value: `0.7`
   */
  dry_level?: number;
  /**
   * Level of the processed (wet) signal in the mix (0.0-1.0) Default value: `0.3`
   */
  wet_level?: number;
  /**
   * Target integrated loudness in LUFS (typically -24 to -14) Default value: `-18`
   */
  loudness_i?: number;
  /**
   * Loudness Range target in LU (typically 5-15) Default value: `8`
   */
  loudness_lra?: number;
  /**
   * Maximum true peak in dBTP (typically -2 to -1) Default value: `-1.5`
   */
  loudness_tp?: number;
  /**
   * Output audio bitrate Default value: `"192k"`
   */
  output_bitrate?: "128k" | "192k" | "256k" | "320k";
};
export type WorkflowUtilitiesImpulseResponseOutput = {
  /**
   * The processed audio file with reverb applied
   */
  audio: AudioFile;
};
export type WorkflowUtilitiesInterleaveVideoInput = {
  /**
   * List of video URLs to interleave in order
   */
  video_urls: Array<string>;
};
export type WorkflowUtilitiesInterleaveVideoOutput = {
  /**
   * The interleaved video output
   */
  video: File;
};
export type XAIImageEditInput = {
  /**
   * Text description of the desired image.
   */
  prompt: string;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * URL of the image to edit.
   */
  image_url: string | Blob | File;
};
export type XAIImageEditOutput = {
  /**
   * The URL of the edited image.
   */
  images: Array<ImageFile>;
  /**
   * The enhanced prompt that was used to generate the image.
   */
  revised_prompt: string;
};
export type XAIImageInput = {
  /**
   * Text description of the desired image.
   */
  prompt: string;
  /**
   * Number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * Aspect ratio of the generated image. Default value: `"1:1"`
   */
  aspect_ratio?:
    | "2:1"
    | "20:9"
    | "19.5:9"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16"
    | "9:19.5"
    | "9:20"
    | "1:2";
  /**
   * The format of the generated image. Default value: `"jpeg"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
};
export type XAIImageOutput = {
  /**
   * The URL of the generated image.
   */
  images: Array<ImageFile>;
  /**
   * The enhanced prompt that was used to generate the image.
   */
  revised_prompt: string;
};
export type XAIImageToVideoInput = {
  /**
   * Text description of desired changes or motion in the video.
   */
  prompt: string;
  /**
   * Video duration in seconds. Default value: `6`
   */
  duration?: number;
  /**
   * Aspect ratio of the generated video. Default value: `"auto"`
   */
  aspect_ratio?:
    | "auto"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16";
  /**
   * Resolution of the output video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
  /**
   * URL of the input image for video generation.
   */
  image_url: string | Blob | File;
};
export type XAIImageToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
};
export type XAilabNsfwInput = {
  /**
   * List of image URLs to check. If more than 10 images are provided, only the first 10 will be checked.
   */
  image_urls: Array<string>;
};
export type XAilabNsfwOutput = {
  /**
   * List of booleans indicating if the image has an NSFW concept
   */
  has_nsfw_concepts: Array<boolean>;
};
export type XAITextToVideoInput = {
  /**
   * Text description of the desired video.
   */
  prompt: string;
  /**
   * Video duration in seconds. Default value: `6`
   */
  duration?: number;
  /**
   * Aspect ratio of the generated video. Default value: `"16:9"`
   */
  aspect_ratio?: "16:9" | "4:3" | "3:2" | "1:1" | "2:3" | "3:4" | "9:16";
  /**
   * Resolution of the output video. Default value: `"720p"`
   */
  resolution?: "480p" | "720p";
};
export type XAITextToVideoOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
};
export type XAIVideoEditInput = {
  /**
   * Text description of the desired edit.
   */
  prompt: string;
  /**
   * URL of the input video to edit. The video will be resized to a maximum area of 854x480 pixels and truncated to 8 seconds.
   */
  video_url: string | Blob | File;
  /**
   * Resolution of the output video. Default value: `"auto"`
   */
  resolution?: "auto" | "480p" | "720p";
};
export type XAIVideoEditOutput = {
  /**
   * The generated video.
   */
  video: VideoFile;
};
export type YouTubeThumbnailInput = {
  /**
   * Image URL for YouTube thumbnail
   */
  image_url: string | Blob | File;
  /**
   * Creative prompt
   */
  user_prompt: string;
  /**
   *  Default value: `""`
   */
  thumbnail_text?: string;
  /**
   *  Default value: `"shocked"`
   */
  reaction_type?:
    | "shocked"
    | "surprised"
    | "excited"
    | "happy"
    | "mind_blown"
    | "jaw_dropped"
    | "clickbait_face"
    | "thumbnail_face"
    | "pointing_excited"
    | "hands_up_amazed"
    | "covering_mouth"
    | "facepalm"
    | "poggers"
    | "sus_face"
    | "fire_eyes"
    | "slay_reaction";
  /**
   *  Default value: `"bold_3d"`
   */
  text_style?:
    | "bold_3d"
    | "stroke_outline"
    | "drop_shadow"
    | "neon_glow"
    | "metallic_chrome"
    | "gradient_fill"
    | "glowing_outline"
    | "fire_effect"
    | "rainbow_gradient"
    | "comic_book"
    | "retro_80s"
    | "cyberpunk_neon"
    | "modern_clean"
    | "explosion_burst"
    | "holographic";
  /**
   * Aspect ratio for 4K output (default: 16:9 for YouTube)
   */
  aspect_ratio?: AspectRatio;
};
export type YouTubeThumbnailOutput = {
  /**
   * YouTube thumbnail with reaction and text
   */
  images: Array<Image>;
  /**
   * Total inference time in milliseconds
   */
  inference_time_ms: number;
};
export type YouTubeThumbnailsInput = {
  /**
   * URL of the image to convert to YouTube thumbnail style.
   */
  image_url: string | Blob | File;
  /**
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`
   */
  guidance_scale?: number;
  /**
   * Number of inference steps for sampling. Default value: `30`
   */
  num_inference_steps?: number;
  /**
   * Whether to enable the safety checker for the generated image. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `0.5`
   */
  lora_scale?: number;
  /**
   * The same seed and the same prompt given to the same version of the model will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The text to include in the YouTube thumbnail. Default value: `"Generate youtube thumbnails"`
   */
  prompt?: string;
};
export type YouTubeThumbnailsOutput = {
  /**
   *
   */
  images: Array<Image>;
  /**
   *
   */
  seed: number;
};
export type yueInput = {
  /**
   * The prompt to generate an image from. Must have two sections. Sections start with either [chorus] or a [verse].
   */
  lyrics: string;
  /**
   * The genres (separated by a space ' ') to guide the music generation.
   */
  genres: string;
};
export type yueOutput = {
  /**
   * Generated music file.
   */
  audio: File;
};
export type ZImageBaseInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The guidance scale to use for the image generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The negative prompt to use for the image generation. Default value: `""`
   */
  negative_prompt?: string;
};
export type ZImageBaseLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The guidance scale to use for the image generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The negative prompt to use for the image generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageBaseLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageBaseOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageBaseTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The guidance scale to use for the image generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The negative prompt to use for the image generation. Default value: `""`
   */
  negative_prompt?: string;
};
export type ZImageBaseTextToImageLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `28`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * The guidance scale to use for the image generation. Default value: `4`
   */
  guidance_scale?: number;
  /**
   * The negative prompt to use for the image generation. Default value: `""`
   */
  negative_prompt?: string;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageBaseTrainerInput = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images and corresponding captions.
   *
   * The images should be named: ROOT.EXT. For example: 001.jpg
   *
   * The corresponding captions should be named: ROOT.txt. For example: 001.txt
   *
   * If no text file is provided for an image, the default_caption will be used.
   */
  image_data_url: string | Blob | File;
  /**
   * Number of steps to train for Default value: `2000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Learning rate. Default value: `0.0005`
   */
  learning_rate?: number;
};
export type ZImageBaseTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type ZImageTrainerInput = {
  /**
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   */
  image_data_url: string | Blob | File;
  /**
   * Total number of training steps. Default value: `1000`
   */
  steps?: number;
  /**
   * Learning rate applied to trainable parameters. Default value: `0.0001`
   */
  learning_rate?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Type of training to perform. Use 'content' to focus on the content of the images, 'style' to focus on the style of the images, and 'balanced' to focus on a combination of both. Default value: `"balanced"`
   */
  training_type?: "content" | "style" | "balanced";
};
export type ZImageTrainerOutput = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type ZImageTurboControlnetInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for ControlNet generation.
   */
  image_url: string | Blob | File;
  /**
   * The scale of the controlnet conditioning. Default value: `0.75`
   */
  control_scale?: number;
  /**
   * The start of the controlnet conditioning.
   */
  control_start?: number;
  /**
   * The end of the controlnet conditioning. Default value: `0.8`
   */
  control_end?: number;
  /**
   * What kind of preprocessing to apply to the image, if any. Default value: `"none"`
   */
  preprocess?: "none" | "canny" | "depth" | "pose";
};
export type ZImageTurboControlNetInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for ControlNet generation.
   */
  image_url: string | Blob | File;
  /**
   * The scale of the controlnet conditioning. Default value: `0.75`
   */
  control_scale?: number;
  /**
   * The start of the controlnet conditioning.
   */
  control_start?: number;
  /**
   * The end of the controlnet conditioning. Default value: `0.8`
   */
  control_end?: number;
  /**
   * What kind of preprocessing to apply to the image, if any. Default value: `"none"`
   */
  preprocess?: "none" | "canny" | "depth" | "pose";
};
export type ZImageTurboControlnetLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for ControlNet generation.
   */
  image_url: string | Blob | File;
  /**
   * The scale of the controlnet conditioning. Default value: `0.75`
   */
  control_scale?: number;
  /**
   * The start of the controlnet conditioning.
   */
  control_start?: number;
  /**
   * The end of the controlnet conditioning. Default value: `0.8`
   */
  control_end?: number;
  /**
   * What kind of preprocessing to apply to the image, if any. Default value: `"none"`
   */
  preprocess?: "none" | "canny" | "depth" | "pose";
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageTurboControlNetLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for ControlNet generation.
   */
  image_url: string | Blob | File;
  /**
   * The scale of the controlnet conditioning. Default value: `0.75`
   */
  control_scale?: number;
  /**
   * The start of the controlnet conditioning.
   */
  control_start?: number;
  /**
   * The end of the controlnet conditioning. Default value: `0.8`
   */
  control_end?: number;
  /**
   * What kind of preprocessing to apply to the image, if any. Default value: `"none"`
   */
  preprocess?: "none" | "canny" | "depth" | "pose";
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageTurboControlnetLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageTurboControlnetOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageTurboControlNetOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageTurboImageToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for Image-to-Image generation.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the image-to-image conditioning. Default value: `0.6`
   */
  strength?: number;
};
export type ZImageTurboImageToImageLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for Image-to-Image generation.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the image-to-image conditioning. Default value: `0.6`
   */
  strength?: number;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageTurboImageToImageLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for Image-to-Image generation.
   */
  image_url: string | Blob | File;
  /**
   * The strength of the image-to-image conditioning. Default value: `0.6`
   */
  strength?: number;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageTurboImageToImageLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageTurboImageToImageOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageTurboInpaintInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for Inpaint generation.
   */
  image_url: string | Blob | File;
  /**
   * The scale of the controlnet conditioning. Default value: `0.75`
   */
  control_scale?: number;
  /**
   * The start of the controlnet conditioning.
   */
  control_start?: number;
  /**
   * The end of the controlnet conditioning. Default value: `0.8`
   */
  control_end?: number;
  /**
   * URL of Mask for Inpaint generation.
   */
  mask_image_url: string | Blob | File;
  /**
   * The strength of the inpaint conditioning. Default value: `1`
   */
  strength?: number;
};
export type ZImageTurboInpaintLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for Inpaint generation.
   */
  image_url: string | Blob | File;
  /**
   * The scale of the controlnet conditioning. Default value: `0.75`
   */
  control_scale?: number;
  /**
   * The start of the controlnet conditioning.
   */
  control_start?: number;
  /**
   * The end of the controlnet conditioning. Default value: `0.8`
   */
  control_end?: number;
  /**
   * URL of Mask for Inpaint generation.
   */
  mask_image_url: string | Blob | File;
  /**
   * The strength of the inpaint conditioning. Default value: `1`
   */
  strength?: number;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageTurboInpaintLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `auto`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | "auto";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * URL of Image for Inpaint generation.
   */
  image_url: string | Blob | File;
  /**
   * The scale of the controlnet conditioning. Default value: `0.75`
   */
  control_scale?: number;
  /**
   * The start of the controlnet conditioning.
   */
  control_start?: number;
  /**
   * The end of the controlnet conditioning. Default value: `0.8`
   */
  control_end?: number;
  /**
   * URL of Mask for Inpaint generation.
   */
  mask_image_url: string | Blob | File;
  /**
   * The strength of the inpaint conditioning. Default value: `1`
   */
  strength?: number;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageTurboInpaintLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageTurboInpaintOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageTurboInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
};
export type ZImageTurboLoraInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageTurboLoraOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageTurboOutput = {
  /**
   * The generated image files info.
   */
  images: Array<ImageFile>;
  /**
   * The timings of the generation process.
   */
  timings: any;
  /**
   * Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed.
   */
  seed: number;
  /**
   * Whether the generated images contain NSFW concepts.
   */
  has_nsfw_concepts: Array<boolean>;
  /**
   * The prompt used for generating the image.
   */
  prompt: string;
};
export type ZImageTurboTextToImageInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
};
export type ZImageTurboTextToImageLoRAInput = {
  /**
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * The size of the generated image. Default value: `landscape_4_3`
   */
  image_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * The number of inference steps to perform. Default value: `8`
   */
  num_inference_steps?: number;
  /**
   * The same seed and the same prompt given to the same version of the model
   * will output the same image every time.
   */
  seed?: number;
  /**
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * The number of images to generate. Default value: `1`
   */
  num_images?: number;
  /**
   * If set to true, the safety checker will be enabled. Default value: `true`
   */
  enable_safety_checker?: boolean;
  /**
   * The format of the generated image. Default value: `"png"`
   */
  output_format?: "jpeg" | "png" | "webp";
  /**
   * The acceleration level to use. Default value: `"regular"`
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.
   */
  enable_prompt_expansion?: boolean;
  /**
   * List of LoRA weights to apply (maximum 3).
   */
  loras?: Array<LoRAInput>;
};
export type ZImageTurboTrainerV2Input = {
  /**
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images and corresponding captions.
   *
   * The images should be named: ROOT.EXT. For example: 001.jpg
   *
   * The corresponding captions should be named: ROOT.txt. For example: 001.txt
   *
   * If no text file is provided for an image, the default_caption will be used.
   */
  image_data_url: string | Blob | File;
  /**
   * Number of steps to train for Default value: `2000`
   */
  steps?: number;
  /**
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Learning rate. Default value: `0.0005`
   */
  learning_rate?: number;
};
export type ZImageTurboTrainerV2Output = {
  /**
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: File;
  /**
   * URL to the configuration file for the trained model.
   */
  config_file: File;
};
export type zonosInput = {
  /**
   * The reference audio.
   */
  reference_audio_url: string | Blob | File;
  /**
   * The content generated using cloned voice.
   */
  prompt: string;
};
export type zonosOutput = {
  /**
   * The generated audio
   */
  audio: File;
};
export type EndpointTypeMap = {
  "fal-ai/flux-pro/kontext": {
    input: FluxProKontextInput;
    output: FluxProKontextOutput;
  };
  "fal-ai/imagen4/preview": {
    input: Imagen4PreviewInput;
    output: Imagen4PreviewOutput;
  };
  "fal-ai/wan-effects": {
    input: WanEffectsInput;
    output: WanEffectsOutput;
  };
  "fal-ai/veo2/image-to-video": {
    input: Veo2ImageToVideoInput;
    output: Veo2ImageToVideoOutput;
  };
  "fal-ai/wan-pro/image-to-video": {
    input: WanProImageToVideoInput;
    output: WanProImageToVideoOutput;
  };
  "fal-ai/kling-video/v1.6/pro/image-to-video": {
    input: KlingVideoV16ProImageToVideoInput;
    output: KlingVideoV16ProImageToVideoOutput;
  };
  "fal-ai/flux-pro/v1.1-ultra": {
    input: FluxProV11UltraInput;
    output: FluxProV11UltraOutput;
  };
  "fal-ai/recraft/v3/text-to-image": {
    input: RecraftV3TextToImageInput;
    output: RecraftV3TextToImageOutput;
  };
  "fal-ai/minimax/video-01/image-to-video": {
    input: MinimaxVideo01ImageToVideoInput;
    output: MinimaxVideo01ImageToVideoOutput;
  };
  "fal-ai/flux-2/lora/edit": {
    input: Flux2LoraEditInput;
    output: Flux2LoraEditOutput;
  };
  "fal-ai/flux-2/lora": {
    input: Flux2LoraInput;
    output: Flux2LoraOutput;
  };
  "fal-ai/flux-2/edit": {
    input: Flux2EditInput;
    output: Flux2EditOutput;
  };
  "fal-ai/flux-2": {
    input: Flux2Input;
    output: Flux2Output;
  };
  "fal-ai/flux-2-pro": {
    input: Flux2ProInput;
    output: Flux2ProOutput;
  };
  "fal-ai/flux-2-pro/edit": {
    input: Flux2ProEditInput;
    output: Flux2ProEditOutput;
  };
  "fal-ai/minimax/hailuo-2.3/pro/image-to-video": {
    input: MinimaxHailuo23ProImageToVideoInput;
    output: MinimaxHailuo23ProImageToVideoOutput;
  };
  "fal-ai/wan-25-preview/image-to-video": {
    input: Wan25PreviewImageToVideoInput;
    output: Wan25PreviewImageToVideoOutput;
  };
  "fal-ai/kling-video/v2.5-turbo/pro/image-to-video": {
    input: KlingVideoV25TurboProImageToVideoInput;
    output: KlingVideoV25TurboProImageToVideoOutput;
  };
  "fal-ai/kling-video/v2.5-turbo/pro/text-to-video": {
    input: KlingVideoV25TurboProTextToVideoInput;
    output: KlingVideoV25TurboProTextToVideoOutput;
  };
  "fal-ai/flux-krea-trainer": {
    input: FluxKreaTrainerInput;
    output: FluxKreaTrainerOutput;
  };
  "fal-ai/veo3/fast": {
    input: Veo3FastInput;
    output: Veo3FastOutput;
  };
  "bria/video/background-removal": {
    input: VideoBackgroundRemovalInput;
    output: VideoBackgroundRemovalOutput;
  };
  "fal-ai/flux-kontext-trainer": {
    input: FluxKontextTrainerInput;
    output: FluxKontextTrainerOutput;
  };
  "fal-ai/minimax/hailuo-02/standard/image-to-video": {
    input: MinimaxHailuo02StandardImageToVideoInput;
    output: MinimaxHailuo02StandardImageToVideoOutput;
  };
  "fal-ai/minimax/hailuo-02/standard/text-to-video": {
    input: MinimaxHailuo02StandardTextToVideoInput;
    output: MinimaxHailuo02StandardTextToVideoOutput;
  };
  "bria/text-to-image/3.2": {
    input: TextToImage32Input;
    output: TextToImage32Output;
  };
  "fal-ai/bytedance/seedance/v1/pro/image-to-video": {
    input: BytedanceSeedanceV1ProImageToVideoInput;
    output: BytedanceSeedanceV1ProImageToVideoOutput;
  };
  "fal-ai/imagen4/preview/fast": {
    input: Imagen4PreviewFastInput;
    output: Imagen4PreviewFastOutput;
  };
  "fal-ai/veo3": {
    input: veo3Input;
    output: veo3Output;
  };
  "fal-ai/kling-video/v2.1/master/image-to-video": {
    input: KlingVideoV21MasterImageToVideoInput;
    output: KlingVideoV21MasterImageToVideoOutput;
  };
  "fal-ai/kling-video/v2.1/standard/image-to-video": {
    input: KlingVideoV21StandardImageToVideoInput;
    output: KlingVideoV21StandardImageToVideoOutput;
  };
  "fal-ai/pixverse/v4.5/image-to-video": {
    input: PixverseV45ImageToVideoInput;
    output: PixverseV45ImageToVideoOutput;
  };
  "fal-ai/kling-video/v2/master/image-to-video": {
    input: KlingVideoV2MasterImageToVideoInput;
    output: KlingVideoV2MasterImageToVideoOutput;
  };
  "fal-ai/kling-video/v2/master/text-to-video": {
    input: KlingVideoV2MasterTextToVideoInput;
    output: KlingVideoV2MasterTextToVideoOutput;
  };
  "fal-ai/hidream-i1-full": {
    input: HidreamI1FullInput;
    output: any;
  };
  "fal-ai/hidream-i1-dev": {
    input: HidreamI1DevInput;
    output: HidreamI1DevOutput;
  };
  "fal-ai/hidream-i1-fast": {
    input: HidreamI1FastInput;
    output: HidreamI1FastOutput;
  };
  "fal-ai/flux/dev": {
    input: FluxDevInput;
    output: FluxDevOutput;
  };
  "fal-ai/wan-i2v": {
    input: WanI2vInput;
    output: WanI2vOutput;
  };
  "fal-ai/flux-lora-fast-training": {
    input: FluxLoraFastTrainingInput;
    output: FluxLoraFastTrainingOutput;
  };
  "fal-ai/mmaudio-v2": {
    input: MmaudioV2Input;
    output: MmaudioV2Output;
  };
  "fal-ai/ideogram/v2": {
    input: IdeogramV2Input;
    output: IdeogramV2Output;
  };
  "fal-ai/flux-lora-portrait-trainer": {
    input: FluxLoraPortraitTrainerInput;
    output: FluxLoraPortraitTrainerOutput;
  };
  "fal-ai/stable-diffusion-v35-large": {
    input: StableDiffusionV35LargeInput;
    output: StableDiffusionV35LargeOutput;
  };
  "fal-ai/flux-general": {
    input: FluxGeneralInput;
    output: FluxGeneralOutput;
  };
  "fal-ai/flux-lora": {
    input: FluxLoraInput;
    output: FluxLoraOutput;
  };
  "fal-ai/flux/dev/image-to-image": {
    input: FluxDevImageToImageInput;
    output: FluxDevImageToImageOutput;
  };
  "fal-ai/aura-sr": {
    input: AuraSrInput;
    output: AuraSrOutput;
  };
  "fal-ai/clarity-upscaler": {
    input: ClarityUpscalerInput;
    output: ClarityUpscalerOutput;
  };
  "fal-ai/qwen-image-trainer-v2": {
    input: QwenImageTrainerV2Input;
    output: QwenImageTrainerV2Output;
  };
  "wan/v2.6/reference-to-video/flash": {
    input: V26ReferenceToVideoFlashInput;
    output: V26ReferenceToVideoFlashOutput;
  };
  "fal-ai/bytedance/dreamactor/v2": {
    input: BytedanceDreamactorV2Input;
    output: BytedanceDreamactorV2Output;
  };
  "fal-ai/workflow-utilities/impulse-response": {
    input: WorkflowUtilitiesImpulseResponseInput;
    output: WorkflowUtilitiesImpulseResponseOutput;
  };
  "fal-ai/workflow-utilities/extract-nth-frame": {
    input: WorkflowUtilitiesExtractNthFrameInput;
    output: WorkflowUtilitiesExtractNthFrameOutput;
  };
  "fal-ai/workflow-utilities/blend-video": {
    input: WorkflowUtilitiesBlendVideoInput;
    output: WorkflowUtilitiesBlendVideoOutput;
  };
  "fal-ai/workflow-utilities/audio-compressor": {
    input: WorkflowUtilitiesAudioCompressorInput;
    output: WorkflowUtilitiesAudioCompressorOutput;
  };
  "fal-ai/kling-video/v3/pro/text-to-video": {
    input: KlingVideoV3ProTextToVideoInput;
    output: KlingVideoV3ProTextToVideoOutput;
  };
  "fal-ai/kling-video/o3/pro/image-to-video": {
    input: KlingVideoO3ProImageToVideoInput;
    output: KlingVideoO3ProImageToVideoOutput;
  };
  "fal-ai/kling-video/o3/pro/reference-to-video": {
    input: KlingVideoO3ProReferenceToVideoInput;
    output: KlingVideoO3ProReferenceToVideoOutput;
  };
  "fal-ai/kling-video/o3/pro/text-to-video": {
    input: KlingVideoO3ProTextToVideoInput;
    output: KlingVideoO3ProTextToVideoOutput;
  };
  "fal-ai/kling-video/o3/standard/text-to-video": {
    input: KlingVideoO3StandardTextToVideoInput;
    output: KlingVideoO3StandardTextToVideoOutput;
  };
  "fal-ai/kling-video/o3/standard/reference-to-video": {
    input: KlingVideoO3StandardReferenceToVideoInput;
    output: KlingVideoO3StandardReferenceToVideoOutput;
  };
  "fal-ai/kling-video/v3/standard/text-to-video": {
    input: KlingVideoV3StandardTextToVideoInput;
    output: KlingVideoV3StandardTextToVideoOutput;
  };
  "fal-ai/kling-video/o3/standard/image-to-video": {
    input: KlingVideoO3StandardImageToVideoInput;
    output: KlingVideoO3StandardImageToVideoOutput;
  };
  "fal-ai/kling-video/o3/standard/video-to-video/edit": {
    input: KlingVideoO3StandardVideoToVideoEditInput;
    output: KlingVideoO3StandardVideoToVideoEditOutput;
  };
  "fal-ai/kling-video/o3/pro/video-to-video/edit": {
    input: KlingVideoO3ProVideoToVideoEditInput;
    output: KlingVideoO3ProVideoToVideoEditOutput;
  };
  "fal-ai/kling-video/o3/standard/video-to-video/reference": {
    input: KlingVideoO3StandardVideoToVideoReferenceInput;
    output: KlingVideoO3StandardVideoToVideoReferenceOutput;
  };
  "fal-ai/kling-video/v3/standard/image-to-video": {
    input: KlingVideoV3StandardImageToVideoInput;
    output: KlingVideoV3StandardImageToVideoOutput;
  };
  "fal-ai/kling-video/v3/pro/image-to-video": {
    input: KlingVideoV3ProImageToVideoInput;
    output: KlingVideoV3ProImageToVideoOutput;
  };
  "fal-ai/kling-video/o3/pro/video-to-video/reference": {
    input: KlingVideoO3ProVideoToVideoReferenceInput;
    output: KlingVideoO3ProVideoToVideoReferenceOutput;
  };
  "fal-ai/minimax/speech-2.8-hd": {
    input: MinimaxSpeech28HdInput;
    output: MinimaxSpeech28HdOutput;
  };
  "fal-ai/minimax/speech-2.8-turbo": {
    input: MinimaxSpeech28TurboInput;
    output: MinimaxSpeech28TurboOutput;
  };
  "fal-ai/kling-image/v3/text-to-image": {
    input: KlingImageV3TextToImageInput;
    output: KlingImageV3TextToImageOutput;
  };
  "fal-ai/kling-image/v3/image-to-image": {
    input: KlingImageV3ImageToImageInput;
    output: KlingImageV3ImageToImageOutput;
  };
  "fal-ai/kling-image/o3/image-to-image": {
    input: KlingImageO3ImageToImageInput;
    output: KlingImageO3ImageToImageOutput;
  };
  "fal-ai/kling-image/o3/text-to-image": {
    input: KlingImageO3TextToImageInput;
    output: KlingImageO3TextToImageOutput;
  };
  "fal-ai/vidu/q3/image-to-video": {
    input: ViduQ3ImageToVideoInput;
    output: ViduQ3ImageToVideoOutput;
  };
  "fal-ai/vidu/q3/text-to-video": {
    input: ViduQ3TextToVideoInput;
    output: ViduQ3TextToVideoOutput;
  };
  "fal-ai/hunyuan-3d/v3.1/rapid/text-to-3d": {
    input: Hunyuan3dV31RapidTextTo3dInput;
    output: Hunyuan3dV31RapidTextTo3dOutput;
  };
  "xai/grok-imagine-image/edit": {
    input: GrokImagineImageEditInput;
    output: GrokImagineImageEditOutput;
  };
  "xai/grok-imagine-image": {
    input: GrokImagineImageInput;
    output: GrokImagineImageOutput;
  };
  "xai/grok-imagine-video/edit-video": {
    input: GrokImagineVideoEditVideoInput;
    output: GrokImagineVideoEditVideoOutput;
  };
  "xai/grok-imagine-video/image-to-video": {
    input: GrokImagineVideoImageToVideoInput;
    output: GrokImagineVideoImageToVideoOutput;
  };
  "xai/grok-imagine-video/text-to-video": {
    input: GrokImagineVideoTextToVideoInput;
    output: GrokImagineVideoTextToVideoOutput;
  };
  "fal-ai/hunyuan-image/v3/instruct/edit": {
    input: HunyuanImageV3InstructEditInput;
    output: HunyuanImageV3InstructEditOutput;
  };
  "fal-ai/hunyuan-image/v3/instruct/text-to-image": {
    input: HunyuanImageV3InstructTextToImageInput;
    output: HunyuanImageV3InstructTextToImageOutput;
  };
  "fal-ai/hunyuan-3d/v3.1/smart-topology": {
    input: Hunyuan3dV31SmartTopologyInput;
    output: Hunyuan3dV31SmartTopologyOutput;
  };
  "fal-ai/hunyuan-3d/v3.1/rapid/image-to-3d": {
    input: Hunyuan3dV31RapidImageTo3dInput;
    output: Hunyuan3dV31RapidImageTo3dOutput;
  };
  "fal-ai/hunyuan-3d/v3.1/pro/text-to-3d": {
    input: Hunyuan3dV31ProTextTo3dInput;
    output: Hunyuan3dV31ProTextTo3dOutput;
  };
  "fal-ai/hunyuan-3d/v3.1/pro/image-to-3d": {
    input: Hunyuan3dV31ProImageTo3dInput;
    output: Hunyuan3dV31ProImageTo3dOutput;
  };
  "fal-ai/z-image-base-trainer": {
    input: ZImageBaseTrainerInput;
    output: ZImageBaseTrainerOutput;
  };
  "fal-ai/hunyuan-3d/v3.1/part": {
    input: Hunyuan3dV31PartInput;
    output: Hunyuan3dV31PartOutput;
  };
  "fal-ai/qwen-image-max/text-to-image": {
    input: QwenImageMaxTextToImageInput;
    output: QwenImageMaxTextToImageOutput;
  };
  "fal-ai/qwen-image-max/edit": {
    input: QwenImageMaxEditInput;
    output: QwenImageMaxEditOutput;
  };
  "fal-ai/workflow-utilities/interleave-video": {
    input: WorkflowUtilitiesInterleaveVideoInput;
    output: WorkflowUtilitiesInterleaveVideoOutput;
  };
  "fal-ai/z-image/base/lora": {
    input: ZImageBaseLoraInput;
    output: ZImageBaseLoraOutput;
  };
  "fal-ai/z-image/base": {
    input: ZImageBaseInput;
    output: ZImageBaseOutput;
  };
  "fal-ai/ltx-2-19b/distilled/audio-to-video/lora": {
    input: Ltx219bDistilledAudioToVideoLoraInput;
    output: Ltx219bDistilledAudioToVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/audio-to-video/lora": {
    input: Ltx219bAudioToVideoLoraInput;
    output: Ltx219bAudioToVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/distilled/audio-to-video": {
    input: Ltx219bDistilledAudioToVideoInput;
    output: Ltx219bDistilledAudioToVideoOutput;
  };
  "fal-ai/ltx-2-19b/audio-to-video": {
    input: Ltx219bAudioToVideoInput;
    output: Ltx219bAudioToVideoOutput;
  };
  "bria/replace-background": {
    input: ReplaceBackgroundInput;
    output: ReplaceBackgroundOutput;
  };
  "fal-ai/pixverse/v5.6/transition": {
    input: PixverseV56TransitionInput;
    output: PixverseV56TransitionOutput;
  };
  "fal-ai/pixverse/v5.6/image-to-video": {
    input: PixverseV56ImageToVideoInput;
    output: PixverseV56ImageToVideoOutput;
  };
  "fal-ai/pixverse/v5.6/text-to-video": {
    input: PixverseV56TextToVideoInput;
    output: PixverseV56TextToVideoOutput;
  };
  "fal-ai/qwen-3-tts/voice-design/1.7b": {
    input: Qwen3TtsVoiceDesign17bInput;
    output: Qwen3TtsVoiceDesign17bOutput;
  };
  "fal-ai/qwen-3-tts/text-to-speech/1.7b": {
    input: Qwen3TtsTextToSpeech17bInput;
    output: Qwen3TtsTextToSpeech17bOutput;
  };
  "fal-ai/qwen-3-tts/text-to-speech/0.6b": {
    input: Qwen3TtsTextToSpeech06bInput;
    output: Qwen3TtsTextToSpeech06bOutput;
  };
  "fal-ai/qwen-3-tts/clone-voice/1.7b": {
    input: Qwen3TtsCloneVoice17bInput;
    output: Qwen3TtsCloneVoice17bOutput;
  };
  "fal-ai/qwen-3-tts/clone-voice/0.6b": {
    input: Qwen3TtsCloneVoice06bInput;
    output: Qwen3TtsCloneVoice06bOutput;
  };
  "fal-ai/z-image-turbo-trainer-v2": {
    input: ZImageTurboTrainerV2Input;
    output: ZImageTurboTrainerV2Output;
  };
  "half-moon-ai/ai-face-swap/faceswapvideo": {
    input: AiFaceSwapFaceswapvideoInput;
    output: AiFaceSwapFaceswapvideoOutput;
  };
  "half-moon-ai/ai-face-swap/faceswapimage": {
    input: AiFaceSwapFaceswapimageInput;
    output: AiFaceSwapFaceswapimageOutput;
  };
  "bria/fibo-edit/edit/structured_instruction": {
    input: FiboEditEditStructuredInstructionInput;
    output: any;
  };
  "bria/fibo-edit/replace_object_by_text": {
    input: FiboEditReplaceObjectByTextInput;
    output: FiboEditReplaceObjectByTextOutput;
  };
  "bria/fibo-edit/sketch_to_colored_image": {
    input: FiboEditSketchToColoredImageInput;
    output: FiboEditSketchToColoredImageOutput;
  };
  "bria/fibo-edit/restore": {
    input: FiboEditRestoreInput;
    output: FiboEditRestoreOutput;
  };
  "bria/fibo-edit/reseason": {
    input: FiboEditReseasonInput;
    output: FiboEditReseasonOutput;
  };
  "bria/fibo-edit/relight": {
    input: FiboEditRelightInput;
    output: FiboEditRelightOutput;
  };
  "bria/fibo-edit/restyle": {
    input: FiboEditRestyleInput;
    output: FiboEditRestyleOutput;
  };
  "bria/fibo-edit/rewrite_text": {
    input: FiboEditRewriteTextInput;
    output: FiboEditRewriteTextOutput;
  };
  "bria/fibo-edit/erase_by_text": {
    input: FiboEditEraseByTextInput;
    output: FiboEditEraseByTextOutput;
  };
  "bria/fibo-edit/edit": {
    input: FiboEditEditInput;
    output: FiboEditEditOutput;
  };
  "bria/fibo-edit/add_object_by_text": {
    input: FiboEditAddObjectByTextInput;
    output: FiboEditAddObjectByTextOutput;
  };
  "bria/fibo-edit/blend": {
    input: FiboEditBlendInput;
    output: FiboEditBlendOutput;
  };
  "bria/fibo-edit/colorize": {
    input: FiboEditColorizeInput;
    output: FiboEditColorizeOutput;
  };
  "fal-ai/vidu/q2/reference-to-video/pro": {
    input: ViduQ2ReferenceToVideoProInput;
    output: ViduQ2ReferenceToVideoProOutput;
  };
  "fal-ai/flux-2/klein/9b/base/edit/lora": {
    input: Flux2Klein9bBaseEditLoraInput;
    output: Flux2Klein9bBaseEditLoraOutput;
  };
  "fal-ai/flux-2/klein/9b/base/lora": {
    input: Flux2Klein9bBaseLoraInput;
    output: Flux2Klein9bBaseLoraOutput;
  };
  "fal-ai/flux-2/klein/4b/base/edit/lora": {
    input: Flux2Klein4bBaseEditLoraInput;
    output: Flux2Klein4bBaseEditLoraOutput;
  };
  "fal-ai/flux-2/klein/4b/base/lora": {
    input: Flux2Klein4bBaseLoraInput;
    output: Flux2Klein4bBaseLoraOutput;
  };
  "fal-ai/nemotron/asr/stream": {
    input: NemotronAsrStreamInput;
    output: any;
  };
  "fal-ai/nemotron/asr": {
    input: NemotronAsrInput;
    output: NemotronAsrOutput;
  };
  "bria/fibo-lite/generate/structured_prompt": {
    input: FiboLiteGenerateStructuredPromptInput;
    output: any;
  };
  "bria/fibo-lite/generate/structured_prompt/lite": {
    input: FiboLiteGenerateStructuredPromptLiteInput;
    output: any;
  };
  "wan/v2.6/image-to-video/flash": {
    input: V26ImageToVideoFlashInput;
    output: V26ImageToVideoFlashOutput;
  };
  "fal-ai/flux-2-klein-9b-base-trainer/edit": {
    input: Flux2Klein9bBaseTrainerEditInput;
    output: Flux2Klein9bBaseTrainerEditOutput;
  };
  "fal-ai/flux-2-klein-9b-base-trainer": {
    input: Flux2Klein9bBaseTrainerInput;
    output: Flux2Klein9bBaseTrainerOutput;
  };
  "fal-ai/flux-2-klein-4b-base-trainer": {
    input: Flux2Klein4bBaseTrainerInput;
    output: Flux2Klein4bBaseTrainerOutput;
  };
  "fal-ai/flux-2-klein-4b-base-trainer/edit": {
    input: Flux2Klein4bBaseTrainerEditInput;
    output: Flux2Klein4bBaseTrainerEditOutput;
  };
  "fal-ai/flux-2/klein/4b/base/edit": {
    input: Flux2Klein4bBaseEditInput;
    output: Flux2Klein4bBaseEditOutput;
  };
  "fal-ai/flux-2/klein/9b/base": {
    input: Flux2Klein9bBaseInput;
    output: Flux2Klein9bBaseOutput;
  };
  "fal-ai/flux-2/klein/9b/base/edit": {
    input: Flux2Klein9bBaseEditInput;
    output: Flux2Klein9bBaseEditOutput;
  };
  "fal-ai/flux-2/klein/4b/base": {
    input: Flux2Klein4bBaseInput;
    output: Flux2Klein4bBaseOutput;
  };
  "fal-ai/flux-2/klein/4b/edit": {
    input: Flux2Klein4bEditInput;
    output: Flux2Klein4bEditOutput;
  };
  "fal-ai/flux-2/klein/9b/edit": {
    input: Flux2Klein9bEditInput;
    output: Flux2Klein9bEditOutput;
  };
  "fal-ai/flux-2/klein/9b": {
    input: Flux2Klein9bInput;
    output: Flux2Klein9bOutput;
  };
  "fal-ai/flux-2/klein/4b": {
    input: Flux2Klein4bInput;
    output: Flux2Klein4bOutput;
  };
  "imagineart/imagineart-1.5-pro-preview/text-to-image": {
    input: Imagineart15ProPreviewTextToImageInput;
    output: Imagineart15ProPreviewTextToImageOutput;
  };
  "fal-ai/qwen-image-2512-trainer-v2": {
    input: QwenImage2512TrainerV2Input;
    output: QwenImage2512TrainerV2Output;
  };
  "fal-ai/elevenlabs/voice-changer": {
    input: ElevenlabsVoiceChangerInput;
    output: ElevenlabsVoiceChangerOutput;
  };
  "fal-ai/elevenlabs/dubbing": {
    input: ElevenlabsDubbingInput;
    output: ElevenlabsDubbingOutput;
  };
  "fal-ai/elevenlabs/speech-to-text/scribe-v2": {
    input: ElevenlabsSpeechToTextScribeV2Input;
    output: ElevenlabsSpeechToTextScribeV2Output;
  };
  "fal-ai/glm-image/image-to-image": {
    input: GlmImageImageToImageInput;
    output: GlmImageImageToImageOutput;
  };
  "fal-ai/glm-image": {
    input: GlmImageInput;
    output: GlmImageOutput;
  };
  "openrouter/router/video/enterprise": {
    input: RouterVideoEnterpriseInput;
    output: RouterVideoEnterpriseOutput;
  };
  "openrouter/router/video": {
    input: RouterVideoInput;
    output: RouterVideoOutput;
  };
  "fal-ai/nova-sr": {
    input: NovaSrInput;
    output: NovaSrOutput;
  };
  "fal-ai/flux-2-trainer-v2/edit": {
    input: Flux2TrainerV2EditInput;
    output: Flux2TrainerV2EditOutput;
  };
  "fal-ai/flux-2-trainer-v2": {
    input: Flux2TrainerV2Input;
    output: Flux2TrainerV2Output;
  };
  "fal-ai/longcat-multi-avatar/image-audio-to-video": {
    input: LongcatMultiAvatarImageAudioToVideoInput;
    output: LongcatMultiAvatarImageAudioToVideoOutput;
  };
  "fal-ai/silero-vad": {
    input: SileroVadInput;
    output: SileroVadOutput;
  };
  "fal-ai/deepfilternet3": {
    input: deepfilternet3Input;
    output: deepfilternet3Output;
  };
  "fal-ai/ltx2-v2v-trainer": {
    input: Ltx2V2vTrainerInput;
    output: Ltx2V2vTrainerOutput;
  };
  "fal-ai/qwen-image-edit-2511-multiple-angles": {
    input: QwenImageEdit2511MultipleAnglesInput;
    output: QwenImageEdit2511MultipleAnglesOutput;
  };
  "fal-ai/ltx-2-19b/distilled/video-to-video/lora": {
    input: Ltx219bDistilledVideoToVideoLoraInput;
    output: Ltx219bDistilledVideoToVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/distilled/video-to-video": {
    input: Ltx219bDistilledVideoToVideoInput;
    output: Ltx219bDistilledVideoToVideoOutput;
  };
  "fal-ai/ltx-2-19b/video-to-video/lora": {
    input: Ltx219bVideoToVideoLoraInput;
    output: Ltx219bVideoToVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/video-to-video": {
    input: Ltx219bVideoToVideoInput;
    output: Ltx219bVideoToVideoOutput;
  };
  "fal-ai/ultrashape": {
    input: ultrashapeInput;
    output: ultrashapeOutput;
  };
  "fal-ai/ltx-2-19b/distilled/extend-video/lora": {
    input: Ltx219bDistilledExtendVideoLoraInput;
    output: Ltx219bDistilledExtendVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/distilled/extend-video": {
    input: Ltx219bDistilledExtendVideoInput;
    output: Ltx219bDistilledExtendVideoOutput;
  };
  "fal-ai/ltx-2-19b/distilled/image-to-video/lora": {
    input: Ltx219bDistilledImageToVideoLoraInput;
    output: Ltx219bDistilledImageToVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/distilled/image-to-video": {
    input: Ltx219bDistilledImageToVideoInput;
    output: Ltx219bDistilledImageToVideoOutput;
  };
  "fal-ai/ltx-2-19b/distilled/text-to-video/lora": {
    input: Ltx219bDistilledTextToVideoLoraInput;
    output: Ltx219bDistilledTextToVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/distilled/text-to-video": {
    input: Ltx219bDistilledTextToVideoInput;
    output: Ltx219bDistilledTextToVideoOutput;
  };
  "fal-ai/ltx-2-19b/extend-video/lora": {
    input: Ltx219bExtendVideoLoraInput;
    output: Ltx219bExtendVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/text-to-video/lora": {
    input: Ltx219bTextToVideoLoraInput;
    output: Ltx219bTextToVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/image-to-video/lora": {
    input: Ltx219bImageToVideoLoraInput;
    output: Ltx219bImageToVideoLoraOutput;
  };
  "fal-ai/ltx-2-19b/extend-video": {
    input: Ltx219bExtendVideoInput;
    output: Ltx219bExtendVideoOutput;
  };
  "fal-ai/ltx-2-19b/text-to-video": {
    input: Ltx219bTextToVideoInput;
    output: Ltx219bTextToVideoOutput;
  };
  "fal-ai/ltx-2-19b/image-to-video": {
    input: Ltx219bImageToVideoInput;
    output: Ltx219bImageToVideoOutput;
  };
  "fal-ai/ltx2-video-trainer": {
    input: Ltx2VideoTrainerInput;
    output: Ltx2VideoTrainerOutput;
  };
  "fal-ai/qwen-image-2512/lora": {
    input: QwenImage2512LoraInput;
    output: QwenImage2512LoraOutput;
  };
  "fal-ai/qwen-image-2512-trainer": {
    input: QwenImage2512TrainerInput;
    output: QwenImage2512TrainerOutput;
  };
  "fal-ai/qwen-image-edit-2511/lora": {
    input: QwenImageEdit2511LoraInput;
    output: QwenImageEdit2511LoraOutput;
  };
  "fal-ai/qwen-image-2512": {
    input: QwenImage2512Input;
    output: QwenImage2512Output;
  };
  "fal-ai/longcat-multi-avatar/image-audio-to-video/multi-speaker": {
    input: any;
    output: any;
  };
  "fal-ai/longcat-single-avatar/image-audio-to-video": {
    input: LongcatSingleAvatarImageAudioToVideoInput;
    output: LongcatSingleAvatarImageAudioToVideoOutput;
  };
  "fal-ai/longcat-single-avatar/audio-to-video": {
    input: LongcatSingleAvatarAudioToVideoInput;
    output: LongcatSingleAvatarAudioToVideoOutput;
  };
  "fal-ai/sam-audio/separate": {
    input: SamAudioSeparateInput;
    output: SamAudioSeparateOutput;
  };
  "fal-ai/sam-audio/span-separate": {
    input: SamAudioSpanSeparateInput;
    output: SamAudioSpanSeparateOutput;
  };
  "fal-ai/sam-audio/visual-separate": {
    input: SamAudioVisualSeparateInput;
    output: SamAudioVisualSeparateOutput;
  };
  "half-moon-ai/ai-home/style": {
    input: AiHomeStyleInput;
    output: AiHomeStyleOutput;
  };
  "half-moon-ai/ai-home/edit": {
    input: AiHomeEditInput;
    output: AiHomeEditOutput;
  };
  "fal-ai/hunyuan-motion/fast": {
    input: HunyuanMotionFastInput;
    output: HunyuanMotionFastOutput;
  };
  "fal-ai/hunyuan-motion": {
    input: HunyuanMotionInput;
    output: HunyuanMotionOutput;
  };
  "fal-ai/arbiter/image/text": {
    input: ArbiterImageTextInput;
    output: ArbiterImageTextOutput;
  };
  "fal-ai/arbiter/image/image": {
    input: ArbiterImageImageInput;
    output: ArbiterImageImageOutput;
  };
  "fal-ai/arbiter/image": {
    input: ArbiterImageInput;
    output: ArbiterImageOutput;
  };
  "fal-ai/wan-move": {
    input: WanMoveInput;
    output: WanMoveOutput;
  };
  "fal-ai/qwen-image-layered/lora": {
    input: QwenImageLayeredLoraInput;
    output: QwenImageLayeredLoraOutput;
  };
  "fal-ai/ffmpeg-api/merge-audios": {
    input: FfmpegApiMergeAudiosInput;
    output: FfmpegApiMergeAudiosOutput;
  };
  "wan/v2.6/text-to-image": {
    input: V26TextToImageInput;
    output: V26TextToImageOutput;
  };
  "wan/v2.6/image-to-image": {
    input: V26ImageToImageInput;
    output: V26ImageToImageOutput;
  };
  "bria/video/erase/keypoints": {
    input: VideoEraseKeypointsInput;
    output: VideoEraseKeypointsOutput;
  };
  "bria/video/erase/prompt": {
    input: VideoErasePromptInput;
    output: VideoErasePromptOutput;
  };
  "bria/video/erase/mask": {
    input: VideoEraseMaskInput;
    output: VideoEraseMaskOutput;
  };
  "fal-ai/qwen-image-edit-2511-trainer": {
    input: QwenImageEdit2511TrainerInput;
    output: QwenImageEdit2511TrainerOutput;
  };
  "fal-ai/kandinsky5-pro/image-to-video": {
    input: Kandinsky5ProImageToVideoInput;
    output: Kandinsky5ProImageToVideoOutput;
  };
  "fal-ai/kandinsky5-pro/text-to-video": {
    input: Kandinsky5ProTextToVideoInput;
    output: Kandinsky5ProTextToVideoOutput;
  };
  "fal-ai/bytedance/seedance/v1.5/pro/text-to-video": {
    input: BytedanceSeedanceV15ProTextToVideoInput;
    output: BytedanceSeedanceV15ProTextToVideoOutput;
  };
  "fal-ai/bytedance/seedance/v1.5/pro/image-to-video": {
    input: BytedanceSeedanceV15ProImageToVideoInput;
    output: BytedanceSeedanceV15ProImageToVideoOutput;
  };
  "fal-ai/qwen-image-layered-trainer": {
    input: QwenImageLayeredTrainerInput;
    output: QwenImageLayeredTrainerOutput;
  };
  "fal-ai/live-avatar": {
    input: LiveAvatarInput;
    output: LiveAvatarOutput;
  };
  "openrouter/router/audio": {
    input: RouterAudioInput;
    output: RouterAudioOutput;
  };
  "fal-ai/elevenlabs/music": {
    input: ElevenlabsMusicInput;
    output: ElevenlabsMusicOutput;
  };
  "fal-ai/lightx/relight": {
    input: LightxRelightInput;
    output: LightxRelightOutput;
  };
  "fal-ai/lightx/recamera": {
    input: LightxRecameraInput;
    output: LightxRecameraOutput;
  };
  "fal-ai/kling-video/v2.6/standard/motion-control": {
    input: KlingVideoV26StandardMotionControlInput;
    output: KlingVideoV26StandardMotionControlOutput;
  };
  "fal-ai/kling-video/v2.6/pro/motion-control": {
    input: KlingVideoV26ProMotionControlInput;
    output: KlingVideoV26ProMotionControlOutput;
  };
  "fal-ai/qwen-image-edit-2511": {
    input: QwenImageEdit2511Input;
    output: QwenImageEdit2511Output;
  };
  "fal-ai/qwen-image-layered": {
    input: QwenImageLayeredInput;
    output: QwenImageLayeredOutput;
  };
  "decart/lucy-restyle": {
    input: LucyRestyleInput;
    output: LucyRestyleOutput;
  };
  "fal-ai/z-image/turbo/inpaint/lora": {
    input: ZImageTurboInpaintLoraInput;
    output: ZImageTurboInpaintLoraOutput;
  };
  "fal-ai/z-image/turbo/inpaint": {
    input: ZImageTurboInpaintInput;
    output: ZImageTurboInpaintOutput;
  };
  "fal-ai/trellis-2": {
    input: Trellis2Input;
    output: Trellis2Output;
  };
  "fal-ai/scail": {
    input: scailInput;
    output: scailOutput;
  };
  "clarityai/crystal-video-upscaler": {
    input: CrystalVideoUpscalerInput;
    output: CrystalVideoUpscalerOutput;
  };
  "fal-ai/vibevoice/0.5b": {
    input: Vibevoice05bInput;
    output: Vibevoice05bOutput;
  };
  "bria/bria_video_eraser/erase/mask": {
    input: BriaVideoEraserEraseMaskInput;
    output: BriaVideoEraserEraseMaskOutput;
  };
  "bria/bria_video_eraser/erase/keypoints": {
    input: BriaVideoEraserEraseKeypointsInput;
    output: BriaVideoEraserEraseKeypointsOutput;
  };
  "bria/bria_video_eraser/erase/prompt": {
    input: BriaVideoEraserErasePromptInput;
    output: BriaVideoEraserErasePromptOutput;
  };
  "fal-ai/hunyuan-video-v1.5/image-to-video": {
    input: HunyuanVideoV15ImageToVideoInput;
    output: HunyuanVideoV15ImageToVideoOutput;
  };
  "fal-ai/hunyuan3d-v3/text-to-3d": {
    input: Hunyuan3dV3TextTo3dInput;
    output: Hunyuan3dV3TextTo3dOutput;
  };
  "fal-ai/hunyuan3d-v3/sketch-to-3d": {
    input: Hunyuan3dV3SketchTo3dInput;
    output: Hunyuan3dV3SketchTo3dOutput;
  };
  "fal-ai/hunyuan3d-v3/image-to-3d": {
    input: Hunyuan3dV3ImageTo3dInput;
    output: Hunyuan3dV3ImageTo3dOutput;
  };
  "fal-ai/flux-2/flash/edit": {
    input: Flux2FlashEditInput;
    output: Flux2FlashEditOutput;
  };
  "fal-ai/flux-2/flash": {
    input: Flux2FlashInput;
    output: Flux2FlashOutput;
  };
  "fal-ai/gpt-image-1.5/edit": {
    input: GptImage15EditInput;
    output: GptImage15EditOutput;
  };
  "fal-ai/gpt-image-1.5": {
    input: GptImage15Input;
    output: GptImage15Output;
  };
  "fal-ai/kling-video/create-voice": {
    input: KlingVideoCreateVoiceInput;
    output: KlingVideoCreateVoiceOutput;
  };
  "bria/fibo-lite/generate": {
    input: FiboLiteGenerateInput;
    output: FiboLiteGenerateOutput;
  };
  "fal-ai/flux-2/turbo/edit": {
    input: Flux2TurboEditInput;
    output: Flux2TurboEditOutput;
  };
  "fal-ai/flux-2/turbo": {
    input: Flux2TurboInput;
    output: Flux2TurboOutput;
  };
  "fal-ai/flux-2-max": {
    input: Flux2MaxInput;
    output: Flux2MaxOutput;
  };
  "fal-ai/flux-2-max/edit": {
    input: Flux2MaxEditInput;
    output: Flux2MaxEditOutput;
  };
  "half-moon-ai/ai-baby-and-aging-generator/multi": {
    input: AiBabyAndAgingGeneratorMultiInput;
    output: AiBabyAndAgingGeneratorMultiOutput;
  };
  "half-moon-ai/ai-baby-and-aging-generator/single": {
    input: AiBabyAndAgingGeneratorSingleInput;
    output: AiBabyAndAgingGeneratorSingleOutput;
  };
  "half-moon-ai/ai-detector/detect-image": {
    input: AiDetectorDetectImageInput;
    output: AiDetectorDetectImageOutput;
  };
  "half-moon-ai/ai-detector/detect-text": {
    input: AiDetectorDetectTextInput;
    output: AiDetectorDetectTextOutput;
  };
  "wan/v2.6/text-to-video": {
    input: V26TextToVideoInput;
    output: V26TextToVideoOutput;
  };
  "wan/v2.6/reference-to-video": {
    input: V26ReferenceToVideoInput;
    output: V26ReferenceToVideoOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/shirt-design": {
    input: QwenImageEdit2509LoraGalleryShirtDesignInput;
    output: QwenImageEdit2509LoraGalleryShirtDesignOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/remove-lighting": {
    input: QwenImageEdit2509LoraGalleryRemoveLightingInput;
    output: QwenImageEdit2509LoraGalleryRemoveLightingOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/remove-element": {
    input: QwenImageEdit2509LoraGalleryRemoveElementInput;
    output: QwenImageEdit2509LoraGalleryRemoveElementOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/lighting-restoration": {
    input: QwenImageEdit2509LoraGalleryLightingRestorationInput;
    output: QwenImageEdit2509LoraGalleryLightingRestorationOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/integrate-product": {
    input: QwenImageEdit2509LoraGalleryIntegrateProductInput;
    output: QwenImageEdit2509LoraGalleryIntegrateProductOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/group-photo": {
    input: QwenImageEdit2509LoraGalleryGroupPhotoInput;
    output: QwenImageEdit2509LoraGalleryGroupPhotoOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/face-to-full-portrait": {
    input: QwenImageEdit2509LoraGalleryFaceToFullPortraitInput;
    output: QwenImageEdit2509LoraGalleryFaceToFullPortraitOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/add-background": {
    input: QwenImageEdit2509LoraGalleryAddBackgroundInput;
    output: QwenImageEdit2509LoraGalleryAddBackgroundOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/next-scene": {
    input: QwenImageEdit2509LoraGalleryNextSceneInput;
    output: QwenImageEdit2509LoraGalleryNextSceneOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora-gallery/multiple-angles": {
    input: QwenImageEdit2509LoraGalleryMultipleAnglesInput;
    output: QwenImageEdit2509LoraGalleryMultipleAnglesOutput;
  };
  "fal-ai/veo3.1/fast/extend-video": {
    input: Veo31FastExtendVideoInput;
    output: Veo31FastExtendVideoOutput;
  };
  "fal-ai/veo3.1/extend-video": {
    input: Veo31ExtendVideoInput;
    output: Veo31ExtendVideoOutput;
  };
  "fal-ai/qwen-image-edit-2509-lora": {
    input: QwenImageEdit2509LoraInput;
    output: QwenImageEdit2509LoraOutput;
  };
  "fal-ai/qwen-image-edit-2509-trainer": {
    input: QwenImageEdit2509TrainerInput;
    output: QwenImageEdit2509TrainerOutput;
  };
  "fal-ai/qwen-image-edit-2509": {
    input: QwenImageEdit2509Input;
    output: QwenImageEdit2509Output;
  };
  "wan/v2.6/image-to-video": {
    input: V26ImageToVideoInput;
    output: V26ImageToVideoOutput;
  };
  "fal-ai/kling-video/o1/standard/reference-to-video": {
    input: KlingVideoO1StandardReferenceToVideoInput;
    output: KlingVideoO1StandardReferenceToVideoOutput;
  };
  "fal-ai/kling-video/o1/standard/image-to-video": {
    input: KlingVideoO1StandardImageToVideoInput;
    output: KlingVideoO1StandardImageToVideoOutput;
  };
  "fal-ai/kling-video/o1/standard/video-to-video/reference": {
    input: KlingVideoO1StandardVideoToVideoReferenceInput;
    output: KlingVideoO1StandardVideoToVideoReferenceOutput;
  };
  "fal-ai/kling-video/o1/standard/video-to-video/edit": {
    input: KlingVideoO1StandardVideoToVideoEditInput;
    output: KlingVideoO1StandardVideoToVideoEditOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/lighting-restoration": {
    input: QwenImageEditPlusLoraGalleryLightingRestorationInput;
    output: QwenImageEditPlusLoraGalleryLightingRestorationOutput;
  };
  "fal-ai/maya/batch": {
    input: MayaBatchInput;
    output: MayaBatchOutput;
  };
  "fal-ai/maya/stream": {
    input: MayaStreamInput;
    output: any;
  };
  "fal-ai/moondream3-preview/segment": {
    input: Moondream3PreviewSegmentInput;
    output: Moondream3PreviewSegmentOutput;
  };
  "veed/fabric-1.0/text": {
    input: Fabric10TextInput;
    output: Fabric10TextOutput;
  };
  "fal-ai/steady-dancer": {
    input: SteadyDancerInput;
    output: SteadyDancerOutput;
  };
  "fal-ai/one-to-all-animation/1.3b": {
    input: OneToAllAnimation13bInput;
    output: OneToAllAnimation13bOutput;
  };
  "fal-ai/one-to-all-animation/14b": {
    input: OneToAllAnimation14bInput;
    output: OneToAllAnimation14bOutput;
  };
  "fal-ai/creatify/aurora": {
    input: CreatifyAuroraInput;
    output: CreatifyAuroraOutput;
  };
  "fal-ai/wan-vision-enhancer": {
    input: WanVisionEnhancerInput;
    output: WanVisionEnhancerOutput;
  };
  "fal-ai/sync-lipsync/react-1": {
    input: SyncLipsyncReact1Input;
    output: SyncLipsyncReact1Output;
  };
  "fal-ai/stepx-edit2": {
    input: StepxEdit2Input;
    output: StepxEdit2Output;
  };
  "fal-ai/z-image/turbo/controlnet/lora": {
    input: ZImageTurboControlnetLoraInput;
    output: ZImageTurboControlnetLoraOutput;
  };
  "fal-ai/z-image/turbo/controlnet": {
    input: ZImageTurboControlnetInput;
    output: ZImageTurboControlnetOutput;
  };
  "fal-ai/z-image/turbo/image-to-image/lora": {
    input: ZImageTurboImageToImageLoraInput;
    output: ZImageTurboImageToImageLoraOutput;
  };
  "fal-ai/z-image/turbo/image-to-image": {
    input: ZImageTurboImageToImageInput;
    output: ZImageTurboImageToImageOutput;
  };
  "fal-ai/longcat-image/edit": {
    input: LongcatImageEditInput;
    output: LongcatImageEditOutput;
  };
  "fal-ai/longcat-image": {
    input: LongcatImageInput;
    output: LongcatImageOutput;
  };
  "fal-ai/kling-video/ai-avatar/v2/pro": {
    input: KlingVideoAiAvatarV2ProInput;
    output: KlingVideoAiAvatarV2ProOutput;
  };
  "fal-ai/kling-video/ai-avatar/v2/standard": {
    input: KlingVideoAiAvatarV2StandardInput;
    output: KlingVideoAiAvatarV2StandardOutput;
  };
  "fal-ai/z-image-trainer": {
    input: ZImageTrainerInput;
    output: ZImageTrainerOutput;
  };
  "fal-ai/bytedance/seedream/v4.5/text-to-image": {
    input: BytedanceSeedreamV45TextToImageInput;
    output: BytedanceSeedreamV45TextToImageOutput;
  };
  "fal-ai/bytedance/seedream/v4.5/edit": {
    input: BytedanceSeedreamV45EditInput;
    output: BytedanceSeedreamV45EditOutput;
  };
  "fal-ai/sam-3/3d-align": {
    input: Sam33dAlignInput;
    output: Sam33dAlignOutput;
  };
  "fal-ai/sam-3/3d-body": {
    input: Sam33dBodyInput;
    output: Sam33dBodyOutput;
  };
  "fal-ai/sam-3/3d-objects": {
    input: Sam33dObjectsInput;
    output: Sam33dObjectsOutput;
  };
  "fal-ai/vidu/q2/reference-to-image": {
    input: ViduQ2ReferenceToImageInput;
    output: ViduQ2ReferenceToImageOutput;
  };
  "fal-ai/vidu/q2/text-to-image": {
    input: ViduQ2TextToImageInput;
    output: ViduQ2TextToImageOutput;
  };
  "fal-ai/kling-video/v2.6/pro/text-to-video": {
    input: KlingVideoV26ProTextToVideoInput;
    output: KlingVideoV26ProTextToVideoOutput;
  };
  "fal-ai/kling-video/v2.6/pro/image-to-video": {
    input: KlingVideoV26ProImageToVideoInput;
    output: KlingVideoV26ProImageToVideoOutput;
  };
  "fal-ai/pixverse/v5.5/effects": {
    input: PixverseV55EffectsInput;
    output: PixverseV55EffectsOutput;
  };
  "fal-ai/pixverse/v5.5/transition": {
    input: PixverseV55TransitionInput;
    output: PixverseV55TransitionOutput;
  };
  "fal-ai/z-image/turbo/lora": {
    input: ZImageTurboLoraInput;
    output: ZImageTurboLoraOutput;
  };
  "fal-ai/pixverse/v5.5/image-to-video": {
    input: PixverseV55ImageToVideoInput;
    output: PixverseV55ImageToVideoOutput;
  };
  "fal-ai/pixverse/v5.5/text-to-video": {
    input: PixverseV55TextToVideoInput;
    output: PixverseV55TextToVideoOutput;
  };
  "veed/video-background-removal/fast": {
    input: VideoBackgroundRemovalFastInput;
    output: VideoBackgroundRemovalFastOutput;
  };
  "fal-ai/kling-video/o1/image-to-video": {
    input: KlingVideoO1ImageToVideoInput;
    output: KlingVideoO1ImageToVideoOutput;
  };
  "fal-ai/kling-video/o1/reference-to-video": {
    input: KlingVideoO1ReferenceToVideoInput;
    output: KlingVideoO1ReferenceToVideoOutput;
  };
  "fal-ai/kling-video/o1/video-to-video/edit": {
    input: KlingVideoO1VideoToVideoEditInput;
    output: KlingVideoO1VideoToVideoEditOutput;
  };
  "fal-ai/kling-video/o1/video-to-video/reference": {
    input: KlingVideoO1VideoToVideoReferenceInput;
    output: KlingVideoO1VideoToVideoReferenceOutput;
  };
  "fal-ai/kling-image/o1": {
    input: KlingImageO1Input;
    output: KlingImageO1Output;
  };
  "fal-ai/ovis-image": {
    input: OvisImageInput;
    output: OvisImageOutput;
  };
  "veed/video-background-removal": {
    input: VideoBackgroundRemovalInput;
    output: VideoBackgroundRemovalOutput;
  };
  "veed/video-background-removal/green-screen": {
    input: VideoBackgroundRemovalGreenScreenInput;
    output: VideoBackgroundRemovalGreenScreenOutput;
  };
  "fal-ai/ltx-2/text-to-video/fast": {
    input: Ltx2TextToVideoFastInput;
    output: Ltx2TextToVideoFastOutput;
  };
  "fal-ai/ltx-2/text-to-video": {
    input: Ltx2TextToVideoInput;
    output: Ltx2TextToVideoOutput;
  };
  "fal-ai/ltx-2/image-to-video/fast": {
    input: Ltx2ImageToVideoFastInput;
    output: Ltx2ImageToVideoFastOutput;
  };
  "fal-ai/ltx-2/image-to-video": {
    input: Ltx2ImageToVideoInput;
    output: Ltx2ImageToVideoOutput;
  };
  "fal-ai/ltx-2/retake-video": {
    input: Ltx2RetakeVideoInput;
    output: Ltx2RetakeVideoOutput;
  };
  "fal-ai/z-image/turbo": {
    input: ZImageTurboInput;
    output: ZImageTurboOutput;
  };
  "decart/lucy-edit/fast": {
    input: LucyEditFastInput;
    output: LucyEditFastOutput;
  };
  "fal-ai/flux-2-lora-gallery/sepia-vintage": {
    input: Flux2LoraGallerySepiaVintageInput;
    output: Flux2LoraGallerySepiaVintageOutput;
  };
  "fal-ai/flux-2-lora-gallery/virtual-tryon": {
    input: Flux2LoraGalleryVirtualTryonInput;
    output: Flux2LoraGalleryVirtualTryonOutput;
  };
  "fal-ai/flux-2-lora-gallery/satellite-view-style": {
    input: Flux2LoraGallerySatelliteViewStyleInput;
    output: Flux2LoraGallerySatelliteViewStyleOutput;
  };
  "fal-ai/flux-2-lora-gallery/realism": {
    input: Flux2LoraGalleryRealismInput;
    output: Flux2LoraGalleryRealismOutput;
  };
  "fal-ai/flux-2-lora-gallery/multiple-angles": {
    input: Flux2LoraGalleryMultipleAnglesInput;
    output: Flux2LoraGalleryMultipleAnglesOutput;
  };
  "fal-ai/flux-2-lora-gallery/hdr-style": {
    input: Flux2LoraGalleryHdrStyleInput;
    output: Flux2LoraGalleryHdrStyleOutput;
  };
  "fal-ai/flux-2-lora-gallery/face-to-full-portrait": {
    input: Flux2LoraGalleryFaceToFullPortraitInput;
    output: Flux2LoraGalleryFaceToFullPortraitOutput;
  };
  "fal-ai/flux-2-lora-gallery/digital-comic-art": {
    input: Flux2LoraGalleryDigitalComicArtInput;
    output: Flux2LoraGalleryDigitalComicArtOutput;
  };
  "fal-ai/flux-2-lora-gallery/ballpoint-pen-sketch": {
    input: Flux2LoraGalleryBallpointPenSketchInput;
    output: Flux2LoraGalleryBallpointPenSketchOutput;
  };
  "fal-ai/flux-2-lora-gallery/apartment-staging": {
    input: Flux2LoraGalleryApartmentStagingInput;
    output: Flux2LoraGalleryApartmentStagingOutput;
  };
  "fal-ai/flux-2-lora-gallery/add-background": {
    input: Flux2LoraGalleryAddBackgroundInput;
    output: Flux2LoraGalleryAddBackgroundOutput;
  };
  "clarityai/crystal-upscaler": {
    input: CrystalUpscalerInput;
    output: CrystalUpscalerOutput;
  };
  "fal-ai/flux-2-trainer/edit": {
    input: Flux2TrainerEditInput;
    output: Flux2TrainerEditOutput;
  };
  "fal-ai/flux-2-trainer": {
    input: Flux2TrainerInput;
    output: Flux2TrainerOutput;
  };
  "fal-ai/flux-2-flex/edit": {
    input: Flux2FlexEditInput;
    output: Flux2FlexEditOutput;
  };
  "fal-ai/flux-2-flex": {
    input: Flux2FlexInput;
    output: Flux2FlexOutput;
  };
  "fal-ai/chrono-edit-lora": {
    input: ChronoEditLoraInput;
    output: ChronoEditLoraOutput;
  };
  "fal-ai/chrono-edit-lora-gallery/paintbrush": {
    input: ChronoEditLoraGalleryPaintbrushInput;
    output: ChronoEditLoraGalleryPaintbrushOutput;
  };
  "fal-ai/chrono-edit-lora-gallery/upscaler": {
    input: ChronoEditLoraGalleryUpscalerInput;
    output: ChronoEditLoraGalleryUpscalerOutput;
  };
  "fal-ai/hunyuan-video-v1.5/text-to-video": {
    input: HunyuanVideoV15TextToVideoInput;
    output: HunyuanVideoV15TextToVideoOutput;
  };
  "fal-ai/sam-3/image-rle": {
    input: Sam3ImageRleInput;
    output: Sam3ImageRleOutput;
  };
  "fal-ai/sam-3/image/embed": {
    input: Sam3ImageEmbedInput;
    output: Sam3ImageEmbedOutput;
  };
  "fal-ai/sam-3/video-rle": {
    input: Sam3VideoRleInput;
    output: Sam3VideoRleOutput;
  };
  "fal-ai/sam-3/video": {
    input: Sam3VideoInput;
    output: Sam3VideoOutput;
  };
  "fal-ai/sam-3/image": {
    input: Sam3ImageInput;
    output: Sam3ImageOutput;
  };
  "fal-ai/gemini-3-pro-image-preview/edit": {
    input: Gemini3ProImagePreviewEditInput;
    output: Gemini3ProImagePreviewEditOutput;
  };
  "fal-ai/gemini-3-pro-image-preview": {
    input: Gemini3ProImagePreviewInput;
    output: Gemini3ProImagePreviewOutput;
  };
  "fal-ai/nano-banana-pro/edit": {
    input: NanoBananaProEditInput;
    output: NanoBananaProEditOutput;
  };
  "fal-ai/nano-banana-pro": {
    input: NanoBananaProInput;
    output: NanoBananaProOutput;
  };
  "imagineart/imagineart-1.5-preview/text-to-image": {
    input: Imagineart15PreviewTextToImageInput;
    output: Imagineart15PreviewTextToImageOutput;
  };
  "bytedance/lynx": {
    input: lynxInput;
    output: lynxOutput;
  };
  "fal-ai/maya": {
    input: mayaInput;
    output: mayaOutput;
  };
  "openrouter/router/openai/v1/responses": {
    input: any;
    output: any;
  };
  "openrouter/router/openai/v1/embeddings": {
    input: any;
    output: any;
  };
  "openrouter/router/vision": {
    input: RouterVisionInput;
    output: RouterVisionOutput;
  };
  "openrouter/router": {
    input: routerInput;
    output: routerOutput;
  };
  "openrouter/router/openai/v1/chat/completions": {
    input: any;
    output: any;
  };
  "fal-ai/editto": {
    input: edittoInput;
    output: edittoOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/multiple-angles": {
    input: QwenImageEditPlusLoraGalleryMultipleAnglesInput;
    output: QwenImageEditPlusLoraGalleryMultipleAnglesOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/shirt-design": {
    input: QwenImageEditPlusLoraGalleryShirtDesignInput;
    output: QwenImageEditPlusLoraGalleryShirtDesignOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/remove-lighting": {
    input: QwenImageEditPlusLoraGalleryRemoveLightingInput;
    output: QwenImageEditPlusLoraGalleryRemoveLightingOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/remove-element": {
    input: QwenImageEditPlusLoraGalleryRemoveElementInput;
    output: QwenImageEditPlusLoraGalleryRemoveElementOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/next-scene": {
    input: QwenImageEditPlusLoraGalleryNextSceneInput;
    output: QwenImageEditPlusLoraGalleryNextSceneOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/integrate-product": {
    input: QwenImageEditPlusLoraGalleryIntegrateProductInput;
    output: QwenImageEditPlusLoraGalleryIntegrateProductOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/group-photo": {
    input: QwenImageEditPlusLoraGalleryGroupPhotoInput;
    output: QwenImageEditPlusLoraGalleryGroupPhotoOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/face-to-full-portrait": {
    input: QwenImageEditPlusLoraGalleryFaceToFullPortraitInput;
    output: QwenImageEditPlusLoraGalleryFaceToFullPortraitOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora-gallery/add-background": {
    input: QwenImageEditPlusLoraGalleryAddBackgroundInput;
    output: QwenImageEditPlusLoraGalleryAddBackgroundOutput;
  };
  "fal-ai/flashvsr/upscale/video": {
    input: FlashvsrUpscaleVideoInput;
    output: FlashvsrUpscaleVideoOutput;
  };
  "fal-ai/pixverse/swap": {
    input: PixverseSwapInput;
    output: PixverseSwapOutput;
  };
  "fal-ai/pika/v2.2/pikaframes": {
    input: PikaV22PikaframesInput;
    output: PikaV22PikaframesOutput;
  };
  "fal-ai/infinity-star/text-to-video": {
    input: InfinityStarTextToVideoInput;
    output: InfinityStarTextToVideoOutput;
  };
  "fal-ai/sana-video": {
    input: SanaVideoInput;
    output: SanaVideoOutput;
  };
  "fal-ai/workflow-utilities/auto-subtitle": {
    input: WorkflowUtilitiesAutoSubtitleInput;
    output: WorkflowUtilitiesAutoSubtitleOutput;
  };
  "fal-ai/reve/fast/remix": {
    input: ReveFastRemixInput;
    output: ReveFastRemixOutput;
  };
  "fal-ai/reve/fast/edit": {
    input: ReveFastEditInput;
    output: ReveFastEditOutput;
  };
  "fal-ai/image-apps-v2/outpaint": {
    input: ImageAppsV2OutpaintInput;
    output: ImageAppsV2OutpaintOutput;
  };
  "fal-ai/flux-vision-upscaler": {
    input: FluxVisionUpscalerInput;
    output: FluxVisionUpscalerOutput;
  };
  "fal-ai/emu-3.5-image/edit-image": {
    input: Emu35ImageEditImageInput;
    output: Emu35ImageEditImageOutput;
  };
  "fal-ai/emu-3.5-image/text-to-image": {
    input: Emu35ImageTextToImageInput;
    output: Emu35ImageTextToImageOutput;
  };
  "simalabs/sima-video-upscaler-lite": {
    input: SimaVideoUpscalerLiteInput;
    output: SimaVideoUpscalerLiteOutput;
  };
  "fal-ai/bytedance-upscaler/upscale/video": {
    input: BytedanceUpscalerUpscaleVideoInput;
    output: BytedanceUpscalerUpscaleVideoOutput;
  };
  "fal-ai/chrono-edit": {
    input: ChronoEditInput;
    output: ChronoEditOutput;
  };
  "fal-ai/minimax-music/v2": {
    input: MinimaxMusicV2Input;
    output: MinimaxMusicV2Output;
  };
  "fal-ai/qwen-image-edit-plus-trainer": {
    input: QwenImageEditPlusTrainerInput;
    output: QwenImageEditPlusTrainerOutput;
  };
  "fal-ai/qwen-image-edit-trainer": {
    input: QwenImageEditTrainerInput;
    output: QwenImageEditTrainerOutput;
  };
  "fal-ai/longcat-video/text-to-video/720p": {
    input: LongcatVideoTextToVideo720pInput;
    output: LongcatVideoTextToVideo720pOutput;
  };
  "fal-ai/longcat-video/image-to-video/720p": {
    input: LongcatVideoImageToVideo720pInput;
    output: LongcatVideoImageToVideo720pOutput;
  };
  "fal-ai/longcat-video/image-to-video/480p": {
    input: LongcatVideoImageToVideo480pInput;
    output: LongcatVideoImageToVideo480pOutput;
  };
  "fal-ai/longcat-video/text-to-video/480p": {
    input: LongcatVideoTextToVideo480pInput;
    output: LongcatVideoTextToVideo480pOutput;
  };
  "fal-ai/longcat-video/distilled/image-to-video/720p": {
    input: LongcatVideoDistilledImageToVideo720pInput;
    output: LongcatVideoDistilledImageToVideo720pOutput;
  };
  "fal-ai/longcat-video/distilled/text-to-video/720p": {
    input: LongcatVideoDistilledTextToVideo720pInput;
    output: LongcatVideoDistilledTextToVideo720pOutput;
  };
  "bria/fibo/generate/structured_prompt": {
    input: FiboGenerateStructuredPromptInput;
    output: any;
  };
  "fal-ai/omnipart": {
    input: omnipartInput;
    output: omnipartOutput;
  };
  "fal-ai/minimax/speech-2.6-turbo": {
    input: MinimaxSpeech26TurboInput;
    output: MinimaxSpeech26TurboOutput;
  };
  "fal-ai/minimax/speech-2.6-hd": {
    input: MinimaxSpeech26HdInput;
    output: MinimaxSpeech26HdOutput;
  };
  "fal-ai/video-as-prompt": {
    input: VideoAsPromptInput;
    output: VideoAsPromptOutput;
  };
  "fal-ai/bytedance/seed3d/image-to-3d": {
    input: BytedanceSeed3dImageTo3dInput;
    output: BytedanceSeed3dImageTo3dOutput;
  };
  "bria/fibo/generate": {
    input: FiboGenerateInput;
    output: FiboGenerateOutput;
  };
  "fal-ai/longcat-video/distilled/image-to-video/480p": {
    input: LongcatVideoDistilledImageToVideo480pInput;
    output: LongcatVideoDistilledImageToVideo480pOutput;
  };
  "fal-ai/longcat-video/distilled/text-to-video/480p": {
    input: LongcatVideoDistilledTextToVideo480pInput;
    output: LongcatVideoDistilledTextToVideo480pOutput;
  };
  "fal-ai/demucs": {
    input: demucsInput;
    output: demucsOutput;
  };
  "fal-ai/piflow": {
    input: piflowInput;
    output: piflowOutput;
  };
  "fal-ai/minimax/hailuo-2.3-fast/standard/image-to-video": {
    input: MinimaxHailuo23FastStandardImageToVideoInput;
    output: MinimaxHailuo23FastStandardImageToVideoOutput;
  };
  "fal-ai/minimax/hailuo-2.3/standard/image-to-video": {
    input: MinimaxHailuo23StandardImageToVideoInput;
    output: MinimaxHailuo23StandardImageToVideoOutput;
  };
  "fal-ai/minimax/hailuo-2.3-fast/pro/image-to-video": {
    input: MinimaxHailuo23FastProImageToVideoInput;
    output: MinimaxHailuo23FastProImageToVideoOutput;
  };
  "fal-ai/minimax/hailuo-2.3/standard/text-to-video": {
    input: MinimaxHailuo23StandardTextToVideoInput;
    output: MinimaxHailuo23StandardTextToVideoOutput;
  };
  "fal-ai/minimax/hailuo-2.3/pro/text-to-video": {
    input: MinimaxHailuo23ProTextToVideoInput;
    output: MinimaxHailuo23ProTextToVideoOutput;
  };
  "fal-ai/birefnet/v2/video": {
    input: BirefnetV2VideoInput;
    output: BirefnetV2VideoOutput;
  };
  "fal-ai/audio-understanding": {
    input: AudioUnderstandingInput;
    output: AudioUnderstandingOutput;
  };
  "fal-ai/bytedance/seedance/v1/pro/fast/image-to-video": {
    input: BytedanceSeedanceV1ProFastImageToVideoInput;
    output: BytedanceSeedanceV1ProFastImageToVideoOutput;
  };
  "fal-ai/bytedance/seedance/v1/pro/fast/text-to-video": {
    input: BytedanceSeedanceV1ProFastTextToVideoInput;
    output: BytedanceSeedanceV1ProFastTextToVideoOutput;
  };
  "fal-ai/vidu/q2/video-extension/pro": {
    input: ViduQ2VideoExtensionProInput;
    output: ViduQ2VideoExtensionProOutput;
  };
  "fal-ai/vidu/q2/image-to-video/turbo": {
    input: ViduQ2ImageToVideoTurboInput;
    output: ViduQ2ImageToVideoTurboOutput;
  };
  "fal-ai/vidu/q2/image-to-video/pro": {
    input: ViduQ2ImageToVideoProInput;
    output: ViduQ2ImageToVideoProOutput;
  };
  "fal-ai/vidu/q2/text-to-video": {
    input: ViduQ2TextToVideoInput;
    output: ViduQ2TextToVideoOutput;
  };
  "fal-ai/kling-video/v2.5-turbo/standard/image-to-video": {
    input: KlingVideoV25TurboStandardImageToVideoInput;
    output: KlingVideoV25TurboStandardImageToVideoOutput;
  };
  "fal-ai/gpt-image-1-mini/edit": {
    input: GptImage1MiniEditInput;
    output: GptImage1MiniEditOutput;
  };
  "fal-ai/gpt-image-1-mini": {
    input: GptImage1MiniInput;
    output: GptImage1MiniOutput;
  };
  "fal-ai/qwen-3-guard": {
    input: Qwen3GuardInput;
    output: Qwen3GuardOutput;
  };
  "fal-ai/krea-wan-14b/text-to-video": {
    input: KreaWan14bTextToVideoInput;
    output: KreaWan14bTextToVideoOutput;
  };
  "beatoven/sound-effect-generation": {
    input: SoundEffectGenerationInput;
    output: SoundEffectGenerationOutput;
  };
  "beatoven/music-generation": {
    input: MusicGenerationInput;
    output: MusicGenerationOutput;
  };
  "fal-ai/meshy/v5/retexture": {
    input: MeshyV5RetextureInput;
    output: MeshyV5RetextureOutput;
  };
  "fal-ai/meshy/v5/remesh": {
    input: MeshyV5RemeshInput;
    output: MeshyV5RemeshOutput;
  };
  "fal-ai/reve/remix": {
    input: ReveRemixInput;
    output: ReveRemixOutput;
  };
  "fal-ai/reve/text-to-image": {
    input: ReveTextToImageInput;
    output: ReveTextToImageOutput;
  };
  "fal-ai/reve/edit": {
    input: ReveEditInput;
    output: ReveEditOutput;
  };
  "fal-ai/wan-alpha": {
    input: WanAlphaInput;
    output: WanAlphaOutput;
  };
  "mirelo-ai/sfx-v1.5/video-to-audio": {
    input: SfxV15VideoToAudioInput;
    output: SfxV15VideoToAudioOutput;
  };
  "mirelo-ai/sfx-v1.5/video-to-video": {
    input: SfxV15VideoToVideoInput;
    output: SfxV15VideoToVideoOutput;
  };
  "fal-ai/krea-wan-14b/video-to-video": {
    input: KreaWan14bVideoToVideoInput;
    output: KreaWan14bVideoToVideoOutput;
  };
  "fal-ai/image2pixel": {
    input: image2pixelInput;
    output: image2pixelOutput;
  };
  "fal-ai/kandinsky5/text-to-video/distill": {
    input: Kandinsky5TextToVideoDistillInput;
    output: Kandinsky5TextToVideoDistillOutput;
  };
  "fal-ai/kandinsky5/text-to-video": {
    input: Kandinsky5TextToVideoInput;
    output: Kandinsky5TextToVideoOutput;
  };
  "fal-ai/dreamomni2/edit": {
    input: Dreamomni2EditInput;
    output: Dreamomni2EditOutput;
  };
  "fal-ai/moondream3-preview/detect": {
    input: Moondream3PreviewDetectInput;
    output: Moondream3PreviewDetectOutput;
  };
  "fal-ai/moondream3-preview/point": {
    input: Moondream3PreviewPointInput;
    output: Moondream3PreviewPointOutput;
  };
  "fal-ai/moondream3-preview/query": {
    input: Moondream3PreviewQueryInput;
    output: Moondream3PreviewQueryOutput;
  };
  "fal-ai/moondream3-preview/caption": {
    input: Moondream3PreviewCaptionInput;
    output: Moondream3PreviewCaptionOutput;
  };
  "fal-ai/kling-video/video-to-audio": {
    input: KlingVideoVideoToAudioInput;
    output: KlingVideoVideoToAudioOutput;
  };
  "fal-ai/sora-2/video-to-video/remix": {
    input: Sora2VideoToVideoRemixInput;
    output: Sora2VideoToVideoRemixOutput;
  };
  "fal-ai/veo3.1/fast/first-last-frame-to-video": {
    input: Veo31FastFirstLastFrameToVideoInput;
    output: Veo31FastFirstLastFrameToVideoOutput;
  };
  "fal-ai/veo3.1/first-last-frame-to-video": {
    input: Veo31FirstLastFrameToVideoInput;
    output: Veo31FirstLastFrameToVideoOutput;
  };
  "fal-ai/veo3.1/reference-to-video": {
    input: Veo31ReferenceToVideoInput;
    output: Veo31ReferenceToVideoOutput;
  };
  "fal-ai/veo3.1/fast": {
    input: Veo31FastInput;
    output: Veo31FastOutput;
  };
  "fal-ai/veo3.1/fast/image-to-video": {
    input: Veo31FastImageToVideoInput;
    output: Veo31FastImageToVideoOutput;
  };
  "fal-ai/veo3.1/image-to-video": {
    input: Veo31ImageToVideoInput;
    output: Veo31ImageToVideoOutput;
  };
  "fal-ai/veo3.1": {
    input: Veo31Input;
    output: Veo31Output;
  };
  "fal-ai/hunyuan-part": {
    input: HunyuanPartInput;
    output: HunyuanPartOutput;
  };
  "fal-ai/wan-vace-apps/long-reframe": {
    input: WanVaceAppsLongReframeInput;
    output: WanVaceAppsLongReframeOutput;
  };
  "fal-ai/index-tts-2/text-to-speech": {
    input: IndexTts2TextToSpeechInput;
    output: IndexTts2TextToSpeechOutput;
  };
  "fal-ai/meshy/v6-preview/text-to-3d": {
    input: MeshyV6PreviewTextTo3dInput;
    output: MeshyV6PreviewTextTo3dOutput;
  };
  "fal-ai/meshy/v5/multi-image-to-3d": {
    input: MeshyV5MultiImageTo3dInput;
    output: MeshyV5MultiImageTo3dOutput;
  };
  "fal-ai/meshy/v6-preview/image-to-3d": {
    input: MeshyV6PreviewImageTo3dInput;
    output: MeshyV6PreviewImageTo3dOutput;
  };
  "fal-ai/sora-2/image-to-video/pro": {
    input: Sora2ImageToVideoProInput;
    output: Sora2ImageToVideoProOutput;
  };
  "fal-ai/sora-2/text-to-video/pro": {
    input: Sora2TextToVideoProInput;
    output: Sora2TextToVideoProOutput;
  };
  "fal-ai/sora-2/text-to-video": {
    input: Sora2TextToVideoInput;
    output: Sora2TextToVideoOutput;
  };
  "fal-ai/sora-2/image-to-video": {
    input: Sora2ImageToVideoInput;
    output: Sora2ImageToVideoOutput;
  };
  "fal-ai/qwen-image-edit-plus-lora": {
    input: QwenImageEditPlusLoraInput;
    output: QwenImageEditPlusLoraOutput;
  };
  "fal-ai/lucidflux": {
    input: lucidfluxInput;
    output: lucidfluxOutput;
  };
  "fal-ai/ovi/image-to-video": {
    input: OviImageToVideoInput;
    output: OviImageToVideoOutput;
  };
  "fal-ai/ovi": {
    input: oviInput;
    output: oviOutput;
  };
  "veed/fabric-1.0/fast": {
    input: Fabric10FastInput;
    output: Fabric10FastOutput;
  };
  "fal-ai/qwen-image-edit/image-to-image": {
    input: QwenImageEditImageToImageInput;
    output: QwenImageEditImageToImageOutput;
  };
  "fal-ai/hunyuan-image/v3/text-to-image": {
    input: HunyuanImageV3TextToImageInput;
    output: HunyuanImageV3TextToImageOutput;
  };
  "fal-ai/hyper3d/rodin/v2": {
    input: Hyper3dRodinV2Input;
    output: Hyper3dRodinV2Output;
  };
  "fal-ai/wan-25-preview/image-to-image": {
    input: Wan25PreviewImageToImageInput;
    output: Wan25PreviewImageToImageOutput;
  };
  "fal-ai/wan-25-preview/text-to-image": {
    input: Wan25PreviewTextToImageInput;
    output: Wan25PreviewTextToImageOutput;
  };
  "fal-ai/wan-25-preview/text-to-video": {
    input: Wan25PreviewTextToVideoInput;
    output: Wan25PreviewTextToVideoOutput;
  };
  "fal-ai/bytedance/omnihuman/v1.5": {
    input: BytedanceOmnihumanV15Input;
    output: BytedanceOmnihumanV15Output;
  };
  "fal-ai/qwen-image-edit-plus": {
    input: QwenImageEditPlusInput;
    output: QwenImageEditPlusOutput;
  };
  "fal-ai/infinitalk/video-to-video": {
    input: InfinitalkVideoToVideoInput;
    output: InfinitalkVideoToVideoOutput;
  };
  "fal-ai/seedvr/upscale/video": {
    input: SeedvrUpscaleVideoInput;
    output: SeedvrUpscaleVideoOutput;
  };
  "fal-ai/seedvr/upscale/image": {
    input: SeedvrUpscaleImageInput;
    output: SeedvrUpscaleImageOutput;
  };
  "fal-ai/wan-vace-apps/video-edit": {
    input: WanVaceAppsVideoEditInput;
    output: WanVaceAppsVideoEditOutput;
  };
  "fal-ai/wan/v2.2-14b/animate/replace": {
    input: WanV2214bAnimateReplaceInput;
    output: WanV2214bAnimateReplaceOutput;
  };
  "fal-ai/wan/v2.2-14b/animate/move": {
    input: WanV2214bAnimateMoveInput;
    output: WanV2214bAnimateMoveOutput;
  };
  "veed/fabric-1.0": {
    input: Fabric10Input;
    output: Fabric10Output;
  };
  "fal-ai/image-apps-v2/product-holding": {
    input: ImageAppsV2ProductHoldingInput;
    output: ImageAppsV2ProductHoldingOutput;
  };
  "fal-ai/image-apps-v2/product-photography": {
    input: ImageAppsV2ProductPhotographyInput;
    output: ImageAppsV2ProductPhotographyOutput;
  };
  "decart/lucy-edit/pro": {
    input: LucyEditProInput;
    output: LucyEditProOutput;
  };
  "decart/lucy-edit/dev": {
    input: LucyEditDevInput;
    output: LucyEditDevOutput;
  };
  "fal-ai/image-apps-v2/virtual-try-on": {
    input: ImageAppsV2VirtualTryOnInput;
    output: ImageAppsV2VirtualTryOnOutput;
  };
  "fal-ai/image-apps-v2/texture-transform": {
    input: ImageAppsV2TextureTransformInput;
    output: ImageAppsV2TextureTransformOutput;
  };
  "fal-ai/image-apps-v2/relighting": {
    input: ImageAppsV2RelightingInput;
    output: ImageAppsV2RelightingOutput;
  };
  "fal-ai/image-apps-v2/style-transfer": {
    input: ImageAppsV2StyleTransferInput;
    output: ImageAppsV2StyleTransferOutput;
  };
  "fal-ai/image-apps-v2/photo-restoration": {
    input: ImageAppsV2PhotoRestorationInput;
    output: ImageAppsV2PhotoRestorationOutput;
  };
  "fal-ai/image-apps-v2/portrait-enhance": {
    input: ImageAppsV2PortraitEnhanceInput;
    output: ImageAppsV2PortraitEnhanceOutput;
  };
  "fal-ai/image-apps-v2/photography-effects": {
    input: ImageAppsV2PhotographyEffectsInput;
    output: ImageAppsV2PhotographyEffectsOutput;
  };
  "fal-ai/image-apps-v2/perspective": {
    input: ImageAppsV2PerspectiveInput;
    output: ImageAppsV2PerspectiveOutput;
  };
  "fal-ai/image-apps-v2/object-removal": {
    input: ImageAppsV2ObjectRemovalInput;
    output: ImageAppsV2ObjectRemovalOutput;
  };
  "fal-ai/image-apps-v2/headshot-photo": {
    input: ImageAppsV2HeadshotPhotoInput;
    output: ImageAppsV2HeadshotPhotoOutput;
  };
  "fal-ai/image-apps-v2/hair-change": {
    input: ImageAppsV2HairChangeInput;
    output: ImageAppsV2HairChangeOutput;
  };
  "fal-ai/image-apps-v2/expression-change": {
    input: ImageAppsV2ExpressionChangeInput;
    output: ImageAppsV2ExpressionChangeOutput;
  };
  "fal-ai/image-apps-v2/city-teleport": {
    input: ImageAppsV2CityTeleportInput;
    output: ImageAppsV2CityTeleportOutput;
  };
  "fal-ai/image-apps-v2/age-modify": {
    input: ImageAppsV2AgeModifyInput;
    output: ImageAppsV2AgeModifyOutput;
  };
  "fal-ai/image-apps-v2/makeup-application": {
    input: ImageAppsV2MakeupApplicationInput;
    output: ImageAppsV2MakeupApplicationOutput;
  };
  "fal-ai/qwen-image-edit/inpaint": {
    input: QwenImageEditInpaintInput;
    output: QwenImageEditInpaintOutput;
  };
  "fal-ai/wan-22-vace-fun-a14b/reframe": {
    input: Wan22VaceFunA14bReframeInput;
    output: Wan22VaceFunA14bReframeOutput;
  };
  "fal-ai/wan-22-vace-fun-a14b/outpainting": {
    input: Wan22VaceFunA14bOutpaintingInput;
    output: Wan22VaceFunA14bOutpaintingOutput;
  };
  "fal-ai/wan-22-vace-fun-a14b/inpainting": {
    input: Wan22VaceFunA14bInpaintingInput;
    output: Wan22VaceFunA14bInpaintingOutput;
  };
  "fal-ai/wan-22-vace-fun-a14b/depth": {
    input: Wan22VaceFunA14bDepthInput;
    output: Wan22VaceFunA14bDepthOutput;
  };
  "fal-ai/wan-22-vace-fun-a14b/pose": {
    input: Wan22VaceFunA14bPoseInput;
    output: Wan22VaceFunA14bPoseOutput;
  };
  "perceptron/isaac-01/openai/v1/chat/completions": {
    input: any;
    output: any;
  };
  "perceptron/isaac-01": {
    input: Isaac01Input;
    output: Isaac01Output;
  };
  "fal-ai/flux/srpo/image-to-image": {
    input: FluxSrpoImageToImageInput;
    output: FluxSrpoImageToImageOutput;
  };
  "fal-ai/flux/srpo": {
    input: FluxSrpoInput;
    output: FluxSrpoOutput;
  };
  "fal-ai/flux-1/srpo/image-to-image": {
    input: Flux1SrpoImageToImageInput;
    output: Flux1SrpoImageToImageOutput;
  };
  "fal-ai/flux-1/srpo": {
    input: Flux1SrpoInput;
    output: Flux1SrpoOutput;
  };
  "fal-ai/pshuman": {
    input: pshumanInput;
    output: pshumanOutput;
  };
  "fal-ai/kling-video/v1/tts": {
    input: KlingVideoV1TtsInput;
    output: KlingVideoV1TtsOutput;
  };
  "fal-ai/kling-video/v1/standard/ai-avatar": {
    input: KlingVideoV1StandardAiAvatarInput;
    output: KlingVideoV1StandardAiAvatarOutput;
  };
  "fal-ai/kling-video/v1/pro/ai-avatar": {
    input: KlingVideoV1ProAiAvatarInput;
    output: KlingVideoV1ProAiAvatarOutput;
  };
  "fal-ai/minimax-music/v1.5": {
    input: MinimaxMusicV15Input;
    output: MinimaxMusicV15Output;
  };
  "decart/lucy-14b/image-to-video": {
    input: Lucy14bImageToVideoInput;
    output: Lucy14bImageToVideoOutput;
  };
  "fal-ai/qwen-image-edit-lora": {
    input: QwenImageEditLoraInput;
    output: QwenImageEditLoraOutput;
  };
  "fal-ai/stable-audio-25/audio-to-audio": {
    input: StableAudio25AudioToAudioInput;
    output: StableAudio25AudioToAudioOutput;
  };
  "fal-ai/stable-audio-25/text-to-audio": {
    input: StableAudio25TextToAudioInput;
    output: StableAudio25TextToAudioOutput;
  };
  "fal-ai/stable-audio-25/inpaint": {
    input: StableAudio25InpaintInput;
    output: StableAudio25InpaintOutput;
  };
  "fal-ai/hunyuan-image/v2.1/text-to-image": {
    input: HunyuanImageV21TextToImageInput;
    output: HunyuanImageV21TextToImageOutput;
  };
  "fal-ai/elevenlabs/text-to-dialogue/eleven-v3": {
    input: ElevenlabsTextToDialogueElevenV3Input;
    output: ElevenlabsTextToDialogueElevenV3Output;
  };
  "fal-ai/vidu/reference-to-image": {
    input: ViduReferenceToImageInput;
    output: ViduReferenceToImageOutput;
  };
  "fal-ai/bytedance/seedream/v4/edit": {
    input: BytedanceSeedreamV4EditInput;
    output: BytedanceSeedreamV4EditOutput;
  };
  "fal-ai/bytedance/seedream/v4/text-to-image": {
    input: BytedanceSeedreamV4TextToImageInput;
    output: BytedanceSeedreamV4TextToImageOutput;
  };
  "fal-ai/hunyuan-video-foley": {
    input: HunyuanVideoFoleyInput;
    output: HunyuanVideoFoleyOutput;
  };
  "fal-ai/chatterbox/text-to-speech/multilingual": {
    input: ChatterboxTextToSpeechMultilingualInput;
    output: ChatterboxTextToSpeechMultilingualOutput;
  };
  "fal-ai/wan/v2.2-a14b/image-to-image": {
    input: WanV22A14bImageToImageInput;
    output: WanV22A14bImageToImageOutput;
  };
  "fal-ai/elevenlabs/sound-effects/v2": {
    input: ElevenlabsSoundEffectsV2Input;
    output: ElevenlabsSoundEffectsV2Output;
  };
  "fal-ai/sync-lipsync/v2/pro": {
    input: SyncLipsyncV2ProInput;
    output: SyncLipsyncV2ProOutput;
  };
  "fal-ai/bytedance/seedance/v1/lite/reference-to-video": {
    input: BytedanceSeedanceV1LiteReferenceToVideoInput;
    output: BytedanceSeedanceV1LiteReferenceToVideoOutput;
  };
  "argil/avatars/text-to-video": {
    input: AvatarsTextToVideoInput;
    output: AvatarsTextToVideoOutput;
  };
  "argil/avatars/audio-to-video": {
    input: AvatarsAudioToVideoInput;
    output: AvatarsAudioToVideoOutput;
  };
  "fal-ai/uso": {
    input: usoInput;
    output: usoOutput;
  };
  "fal-ai/wan-ati": {
    input: WanAtiInput;
    output: WanAtiOutput;
  };
  "fal-ai/decart/lucy-5b/image-to-video": {
    input: DecartLucy5bImageToVideoInput;
    output: DecartLucy5bImageToVideoOutput;
  };
  "fal-ai/wan-fun-control": {
    input: WanFunControlInput;
    output: WanFunControlOutput;
  };
  "fal-ai/vibevoice/7b": {
    input: Vibevoice7bInput;
    output: Vibevoice7bOutput;
  };
  "fal-ai/vibevoice": {
    input: vibevoiceInput;
    output: vibevoiceOutput;
  };
  "fal-ai/wan/v2.2-14b/speech-to-video": {
    input: WanV2214bSpeechToVideoInput;
    output: WanV2214bSpeechToVideoOutput;
  };
  "bria/video/increase-resolution": {
    input: VideoIncreaseResolutionInput;
    output: VideoIncreaseResolutionOutput;
  };
  "fal-ai/gemini-25-flash-image/edit": {
    input: Gemini25FlashImageEditInput;
    output: Gemini25FlashImageEditOutput;
  };
  "fal-ai/gemini-25-flash-image": {
    input: Gemini25FlashImageInput;
    output: Gemini25FlashImageOutput;
  };
  "fal-ai/qwen-image/image-to-image": {
    input: QwenImageImageToImageInput;
    output: QwenImageImageToImageOutput;
  };
  "sonauto/v2/extend": {
    input: V2ExtendInput;
    output: V2ExtendOutput;
  };
  "sonauto/v2/inpaint": {
    input: V2InpaintInput;
    output: V2InpaintOutput;
  };
  "sonauto/v2/text-to-music": {
    input: V2TextToMusicInput;
    output: V2TextToMusicOutput;
  };
  "fal-ai/pixverse/v5/transition": {
    input: PixverseV5TransitionInput;
    output: PixverseV5TransitionOutput;
  };
  "fal-ai/pixverse/v5/effects": {
    input: PixverseV5EffectsInput;
    output: PixverseV5EffectsOutput;
  };
  "fal-ai/pixverse/v5/image-to-video": {
    input: PixverseV5ImageToVideoInput;
    output: PixverseV5ImageToVideoOutput;
  };
  "fal-ai/pixverse/v5/text-to-video": {
    input: PixverseV5TextToVideoInput;
    output: PixverseV5TextToVideoOutput;
  };
  "fal-ai/infinitalk/single-text": {
    input: InfinitalkSingleTextInput;
    output: InfinitalkSingleTextOutput;
  };
  "fal-ai/infinitalk": {
    input: infinitalkInput;
    output: infinitalkOutput;
  };
  "fal-ai/elevenlabs/tts/eleven-v3": {
    input: ElevenlabsTtsElevenV3Input;
    output: ElevenlabsTtsElevenV3Output;
  };
  "bria/reimagine/3.2": {
    input: Reimagine32Input;
    output: Reimagine32Output;
  };
  "fal-ai/nano-banana/edit": {
    input: NanoBananaEditInput;
    output: NanoBananaEditOutput;
  };
  "fal-ai/nano-banana": {
    input: NanoBananaInput;
    output: NanoBananaOutput;
  };
  "fal-ai/nextstep-1": {
    input: Nextstep1Input;
    output: Nextstep1Output;
  };
  "fal-ai/qwen-image-edit": {
    input: QwenImageEditInput;
    output: QwenImageEditOutput;
  };
  "mirelo-ai/sfx-v1/video-to-audio": {
    input: SfxV1VideoToAudioInput;
    output: SfxV1VideoToAudioOutput;
  };
  "mirelo-ai/sfx-v1/video-to-video": {
    input: SfxV1VideoToVideoInput;
    output: SfxV1VideoToVideoOutput;
  };
  "fal-ai/stable-avatar": {
    input: StableAvatarInput;
    output: StableAvatarOutput;
  };
  "moonvalley/marey/pose-transfer": {
    input: MareyPoseTransferInput;
    output: MareyPoseTransferOutput;
  };
  "moonvalley/marey/motion-transfer": {
    input: MareyMotionTransferInput;
    output: MareyMotionTransferOutput;
  };
  "moonvalley/marey/i2v": {
    input: MareyI2vInput;
    output: MareyI2vOutput;
  };
  "fal-ai/qwen-image-trainer": {
    input: QwenImageTrainerInput;
    output: QwenImageTrainerOutput;
  };
  "moonvalley/marey/t2v": {
    input: MareyT2vInput;
    output: MareyT2vOutput;
  };
  "fal-ai/echomimic-v3": {
    input: EchomimicV3Input;
    output: EchomimicV3Output;
  };
  "fal-ai/bytedance/video-stylize": {
    input: BytedanceVideoStylizeInput;
    output: any;
  };
  "fal-ai/ffmpeg-api/merge-videos": {
    input: FfmpegApiMergeVideosInput;
    output: FfmpegApiMergeVideosOutput;
  };
  "fal-ai/minimax/preview/speech-2.5-hd": {
    input: MinimaxPreviewSpeech25HdInput;
    output: MinimaxPreviewSpeech25HdOutput;
  };
  "fal-ai/minimax/preview/speech-2.5-turbo": {
    input: MinimaxPreviewSpeech25TurboInput;
    output: MinimaxPreviewSpeech25TurboOutput;
  };
  "fal-ai/wan-22-image-trainer": {
    input: Wan22ImageTrainerInput;
    output: Wan22ImageTrainerOutput;
  };
  "fal-ai/ideogram/character/edit": {
    input: IdeogramCharacterEditInput;
    output: IdeogramCharacterEditOutput;
  };
  "fal-ai/ideogram/character": {
    input: IdeogramCharacterInput;
    output: IdeogramCharacterOutput;
  };
  "fal-ai/ideogram/character/remix": {
    input: IdeogramCharacterRemixInput;
    output: IdeogramCharacterRemixOutput;
  };
  "fal-ai/wan/v2.2-a14b/text-to-video/lora": {
    input: WanV22A14bTextToVideoLoraInput;
    output: WanV22A14bTextToVideoLoraOutput;
  };
  "fal-ai/wan/v2.2-a14b/image-to-video/lora": {
    input: WanV22A14bImageToVideoLoraInput;
    output: WanV22A14bImageToVideoLoraOutput;
  };
  "fal-ai/wan/v2.2-5b/text-to-video/distill": {
    input: WanV225bTextToVideoDistillInput;
    output: WanV225bTextToVideoDistillOutput;
  };
  "fal-ai/minimax/hailuo-02-fast/image-to-video": {
    input: MinimaxHailuo02FastImageToVideoInput;
    output: MinimaxHailuo02FastImageToVideoOutput;
  };
  "fal-ai/bytedance/dreamina/v3.1/text-to-image": {
    input: BytedanceDreaminaV31TextToImageInput;
    output: BytedanceDreaminaV31TextToImageOutput;
  };
  "fal-ai/wan/v2.2-5b/text-to-video/fast-wan": {
    input: WanV225bTextToVideoFastWanInput;
    output: WanV225bTextToVideoFastWanOutput;
  };
  "fal-ai/wan/v2.2-a14b/text-to-image/lora": {
    input: WanV22A14bTextToImageLoraInput;
    output: WanV22A14bTextToImageLoraOutput;
  };
  "fal-ai/wan/v2.2-5b/text-to-image": {
    input: WanV225bTextToImageInput;
    output: WanV225bTextToImageOutput;
  };
  "fal-ai/wan/v2.2-a14b/text-to-image": {
    input: WanV22A14bTextToImageInput;
    output: WanV22A14bTextToImageOutput;
  };
  "fal-ai/qwen-image": {
    input: QwenImageInput;
    output: QwenImageOutput;
  };
  "fal-ai/wan/v2.2-a14b/video-to-video": {
    input: WanV22A14bVideoToVideoInput;
    output: WanV22A14bVideoToVideoOutput;
  };
  "fal-ai/flux-krea-lora/stream": {
    input: FluxKreaLoraStreamInput;
    output: FluxKreaLoraStreamOutput;
  };
  "fal-ai/flux-krea-lora/inpainting": {
    input: FluxKreaLoraInpaintingInput;
    output: FluxKreaLoraInpaintingOutput;
  };
  "fal-ai/flux-krea-lora": {
    input: FluxKreaLoraInput;
    output: FluxKreaLoraOutput;
  };
  "fal-ai/flux-krea-lora/image-to-image": {
    input: FluxKreaLoraImageToImageInput;
    output: FluxKreaLoraImageToImageOutput;
  };
  "fal-ai/veo3/image-to-video": {
    input: Veo3ImageToVideoInput;
    output: Veo3ImageToVideoOutput;
  };
  "fal-ai/wan/v2.2-a14b/image-to-video/turbo": {
    input: WanV22A14bImageToVideoTurboInput;
    output: WanV22A14bImageToVideoTurboOutput;
  };
  "fal-ai/wan/v2.2-a14b/text-to-video/turbo": {
    input: WanV22A14bTextToVideoTurboInput;
    output: WanV22A14bTextToVideoTurboOutput;
  };
  "fal-ai/flux/krea/image-to-image": {
    input: FluxKreaImageToImageInput;
    output: FluxKreaImageToImageOutput;
  };
  "fal-ai/flux/krea/redux": {
    input: FluxKreaReduxInput;
    output: FluxKreaReduxOutput;
  };
  "fal-ai/flux/krea": {
    input: FluxKreaInput;
    output: FluxKreaOutput;
  };
  "fal-ai/flux-1/krea/image-to-image": {
    input: Flux1KreaImageToImageInput;
    output: Flux1KreaImageToImageOutput;
  };
  "fal-ai/flux-1/krea/redux": {
    input: Flux1KreaReduxInput;
    output: Flux1KreaReduxOutput;
  };
  "fal-ai/flux-1/krea": {
    input: Flux1KreaInput;
    output: Flux1KreaOutput;
  };
  "fal-ai/wan/v2.2-5b/image-to-video": {
    input: WanV225bImageToVideoInput;
    output: WanV225bImageToVideoOutput;
  };
  "fal-ai/flux-kontext-lora/inpaint": {
    input: FluxKontextLoraInpaintInput;
    output: FluxKontextLoraInpaintOutput;
  };
  "fal-ai/wan/v2.2-5b/text-to-video": {
    input: WanV225bTextToVideoInput;
    output: WanV225bTextToVideoOutput;
  };
  "fal-ai/wan/v2.2-a14b/text-to-video": {
    input: WanV22A14bTextToVideoInput;
    output: WanV22A14bTextToVideoOutput;
  };
  "fal-ai/wan/v2.2-a14B/image-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/hunyuan_world": {
    input: HunyuanWorldInput;
    output: HunyuanWorldOutput;
  };
  "fal-ai/hunyuan_world/image-to-world": {
    input: HunyuanWorldImageToWorldInput;
    output: HunyuanWorldImageToWorldOutput;
  };
  "fal-ai/x-ailab/nsfw": {
    input: XAilabNsfwInput;
    output: XAilabNsfwOutput;
  };
  "fal-ai/bytedance/omnihuman": {
    input: BytedanceOmnihumanInput;
    output: BytedanceOmnihumanOutput;
  };
  "fal-ai/sky-raccoon": {
    input: SkyRaccoonInput;
    output: SkyRaccoonOutput;
  };
  "fal-ai/image-editing/retouch": {
    input: ImageEditingRetouchInput;
    output: ImageEditingRetouchOutput;
  };
  "fal-ai/hidream-e1-1": {
    input: HidreamE11Input;
    output: HidreamE11Output;
  };
  "fal-ai/ltxv-13b-098-distilled/extend": {
    input: Ltxv13b098DistilledExtendInput;
    output: Ltxv13b098DistilledExtendOutput;
  };
  "fal-ai/rife/video": {
    input: RifeVideoInput;
    output: RifeVideoOutput;
  };
  "fal-ai/rife": {
    input: rifeInput;
    output: rifeOutput;
  };
  "fal-ai/film/video": {
    input: FilmVideoInput;
    output: FilmVideoOutput;
  };
  "fal-ai/film": {
    input: filmInput;
    output: filmOutput;
  };
  "fal-ai/minimax/voice-design": {
    input: MinimaxVoiceDesignInput;
    output: MinimaxVoiceDesignOutput;
  };
  "fal-ai/luma-dream-machine/ray-2-flash/modify": {
    input: LumaDreamMachineRay2FlashModifyInput;
    output: LumaDreamMachineRay2FlashModifyOutput;
  };
  "fal-ai/ltxv-13b-098-distilled/image-to-video": {
    input: Ltxv13b098DistilledImageToVideoInput;
    output: Ltxv13b098DistilledImageToVideoOutput;
  };
  "fal-ai/ltxv-13b-098-distilled": {
    input: Ltxv13b098DistilledInput;
    output: Ltxv13b098DistilledOutput;
  };
  "fal-ai/ltxv-13b-098-distilled/multiconditioning": {
    input: Ltxv13b098DistilledMulticonditioningInput;
    output: Ltxv13b098DistilledMulticonditioningOutput;
  };
  "fal-ai/calligrapher": {
    input: calligrapherInput;
    output: calligrapherOutput;
  };
  "fal-ai/veo3/fast/image-to-video": {
    input: Veo3FastImageToVideoInput;
    output: Veo3FastImageToVideoOutput;
  };
  "fal-ai/ffmpeg-api/loudnorm": {
    input: FfmpegApiLoudnormInput;
    output: FfmpegApiLoudnormOutput;
  };
  "fal-ai/vidu/q1/reference-to-video": {
    input: ViduQ1ReferenceToVideoInput;
    output: ViduQ1ReferenceToVideoOutput;
  };
  "fal-ai/bria/reimagine": {
    input: BriaReimagineInput;
    output: BriaReimagineOutput;
  };
  "fal-ai/pixverse/sound-effects": {
    input: PixverseSoundEffectsInput;
    output: PixverseSoundEffectsOutput;
  };
  "fal-ai/image-editing/realism": {
    input: ImageEditingRealismInput;
    output: ImageEditingRealismOutput;
  };
  "fal-ai/thinksound/audio": {
    input: ThinksoundAudioInput;
    output: ThinksoundAudioOutput;
  };
  "fal-ai/thinksound": {
    input: thinksoundInput;
    output: thinksoundOutput;
  };
  "fal-ai/post-processing/vignette": {
    input: PostProcessingVignetteInput;
    output: PostProcessingVignetteOutput;
  };
  "fal-ai/post-processing/solarize": {
    input: PostProcessingSolarizeInput;
    output: PostProcessingSolarizeOutput;
  };
  "fal-ai/post-processing/sharpen": {
    input: PostProcessingSharpenInput;
    output: PostProcessingSharpenOutput;
  };
  "fal-ai/post-processing/parabolize": {
    input: PostProcessingParabolizeInput;
    output: PostProcessingParabolizeOutput;
  };
  "fal-ai/post-processing/grain": {
    input: PostProcessingGrainInput;
    output: PostProcessingGrainOutput;
  };
  "fal-ai/post-processing/dodge-burn": {
    input: PostProcessingDodgeBurnInput;
    output: PostProcessingDodgeBurnOutput;
  };
  "fal-ai/post-processing/dissolve": {
    input: PostProcessingDissolveInput;
    output: PostProcessingDissolveOutput;
  };
  "fal-ai/post-processing/desaturate": {
    input: PostProcessingDesaturateInput;
    output: PostProcessingDesaturateOutput;
  };
  "fal-ai/post-processing/color-tint": {
    input: PostProcessingColorTintInput;
    output: PostProcessingColorTintOutput;
  };
  "fal-ai/post-processing/color-correction": {
    input: PostProcessingColorCorrectionInput;
    output: PostProcessingColorCorrectionOutput;
  };
  "fal-ai/post-processing/chromatic-aberration": {
    input: PostProcessingChromaticAberrationInput;
    output: PostProcessingChromaticAberrationOutput;
  };
  "fal-ai/post-processing/blur": {
    input: PostProcessingBlurInput;
    output: PostProcessingBlurOutput;
  };
  "fal-ai/pixverse/extend/fast": {
    input: PixverseExtendFastInput;
    output: PixverseExtendFastOutput;
  };
  "fal-ai/pixverse/extend": {
    input: PixverseExtendInput;
    output: PixverseExtendOutput;
  };
  "fal-ai/pixverse/lipsync": {
    input: PixverseLipsyncInput;
    output: PixverseLipsyncOutput;
  };
  "fal-ai/image-editing/youtube-thumbnails": {
    input: ImageEditingYoutubeThumbnailsInput;
    output: ImageEditingYoutubeThumbnailsOutput;
  };
  "fal-ai/luma-dream-machine/ray-2/modify": {
    input: LumaDreamMachineRay2ModifyInput;
    output: LumaDreamMachineRay2ModifyOutput;
  };
  "fal-ai/topaz/upscale/image": {
    input: TopazUpscaleImageInput;
    output: TopazUpscaleImageOutput;
  };
  "fal-ai/bytedance/seededit/v3/edit-image": {
    input: BytedanceSeededitV3EditImageInput;
    output: BytedanceSeededitV3EditImageOutput;
  };
  "fal-ai/image-editing/broccoli-haircut": {
    input: ImageEditingBroccoliHaircutInput;
    output: ImageEditingBroccoliHaircutOutput;
  };
  "fal-ai/image-editing/wojak-style": {
    input: ImageEditingWojakStyleInput;
    output: ImageEditingWojakStyleOutput;
  };
  "fal-ai/image-editing/plushie-style": {
    input: ImageEditingPlushieStyleInput;
    output: ImageEditingPlushieStyleOutput;
  };
  "fal-ai/flux-kontext-lora/text-to-image": {
    input: FluxKontextLoraTextToImageInput;
    output: FluxKontextLoraTextToImageOutput;
  };
  "fal-ai/flux-kontext-lora": {
    input: FluxKontextLoraInput;
    output: FluxKontextLoraOutput;
  };
  "fal-ai/omnigen-v2": {
    input: OmnigenV2Input;
    output: OmnigenV2Output;
  };
  "fal-ai/fashn/tryon/v1.6": {
    input: FashnTryonV16Input;
    output: FashnTryonV16Output;
  };
  "fal-ai/ai-avatar/single-text": {
    input: AiAvatarSingleTextInput;
    output: AiAvatarSingleTextOutput;
  };
  "fal-ai/ai-avatar": {
    input: AiAvatarInput;
    output: AiAvatarOutput;
  };
  "fal-ai/ai-avatar/multi-text": {
    input: AiAvatarMultiTextInput;
    output: AiAvatarMultiTextOutput;
  };
  "fal-ai/ai-avatar/multi": {
    input: AiAvatarMultiInput;
    output: AiAvatarMultiOutput;
  };
  "fal-ai/video-understanding": {
    input: VideoUnderstandingInput;
    output: VideoUnderstandingOutput;
  };
  "fal-ai/wan-vace-14b/reframe": {
    input: WanVace14bReframeInput;
    output: WanVace14bReframeOutput;
  };
  "fal-ai/wan-vace-14b/outpainting": {
    input: WanVace14bOutpaintingInput;
    output: WanVace14bOutpaintingOutput;
  };
  "fal-ai/wan-vace-14b/inpainting": {
    input: WanVace14bInpaintingInput;
    output: WanVace14bInpaintingOutput;
  };
  "fal-ai/wan-vace-14b/pose": {
    input: WanVace14bPoseInput;
    output: WanVace14bPoseOutput;
  };
  "fal-ai/wan-vace-14b/depth": {
    input: WanVace14bDepthInput;
    output: WanVace14bDepthOutput;
  };
  "fal-ai/chain-of-zoom": {
    input: ChainOfZoomInput;
    output: ChainOfZoomOutput;
  };
  "tripo3d/tripo/v2.5/multiview-to-3d": {
    input: TripoV25MultiviewTo3dInput;
    output: TripoV25MultiviewTo3dOutput;
  };
  "fal-ai/minimax/hailuo-02/pro/image-to-video": {
    input: MinimaxHailuo02ProImageToVideoInput;
    output: MinimaxHailuo02ProImageToVideoOutput;
  };
  "fal-ai/minimax/hailuo-02/pro/text-to-video": {
    input: MinimaxHailuo02ProTextToVideoInput;
    output: MinimaxHailuo02ProTextToVideoOutput;
  };
  "fal-ai/pasd": {
    input: pasdInput;
    output: pasdOutput;
  };
  "fal-ai/object-removal/bbox": {
    input: ObjectRemovalBboxInput;
    output: ObjectRemovalBboxOutput;
  };
  "fal-ai/object-removal/mask": {
    input: ObjectRemovalMaskInput;
    output: ObjectRemovalMaskOutput;
  };
  "fal-ai/object-removal": {
    input: ObjectRemovalInput;
    output: ObjectRemovalOutput;
  };
  "fal-ai/bytedance/seedance/v1/pro/text-to-video": {
    input: BytedanceSeedanceV1ProTextToVideoInput;
    output: BytedanceSeedanceV1ProTextToVideoOutput;
  };
  "fal-ai/dwpose/video": {
    input: DwposeVideoInput;
    output: DwposeVideoOutput;
  };
  "fal-ai/hunyuan3d-v21": {
    input: Hunyuan3dV21Input;
    output: Hunyuan3dV21Output;
  };
  "fal-ai/bytedance/seedance/v1/lite/image-to-video": {
    input: BytedanceSeedanceV1LiteImageToVideoInput;
    output: BytedanceSeedanceV1LiteImageToVideoOutput;
  };
  "fal-ai/bytedance/seedance/v1/lite/text-to-video": {
    input: BytedanceSeedanceV1LiteTextToVideoInput;
    output: BytedanceSeedanceV1LiteTextToVideoOutput;
  };
  "fal-ai/recraft/vectorize": {
    input: RecraftVectorizeInput;
    output: RecraftVectorizeOutput;
  };
  "fal-ai/wan-trainer/t2v": {
    input: WanTrainerT2vInput;
    output: WanTrainerT2vOutput;
  };
  "fal-ai/wan-trainer/t2v-14b": {
    input: WanTrainerT2v14bInput;
    output: WanTrainerT2v14bOutput;
  };
  "fal-ai/wan-trainer/i2v-720p": {
    input: WanTrainerI2v720pInput;
    output: WanTrainerI2v720pOutput;
  };
  "fal-ai/wan-trainer/flf2v-720p": {
    input: WanTrainerFlf2v720pInput;
    output: WanTrainerFlf2v720pOutput;
  };
  "fal-ai/bytedance/seedream/v3/text-to-image": {
    input: BytedanceSeedreamV3TextToImageInput;
    output: BytedanceSeedreamV3TextToImageOutput;
  };
  "fal-ai/ffmpeg-api/extract-frame": {
    input: FfmpegApiExtractFrameInput;
    output: FfmpegApiExtractFrameOutput;
  };
  "fal-ai/ffmpeg-api/merge-audio-video": {
    input: FfmpegApiMergeAudioVideoInput;
    output: FfmpegApiMergeAudioVideoOutput;
  };
  "fal-ai/luma-photon/flash/modify": {
    input: LumaPhotonFlashModifyInput;
    output: LumaPhotonFlashModifyOutput;
  };
  "fal-ai/luma-photon/modify": {
    input: LumaPhotonModifyInput;
    output: LumaPhotonModifyOutput;
  };
  "fal-ai/image-editing/reframe": {
    input: ImageEditingReframeInput;
    output: ImageEditingReframeOutput;
  };
  "fal-ai/wan-vace-1-3b": {
    input: WanVace13bInput;
    output: WanVace13bOutput;
  };
  "fal-ai/image-editing/baby-version": {
    input: ImageEditingBabyVersionInput;
    output: ImageEditingBabyVersionOutput;
  };
  "fal-ai/luma-dream-machine/ray-2-flash/reframe": {
    input: LumaDreamMachineRay2FlashReframeInput;
    output: LumaDreamMachineRay2FlashReframeOutput;
  };
  "fal-ai/luma-dream-machine/ray-2/reframe": {
    input: LumaDreamMachineRay2ReframeInput;
    output: LumaDreamMachineRay2ReframeOutput;
  };
  "fal-ai/luma-photon/flash/reframe": {
    input: LumaPhotonFlashReframeInput;
    output: LumaPhotonFlashReframeOutput;
  };
  "fal-ai/luma-photon/reframe": {
    input: LumaPhotonReframeInput;
    output: LumaPhotonReframeOutput;
  };
  "resemble-ai/chatterboxhd/speech-to-speech": {
    input: ChatterboxhdSpeechToSpeechInput;
    output: ChatterboxhdSpeechToSpeechOutput;
  };
  "resemble-ai/chatterboxhd/text-to-speech": {
    input: ChatterboxhdTextToSpeechInput;
    output: ChatterboxhdTextToSpeechOutput;
  };
  "fal-ai/flux-1/schnell/redux": {
    input: Flux1SchnellReduxInput;
    output: Flux1SchnellReduxOutput;
  };
  "fal-ai/flux-1/dev/redux": {
    input: Flux1DevReduxInput;
    output: Flux1DevReduxOutput;
  };
  "fal-ai/flux-1/dev/image-to-image": {
    input: Flux1DevImageToImageInput;
    output: Flux1DevImageToImageOutput;
  };
  "fal-ai/flux-1/schnell": {
    input: Flux1SchnellInput;
    output: Flux1SchnellOutput;
  };
  "fal-ai/flux-1/dev": {
    input: Flux1DevInput;
    output: Flux1DevOutput;
  };
  "fal-ai/image-editing/text-removal": {
    input: ImageEditingTextRemovalInput;
    output: ImageEditingTextRemovalOutput;
  };
  "fal-ai/image-editing/photo-restoration": {
    input: ImageEditingPhotoRestorationInput;
    output: ImageEditingPhotoRestorationOutput;
  };
  "fal-ai/chatterbox/speech-to-speech": {
    input: ChatterboxSpeechToSpeechInput;
    output: ChatterboxSpeechToSpeechOutput;
  };
  "fal-ai/chatterbox/text-to-speech": {
    input: ChatterboxTextToSpeechInput;
    output: any;
  };
  "fal-ai/image-editing/weather-effect": {
    input: ImageEditingWeatherEffectInput;
    output: ImageEditingWeatherEffectOutput;
  };
  "fal-ai/image-editing/time-of-day": {
    input: ImageEditingTimeOfDayInput;
    output: ImageEditingTimeOfDayOutput;
  };
  "fal-ai/image-editing/style-transfer": {
    input: ImageEditingStyleTransferInput;
    output: ImageEditingStyleTransferOutput;
  };
  "fal-ai/image-editing/scene-composition": {
    input: ImageEditingSceneCompositionInput;
    output: ImageEditingSceneCompositionOutput;
  };
  "fal-ai/image-editing/professional-photo": {
    input: ImageEditingProfessionalPhotoInput;
    output: ImageEditingProfessionalPhotoOutput;
  };
  "fal-ai/image-editing/object-removal": {
    input: ImageEditingObjectRemovalInput;
    output: ImageEditingObjectRemovalOutput;
  };
  "fal-ai/image-editing/hair-change": {
    input: ImageEditingHairChangeInput;
    output: ImageEditingHairChangeOutput;
  };
  "fal-ai/image-editing/face-enhancement": {
    input: ImageEditingFaceEnhancementInput;
    output: ImageEditingFaceEnhancementOutput;
  };
  "fal-ai/image-editing/expression-change": {
    input: ImageEditingExpressionChangeInput;
    output: ImageEditingExpressionChangeOutput;
  };
  "fal-ai/image-editing/color-correction": {
    input: ImageEditingColorCorrectionInput;
    output: ImageEditingColorCorrectionOutput;
  };
  "fal-ai/image-editing/cartoonify": {
    input: ImageEditingCartoonifyInput;
    output: ImageEditingCartoonifyOutput;
  };
  "fal-ai/image-editing/background-change": {
    input: ImageEditingBackgroundChangeInput;
    output: ImageEditingBackgroundChangeOutput;
  };
  "fal-ai/image-editing/age-progression": {
    input: ImageEditingAgeProgressionInput;
    output: ImageEditingAgeProgressionOutput;
  };
  "fal-ai/flux-pro/kontext/max/multi": {
    input: FluxProKontextMaxMultiInput;
    output: FluxProKontextMaxMultiOutput;
  };
  "fal-ai/flux-pro/kontext/multi": {
    input: FluxProKontextMultiInput;
    output: FluxProKontextMultiOutput;
  };
  "fal-ai/hunyuan-avatar": {
    input: HunyuanAvatarInput;
    output: HunyuanAvatarOutput;
  };
  "fal-ai/flux-pro/kontext/max": {
    input: FluxProKontextMaxInput;
    output: FluxProKontextMaxOutput;
  };
  "fal-ai/flux-pro/kontext/max/text-to-image": {
    input: FluxProKontextMaxTextToImageInput;
    output: FluxProKontextMaxTextToImageOutput;
  };
  "fal-ai/kling-video/v2.1/master/text-to-video": {
    input: KlingVideoV21MasterTextToVideoInput;
    output: KlingVideoV21MasterTextToVideoOutput;
  };
  "fal-ai/kling-video/v2.1/pro/image-to-video": {
    input: KlingVideoV21ProImageToVideoInput;
    output: KlingVideoV21ProImageToVideoOutput;
  };
  "fal-ai/flux-pro/kontext/text-to-image": {
    input: FluxProKontextTextToImageInput;
    output: FluxProKontextTextToImageOutput;
  };
  "fal-ai/flux-kontext/dev": {
    input: FluxKontextDevInput;
    output: FluxKontextDevOutput;
  };
  "veed/lipsync": {
    input: lipsyncInput;
    output: lipsyncOutput;
  };
  "veed/avatars/text-to-video": {
    input: AvatarsTextToVideoInput;
    output: AvatarsTextToVideoOutput;
  };
  "veed/avatars/audio-to-video": {
    input: AvatarsAudioToVideoInput;
    output: AvatarsAudioToVideoOutput;
  };
  "fal-ai/hunyuan-portrait": {
    input: HunyuanPortraitInput;
    output: HunyuanPortraitOutput;
  };
  "fal-ai/wan-vace-14b": {
    input: WanVace14bInput;
    output: WanVace14bOutput;
  };
  "fal-ai/bagel/understand": {
    input: BagelUnderstandInput;
    output: BagelUnderstandOutput;
  };
  "fal-ai/bagel/edit": {
    input: BagelEditInput;
    output: BagelEditOutput;
  };
  "fal-ai/bagel": {
    input: bagelInput;
    output: bagelOutput;
  };
  "fal-ai/lyria2": {
    input: lyria2Input;
    output: lyria2Output;
  };
  "fal-ai/imagen4/preview/ultra": {
    input: Imagen4PreviewUltraInput;
    output: Imagen4PreviewUltraOutput;
  };
  "fal-ai/kling-video/v1.6/standard/elements": {
    input: KlingVideoV16StandardElementsInput;
    output: KlingVideoV16StandardElementsOutput;
  };
  "fal-ai/kling-video/v1.6/pro/elements": {
    input: KlingVideoV16ProElementsInput;
    output: KlingVideoV16ProElementsOutput;
  };
  "fal-ai/dreamo": {
    input: dreamoInput;
    output: dreamoOutput;
  };
  "fal-ai/ltx-video-13b-distilled/extend": {
    input: LtxVideo13bDistilledExtendInput;
    output: LtxVideo13bDistilledExtendOutput;
  };
  "fal-ai/ltx-video-13b-distilled/multiconditioning": {
    input: LtxVideo13bDistilledMulticonditioningInput;
    output: LtxVideo13bDistilledMulticonditioningOutput;
  };
  "fal-ai/ltx-video-13b-distilled/image-to-video": {
    input: LtxVideo13bDistilledImageToVideoInput;
    output: LtxVideo13bDistilledImageToVideoOutput;
  };
  "fal-ai/ltx-video-13b-dev/multiconditioning": {
    input: LtxVideo13bDevMulticonditioningInput;
    output: LtxVideo13bDevMulticonditioningOutput;
  };
  "fal-ai/ltx-video-13b-dev/extend": {
    input: LtxVideo13bDevExtendInput;
    output: LtxVideo13bDevExtendOutput;
  };
  "fal-ai/ltx-video-13b-dev/image-to-video": {
    input: LtxVideo13bDevImageToVideoInput;
    output: LtxVideo13bDevImageToVideoOutput;
  };
  "fal-ai/ltx-video-13b-dev": {
    input: LtxVideo13bDevInput;
    output: LtxVideo13bDevOutput;
  };
  "fal-ai/ltx-video-13b-distilled": {
    input: LtxVideo13bDistilledInput;
    output: LtxVideo13bDistilledOutput;
  };
  "fal-ai/flux-lora/stream": {
    input: FluxLoraStreamInput;
    output: FluxLoraStreamOutput;
  };
  "fal-ai/ltx-video-lora/multiconditioning": {
    input: LtxVideoLoraMulticonditioningInput;
    output: LtxVideoLoraMulticonditioningOutput;
  };
  "fal-ai/ltx-video-lora/image-to-video": {
    input: LtxVideoLoraImageToVideoInput;
    output: LtxVideoLoraImageToVideoOutput;
  };
  "fal-ai/ltx-video-lora": {
    input: LtxVideoLoraInput;
    output: LtxVideoLoraOutput;
  };
  "fal-ai/pixverse/v4.5/transition": {
    input: PixverseV45TransitionInput;
    output: PixverseV45TransitionOutput;
  };
  "fal-ai/pixverse/v4.5/image-to-video/fast": {
    input: PixverseV45ImageToVideoFastInput;
    output: PixverseV45ImageToVideoFastOutput;
  };
  "fal-ai/pixverse/v4.5/text-to-video/fast": {
    input: PixverseV45TextToVideoFastInput;
    output: PixverseV45TextToVideoFastOutput;
  };
  "fal-ai/pixverse/v4.5/text-to-video": {
    input: PixverseV45TextToVideoInput;
    output: PixverseV45TextToVideoOutput;
  };
  "fal-ai/pixverse/v4.5/effects": {
    input: PixverseV45EffectsInput;
    output: PixverseV45EffectsOutput;
  };
  "fal-ai/hunyuan-custom": {
    input: HunyuanCustomInput;
    output: HunyuanCustomOutput;
  };
  "fal-ai/framepack/f1": {
    input: FramepackF1Input;
    output: FramepackF1Output;
  };
  "fal-ai/ace-step/audio-outpaint": {
    input: AceStepAudioOutpaintInput;
    output: AceStepAudioOutpaintOutput;
  };
  "fal-ai/ace-step/audio-inpaint": {
    input: AceStepAudioInpaintInput;
    output: AceStepAudioInpaintOutput;
  };
  "fal-ai/ace-step/audio-to-audio": {
    input: AceStepAudioToAudioInput;
    output: AceStepAudioToAudioOutput;
  };
  "fal-ai/ace-step/prompt-to-audio": {
    input: AceStepPromptToAudioInput;
    output: AceStepPromptToAudioOutput;
  };
  "smoretalk-ai/rembg-enhance": {
    input: RembgEnhanceInput;
    output: RembgEnhanceOutput;
  };
  "fal-ai/vidu/q1/start-end-to-video": {
    input: ViduQ1StartEndToVideoInput;
    output: ViduQ1StartEndToVideoOutput;
  };
  "fal-ai/vidu/q1/text-to-video": {
    input: ViduQ1TextToVideoInput;
    output: ViduQ1TextToVideoOutput;
  };
  "fal-ai/vidu/q1/image-to-video": {
    input: ViduQ1ImageToVideoInput;
    output: ViduQ1ImageToVideoOutput;
  };
  "fal-ai/ace-step": {
    input: AceStepInput;
    output: AceStepOutput;
  };
  "fal-ai/ltx-video-trainer": {
    input: LtxVideoTrainerInput;
    output: LtxVideoTrainerOutput;
  };
  "fal-ai/recraft/upscale/creative": {
    input: RecraftUpscaleCreativeInput;
    output: RecraftUpscaleCreativeOutput;
  };
  "fal-ai/recraft/upscale/crisp": {
    input: RecraftUpscaleCrispInput;
    output: RecraftUpscaleCrispOutput;
  };
  "fal-ai/recraft/v3/create-style": {
    input: RecraftV3CreateStyleInput;
    output: RecraftV3CreateStyleOutput;
  };
  "fal-ai/recraft/v3/image-to-image": {
    input: RecraftV3ImageToImageInput;
    output: RecraftV3ImageToImageOutput;
  };
  "fal-ai/ltx-video-v097/extend": {
    input: LtxVideoV097ExtendInput;
    output: LtxVideoV097ExtendOutput;
  };
  "fal-ai/minimax/voice-clone": {
    input: MinimaxVoiceCloneInput;
    output: MinimaxVoiceCloneOutput;
  };
  "fal-ai/minimax/speech-02-turbo": {
    input: MinimaxSpeech02TurboInput;
    output: MinimaxSpeech02TurboOutput;
  };
  "fal-ai/ltx-video-v097/multiconditioning": {
    input: LtxVideoV097MulticonditioningInput;
    output: LtxVideoV097MulticonditioningOutput;
  };
  "fal-ai/minimax/speech-02-hd": {
    input: MinimaxSpeech02HdInput;
    output: MinimaxSpeech02HdOutput;
  };
  "fal-ai/ltx-video-v097": {
    input: LtxVideoV097Input;
    output: LtxVideoV097Output;
  };
  "fal-ai/ltx-video-v097/image-to-video": {
    input: LtxVideoV097ImageToVideoInput;
    output: LtxVideoV097ImageToVideoOutput;
  };
  "fal-ai/minimax/image-01/subject-reference": {
    input: MinimaxImage01SubjectReferenceInput;
    output: MinimaxImage01SubjectReferenceOutput;
  };
  "fal-ai/minimax/image-01": {
    input: MinimaxImage01Input;
    output: MinimaxImage01Output;
  };
  "fal-ai/hidream-i1-full/image-to-image": {
    input: HidreamI1FullImageToImageInput;
    output: HidreamI1FullImageToImageOutput;
  };
  "fal-ai/pony-v7": {
    input: PonyV7Input;
    output: PonyV7Output;
  };
  "fal-ai/trellis/multi": {
    input: TrellisMultiInput;
    output: TrellisMultiOutput;
  };
  "fal-ai/ideogram/v3/reframe": {
    input: IdeogramV3ReframeInput;
    output: IdeogramV3ReframeOutput;
  };
  "fal-ai/ideogram/v3": {
    input: IdeogramV3Input;
    output: IdeogramV3Output;
  };
  "fal-ai/ideogram/v3/replace-background": {
    input: IdeogramV3ReplaceBackgroundInput;
    output: IdeogramV3ReplaceBackgroundOutput;
  };
  "fal-ai/ideogram/v3/remix": {
    input: IdeogramV3RemixInput;
    output: IdeogramV3RemixOutput;
  };
  "fal-ai/ideogram/v3/edit": {
    input: IdeogramV3EditInput;
    output: IdeogramV3EditOutput;
  };
  "fal-ai/hidream-e1-full": {
    input: HidreamE1FullInput;
    output: HidreamE1FullOutput;
  };
  "fal-ai/f-lite/standard": {
    input: FLiteStandardInput;
    output: FLiteStandardOutput;
  };
  "fal-ai/f-lite/texture": {
    input: FLiteTextureInput;
    output: FLiteTextureOutput;
  };
  "fal-ai/moondream2/visual-query": {
    input: Moondream2VisualQueryInput;
    output: Moondream2VisualQueryOutput;
  };
  "fal-ai/moondream2": {
    input: moondream2Input;
    output: moondream2Output;
  };
  "fal-ai/moondream2/point-object-detection": {
    input: Moondream2PointObjectDetectionInput;
    output: Moondream2PointObjectDetectionOutput;
  };
  "fal-ai/moondream2/object-detection": {
    input: Moondream2ObjectDetectionInput;
    output: Moondream2ObjectDetectionOutput;
  };
  "fal-ai/step1x-edit": {
    input: Step1xEditInput;
    output: Step1xEditOutput;
  };
  "tripo3d/tripo/v2.5/image-to-3d": {
    input: TripoV25ImageTo3dInput;
    output: TripoV25ImageTo3dOutput;
  };
  "fal-ai/image2svg": {
    input: image2svgInput;
    output: image2svgOutput;
  };
  "fal-ai/uno": {
    input: unoInput;
    output: unoOutput;
  };
  "fal-ai/magi/extend-video": {
    input: MagiExtendVideoInput;
    output: MagiExtendVideoOutput;
  };
  "fal-ai/magi": {
    input: magiInput;
    output: magiOutput;
  };
  "fal-ai/magi/image-to-video": {
    input: MagiImageToVideoInput;
    output: MagiImageToVideoOutput;
  };
  "fal-ai/gpt-image-1/text-to-image": {
    input: GptImage1TextToImageInput;
    output: GptImage1TextToImageOutput;
  };
  "fal-ai/gpt-image-1/edit-image": {
    input: GptImage1EditImageInput;
    output: GptImage1EditImageOutput;
  };
  "fal-ai/pixverse/v4/effects": {
    input: PixverseV4EffectsInput;
    output: PixverseV4EffectsOutput;
  };
  "fal-ai/magi-distilled/extend-video": {
    input: MagiDistilledExtendVideoInput;
    output: MagiDistilledExtendVideoOutput;
  };
  "fal-ai/magi-distilled/image-to-video": {
    input: MagiDistilledImageToVideoInput;
    output: MagiDistilledImageToVideoOutput;
  };
  "fal-ai/dia-tts/voice-clone": {
    input: DiaTtsVoiceCloneInput;
    output: DiaTtsVoiceCloneOutput;
  };
  "fal-ai/framepack/flf2v": {
    input: FramepackFlf2vInput;
    output: FramepackFlf2vOutput;
  };
  "fal-ai/dia-tts": {
    input: DiaTtsInput;
    output: DiaTtsOutput;
  };
  "fal-ai/magi-distilled": {
    input: MagiDistilledInput;
    output: MagiDistilledOutput;
  };
  "fal-ai/smart-turn": {
    input: SmartTurnInput;
    output: SmartTurnOutput;
  };
  "rundiffusion-fal/juggernaut-flux-lora/inpainting": {
    input: JuggernautFluxLoraInpaintingInput;
    output: JuggernautFluxLoraInpaintingOutput;
  };
  "fal-ai/fashn/tryon/v1.5": {
    input: FashnTryonV15Input;
    output: FashnTryonV15Output;
  };
  "fal-ai/plushify": {
    input: plushifyInput;
    output: plushifyOutput;
  };
  "fal-ai/instant-character": {
    input: InstantCharacterInput;
    output: InstantCharacterOutput;
  };
  "fal-ai/wan-flf2v": {
    input: WanFlf2vInput;
    output: WanFlf2vOutput;
  };
  "fal-ai/turbo-flux-trainer": {
    input: TurboFluxTrainerInput;
    output: TurboFluxTrainerOutput;
  };
  "fal-ai/framepack": {
    input: framepackInput;
    output: framepackOutput;
  };
  "fal-ai/cartoonify": {
    input: cartoonifyInput;
    output: cartoonifyOutput;
  };
  "fal-ai/wan-vace": {
    input: WanVaceInput;
    output: WanVaceOutput;
  };
  "fal-ai/finegrain-eraser/mask": {
    input: FinegrainEraserMaskInput;
    output: FinegrainEraserMaskOutput;
  };
  "fal-ai/finegrain-eraser/bbox": {
    input: FinegrainEraserBboxInput;
    output: FinegrainEraserBboxOutput;
  };
  "fal-ai/finegrain-eraser": {
    input: FinegrainEraserInput;
    output: FinegrainEraserOutput;
  };
  "cassetteai/video-sound-effects-generator": {
    input: VideoSoundEffectsGeneratorInput;
    output: VideoSoundEffectsGeneratorOutput;
  };
  "fal-ai/speech-to-text/turbo": {
    input: SpeechToTextTurboInput;
    output: SpeechToTextTurboOutput;
  };
  "fal-ai/speech-to-text/turbo/stream": {
    input: SpeechToTextTurboStreamInput;
    output: any;
  };
  "fal-ai/speech-to-text/stream": {
    input: SpeechToTextStreamInput;
    output: any;
  };
  "fal-ai/speech-to-text": {
    input: SpeechToTextInput;
    output: SpeechToTextOutput;
  };
  "cassetteai/sound-effects-generator": {
    input: SoundEffectsGeneratorInput;
    output: SoundEffectsGeneratorOutput;
  };
  "fal-ai/sync-lipsync/v2": {
    input: SyncLipsyncV2Input;
    output: SyncLipsyncV2Output;
  };
  "fal-ai/star-vector": {
    input: StarVectorInput;
    output: StarVectorOutput;
  };
  "fal-ai/pixverse/v4/image-to-video/fast": {
    input: PixverseV4ImageToVideoFastInput;
    output: PixverseV4ImageToVideoFastOutput;
  };
  "fal-ai/pixverse/v4/image-to-video": {
    input: PixverseV4ImageToVideoInput;
    output: PixverseV4ImageToVideoOutput;
  };
  "fal-ai/pixverse/v3.5/effects": {
    input: PixverseV35EffectsInput;
    output: PixverseV35EffectsOutput;
  };
  "fal-ai/pixverse/v4/text-to-video": {
    input: PixverseV4TextToVideoInput;
    output: PixverseV4TextToVideoOutput;
  };
  "fal-ai/pixverse/v3.5/transition": {
    input: PixverseV35TransitionInput;
    output: PixverseV35TransitionOutput;
  };
  "fal-ai/pixverse/v4/text-to-video/fast": {
    input: PixverseV4TextToVideoFastInput;
    output: PixverseV4TextToVideoFastOutput;
  };
  "fal-ai/ghiblify": {
    input: ghiblifyInput;
    output: ghiblifyOutput;
  };
  "fal-ai/orpheus-tts": {
    input: OrpheusTtsInput;
    output: OrpheusTtsOutput;
  };
  "fal-ai/sana/v1.5/1.6b": {
    input: SanaV1516bInput;
    output: SanaV1516bOutput;
  };
  "fal-ai/sana/v1.5/4.8b": {
    input: SanaV1548bInput;
    output: SanaV1548bOutput;
  };
  "fal-ai/sana/sprint": {
    input: SanaSprintInput;
    output: SanaSprintOutput;
  };
  "CassetteAI/music-generator": {
    input: MusicGeneratorInput;
    output: MusicGeneratorOutput;
  };
  "fal-ai/kling-video/lipsync/audio-to-video": {
    input: KlingVideoLipsyncAudioToVideoInput;
    output: KlingVideoLipsyncAudioToVideoOutput;
  };
  "fal-ai/kling-video/lipsync/text-to-video": {
    input: KlingVideoLipsyncTextToVideoInput;
    output: KlingVideoLipsyncTextToVideoOutput;
  };
  "fal-ai/latentsync": {
    input: latentsyncInput;
    output: latentsyncOutput;
  };
  "fal-ai/wan-t2v-lora": {
    input: WanT2vLoraInput;
    output: WanT2vLoraOutput;
  };
  "fal-ai/wan-trainer": {
    input: WanTrainerInput;
    output: WanTrainerOutput;
  };
  "fal-ai/thera": {
    input: theraInput;
    output: theraOutput;
  };
  "fal-ai/mix-dehaze-net": {
    input: MixDehazeNetInput;
    output: MixDehazeNetOutput;
  };
  "fal-ai/hunyuan3d/v2/multi-view": {
    input: Hunyuan3dV2MultiViewInput;
    output: Hunyuan3dV2MultiViewOutput;
  };
  "fal-ai/hunyuan3d/v2/mini": {
    input: Hunyuan3dV2MiniInput;
    output: Hunyuan3dV2MiniOutput;
  };
  "fal-ai/hunyuan3d/v2/turbo": {
    input: Hunyuan3dV2TurboInput;
    output: Hunyuan3dV2TurboOutput;
  };
  "fal-ai/hunyuan3d/v2/mini/turbo": {
    input: Hunyuan3dV2MiniTurboInput;
    output: Hunyuan3dV2MiniTurboOutput;
  };
  "fal-ai/gemini-flash-edit": {
    input: GeminiFlashEditInput;
    output: GeminiFlashEditOutput;
  };
  "fal-ai/gemini-flash-edit/multi": {
    input: GeminiFlashEditMultiInput;
    output: GeminiFlashEditMultiOutput;
  };
  "fal-ai/hunyuan3d/v2": {
    input: Hunyuan3dV2Input;
    output: Hunyuan3dV2Output;
  };
  "fal-ai/hunyuan3d/v2/multi-view/turbo": {
    input: Hunyuan3dV2MultiViewTurboInput;
    output: Hunyuan3dV2MultiViewTurboOutput;
  };
  "fal-ai/luma-dream-machine/ray-2-flash": {
    input: LumaDreamMachineRay2FlashInput;
    output: LumaDreamMachineRay2FlashOutput;
  };
  "fal-ai/luma-dream-machine/ray-2-flash/image-to-video": {
    input: LumaDreamMachineRay2FlashImageToVideoInput;
    output: LumaDreamMachineRay2FlashImageToVideoOutput;
  };
  "fal-ai/invisible-watermark": {
    input: InvisibleWatermarkInput;
    output: InvisibleWatermarkOutput;
  };
  "fal-ai/pika/v1.5/pikaffects": {
    input: PikaV15PikaffectsInput;
    output: PikaV15PikaffectsOutput;
  };
  "fal-ai/pika/v2.1/image-to-video": {
    input: PikaV21ImageToVideoInput;
    output: PikaV21ImageToVideoOutput;
  };
  "fal-ai/pika/v2.1/text-to-video": {
    input: PikaV21TextToVideoInput;
    output: PikaV21TextToVideoOutput;
  };
  "fal-ai/pika/v2.2/image-to-video": {
    input: PikaV22ImageToVideoInput;
    output: PikaV22ImageToVideoOutput;
  };
  "fal-ai/pika/v2.2/pikascenes": {
    input: PikaV22PikascenesInput;
    output: PikaV22PikascenesOutput;
  };
  "fal-ai/pika/v2.2/text-to-video": {
    input: PikaV22TextToVideoInput;
    output: PikaV22TextToVideoOutput;
  };
  "fal-ai/pika/v2/pikadditions": {
    input: PikaV2PikadditionsInput;
    output: PikaV2PikadditionsOutput;
  };
  "fal-ai/pika/v2/turbo/image-to-video": {
    input: PikaV2TurboImageToVideoInput;
    output: PikaV2TurboImageToVideoOutput;
  };
  "fal-ai/pika/v2/turbo/text-to-video": {
    input: PikaV2TurboTextToVideoInput;
    output: PikaV2TurboTextToVideoOutput;
  };
  "fal-ai/csm-1b": {
    input: Csm1bInput;
    output: Csm1bOutput;
  };
  "fal-ai/vidu/image-to-video": {
    input: ViduImageToVideoInput;
    output: ViduImageToVideoOutput;
  };
  "fal-ai/vidu/reference-to-video": {
    input: ViduReferenceToVideoInput;
    output: ViduReferenceToVideoOutput;
  };
  "fal-ai/vidu/start-end-to-video": {
    input: ViduStartEndToVideoInput;
    output: ViduStartEndToVideoOutput;
  };
  "fal-ai/vidu/template-to-video": {
    input: ViduTemplateToVideoInput;
    output: ViduTemplateToVideoOutput;
  };
  "fal-ai/wan-pro/text-to-video": {
    input: WanProTextToVideoInput;
    output: WanProTextToVideoOutput;
  };
  "fal-ai/wan-i2v-lora": {
    input: WanI2vLoraInput;
    output: WanI2vLoraOutput;
  };
  "fal-ai/kling-video/v1.6/pro/effects": {
    input: KlingVideoV16ProEffectsInput;
    output: KlingVideoV16ProEffectsOutput;
  };
  "fal-ai/kling-video/v1.6/standard/effects": {
    input: KlingVideoV16StandardEffectsInput;
    output: KlingVideoV16StandardEffectsOutput;
  };
  "fal-ai/hunyuan-video-image-to-video": {
    input: HunyuanVideoImageToVideoInput;
    output: HunyuanVideoImageToVideoOutput;
  };
  "fal-ai/kling-video/v1.5/pro/effects": {
    input: KlingVideoV15ProEffectsInput;
    output: KlingVideoV15ProEffectsOutput;
  };
  "fal-ai/kling-video/v1/standard/effects": {
    input: KlingVideoV1StandardEffectsInput;
    output: KlingVideoV1StandardEffectsOutput;
  };
  "fal-ai/ltx-video-v095/image-to-video": {
    input: LtxVideoV095ImageToVideoInput;
    output: LtxVideoV095ImageToVideoOutput;
  };
  "fal-ai/ltx-video-v095": {
    input: LtxVideoV095Input;
    output: LtxVideoV095Output;
  };
  "fal-ai/ltx-video-v095/extend": {
    input: LtxVideoV095ExtendInput;
    output: LtxVideoV095ExtendOutput;
  };
  "fal-ai/ltx-video-v095/multiconditioning": {
    input: LtxVideoV095MulticonditioningInput;
    output: LtxVideoV095MulticonditioningOutput;
  };
  "rundiffusion-fal/juggernaut-flux-lora": {
    input: JuggernautFluxLoraInput;
    output: JuggernautFluxLoraOutput;
  };
  "rundiffusion-fal/juggernaut-flux/base": {
    input: JuggernautFluxBaseInput;
    output: JuggernautFluxBaseOutput;
  };
  "rundiffusion-fal/juggernaut-flux/base/image-to-image": {
    input: JuggernautFluxBaseImageToImageInput;
    output: JuggernautFluxBaseImageToImageOutput;
  };
  "rundiffusion-fal/juggernaut-flux/lightning": {
    input: JuggernautFluxLightningInput;
    output: JuggernautFluxLightningOutput;
  };
  "rundiffusion-fal/juggernaut-flux/pro": {
    input: JuggernautFluxProInput;
    output: JuggernautFluxProOutput;
  };
  "rundiffusion-fal/juggernaut-flux/pro/image-to-image": {
    input: JuggernautFluxProImageToImageInput;
    output: JuggernautFluxProImageToImageOutput;
  };
  "rundiffusion-fal/rundiffusion-photo-flux": {
    input: RundiffusionPhotoFluxInput;
    output: RundiffusionPhotoFluxOutput;
  };
  "fal-ai/diffrhythm": {
    input: diffrhythmInput;
    output: diffrhythmOutput;
  };
  "fal-ai/cogview4": {
    input: cogview4Input;
    output: cogview4Output;
  };
  "fal-ai/topaz/upscale/video": {
    input: TopazUpscaleVideoInput;
    output: TopazUpscaleVideoOutput;
  };
  "fal-ai/docres/dewarp": {
    input: DocresDewarpInput;
    output: DocresDewarpOutput;
  };
  "fal-ai/docres": {
    input: docresInput;
    output: docresOutput;
  };
  "fal-ai/swin2sr": {
    input: swin2srInput;
    output: swin2srOutput;
  };
  "fal-ai/ideogram/v2a/remix": {
    input: IdeogramV2aRemixInput;
    output: IdeogramV2aRemixOutput;
  };
  "fal-ai/elevenlabs/speech-to-text": {
    input: ElevenlabsSpeechToTextInput;
    output: ElevenlabsSpeechToTextOutput;
  };
  "fal-ai/kling-video/v1.6/pro/text-to-video": {
    input: KlingVideoV16ProTextToVideoInput;
    output: KlingVideoV16ProTextToVideoOutput;
  };
  "fal-ai/ideogram/v2a/turbo/remix": {
    input: IdeogramV2aTurboRemixInput;
    output: IdeogramV2aTurboRemixOutput;
  };
  "fal-ai/elevenlabs/tts/multilingual-v2": {
    input: ElevenlabsTtsMultilingualV2Input;
    output: ElevenlabsTtsMultilingualV2Output;
  };
  "fal-ai/elevenlabs/tts/turbo-v2.5": {
    input: ElevenlabsTtsTurboV25Input;
    output: ElevenlabsTtsTurboV25Output;
  };
  "fal-ai/elevenlabs/audio-isolation": {
    input: ElevenlabsAudioIsolationInput;
    output: ElevenlabsAudioIsolationOutput;
  };
  "fal-ai/ideogram/v2a": {
    input: IdeogramV2aInput;
    output: IdeogramV2aOutput;
  };
  "fal-ai/ideogram/v2a/turbo": {
    input: IdeogramV2aTurboInput;
    output: IdeogramV2aTurboOutput;
  };
  "fal-ai/wan/v2.1/1.3b/text-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/evf-sam": {
    input: EvfSamInput;
    output: EvfSamOutput;
  };
  "fal-ai/ddcolor": {
    input: ddcolorInput;
    output: ddcolorOutput;
  };
  "fal-ai/video-prompt-generator": {
    input: VideoPromptGeneratorInput;
    output: VideoPromptGeneratorOutput;
  };
  "fal-ai/sam2/auto-segment": {
    input: Sam2AutoSegmentInput;
    output: Sam2AutoSegmentOutput;
  };
  "fal-ai/wan-t2v": {
    input: WanT2vInput;
    output: WanT2vOutput;
  };
  "fal-ai/minimax/video-01-director/image-to-video": {
    input: MinimaxVideo01DirectorImageToVideoInput;
    output: MinimaxVideo01DirectorImageToVideoOutput;
  };
  "fal-ai/drct-super-resolution": {
    input: DrctSuperResolutionInput;
    output: DrctSuperResolutionOutput;
  };
  "fal-ai/nafnet/deblur": {
    input: NafnetDeblurInput;
    output: NafnetDeblurOutput;
  };
  "fal-ai/nafnet/denoise": {
    input: NafnetDenoiseInput;
    output: NafnetDenoiseOutput;
  };
  "fal-ai/veo2": {
    input: veo2Input;
    output: veo2Output;
  };
  "fal-ai/post-processing": {
    input: PostProcessingInput;
    output: PostProcessingOutput;
  };
  "fal-ai/skyreels-i2v": {
    input: SkyreelsI2vInput;
    output: SkyreelsI2vOutput;
  };
  "fal-ai/flowedit": {
    input: floweditInput;
    output: floweditOutput;
  };
  "fal-ai/kokoro/hindi": {
    input: KokoroHindiInput;
    output: KokoroHindiOutput;
  };
  "fal-ai/kokoro/british-english": {
    input: KokoroBritishEnglishInput;
    output: KokoroBritishEnglishOutput;
  };
  "fal-ai/kokoro/american-english": {
    input: KokoroAmericanEnglishInput;
    output: KokoroAmericanEnglishOutput;
  };
  "fal-ai/zonos": {
    input: zonosInput;
    output: zonosOutput;
  };
  "fal-ai/kokoro/italian": {
    input: KokoroItalianInput;
    output: KokoroItalianOutput;
  };
  "fal-ai/kokoro/brazilian-portuguese": {
    input: KokoroBrazilianPortugueseInput;
    output: KokoroBrazilianPortugueseOutput;
  };
  "fal-ai/kokoro/french": {
    input: KokoroFrenchInput;
    output: KokoroFrenchOutput;
  };
  "fal-ai/kokoro/japanese": {
    input: KokoroJapaneseInput;
    output: KokoroJapaneseOutput;
  };
  "fal-ai/kokoro/mandarin-chinese": {
    input: KokoroMandarinChineseInput;
    output: KokoroMandarinChineseOutput;
  };
  "fal-ai/kokoro/spanish": {
    input: KokoroSpanishInput;
    output: KokoroSpanishOutput;
  };
  "fal-ai/luma-dream-machine/ray-2/image-to-video": {
    input: LumaDreamMachineRay2ImageToVideoInput;
    output: LumaDreamMachineRay2ImageToVideoOutput;
  };
  "fal-ai/got-ocr/v2": {
    input: GotOcrV2Input;
    output: GotOcrV2Output;
  };
  "fal-ai/flux-control-lora-canny": {
    input: FluxControlLoraCannyInput;
    output: FluxControlLoraCannyOutput;
  };
  "fal-ai/ben/v2/image": {
    input: BenV2ImageInput;
    output: BenV2ImageOutput;
  };
  "fal-ai/ben/v2/video": {
    input: BenV2VideoInput;
    output: BenV2VideoOutput;
  };
  "fal-ai/flux-control-lora-canny/image-to-image": {
    input: FluxControlLoraCannyImageToImageInput;
    output: FluxControlLoraCannyImageToImageOutput;
  };
  "fal-ai/flux-control-lora-depth": {
    input: FluxControlLoraDepthInput;
    output: FluxControlLoraDepthOutput;
  };
  "fal-ai/flux-control-lora-depth/image-to-image": {
    input: FluxControlLoraDepthImageToImageInput;
    output: FluxControlLoraDepthImageToImageOutput;
  };
  "fal-ai/minimax/video-01-director": {
    input: MinimaxVideo01DirectorInput;
    output: MinimaxVideo01DirectorOutput;
  };
  "fal-ai/imagen3/fast": {
    input: Imagen3FastInput;
    output: Imagen3FastOutput;
  };
  "fal-ai/ideogram/upscale": {
    input: IdeogramUpscaleInput;
    output: IdeogramUpscaleOutput;
  };
  "fal-ai/imagen3": {
    input: imagen3Input;
    output: imagen3Output;
  };
  "fal-ai/hunyuan-video-img2vid-lora": {
    input: HunyuanVideoImg2vidLoraInput;
    output: HunyuanVideoImg2vidLoraOutput;
  };
  "fal-ai/codeformer": {
    input: codeformerInput;
    output: codeformerOutput;
  };
  "fal-ai/lumina-image/v2": {
    input: LuminaImageV2Input;
    output: LuminaImageV2Output;
  };
  "fal-ai/hunyuan-video-lora/video-to-video": {
    input: HunyuanVideoLoraVideoToVideoInput;
    output: HunyuanVideoLoraVideoToVideoOutput;
  };
  "fal-ai/hunyuan-video/video-to-video": {
    input: HunyuanVideoVideoToVideoInput;
    output: HunyuanVideoVideoToVideoOutput;
  };
  "fal-ai/pixverse/v3.5/image-to-video/fast": {
    input: PixverseV35ImageToVideoFastInput;
    output: PixverseV35ImageToVideoFastOutput;
  };
  "fal-ai/pixverse/v3.5/image-to-video": {
    input: PixverseV35ImageToVideoInput;
    output: PixverseV35ImageToVideoOutput;
  };
  "fal-ai/pixverse/v3.5/text-to-video": {
    input: PixverseV35TextToVideoInput;
    output: PixverseV35TextToVideoOutput;
  };
  "fal-ai/pixverse/v3.5/text-to-video/fast": {
    input: PixverseV35TextToVideoFastInput;
    output: PixverseV35TextToVideoFastOutput;
  };
  "fal-ai/janus": {
    input: janusInput;
    output: janusOutput;
  };
  "fal-ai/yue": {
    input: yueInput;
    output: yueOutput;
  };
  "fal-ai/luma-dream-machine/ray-2": {
    input: LumaDreamMachineRay2Input;
    output: LumaDreamMachineRay2Output;
  };
  "fal-ai/kling/v1-5/kolors-virtual-try-on": {
    input: KlingV15KolorsVirtualTryOnInput;
    output: KlingV15KolorsVirtualTryOnOutput;
  };
  "fal-ai/ffmpeg-api/waveform": {
    input: FfmpegApiWaveformInput;
    output: FfmpegApiWaveformOutput;
  };
  "fal-ai/ffmpeg-api/metadata": {
    input: FfmpegApiMetadataInput;
    output: FfmpegApiMetadataOutput;
  };
  "fal-ai/ffmpeg-api/compose": {
    input: FfmpegApiComposeInput;
    output: FfmpegApiComposeOutput;
  };
  "fal-ai/minimax/video-01-subject-reference": {
    input: MinimaxVideo01SubjectReferenceInput;
    output: MinimaxVideo01SubjectReferenceOutput;
  };
  "fal-ai/moondream-next/batch": {
    input: MoondreamNextBatchInput;
    output: MoondreamNextBatchOutput;
  };
  "fal-ai/flux-lora-canny": {
    input: FluxLoraCannyInput;
    output: FluxLoraCannyOutput;
  };
  "fal-ai/flux-pro/v1.1": {
    input: FluxProV11Input;
    output: FluxProV11Output;
  };
  "fal-ai/flux-pro/v1.1-ultra-finetuned": {
    input: FluxProV11UltraFinetunedInput;
    output: FluxProV11UltraFinetunedOutput;
  };
  "fal-ai/flux-pro/v1/fill-finetuned": {
    input: FluxProV1FillFinetunedInput;
    output: FluxProV1FillFinetunedOutput;
  };
  "fal-ai/hunyuan-video-lora": {
    input: HunyuanVideoLoraInput;
    output: HunyuanVideoLoraOutput;
  };
  "fal-ai/transpixar": {
    input: transpixarInput;
    output: transpixarOutput;
  };
  "fal-ai/cogvideox-5b": {
    input: Cogvideox5bInput;
    output: Cogvideox5bOutput;
  };
  "fal-ai/hunyuan-video-lora-training": {
    input: HunyuanVideoLoraTrainingInput;
    output: HunyuanVideoLoraTrainingOutput;
  };
  "fal-ai/sa2va/4b/video": {
    input: Sa2va4bVideoInput;
    output: Sa2va4bVideoOutput;
  };
  "fal-ai/sa2va/8b/video": {
    input: Sa2va8bVideoInput;
    output: Sa2va8bVideoOutput;
  };
  "fal-ai/sync-lipsync": {
    input: SyncLipsyncInput;
    output: SyncLipsyncOutput;
  };
  "fal-ai/sa2va/4b/image": {
    input: Sa2va4bImageInput;
    output: Sa2va4bImageOutput;
  };
  "fal-ai/sa2va/8b/image": {
    input: any;
    output: any;
  };
  "fal-ai/moondream-next/detection": {
    input: any;
    output: any;
  };
  "fal-ai/moondream-next": {
    input: any;
    output: any;
  };
  "fal-ai/kling-video/v1.6/standard/image-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/kling-video/v1.6/standard/text-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/auto-caption": {
    input: any;
    output: any;
  };
  "fal-ai/switti": {
    input: any;
    output: any;
  };
  "fal-ai/switti/512": {
    input: any;
    output: any;
  };
  "fal-ai/dubbing": {
    input: any;
    output: any;
  };
  "fal-ai/sadtalker/reference": {
    input: any;
    output: any;
  };
  "fal-ai/mmaudio-v2/text-to-audio": {
    input: any;
    output: any;
  };
  "fal-ai/bria/eraser": {
    input: any;
    output: any;
  };
  "fal-ai/bria/expand": {
    input: any;
    output: any;
  };
  "fal-ai/bria/genfill": {
    input: any;
    output: any;
  };
  "fal-ai/bria/text-to-image/base": {
    input: any;
    output: any;
  };
  "fal-ai/bria/text-to-image/fast": {
    input: any;
    output: any;
  };
  "fal-ai/bria/text-to-image/hd": {
    input: any;
    output: any;
  };
  "fal-ai/bria/product-shot": {
    input: any;
    output: any;
  };
  "fal-ai/bria/background/remove": {
    input: any;
    output: any;
  };
  "fal-ai/bria/background/replace": {
    input: any;
    output: any;
  };
  "fal-ai/flux-lora-fill": {
    input: any;
    output: any;
  };
  "fal-ai/cat-vton": {
    input: any;
    output: any;
  };
  "fal-ai/leffa/pose-transfer": {
    input: any;
    output: any;
  };
  "fal-ai/leffa/virtual-tryon": {
    input: any;
    output: any;
  };
  "fal-ai/minimax-music": {
    input: any;
    output: any;
  };
  "fal-ai/hyper3d/rodin": {
    input: any;
    output: any;
  };
  "fal-ai/minimax/video-01-live": {
    input: any;
    output: any;
  };
  "fal-ai/minimax/video-01-live/image-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/recraft-20b": {
    input: any;
    output: any;
  };
  "fal-ai/ideogram/v2/edit": {
    input: any;
    output: any;
  };
  "fal-ai/trellis": {
    input: any;
    output: any;
  };
  "fal-ai/ideogram/v2/turbo": {
    input: any;
    output: any;
  };
  "fal-ai/ideogram/v2/remix": {
    input: any;
    output: any;
  };
  "fal-ai/video-upscaler": {
    input: any;
    output: any;
  };
  "fal-ai/ideogram/v2/turbo/edit": {
    input: any;
    output: any;
  };
  "fal-ai/ideogram/v2/turbo/remix": {
    input: any;
    output: any;
  };
  "fal-ai/kling-video/v1/standard/text-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/luma-photon/flash": {
    input: any;
    output: any;
  };
  "fal-ai/aura-flow": {
    input: any;
    output: any;
  };
  "fal-ai/omnigen-v1": {
    input: any;
    output: any;
  };
  "fal-ai/flux/schnell/redux": {
    input: any;
    output: any;
  };
  "fal-ai/kling-video/v1.5/pro/text-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/flux/schnell": {
    input: any;
    output: any;
  };
  "fal-ai/flux-pro/v1.1/redux": {
    input: any;
    output: any;
  };
  "fal-ai/flux-lora-depth": {
    input: any;
    output: any;
  };
  "fal-ai/flux-pro/v1.1-ultra/redux": {
    input: any;
    output: any;
  };
  "fal-ai/flux-pro/v1/fill": {
    input: any;
    output: any;
  };
  "fal-ai/flux-pro/v1/redux": {
    input: any;
    output: any;
  };
  "fal-ai/flux/dev/redux": {
    input: any;
    output: any;
  };
  "fal-ai/ltx-video/image-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/kolors/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/iclight-v2": {
    input: any;
    output: any;
  };
  "fal-ai/mochi-v1": {
    input: any;
    output: any;
  };
  "fal-ai/flux-differential-diffusion": {
    input: any;
    output: any;
  };
  "fal-ai/flux-pulid": {
    input: any;
    output: any;
  };
  "fal-ai/birefnet/v2": {
    input: any;
    output: any;
  };
  "fal-ai/stable-diffusion-v35-medium": {
    input: any;
    output: any;
  };
  "fal-ai/hunyuan-video": {
    input: any;
    output: any;
  };
  "fal-ai/cogvideox-5b/image-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/f5-tts": {
    input: any;
    output: any;
  };
  "fal-ai/cogvideox-5b/video-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/llavav15-13b": {
    input: any;
    output: any;
  };
  "fal-ai/ltx-video": {
    input: any;
    output: any;
  };
  "fal-ai/kling-video/v1.5/pro/image-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/kling-video/v1/pro/image-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/kling-video/v1/pro/text-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/kling-video/v1/standard/image-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/flux-pro/new": {
    input: any;
    output: any;
  };
  "fal-ai/live-portrait/image": {
    input: any;
    output: any;
  };
  "fal-ai/flux-lora/inpainting": {
    input: any;
    output: any;
  };
  "fal-ai/flux-general/rf-inversion": {
    input: any;
    output: any;
  };
  "fal-ai/stable-video": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/hed": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/depth-anything/v2": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/scribble": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/mlsd": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/sam": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/midas": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/teed": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/lineart": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/zoe": {
    input: any;
    output: any;
  };
  "fal-ai/fast-svd/text-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/image-preprocessors/pidi": {
    input: any;
    output: any;
  };
  "fal-ai/controlnext": {
    input: any;
    output: any;
  };
  "fal-ai/stable-diffusion-v3-medium": {
    input: any;
    output: any;
  };
  "fal-ai/imageutils/sam": {
    input: any;
    output: any;
  };
  "fal-ai/sam2/image": {
    input: any;
    output: any;
  };
  "fal-ai/sam2/video": {
    input: any;
    output: any;
  };
  "fal-ai/flux-general/inpainting": {
    input: any;
    output: any;
  };
  "fal-ai/flux-general/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/flux-general/differential-diffusion": {
    input: any;
    output: any;
  };
  "fal-ai/flux-lora/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/fooocus/upscale-or-vary": {
    input: any;
    output: any;
  };
  "fal-ai/sana": {
    input: any;
    output: any;
  };
  "fal-ai/pixart-sigma": {
    input: any;
    output: any;
  };
  "fal-ai/flux-subject": {
    input: any;
    output: any;
  };
  "fal-ai/sdxl-controlnet-union": {
    input: any;
    output: any;
  };
  "fal-ai/sdxl-controlnet-union/inpainting": {
    input: any;
    output: any;
  };
  "fal-ai/sdxl-controlnet-union/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/kolors": {
    input: any;
    output: any;
  };
  "fal-ai/amt-interpolation/frame-interpolation": {
    input: any;
    output: any;
  };
  "fal-ai/muse-pose": {
    input: any;
    output: any;
  };
  "fal-ai/live-portrait": {
    input: any;
    output: any;
  };
  "fal-ai/era-3d": {
    input: any;
    output: any;
  };
  "fal-ai/stable-cascade": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/dense-region-caption": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/referring-expression-segmentation": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/object-detection": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/open-vocabulary-detection": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/region-to-description": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/ocr": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/more-detailed-caption": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/caption-to-phrase-grounding": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/region-proposal": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/region-to-category": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/caption": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/ocr-with-region": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/detailed-caption": {
    input: any;
    output: any;
  };
  "fal-ai/florence-2-large/region-to-segmentation": {
    input: any;
    output: any;
  };
  "fal-ai/fast-sdxl": {
    input: any;
    output: any;
  };
  "fal-ai/stable-diffusion-v3-medium/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/stable-cascade/sote-diffusion": {
    input: any;
    output: any;
  };
  "fal-ai/fast-svd-lcm/text-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/luma-photon": {
    input: any;
    output: any;
  };
  "fal-ai/dwpose": {
    input: any;
    output: any;
  };
  "fal-ai/sd15-depth-controlnet": {
    input: any;
    output: any;
  };
  "fal-ai/ccsr": {
    input: any;
    output: any;
  };
  "fal-ai/omni-zero": {
    input: any;
    output: any;
  };
  "fal-ai/lightning-models": {
    input: any;
    output: any;
  };
  "fal-ai/playground-v25": {
    input: any;
    output: any;
  };
  "fal-ai/hyper-sdxl/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/realistic-vision": {
    input: any;
    output: any;
  };
  "fal-ai/dreamshaper": {
    input: any;
    output: any;
  };
  "fal-ai/hyper-sdxl/inpainting": {
    input: any;
    output: any;
  };
  "fal-ai/ip-adapter-face-id": {
    input: any;
    output: any;
  };
  "fal-ai/lora/inpaint": {
    input: any;
    output: any;
  };
  "fal-ai/lora/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/stable-diffusion-v15": {
    input: any;
    output: any;
  };
  "fal-ai/fast-sdxl/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/fast-sdxl/inpainting": {
    input: any;
    output: any;
  };
  "fal-ai/layer-diffusion": {
    input: any;
    output: any;
  };
  "fal-ai/musetalk": {
    input: any;
    output: any;
  };
  "fal-ai/fast-lightning-sdxl": {
    input: any;
    output: any;
  };
  "fal-ai/sadtalker": {
    input: any;
    output: any;
  };
  "fal-ai/wizper": {
    input: any;
    output: any;
  };
  "fal-ai/imageutils/nsfw": {
    input: any;
    output: any;
  };
  "fal-ai/moondream/batched": {
    input: any;
    output: any;
  };
  "fal-ai/fast-fooocus-sdxl/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/face-to-sticker": {
    input: any;
    output: any;
  };
  "fal-ai/photomaker": {
    input: any;
    output: any;
  };
  "fal-ai/t2v-turbo": {
    input: any;
    output: any;
  };
  "fal-ai/fast-sdxl-controlnet-canny": {
    input: any;
    output: any;
  };
  "fal-ai/creative-upscaler": {
    input: any;
    output: any;
  };
  "fal-ai/birefnet": {
    input: any;
    output: any;
  };
  "fal-ai/fast-animatediff/text-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/playground-v25/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/amt-interpolation": {
    input: any;
    output: any;
  };
  "fal-ai/fast-lightning-sdxl/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/fast-lightning-sdxl/inpainting": {
    input: any;
    output: any;
  };
  "fal-ai/hyper-sdxl": {
    input: any;
    output: any;
  };
  "fal-ai/playground-v25/inpainting": {
    input: any;
    output: any;
  };
  "fal-ai/fast-lcm-diffusion/inpainting": {
    input: any;
    output: any;
  };
  "fal-ai/fast-lcm-diffusion": {
    input: any;
    output: any;
  };
  "fal-ai/fast-lcm-diffusion/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/whisper": {
    input: any;
    output: any;
  };
  "fal-ai/fast-fooocus-sdxl": {
    input: any;
    output: any;
  };
  "fal-ai/llava-next": {
    input: any;
    output: any;
  };
  "fal-ai/fast-animatediff/turbo/text-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/retoucher": {
    input: any;
    output: any;
  };
  "fal-ai/illusion-diffusion": {
    input: any;
    output: any;
  };
  "fal-ai/fooocus/image-prompt": {
    input: any;
    output: any;
  };
  "fal-ai/imageutils/depth": {
    input: any;
    output: any;
  };
  "fal-ai/fast-animatediff/turbo/video-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/fooocus/inpaint": {
    input: any;
    output: any;
  };
  "fal-ai/fast-svd-lcm": {
    input: any;
    output: any;
  };
  "fal-ai/fast-animatediff/video-to-video": {
    input: any;
    output: any;
  };
  "fal-ai/minimax/video-01": {
    input: any;
    output: any;
  };
  "fal-ai/lcm": {
    input: any;
    output: any;
  };
  "fal-ai/triposr": {
    input: any;
    output: any;
  };
  "fal-ai/diffusion-edge": {
    input: any;
    output: any;
  };
  "fal-ai/stable-audio": {
    input: any;
    output: any;
  };
  "fal-ai/imageutils/marigold-depth": {
    input: any;
    output: any;
  };
  "fal-ai/pulid": {
    input: any;
    output: any;
  };
  "fal-ai/fast-sdxl-controlnet-canny/image-to-image": {
    input: any;
    output: any;
  };
  "fal-ai/fast-sdxl-controlnet-canny/inpainting": {
    input: any;
    output: any;
  };
  "fal-ai/fooocus": {
    input: any;
    output: any;
  };
  "fal-ai/lcm-sd15-i2i": {
    input: any;
    output: any;
  };
  "fal-ai/animatediff-sparsectrl-lcm": {
    input: any;
    output: any;
  };
  "fal-ai/inpaint": {
    input: any;
    output: any;
  };
  "fal-ai/controlnetsdxl": {
    input: any;
    output: any;
  };
  "fal-ai/esrgan": {
    input: any;
    output: any;
  };
  "fal-ai/imageutils/rembg": {
    input: any;
    output: any;
  };
  "fal-ai/lora": {
    input: any;
    output: any;
  };
};
