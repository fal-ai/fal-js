// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: "https://queue.fal.run" | (string & {});
};

/**
 * AnimateDiffV2VOutput
 */
export type FastAnimatediffVideoToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file.
   */
  video: FileType2;
};

/**
 * File
 */
export type FileType2 = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
};

/**
 * AnimateDiffV2VInput
 */
export type FastAnimatediffVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use for generating the image. Be as descriptive as possible for best results.
   */
  prompt: string;
  /**
   * Video Url
   *
   * URL of the video.
   */
  video_url: string;
  /**
   * First N Seconds
   *
   * The first N number of seconds of video to animate.
   */
  first_n_seconds?: number;
  /**
   * Fps
   *
   * Number of frames per second to extract from the video.
   */
  fps?: number;
  /**
   * Strength
   *
   * The strength of the input video in the final output.
   */
  strength?: number;
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you.
   *
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   *
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution).
   *
   */
  negative_prompt?: string;
  /**
   * Motions
   *
   * The motions to apply to the video.
   */
  motions?: Array<
    "zoom-out" | "zoom-in" | "pan-left" | "pan-right" | "tilt-up" | "tilt-down"
  >;
};

/**
 * AnimateDiffV2VOutput
 */
export type FastAnimatediffTurboVideoToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file.
   */
  video: FileType2;
};

/**
 * AnimateDiffV2VTurboInput
 */
export type FastAnimatediffTurboVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use for generating the image. Be as descriptive as possible for best results.
   */
  prompt: string;
  /**
   * Video Url
   *
   * URL of the video.
   */
  video_url: string;
  /**
   * First N Seconds
   *
   * The first N number of seconds of video to animate.
   */
  first_n_seconds?: number;
  /**
   * Fps
   *
   * Number of frames per second to extract from the video.
   */
  fps?: number;
  /**
   * Strength
   *
   * The strength of the input video in the final output.
   */
  strength?: number;
  /**
   * Guidance Scale
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform. 4-12 is recommended for turbo mode.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   *
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution).
   *
   */
  negative_prompt?: string;
  /**
   * Motions
   *
   * The motions to apply to the video.
   */
  motions?: Array<
    "zoom-out" | "zoom-in" | "pan-left" | "pan-right" | "tilt-up" | "tilt-down"
  >;
};

/**
 * AMTInterpolationOutput
 */
export type AmtInterpolationOutput = {
  /**
   * Video
   *
   * Generated video
   */
  video: File;
};

/**
 * File
 */
export type File = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File;
};

/**
 * AMTInterpolationInput
 */
export type AmtInterpolationInput = {
  /**
   * Video URL
   *
   * URL of the video to be processed
   */
  video_url: string;
  /**
   * Recursive Interpolation Passes
   *
   * Number of recursive interpolation passes
   */
  recursive_interpolation_passes?: number;
  /**
   * Output FPS
   *
   * Output frames per second
   */
  output_fps?: number;
};

/**
 * SAM2VideoOutput
 */
export type Sam2VideoOutput = {
  /**
   * Boundingbox Frames Zip
   *
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
  /**
   * Video
   *
   * The segmented video.
   */
  video: File;
};

/**
 * SAM2VideoRLEInput
 */
export type Sam2VideoInput = {
  /**
   * Boundingbox Zip
   *
   * Return per-frame bounding box overlays as a zip archive.
   */
  boundingbox_zip?: boolean;
  /**
   * Prompts
   *
   * List of prompts to segment the video
   */
  prompts?: Array<PointPromptType2>;
  /**
   * Video Url
   *
   * The URL of the video to be segmented.
   */
  video_url: string;
  /**
   * Box Prompts
   *
   * Coordinates for boxes
   */
  box_prompts?: Array<BoxPromptType2>;
  /**
   * Apply Mask
   *
   * Apply the mask on the video.
   */
  apply_mask?: boolean;
  /**
   * Mask Url
   *
   * The URL of the mask to be applied initially.
   */
  mask_url?: string;
};

/**
 * BoxPrompt
 */
export type BoxPromptType2 = {
  /**
   * Y Min
   *
   * Y Min Coordinate of the box
   */
  y_min?: number;
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number;
  /**
   * X Max
   *
   * X Max Coordinate of the prompt
   */
  x_max?: number;
  /**
   * X Min
   *
   * X Min Coordinate of the box
   */
  x_min?: number;
  /**
   * Y Max
   *
   * Y Max Coordinate of the prompt
   */
  y_max?: number;
};

/**
 * PointPrompt
 */
export type PointPromptType2 = {
  /**
   * Y
   *
   * Y Coordinate of the prompt
   */
  y?: number;
  /**
   * Label
   *
   * Label of the prompt. 1 for foreground, 0 for background
   */
  label?: 0 | 1;
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number;
  /**
   * X
   *
   * X Coordinate of the prompt
   */
  x?: number;
};

/**
 * ControlNeXtOutput
 */
export type ControlnextOutput = {
  /**
   * The generated video.
   */
  video: FileType2;
};

/**
 * ControlNeXtInput
 */
export type ControlnextInput = {
  /**
   * Controlnext Cond Scale
   *
   * Condition scale for ControlNeXt.
   */
  controlnext_cond_scale?: number;
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string;
  /**
   * Fps
   *
   * Frames per second for the output video.
   */
  fps?: number;
  /**
   * Max Frame Num
   *
   * Maximum number of frames to process.
   */
  max_frame_num?: number;
  /**
   * Width
   *
   * Width of the output video.
   */
  width?: number;
  /**
   * Overlap
   *
   * Number of overlapping frames between batches.
   */
  overlap?: number;
  /**
   * Guidance Scale
   *
   * Guidance scale for the diffusion process.
   */
  guidance_scale?: number;
  /**
   * Batch Frames
   *
   * Number of frames to process in each batch.
   */
  batch_frames?: number;
  /**
   * Height
   *
   * Height of the output video.
   */
  height?: number;
  /**
   * Sample Stride
   *
   * Stride for sampling frames from the input video.
   */
  sample_stride?: number;
  /**
   * Image Url
   *
   * URL of the reference image.
   */
  image_url: string;
  /**
   * Decode Chunk Size
   *
   * Chunk size for decoding frames.
   */
  decode_chunk_size?: number;
  /**
   * Motion Bucket Id
   *
   * Motion bucket ID for the pipeline.
   */
  motion_bucket_id?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps.
   */
  num_inference_steps?: number;
};

/**
 * Output
 */
export type Cogvideox5bVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generating the video.
   */
  prompt: string;
  /**
   * Timings
   */
  timings: {
    [key: string]: number;
  };
  /**
   * Seed
   *
   *
   * Seed of the generated video. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   */
  seed: number;
  /**
   * Video
   *
   * The URL to the generated video
   */
  video: File;
};

/**
 * VideoToVideoInput
 */
export type Cogvideox5bVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Input Video Url
   *
   * The video to generate the video from.
   */
  video_url: string;
  /**
   * Use Rife
   *
   * Use RIFE for video interpolation
   */
  use_rife?: boolean;
  /**
   * Loras
   *
   *
   * The LoRAs to use for the image generation. We currently support one lora.
   *
   */
  loras?: Array<LoraWeight>;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Strength
   *
   * The strength to use for Video to Video.  1.0 completely remakes the video while 0.0 preserves the original.
   */
  strength?: number;
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you.
   *
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number;
  /**
   * Export Fps
   *
   * The target FPS of the video
   */
  export_fps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate video from
   */
  negative_prompt?: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
};

/**
 * ImageSize
 */
export type ImageSize = {
  /**
   * Height
   *
   * The height of the generated image.
   */
  height?: number;
  /**
   * Width
   *
   * The width of the generated image.
   */
  width?: number;
};

/**
 * LoraWeight
 */
export type LoraWeight = {
  /**
   * Path
   *
   * URL or the path to the LoRA weights.
   */
  path: string;
  /**
   * Scale
   *
   *
   * The scale of the LoRA weight. This is used to scale the LoRA weight
   * before merging it with the base model.
   *
   */
  scale?: number;
};

/**
 * Output
 */
export type VideoUpscalerOutput = {
  /**
   * Video
   *
   * The stitched video
   */
  video: File;
};

/**
 * Input
 */
export type VideoUpscalerInput = {
  /**
   * Video Url
   *
   * The URL of the video to upscale
   */
  video_url: string;
  /**
   * Scale
   *
   * The scale factor
   */
  scale?: number;
};

/**
 * OutputModel
 */
export type DubbingOutput = {
  /**
   * Video
   *
   * The generated video with the lip sync.
   */
  video: File;
};

/**
 * InputModel
 */
export type DubbingInput = {
  /**
   * Do Lipsync
   *
   * Whether to lip sync the audio to the video
   */
  do_lipsync?: boolean;
  /**
   * Video Url
   *
   * Input video URL to be dubbed.
   */
  video_url: string;
  /**
   * Target Language
   *
   * Target language to dub the video to
   */
  target_language?: "hindi" | "turkish" | "english";
};

/**
 * Output
 */
export type AutoCaptionOutput = {
  /**
   * Video Url
   *
   * URL to the caption .mp4 video.
   */
  video_url: string;
};

/**
 * CaptionInput
 */
export type AutoCaptionInput = {
  /**
   * Txt Font
   *
   * Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file
   */
  txt_font?: string;
  /**
   * Video Url
   *
   * URL to the .mp4 video with audio. Only videos of size <100MB are allowed.
   */
  video_url: string;
  /**
   * Top Align
   *
   * Top-to-bottom alignment of the text. Can be a string ('top', 'center', 'bottom') or a float (0.0-1.0)
   */
  top_align?: string | number;
  /**
   * Txt Color
   *
   * Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation.
   */
  txt_color?: string;
  /**
   * Stroke Width
   *
   * Width of the text strokes in pixels
   */
  stroke_width?: number;
  /**
   * Refresh Interval
   *
   * Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once.
   */
  refresh_interval?: number;
  /**
   * Font Size
   *
   * Size of text in generated captions.
   */
  font_size?: number;
  /**
   * Left Align
   *
   * Left-to-right alignment of the text. Can be a string ('left', 'center', 'right') or a float (0.0-1.0)
   */
  left_align?: string | number;
};

/**
 * LipSyncOutput
 */
export type SyncLipsyncOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LipSyncInput
 */
export type SyncLipsyncInput = {
  /**
   * Model
   *
   * The model to use for lipsyncing
   */
  model?: "lipsync-1.8.0" | "lipsync-1.7.1" | "lipsync-1.9.0-beta";
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string;
  /**
   * Sync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  sync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
  /**
   * Audio Url
   *
   * URL of the input audio
   */
  audio_url: string;
};

/**
 * Keyframe
 */
export type Keyframe = {
  /**
   * Duration
   *
   * The duration in milliseconds of this keyframe
   */
  duration: number;
  /**
   * Timestamp
   *
   * The timestamp in milliseconds where this keyframe starts
   */
  timestamp: number;
  /**
   * Url
   *
   * The URL where this keyframe's media file can be accessed
   */
  url: string;
};

/**
 * Track
 */
export type Track = {
  /**
   * Type
   *
   * Type of track ('video' or 'audio')
   */
  type: string;
  /**
   * Id
   *
   * Unique identifier for the track
   */
  id: string;
  /**
   * Keyframes
   *
   * List of keyframes that make up this track
   */
  keyframes: Array<Keyframe>;
};

/**
 * ComposeOutput
 */
export type FfmpegApiComposeOutput = {
  /**
   * Video Url
   *
   * URL of the processed video file
   */
  video_url: string;
  /**
   * Thumbnail Url
   *
   * URL of the video's thumbnail image
   */
  thumbnail_url: string;
};

/**
 * Input
 */
export type FfmpegApiComposeInput = {
  /**
   * Tracks
   *
   * List of tracks to be combined into the final media
   */
  tracks: Array<Track>;
};

/**
 * HunyuanT2VResponse
 */
export type HunyuanVideoVideoToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   */
  video: File;
};

/**
 * HunyuanV2VRequest
 */
export type HunyuanVideoVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Video Url
   *
   * URL of the video input.
   */
  video_url: string;
  /**
   * Strength
   *
   * Strength for Video-to-Video
   */
  strength?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to run. Lower gets faster results, higher gets better results.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: "129" | "85";
  /**
   * Pro Mode
   *
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean;
};

/**
 * HunyuanV2VResponse
 */
export type HunyuanVideoLoraVideoToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   */
  video: File;
};

/**
 * HunyuanV2VRequest
 */
export type HunyuanVideoLoraVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Video Url
   *
   * URL of the video
   */
  video_url: string;
  /**
   * Loras
   *
   *
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   *
   */
  loras?: Array<LoraWeight>;
  /**
   * Strength
   *
   * Strength of video-to-video
   */
  strength?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: "129" | "85";
  /**
   * Pro Mode
   *
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean;
};

/**
 * Ben2OutputVideo
 */
export type BenV2VideoOutput = {
  /**
   * Seed
   *
   *
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * Ben2InputVideo
 */
export type BenV2VideoInput = {
  /**
   * Video Url
   *
   * URL of video to be used for background removal.
   */
  video_url: string;
  /**
   * Seed
   *
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * Background Color
   *
   * Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]
   */
  background_color?: [unknown, unknown, unknown];
};

/**
 * VideoUpscaleOutput
 */
export type TopazUpscaleVideoOutput = {
  /**
   * Video
   *
   * The upscaled video file
   */
  video: File;
};

/**
 * VideoUpscaleRequest
 */
export type TopazUpscaleVideoInput = {
  /**
   * H264 Output
   *
   * Whether to use H264 codec for output video. Default is H265.
   */
  H264_output?: boolean;
  /**
   * Video Url
   *
   * URL of the video to upscale
   */
  video_url: string;
  /**
   * Upscale Factor
   *
   * Factor to upscale the video by (e.g. 2.0 doubles width and height)
   */
  upscale_factor?: number;
  /**
   * Target Fps
   *
   * Target FPS for frame interpolation. If set, frame interpolation will be enabled.
   */
  target_fps?: number;
};

/**
 * MulticonditioningVideoOutput
 */
export type LtxVideoV095MulticonditioningOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MultiConditioningVideoInput
 */
export type LtxVideoV095MulticonditioningInput = {
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using the model's own capabilities.
   */
  expand_prompt?: boolean;
  /**
   * Images
   *
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInputType2>;
  /**
   * Videos
   *
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInputType2>;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
};

/**
 * VideoConditioningInput
 */
export type VideoConditioningInputType2 = {
  /**
   * Video Url
   *
   * URL of video to be extended
   */
  video_url: string;
  /**
   * Start Frame Num
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num: number;
};

/**
 * ImageConditioningInput
 */
export type ImageConditioningInputType2 = {
  /**
   * Start Frame Num
   *
   * Frame number of the image from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num: number;
  /**
   * Image Url
   *
   * URL of image to use as conditioning
   */
  image_url: string;
};

/**
 * ExtendVideoOutput
 */
export type LtxVideoV095ExtendOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * ExtendVideoInput
 */
export type LtxVideoV095ExtendInput = {
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using the model's own capabilities.
   */
  expand_prompt?: boolean;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Video
   *
   * Video to be extended.
   */
  video: VideoConditioningInputType2;
};

/**
 * PikadditionsOutput
 *
 * Output from Pikadditions generation
 */
export type PikaV2PikadditionsOutput = {
  /**
   * Video
   *
   * The generated video with added objects/images
   */
  video: File;
};

/**
 * PikadditionsRequest
 *
 * Request model for Pikadditions endpoint
 */
export type PikaV2PikadditionsInput = {
  /**
   * Prompt
   *
   * Text prompt describing what to add
   */
  prompt?: string;
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to guide the model
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * URL of the image to add
   */
  image_url: string;
};

/**
 * Output
 */
export type LatentsyncOutput = {
  /**
   * Video
   *
   * The generated video with the lip sync.
   */
  video: File;
};

/**
 * Input
 */
export type LatentsyncInput = {
  /**
   * Video Url
   *
   * The URL of the video to generate the lip sync for.
   */
  video_url: string;
  /**
   * Guidance Scale
   *
   * Guidance scale for the model inference
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * Random seed for generation. If None, a random seed will be used.
   */
  seed?: number;
  /**
   * Audio Url
   *
   * The URL of the audio to generate the lip sync for.
   */
  audio_url: string;
  /**
   * Loop Mode
   *
   * Video loop mode when audio is longer than video. Options: pingpong, loop
   */
  loop_mode?: "pingpong" | "loop";
};

/**
 * LipSyncV2Output
 */
export type SyncLipsyncV2Output = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LipSyncV2Input
 */
export type SyncLipsyncV2Input = {
  /**
   * Model
   *
   * The model to use for lipsyncing. `lipsync-2-pro` will cost roughly 1.67 times as much as `lipsync-2` for the same duration.
   */
  model?: "lipsync-2" | "lipsync-2-pro";
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string;
  /**
   * Sync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  sync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
  /**
   * Audio Url
   *
   * URL of the input audio
   */
  audio_url: string;
};

/**
 * VideoOutput
 *
 * Pydantic model for returning the re-sounded video back to the client.
 */
export type VideoSoundEffectsGeneratorOutput = {
  video: FileType2;
};

/**
 * VideoInput
 *
 * Pydantic model for receiving a video file to analyze and re-sound.
 */
export type VideoSoundEffectsGeneratorInput = {
  video_url: VideoType2;
};

/**
 * Video
 *
 * Represents a video file.
 */
export type VideoType2 = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File | unknown;
};

/**
 * WanT2VResponse
 */
export type WanVaceOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanT2VRequest
 */
export type WanVaceInput = {
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Video Url
   *
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Ref Image Urls
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Task
   *
   * Task type for the model.
   */
  task?: "depth" | "inpainting";
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number;
  /**
   * Mask Image Url
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames
   */
  num_frames?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "auto" | "9:16" | "16:9";
  /**
   * Resolution
   *
   * Resolution of the generated video (480p,580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Mask Video Url
   *
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * MagiVideoExtensionResponse
 */
export type MagiDistilledExtendVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MagiVideoExtensionRequest
 */
export type MagiDistilledExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Video Url
   *
   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
   */
  video_url: string;
  /**
   * Start Frame
   *
   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
   */
  start_frame?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number;
};

/**
 * MagiVideoExtensionResponse
 */
export type MagiExtendVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MagiVideoExtensionRequest
 */
export type MagiExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Video Url
   *
   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.
   */
  video_url: string;
  /**
   * Start Frame
   *
   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.
   */
  start_frame?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32 | 64;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number;
};

/**
 * VideoCondition
 *
 * Video condition to use for generation.
 */
export type VideoCondition = {
  /**
   * Strength
   *
   * The strength of the condition.
   */
  strength?: number;
  /**
   * Start Frame Number
   *
   * The frame number to start the condition on.
   */
  start_frame_number?: number;
  /**
   * Video Url
   *
   * The URL of the video to use as input.
   */
  video_url: string;
};

/**
 * ImageCondition
 *
 * Image condition to use for generation.
 */
export type ImageCondition = {
  /**
   * Strength
   *
   * The strength of the condition.
   */
  strength?: number;
  /**
   * Start Frame Number
   *
   * The frame number to start the condition on.
   */
  start_frame_number?: number;
  /**
   * Image Url
   *
   * The URL of the image to use as input.
   */
  image_url: string;
};

/**
 * MulticonditioningVideoOutput
 */
export type LtxVideoLoraMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * MulticonditioningVideoInput
 *
 * Request model for text-to-video generation with multiple conditions.
 */
export type LtxVideoLoraMulticonditioningInput = {
  /**
   * Number Of Steps
   *
   * The number of inference steps to use.
   */
  number_of_steps?: number;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using the LLM.
   */
  expand_prompt?: boolean;
  /**
   * Number Of Frames
   *
   * The number of frames in the video.
   */
  number_of_frames?: number;
  /**
   * Loras
   *
   * The LoRA weights to use for generation.
   */
  loras?: Array<LoRaWeightType3>;
  /**
   * Images
   *
   * The image conditions to use for generation.
   */
  images?: Array<ImageCondition>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to use.
   */
  negative_prompt?: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16" | "auto";
  /**
   * Resolution
   *
   * The resolution of the video.
   */
  resolution?: "480p" | "720p";
  /**
   * Videos
   *
   * The video conditions to use for generation.
   */
  videos?: Array<VideoCondition>;
  /**
   * Seed
   *
   * The seed to use for generation.
   */
  seed?: number;
};

/**
 * LoRAWeight
 *
 * LoRA weight to use for generation.
 */
export type LoRaWeightType3 = {
  /**
   * Path
   *
   * URL or path to the LoRA weights.
   */
  path: string;
  /**
   * Scale
   *
   * Scale of the LoRA weight. This is a multiplier applied to the LoRA weight when loading it.
   */
  scale?: number;
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string;
};

/**
 * ExtendVideoOutput
 */
export type LtxVideo13bDevExtendOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * ExtendVideoInput
 */
export type LtxVideo13bDevExtendInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Video
   *
   * Video to be extended.
   */
  video: VideoConditioningInput;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number;
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * VideoConditioningInput
 */
export type VideoConditioningInput = {
  /**
   * Video URL
   *
   * URL of video to use as conditioning
   */
  video_url: string;
  /**
   * Start Frame Number
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean;
  /**
   * Limit Number of Frames
   *
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean;
  /**
   * Resample FPS
   *
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean;
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number;
  /**
   * Target FPS
   *
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True.
   */
  target_fps?: number;
  /**
   * Maximum Number of Frames
   *
   * Maximum number of frames to use from the video. If None, all frames will be used.
   */
  max_num_frames?: number;
  /**
   * Conditioning Type
   *
   * Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.
   */
  conditioning_type?: "rgb" | "depth" | "pose" | "canny";
  /**
   * Preprocess
   *
   * Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.
   */
  preprocess?: boolean;
};

/**
 * LoRAWeight
 */
export type LoRaWeight = {
  /**
   * Path
   *
   * URL or path to the LoRA weights.
   */
  path: string;
  /**
   * Scale
   *
   * Scale of the LoRA weight. This is a multiplier applied to the LoRA weight when loading it.
   */
  scale?: number;
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string;
};

/**
 * MultiConditioningVideoOutput
 */
export type LtxVideo13bDevMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MultiConditioningVideoInput
 */
export type LtxVideo13bDevMulticonditioningInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Images
   *
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Videos
   *
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number;
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * ImageConditioningInput
 */
export type ImageConditioningInput = {
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number;
  /**
   * Start Frame Number
   *
   * Frame number of the image from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number;
  /**
   * Image URL
   *
   * URL of image to use as conditioning
   */
  image_url: string;
};

/**
 * MultiConditioningVideoOutput
 */
export type LtxVideo13bDistilledMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * DistilledMultiConditioningVideoInput
 *
 * Distilled model input
 */
export type LtxVideo13bDistilledMulticonditioningInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Images
   *
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number;
  /**
   * Videos
   *
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * ExtendVideoOutput
 */
export type LtxVideo13bDistilledExtendOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * DistilledExtendVideoInput
 *
 * Distilled model input
 */
export type LtxVideo13bDistilledExtendInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Video
   *
   * Video to be extended.
   */
  video: VideoConditioningInput;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number;
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * WanVACEResponse
 */
export type WanVace14bOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * VideoFile
 */
export type VideoFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown;
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number | unknown;
  /**
   * Height
   *
   * The height of the video
   */
  height?: number | unknown;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number | unknown;
  /**
   * Width
   *
   * The width of the video
   */
  width?: number | unknown;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown;
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number | unknown;
};

/**
 * WanVACERequest
 */
export type WanVace14bInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string | unknown;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Mask Video URL
   *
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string | unknown;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Mask Image URL
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string | unknown;
  /**
   * Task
   *
   * Task type for the model.
   */
  task?: "depth" | "pose" | "inpainting" | "outpainting" | "reframe";
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * LipsyncAppOutput
 */
export type LipsyncOutput = {
  video: FileType2;
};

/**
 * LipsyncInput
 */
export type LipsyncInput = {
  /**
   * Video Url
   */
  video_url: string;
  /**
   * Audio Url
   */
  audio_url: string;
};

/**
 * ReframeOutput
 */
export type LumaDreamMachineRay2ReframeOutput = {
  /**
   * Video
   *
   * URL of the reframed video
   */
  video: File;
};

/**
 * ReframeVideoRequest
 */
export type LumaDreamMachineRay2ReframeInput = {
  /**
   * Prompt
   *
   * Optional prompt for reframing
   */
  prompt?: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the reframed video
   */
  aspect_ratio: "1:1" | "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Y Start
   *
   * Start Y coordinate for reframing
   */
  y_start?: number;
  /**
   * X End
   *
   * End X coordinate for reframing
   */
  x_end?: number;
  /**
   * Video Url
   *
   * URL of the input video to reframe
   */
  video_url: string;
  /**
   * Y End
   *
   * End Y coordinate for reframing
   */
  y_end?: number;
  /**
   * X Start
   *
   * Start X coordinate for reframing
   */
  x_start?: number;
  /**
   * Grid Position Y
   *
   * Y position of the grid for reframing
   */
  grid_position_y?: number;
  /**
   * Grid Position X
   *
   * X position of the grid for reframing
   */
  grid_position_x?: number;
  /**
   * Image Url
   *
   * Optional URL of the first frame image for reframing
   */
  image_url?: string;
};

/**
 * ReframeOutput
 */
export type LumaDreamMachineRay2FlashReframeOutput = {
  /**
   * Video
   *
   * URL of the reframed video
   */
  video: File;
};

/**
 * ReframeVideoRequest
 */
export type LumaDreamMachineRay2FlashReframeInput = {
  /**
   * Prompt
   *
   * Optional prompt for reframing
   */
  prompt?: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the reframed video
   */
  aspect_ratio: "1:1" | "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Y Start
   *
   * Start Y coordinate for reframing
   */
  y_start?: number;
  /**
   * X End
   *
   * End X coordinate for reframing
   */
  x_end?: number;
  /**
   * Video Url
   *
   * URL of the input video to reframe
   */
  video_url: string;
  /**
   * Y End
   *
   * End Y coordinate for reframing
   */
  y_end?: number;
  /**
   * X Start
   *
   * Start X coordinate for reframing
   */
  x_start?: number;
  /**
   * Grid Position Y
   *
   * Y position of the grid for reframing
   */
  grid_position_y?: number;
  /**
   * Grid Position X
   *
   * X position of the grid for reframing
   */
  grid_position_x?: number;
  /**
   * Image Url
   *
   * Optional URL of the first frame image for reframing
   */
  image_url?: string;
};

/**
 * WanT2VResponse
 */
export type WanVace13bOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanT2VRequest
 */
export type WanVace13bInput = {
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Video Url
   *
   * URL to the source video file. If provided, the model will use this video as a reference.
   */
  video_url?: string;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Mask Image Url
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.
   */
  mask_image_url?: string;
  /**
   * Task
   *
   * Task type for the model.
   */
  task?: "depth" | "inpainting" | "pose";
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number;
  /**
   * Ref Image Urls
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames
   */
  num_frames?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p,580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "auto" | "9:16" | "16:9";
  /**
   * Mask Video Url
   *
   * URL to the source mask file. If provided, the model will use this mask as a reference.
   */
  mask_video_url?: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * CombineOutput
 */
export type FfmpegApiMergeAudioVideoOutput = {
  video: FileType2;
};

/**
 * CombineInput
 */
export type FfmpegApiMergeAudioVideoInput = {
  /**
   * Video Url
   *
   * URL of the video file to use as the video track
   */
  video_url: string;
  /**
   * Start Offset
   *
   * Offset in seconds for when the audio should start relative to the video
   */
  start_offset?: number;
  /**
   * Audio Url
   *
   * URL of the audio file to use as the audio track
   */
  audio_url: string;
};

/**
 * DWPoseVideoOutput
 */
export type DwposeVideoOutput = {
  /**
   * Video
   *
   * The output video with pose estimation.
   */
  video: File;
};

/**
 * DWPoseVideoInput
 */
export type DwposeVideoInput = {
  /**
   * Video Url
   *
   * URL of video to be used for pose estimation
   */
  video_url: string;
  /**
   * Draw Mode
   *
   * Mode of drawing the pose on the video. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'.
   */
  draw_mode?:
    | "full-pose"
    | "body-pose"
    | "face-pose"
    | "hand-pose"
    | "face-hand-mask"
    | "face-mask"
    | "hand-mask";
};

/**
 * WanVACEDepthResponse
 */
export type WanVace14bDepthOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEDepthRequest
 */
export type WanVace14bDepthInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. Required for depth task.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * WanVACEPoseResponse
 */
export type WanVace14bPoseOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEPoseRequest
 */
export type WanVace14bPoseInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. Required for pose task.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * WanVACEInpaintingResponse
 */
export type WanVace14bInpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEInpaintingRequest
 */
export type WanVace14bInpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. Required for inpainting.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Reference Image URLs
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Mask Video URL
   *
   * URL to the source mask file. Required for inpainting.
   */
  mask_video_url: string | unknown;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Mask Image URL
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.
   */
  mask_image_url?: string | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * WanVACEOutpaintingResponse
 */
export type WanVace14bOutpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEOutpaintingRequest
 */
export type WanVace14bOutpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. Required for outpainting.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Expand Ratio
   *
   * Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.
   */
  expand_ratio?: number;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Expand Bottom
   *
   * Whether to expand the video to the bottom.
   */
  expand_bottom?: boolean;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Expand Top
   *
   * Whether to expand the video to the top.
   */
  expand_top?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Expand Left
   *
   * Whether to expand the video to the left.
   */
  expand_left?: boolean;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Expand Right
   *
   * Whether to expand the video to the right.
   */
  expand_right?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * WanVACEReframeResponse
 */
export type WanVace14bReframeOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEReframeRequest
 */
export type WanVace14bReframeInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. Optional for reframing.
   */
  prompt?: string;
  /**
   * Video URL
   *
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Trim Borders
   *
   * Whether to trim borders from the video.
   */
  trim_borders?: boolean;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Zoom Factor
   *
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * ModifyOutput
 */
export type LumaDreamMachineRay2ModifyOutput = {
  /**
   * Video
   *
   * URL of the modified video
   */
  video: File;
};

/**
 * ModifyVideoRequest
 */
export type LumaDreamMachineRay2ModifyInput = {
  /**
   * Prompt
   *
   * Instruction for modifying the video
   */
  prompt?: string;
  /**
   * Video Url
   *
   * URL of the input video to modify
   */
  video_url: string;
  /**
   * Mode
   *
   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most
   */
  mode?:
    | "adhere_1"
    | "adhere_2"
    | "adhere_3"
    | "flex_1"
    | "flex_2"
    | "flex_3"
    | "reimagine_1"
    | "reimagine_2"
    | "reimagine_3";
  /**
   * Image Url
   *
   * Optional URL of the first frame image for modification
   */
  image_url?: string;
};

/**
 * LipsyncOutput
 */
export type PixverseLipsyncOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LipsyncRequest
 */
export type PixverseLipsyncInput = {
  /**
   * Text
   *
   * Text content for TTS when audio_url is not provided
   */
  text?: string;
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string;
  /**
   * Audio Url
   *
   * URL of the input audio. If not provided, TTS will be used.
   */
  audio_url?: string;
  /**
   * Voice Id
   *
   * Voice to use for TTS when audio_url is not provided
   */
  voice_id?:
    | "Emily"
    | "James"
    | "Isabella"
    | "Liam"
    | "Chloe"
    | "Adrian"
    | "Harper"
    | "Ava"
    | "Sophia"
    | "Julia"
    | "Mason"
    | "Jack"
    | "Oliver"
    | "Ethan"
    | "Auto";
};

/**
 * ExtendOutput
 */
export type PixverseExtendOutput = {
  /**
   * Video
   *
   * The extended video
   */
  video: File;
};

/**
 * ExtendRequest
 */
export type PixverseExtendInput = {
  /**
   * Prompt
   *
   * Prompt describing how to extend the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 1080p videos are limited to 5 seconds
   */
  duration?: "5" | "8";
  /**
   * Style
   *
   * The style of the extended video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Video Url
   *
   * URL of the input video to extend
   */
  video_url: string;
  /**
   * Model
   *
   * The model version to use for generation
   */
  model?: "v3.5" | "v4" | "v4.5" | "v5" | "v5.5" | "v5.6";
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * ExtendOutput
 */
export type PixverseExtendFastOutput = {
  /**
   * Video
   *
   * The extended video
   */
  video: File;
};

/**
 * FastExtendRequest
 */
export type PixverseExtendFastInput = {
  /**
   * Prompt
   *
   * Prompt describing how to extend the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video. Fast mode doesn't support 1080p
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Video Url
   *
   * URL of the input video to extend
   */
  video_url: string;
  /**
   * Style
   *
   * The style of the extended video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Model
   *
   * The model version to use for generation
   */
  model?: "v3.5" | "v4" | "v4.5" | "v5" | "v5.5" | "v5.6";
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * Output
 */
export type ThinksoundOutput = {
  /**
   * Prompt
   *
   * The prompt used to generate the audio.
   */
  prompt: string;
  /**
   * Video
   *
   * The generated video with audio.
   */
  video: File;
};

/**
 * Input
 */
export type ThinksoundInput = {
  /**
   * Prompt
   *
   * A prompt to guide the audio generation. If not provided, it will be extracted from the video.
   */
  prompt?: string;
  /**
   * Video Url
   *
   * The URL of the video to generate the audio for.
   */
  video_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps for audio generation.
   */
  num_inference_steps?: number;
  /**
   * CFG Scale
   *
   * The classifier-free guidance scale for audio generation.
   */
  cfg_scale?: number;
};

/**
 * AudioOutput
 */
export type ThinksoundAudioOutput = {
  /**
   * Prompt
   *
   * The prompt used to generate the audio.
   */
  prompt: string;
  /**
   * Audio
   *
   * The generated audio file.
   */
  audio: File;
};

/**
 * Input
 */
export type ThinksoundAudioInput = {
  /**
   * Prompt
   *
   * A prompt to guide the audio generation. If not provided, it will be extracted from the video.
   */
  prompt?: string;
  /**
   * Video Url
   *
   * The URL of the video to generate the audio for.
   */
  video_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps for audio generation.
   */
  num_inference_steps?: number;
  /**
   * CFG Scale
   *
   * The classifier-free guidance scale for audio generation.
   */
  cfg_scale?: number;
};

/**
 * SoundEffectOutput
 */
export type PixverseSoundEffectsOutput = {
  /**
   * Video
   *
   * The video with added sound effects
   */
  video: File;
};

/**
 * SoundEffectRequest
 */
export type PixverseSoundEffectsInput = {
  /**
   * Prompt
   *
   * Description of the sound effect to generate. If empty, a random sound effect will be generated
   */
  prompt?: string;
  /**
   * Video Url
   *
   * URL of the input video to add sound effects to
   */
  video_url: string;
  /**
   * Original Sound Switch
   *
   * Whether to keep the original audio from the video
   */
  original_sound_switch?: boolean;
};

/**
 * MultiConditioningVideoOutput
 */
export type Ltxv13B098DistilledMulticonditioningOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * DistilledMultiConditioningVideoInput
 *
 * Distilled model input
 */
export type Ltxv13B098DistilledMulticonditioningInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Temporal AdaIN Factor
   *
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
   */
  temporal_adain_factor?: number;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Images
   *
   * URL of images to use as conditioning
   */
  images?: Array<ImageConditioningInput>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Second Pass Number of Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Enable Detail Pass
   *
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Tone Map Compression Ratio
   *
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number;
  /**
   * Videos
   *
   * Videos to use as conditioning
   */
  videos?: Array<VideoConditioningInput>;
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * ModifyOutput
 */
export type LumaDreamMachineRay2FlashModifyOutput = {
  /**
   * Video
   *
   * URL of the modified video
   */
  video: File;
};

/**
 * ModifyVideoRequest
 */
export type LumaDreamMachineRay2FlashModifyInput = {
  /**
   * Prompt
   *
   * Instruction for modifying the video
   */
  prompt?: string;
  /**
   * Video Url
   *
   * URL of the input video to modify
   */
  video_url: string;
  /**
   * Mode
   *
   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most
   */
  mode?:
    | "adhere_1"
    | "adhere_2"
    | "adhere_3"
    | "flex_1"
    | "flex_2"
    | "flex_3"
    | "reimagine_1"
    | "reimagine_2"
    | "reimagine_3";
  /**
   * Image Url
   *
   * Optional URL of the first frame image for modification
   */
  image_url?: string;
};

/**
 * FILMVideoOutput
 */
export type FilmVideoOutput = {
  /**
   * Video
   *
   * The generated video file with interpolated frames.
   */
  video: VideoFileType2;
};

/**
 * VideoFile
 */
export type VideoFileType2 = {
  /**
   * Height
   *
   * The height of the video
   */
  height?: number;
  /**
   * Duration
   *
   * The duration of the video
   */
  duration?: number;
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * Fps
   *
   * The FPS of the video
   */
  fps?: number;
  /**
   * Width
   *
   * The width of the video
   */
  width?: number;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string;
  /**
   * Num Frames
   *
   * The number of frames in the video
   */
  num_frames?: number;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string;
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File;
};

/**
 * FILMVideoInput
 */
export type FilmVideoInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Only applicable if output_type is 'video'.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video URL
   *
   * The URL of the video to use for interpolation.
   */
  video_url: string;
  /**
   * Use Calculated FPS
   *
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.
   */
  use_calculated_fps?: boolean;
  /**
   * Loop
   *
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean;
  /**
   * Frames Per Second
   *
   * Frames per second for the output video. Only applicable if use_calculated_fps is False.
   */
  fps?: number;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the output video. Only applicable if output_type is 'video'.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Use Scene Detection
   *
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate between the input video frames.
   */
  num_frames?: number;
};

/**
 * RIFEVideoOutput
 */
export type RifeVideoOutput = {
  /**
   * Video
   *
   * The generated video file with interpolated frames.
   */
  video: File;
};

/**
 * RIFEVideoInput
 */
export type RifeVideoInput = {
  /**
   * Video URL
   *
   * The URL of the video to use for interpolation.
   */
  video_url: string;
  /**
   * Use Scene Detection
   *
   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.
   */
  use_scene_detection?: boolean;
  /**
   * Loop
   *
   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.
   */
  loop?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate between the input video frames.
   */
  num_frames?: number;
  /**
   * Use Calculated FPS
   *
   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.
   */
  use_calculated_fps?: boolean;
  /**
   * Frames Per Second
   *
   * Frames per second for the output video. Only applicable if use_calculated_fps is False.
   */
  fps?: number;
};

/**
 * ExtendVideoConditioningInput
 */
export type ExtendVideoConditioningInput = {
  /**
   * Video URL
   *
   * URL of video to use as conditioning
   */
  video_url: string;
  /**
   * Start Frame Number
   *
   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.
   */
  start_frame_num?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.
   */
  reverse_video?: boolean;
  /**
   * Limit Number of Frames
   *
   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.
   */
  limit_num_frames?: boolean;
  /**
   * Resample FPS
   *
   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.
   */
  resample_fps?: boolean;
  /**
   * Strength
   *
   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.
   */
  strength?: number;
  /**
   * Target FPS
   *
   * Target FPS to resample the video to. Only relevant if `resample_fps` is True.
   */
  target_fps?: number;
  /**
   * Maximum Number of Frames
   *
   * Maximum number of frames to use from the video. If None, all frames will be used.
   */
  max_num_frames?: number;
};

/**
 * ExtendVideoOutput
 */
export type Ltxv13B098DistilledExtendOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * DistilledExtendVideoInput
 *
 * Distilled model input
 */
export type Ltxv13B098DistilledExtendInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Temporal AdaIN Factor
   *
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
   */
  temporal_adain_factor?: number;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Second Pass Number of Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Video
   *
   * Video to be extended.
   */
  video: ExtendVideoConditioningInput;
  /**
   * Enable Detail Pass
   *
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Tone Map Compression Ratio
   *
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number;
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * WanV2VResponse
 */
export type WanV22A14bVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanV2VRequest
 */
export type WanV22A14bVideoToVideoInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string;
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: "none" | "regular";
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resample Video Frame Rate
   *
   * If true, the video will be resampled to the passed frames per second. If false, the video will not be resampled.
   */
  resample_fps?: boolean;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number;
  /**
   * Guidance Scale (1st Stage)
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input video.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Guidance Scale (2nd Stage)
   *
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
   */
  guidance_scale_2?: number;
  /**
   * Strength
   *
   * Strength of the video transformation. A value of 1.0 means the output will be completely based on the prompt, while a value of 0.0 means the output will be identical to the input video.
   */
  strength?: number;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean;
};

/**
 * MergeVideosOutput
 */
export type FfmpegApiMergeVideosOutput = {
  /**
   * Metadata
   *
   * Metadata about the merged video including original video info
   */
  metadata: {
    [key: string]: unknown;
  };
  video: FileType2;
};

/**
 * MergeVideosInput
 */
export type FfmpegApiMergeVideosInput = {
  /**
   * Resolution
   *
   * Resolution of the final video. Width and height must be between 512 and 2048.
   */
  resolution?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9"
    | unknown;
  /**
   * Video Urls
   *
   * List of video URLs to merge in order
   */
  video_urls: Array<string>;
  /**
   * Target Fps
   *
   * Target FPS for the output video. If not provided, uses the lowest FPS from input videos.
   */
  target_fps?: number | unknown;
};

/**
 * MareyOutput
 */
export type MareyMotionTransferOutput = {
  video: FileType2;
};

/**
 * MareyInputMotionTransfer
 */
export type MareyMotionTransferInput = {
  /**
   * Prompt
   *
   * The prompt to generate a video from
   */
  prompt: string;
  /**
   * Video Url
   *
   * The URL of the video to use as the control video.
   */
  video_url: string;
  /**
   * Seed
   *
   * Seed for random number generation. Use -1 for random seed each run.
   */
  seed?: number | unknown;
  /**
   * Reference Image Url
   *
   * Optional reference image URL to use for pose control or as a starting frame
   */
  reference_image_url?: string | unknown;
  /**
   * Negative Prompt
   *
   * Negative prompt used to guide the model away from undesirable features.
   */
  negative_prompt?: string | unknown;
  /**
   * First Frame Image Url
   *
   * Optional first frame image URL to use as the first frame of the generated video
   */
  first_frame_image_url?: string | unknown;
};

/**
 * MareyOutput
 */
export type MareyPoseTransferOutput = {
  video: FileType2;
};

/**
 * MareyInputPoseTransfer
 */
export type MareyPoseTransferInput = {
  /**
   * Prompt
   *
   * The prompt to generate a video from
   */
  prompt: string;
  /**
   * Video Url
   *
   * The URL of the video to use as the control video.
   */
  video_url: string;
  /**
   * Seed
   *
   * Seed for random number generation. Use -1 for random seed each run.
   */
  seed?: number | unknown;
  /**
   * Reference Image Url
   *
   * Optional reference image URL to use for pose control or as a starting frame
   */
  reference_image_url?: string | unknown;
  /**
   * Negative Prompt
   *
   * Negative prompt used to guide the model away from undesirable features.
   */
  negative_prompt?: string | unknown;
  /**
   * First Frame Image Url
   *
   * Optional first frame image URL to use as the first frame of the generated video
   */
  first_frame_image_url?: string | unknown;
};

/**
 * VideoOutput
 */
export type SfxV1VideoToVideoOutput = {
  /**
   * Video
   *
   * The processed video with sound effects
   */
  video: Array<Video>;
};

/**
 * Video
 */
export type Video = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
};

/**
 * Input
 */
export type SfxV1VideoToVideoInput = {
  /**
   * Num Samples
   *
   * The number of samples to generate from the model
   */
  num_samples?: number | unknown;
  /**
   * Video Url
   *
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string;
  /**
   * Duration
   *
   * The duration of the generated audio in seconds
   */
  duration?: number | unknown;
  /**
   * Seed
   *
   * The seed to use for the generation. If not provided, a random seed will be used
   */
  seed?: number | unknown;
  /**
   * Text Prompt
   *
   * Additional description to guide the model
   */
  text_prompt?: string | unknown;
};

/**
 * AvatarSingleAudioResponse
 */
export type InfinitalkOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * InfiniTalkSingleAudioRequest
 */
export type InfinitalkInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: "480p" | "720p";
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Audio URL
   *
   * The URL of the audio file.
   */
  audio_url: string;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 41 to 721.
   */
  num_frames?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};

/**
 * OutputIncreaseResolutionModel
 */
export type VideoIncreaseResolutionOutput = {
  /**
   * Video
   *
   * Video with removed background and audio.
   */
  video: Video | FileType2;
};

/**
 * InputIncreaseResolutionModel
 */
export type VideoIncreaseResolutionInput = {
  /**
   * Video Url
   *
   * Input video to increase resolution. Size should be less than 14142x14142 and duration less than 30s.
   */
  video_url: string;
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h265"
    | "mkv_h264"
    | "mkv_vp9"
    | "gif";
  /**
   * Desired Increase
   *
   * desired_increase factor. Options: 2x, 4x.
   */
  desired_increase?: "2" | "4";
};

/**
 * WanFunControlResponse
 */
export type WanFunControlOutput = {
  /**
   * Video
   *
   * The video generated by the model.
   */
  video: File;
};

/**
 * WanFunControlRequest
 */
export type WanFunControlInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video.
   */
  prompt: string;
  /**
   * Shift
   *
   * The shift for the scheduler.
   */
  shift?: number;
  /**
   * Preprocess Video
   *
   * Whether to preprocess the video. If True, the video will be preprocessed to depth or pose.
   */
  preprocess_video?: boolean;
  /**
   * Reference Image URL
   *
   * The URL of the reference image to use as a reference for the video generation.
   */
  reference_image_url?: string;
  /**
   * FPS
   *
   * The fps to generate. Only used when match_input_fps is False.
   */
  fps?: number;
  /**
   * Match Input Number of Frames
   *
   * Whether to match the number of frames in the input video.
   */
  match_input_num_frames?: boolean;
  /**
   * Guidance Scale
   *
   * The guidance scale.
   */
  guidance_scale?: number;
  /**
   * Preprocess Type
   *
   * The type of preprocess to apply to the video. Only used when preprocess_video is True.
   */
  preprocess_type?: "depth" | "pose";
  /**
   * Control Video URL
   *
   * The URL of the control video to use as a reference for the video generation.
   */
  control_video_url: string;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video.
   */
  negative_prompt?: string;
  /**
   * Number of Frames
   *
   * The number of frames to generate. Only used when match_input_num_frames is False.
   */
  num_frames?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps.
   */
  num_inference_steps?: number;
  /**
   * Match Input FPS
   *
   * Whether to match the fps in the input video.
   */
  match_input_fps?: boolean;
};

/**
 * LipSyncV2ProOutput
 */
export type SyncLipsyncV2ProOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LipSyncV2ProInput
 */
export type SyncLipsyncV2ProInput = {
  /**
   * Sync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  sync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
  /**
   * Video Url
   *
   * URL of the input video
   */
  video_url: string;
  /**
   * Audio Url
   *
   * URL of the input audio
   */
  audio_url: string;
};

/**
 * HunyuanFoleyResponse
 */
export type HunyuanVideoFoleyOutput = {
  /**
   * Video
   *
   * List of generated video files with audio.
   */
  video: File;
};

/**
 * HunyuanFoleyRequest
 */
export type HunyuanVideoFoleyInput = {
  /**
   * Video Url
   *
   * The URL of the video to generate audio for.
   */
  video_url: string;
  /**
   * Guidance Scale
   *
   * Guidance scale for audio generation.
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for generation.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to avoid certain audio characteristics.
   */
  negative_prompt?: string;
  /**
   * Text Prompt
   *
   * Text description of the desired audio (optional).
   */
  text_prompt: string;
};

/**
 * WanVACEPoseResponse
 */
export type Wan22VaceFunA14bPoseOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEPoseRequest
 */
export type Wan22VaceFunA14bPoseInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. Required for pose task.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * WanVACEDepthResponse
 */
export type Wan22VaceFunA14bDepthOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEDepthRequest
 */
export type Wan22VaceFunA14bDepthInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. Required for depth task.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * WanVACEInpaintingResponse
 */
export type Wan22VaceFunA14bInpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEInpaintingRequest
 */
export type Wan22VaceFunA14bInpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. Required for inpainting.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Reference Image URLs
   *
   * Urls to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Mask Video URL
   *
   * URL to the source mask file. Required for inpainting.
   */
  mask_video_url: string | unknown;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Preprocess
   *
   * Whether to preprocess the input video.
   */
  preprocess?: boolean;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Mask Image URL
   *
   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.
   */
  mask_image_url?: string | unknown;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * WanVACEOutpaintingResponse
 */
export type Wan22VaceFunA14bOutpaintingOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEOutpaintingRequest
 */
export type Wan22VaceFunA14bOutpaintingInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. Required for outpainting.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Reference Image URLs
   *
   * URLs to source reference image. If provided, the model will use this image as reference.
   */
  ref_image_urls?: Array<string>;
  /**
   * Expand Ratio
   *
   * Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.
   */
  expand_ratio?: number;
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Expand Bottom
   *
   * Whether to expand the video to the bottom.
   */
  expand_bottom?: boolean;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Expand Left
   *
   * Whether to expand the video to the left.
   */
  expand_left?: boolean;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Expand Top
   *
   * Whether to expand the video to the top.
   */
  expand_top?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Expand Right
   *
   * Whether to expand the video to the right.
   */
  expand_right?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * WanVACEReframeResponse
 */
export type Wan22VaceFunA14bReframeOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * WanVACEReframeRequest
 */
export type Wan22VaceFunA14bReframeInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation. Optional for reframing.
   */
  prompt?: string;
  /**
   * Video URL
   *
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * First Frame URL
   *
   * URL to the first frame of the video. If provided, the model will use this frame as a reference.
   */
  first_frame_url?: string | unknown;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.
   */
  auto_downsample_min_fps?: number;
  /**
   * Trim Borders
   *
   * Whether to trim borders from the video.
   */
  trim_borders?: boolean;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Zoom Factor
   *
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Last Frame URL
   *
   * URL to the last frame of the video. If provided, the model will use this frame as a reference.
   */
  last_frame_url?: string | unknown;
};

/**
 * LucyEditDevOutput
 */
export type LucyEditDevOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LucyEditDevInput
 */
export type LucyEditDevInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean;
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string;
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
};

/**
 * LucyEditProOutput
 */
export type LucyEditProOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LucyEditProInput
 */
export type LucyEditProInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean;
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string;
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video
   */
  resolution?: "720p";
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
};

/**
 * WanAnimateMoveResponse
 */
export type WanV2214bAnimateMoveOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation (auto-generated by the model)
   */
  prompt: string;
  /**
   * Frames Zip
   *
   * ZIP archive of generated frames (if requested).
   */
  frames_zip?: File;
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanAnimateMoveRequest
 */
export type WanV2214bAnimateMoveInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string;
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Return Frames ZIP
   *
   * If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).
   */
  return_frames_zip?: boolean;
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Use Turbo
   *
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.
   */
  use_turbo?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
};

/**
 * WanAnimateReplaceResponse
 */
export type WanV2214bAnimateReplaceOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation (auto-generated by the model)
   */
  prompt: string;
  /**
   * Frames Zip
   *
   * ZIP archive of generated frames (if requested).
   */
  frames_zip?: File;
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanAnimateMoveRequest
 */
export type WanV2214bAnimateReplaceInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string;
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Return Frames ZIP
   *
   * If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).
   */
  return_frames_zip?: boolean;
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Use Turbo
   *
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.
   */
  use_turbo?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
};

/**
 * WanVACEVideoEditResponse
 */
export type WanVaceAppsVideoEditOutput = {
  /**
   * Frames Zip
   *
   * ZIP archive of generated frames if requested.
   */
  frames_zip?: File;
  /**
   * Video
   *
   * The edited video.
   */
  video: VideoFileType2;
};

/**
 * WanVACEVideoEditRequest
 */
export type WanVaceAppsVideoEditInput = {
  /**
   * Prompt
   *
   * Prompt to edit the video.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL of the input video.
   */
  video_url: string;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * Resolution
   *
   * Resolution of the edited video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Return Frames ZIP
   *
   * Whether to include a ZIP archive containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the edited video.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Video Type
   *
   * The type of video you're editing. Use 'general' for most videos, and 'human' for videos emphasizing human subjects and motions. The default value 'auto' means the model will guess based on the first frame of the video.
   */
  video_type?: "auto" | "general" | "human";
  /**
   * Image URLs
   *
   * URLs of the input images to use as a reference for the generation.
   */
  image_urls?: Array<string>;
  /**
   * Enable Auto Downsampling
   *
   * Whether to enable automatic downsampling. If your video has a high frame rate or is long, enabling longer sequences to be generated. The video will be interpolated back to the original frame rate after generation.
   */
  enable_auto_downsample?: boolean;
  /**
   * Auto Downsample Min FPS
   *
   * The minimum frames per second to downsample the video to.
   */
  auto_downsample_min_fps?: number;
};

/**
 * SeedVRVideoOutput
 */
export type SeedvrUpscaleVideoOutput = {
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed: number;
  video: FileType2;
};

/**
 * SeedVRVideoInput
 */
export type SeedvrUpscaleVideoInput = {
  /**
   * Upscale Mode
   *
   * The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly.
   */
  upscale_mode?: "target" | "factor";
  /**
   * Video Url
   *
   * The input video to be processed
   */
  video_url: string;
  /**
   * Noise Scale
   *
   * The noise scale to use for the generation process.
   */
  noise_scale?: number;
  /**
   * Output Format
   *
   * The format of the output video.
   */
  output_format?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Output Write Mode
   *
   * The write mode of the output video.
   */
  output_write_mode?: "fast" | "balanced" | "small";
  /**
   * Target Resolution
   *
   * The target resolution to upscale to when `upscale_mode` is `target`.
   */
  target_resolution?: "720p" | "1080p" | "1440p" | "2160p";
  /**
   * Output Quality
   *
   * The quality of the output video.
   */
  output_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Upscale Factor
   *
   * Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`.
   */
  upscale_factor?: number;
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed?: number | unknown;
};

/**
 * InfinitalkVid2VidResponse
 */
export type InfinitalkVideoToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * InfiniTalkVid2VidAudioRequest
 */
export type InfinitalkVideoToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: "480p" | "720p";
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string;
  /**
   * Audio URL
   *
   * The URL of the audio file.
   */
  audio_url: string;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};

/**
 * LongWanVACEReframeResponse
 */
export type WanVaceAppsLongReframeOutput = {
  /**
   * Video
   *
   * The output video file.
   */
  video: VideoFileType2;
};

/**
 * LongWanVACEReframeRequest
 */
export type WanVaceAppsLongReframeInput = {
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Video URL
   *
   * URL to the source video file. This video will be used as a reference for the reframe task.
   */
  video_url: string;
  /**
   * Zoom Factor
   *
   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.
   */
  zoom_factor?: number;
  /**
   * Paste Back
   *
   * Whether to paste back the reframed scene to the original video.
   */
  paste_back?: boolean;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular";
  /**
   * Prompt
   *
   * The text prompt to guide video generation. Optional for reframing.
   */
  prompt?: string;
  /**
   * Scene Threshold
   *
   * Threshold for scene detection sensitivity (0-100). Lower values detect more scenes.
   */
  scene_threshold?: number;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Auto Downsample Min Fps
   *
   * Minimum FPS for auto downsample.
   */
  auto_downsample_min_fps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Transparency Mode
   *
   * The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.
   */
  transparency_mode?: "content_aware" | "white" | "black";
  /**
   * Trim Borders
   *
   * Whether to trim borders from the video.
   */
  trim_borders?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. Options are 'rife' or 'film'.
   */
  interpolator_model?: "rife" | "film";
  /**
   * Enable Auto Downsample
   *
   * Whether to enable auto downsample.
   */
  enable_auto_downsample?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
};

/**
 * RemixOutput
 */
export type Sora2VideoToVideoRemixOutput = {
  /**
   * Spritesheet
   *
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
  /**
   * Thumbnail
   *
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Video ID
   *
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Video
   *
   * The generated video
   */
  video: VideoFileType2;
};

/**
 * ImageFile
 */
export type ImageFile = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number;
  /**
   * Height
   *
   * The height of the image
   */
  height?: number;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * Width
   *
   * The width of the image
   */
  width?: number;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string;
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File;
};

/**
 * RemixInput
 */
export type Sora2VideoToVideoRemixInput = {
  /**
   * Prompt
   *
   * Updated text prompt that directs the remix generation
   */
  prompt: string;
  /**
   * Video ID
   *
   * The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos.
   */
  video_id: string;
  /**
   * Delete Video
   *
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
   */
  delete_video?: boolean;
};

/**
 * VideoToVideoOutput
 */
export type KreaWan14bVideoToVideoOutput = {
  video: FileType2;
};

/**
 * VideoToVideoInput
 */
export type KreaWan14bVideoToVideoInput = {
  /**
   * Prompt
   *
   * Prompt for the video-to-video generation.
   */
  prompt: string;
  /**
   * Video Url
   *
   * URL of the input video. Currently, only outputs of 16:9 aspect ratio and 480p resolution are supported. Video duration should be less than 1000 frames at 16fps, and output frames will be 6 plus a multiple of 12, for example 18, 30, 42, etc.
   */
  video_url: string;
  /**
   * Strength
   *
   * Denoising strength for the video-to-video generation. 0.0 preserves the original, 1.0 completely remakes the video.
   */
  strength?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Seed for the video-to-video generation.
   */
  seed?: number | unknown;
};

/**
 * Video
 */
export type VideoOutput = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
};

/**
 * VideoOutput
 */
export type SfxV15VideoToVideoOutput = {
  /**
   * Video
   *
   * The processed video with sound effects
   */
  video: Array<VideoOutput>;
};

/**
 * Input
 */
export type SfxV15VideoToVideoInput = {
  /**
   * Num Samples
   *
   * The number of samples to generate from the model
   */
  num_samples?: number | unknown;
  /**
   * Duration
   *
   * The duration of the generated audio in seconds
   */
  duration?: number | unknown;
  /**
   * Start Offset
   *
   * The start offset in seconds to start the audio generation from
   */
  start_offset?: number | unknown;
  /**
   * Video Url
   *
   * A video url that can accessed from the API to process and add sound effects
   */
  video_url: string;
  /**
   * Seed
   *
   * The seed to use for the generation. If not provided, a random seed will be used
   */
  seed?: number | unknown;
  /**
   * Text Prompt
   *
   * Additional description to guide the model
   */
  text_prompt?: string | unknown;
};

/**
 * Q2VideoExtensionOutput
 */
export type ViduQ2VideoExtensionProOutput = {
  /**
   * Video
   *
   * The extended video using the Q2 model
   */
  video: File;
};

/**
 * Q2VideoExtensionRequest
 */
export type ViduQ2VideoExtensionProInput = {
  /**
   * Prompt
   *
   * text prompt to guide the video extension
   */
  prompt?: string;
  /**
   * Duration
   *
   * Duration of the extension in seconds
   */
  duration?: 2 | 3 | 4 | 5 | 6 | 7;
  /**
   * Resolution
   *
   * Output video resolution
   */
  resolution?: "720p" | "1080p";
  /**
   * Video Url
   *
   * URL of the video to extend
   */
  video_url: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};

/**
 * VideoOutput
 */
export type BirefnetV2VideoOutput = {
  /**
   * Video
   *
   * Video with background removed
   */
  video: VideoFileType2;
  /**
   * Mask Video
   *
   * Mask used to remove the background
   */
  mask_video?: VideoFileType2;
};

/**
 * VideoInputV2
 */
export type BirefnetV2VideoInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Operating Resolution
   *
   * The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model.
   */
  operating_resolution?: "1024x1024" | "2048x2048" | "2304x2304";
  /**
   * Video Url
   *
   * URL of the video to remove background from
   */
  video_url: string;
  /**
   * Model
   *
   *
   * Model to use for background removal.
   * The 'General Use (Light)' model is the original model used in the BiRefNet repository.
   * The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images.
   * The 'General Use (Heavy)' model is a slower but more accurate model.
   * The 'Matting' model is a model trained specifically for matting images.
   * The 'Portrait' model is a model trained specifically for portrait images.
   * The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304.
   * The 'General Use (Light)' model is recommended for most use cases.
   *
   * The corresponding models are as follows:
   * - 'General Use (Light)': BiRefNet
   * - 'General Use (Light 2K)': BiRefNet_lite-2K
   * - 'General Use (Heavy)': BiRefNet_lite
   * - 'Matting': BiRefNet-matting
   * - 'Portrait': BiRefNet-portrait
   * - 'General Use (Dynamic)': BiRefNet_dynamic
   *
   */
  model?:
    | "General Use (Light)"
    | "General Use (Light 2K)"
    | "General Use (Heavy)"
    | "Matting"
    | "Portrait"
    | "General Use (Dynamic)";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Output Mask
   *
   * Whether to output the mask used to remove the background
   */
  output_mask?: boolean;
  /**
   * Refine Foreground
   *
   * Whether to refine the foreground using the estimated mask
   */
  refine_foreground?: boolean;
};

/**
 * VideoEffectOutput
 */
export type VideoAsPromptOutput = {
  video: FileType2;
};

/**
 * VideoEffectInputWan
 */
export type VideoAsPromptInput = {
  /**
   * Prompt
   *
   * The prompt to generate an image from.
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Video Url
   *
   * reference video to generate effect video from.
   */
  video_url: string;
  /**
   * Image Url
   *
   * Input image to generate the effect video for.
   */
  image_url: string;
  /**
   * Frames Per Second
   *
   * Frames per second for the output video. Only applicable if output_type is 'video'.
   */
  fps?: number;
  /**
   * Video Description
   *
   * A brief description of the input video content.
   */
  video_description: string;
  /**
   * Seed
   *
   * Random seed for reproducible generation. If set none, a random seed will be used.
   */
  seed?: number | unknown;
  /**
   * Guidance Scale
   *
   * Guidance scale for generation.
   */
  guidance_scale?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
};

/**
 * UpscaleOutput
 */
export type BytedanceUpscalerUpscaleVideoOutput = {
  /**
   * Duration
   *
   * Duration of audio input/video output as used for billing.
   */
  duration: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * UpscaleInput
 */
export type BytedanceUpscalerUpscaleVideoInput = {
  /**
   * Target Fps
   *
   * The target FPS of the video to upscale.
   */
  target_fps?: "30fps" | "60fps";
  /**
   * Video Url
   *
   * The URL of the video to upscale.
   */
  video_url: string;
  /**
   * Target Resolution
   *
   * The target resolution of the video to upscale.
   */
  target_resolution?: "1080p" | "2k" | "4k";
};

/**
 * AutoSubtitleOutput
 *
 * Output model for video with automatic subtitles
 */
export type WorkflowUtilitiesAutoSubtitleOutput = {
  /**
   * Transcription
   *
   * Full transcription text
   */
  transcription: string;
  /**
   * Subtitle Count
   *
   * Number of subtitle segments generated
   */
  subtitle_count: number;
  /**
   * Transcription Metadata
   *
   * Additional transcription metadata from ElevenLabs (language, segments, etc.)
   */
  transcription_metadata?: {
    [key: string]: unknown;
  };
  /**
   * Words
   *
   * Word-level timing information from transcription service
   */
  words?: Array<{
    [key: string]: unknown;
  }>;
  /**
   * Video
   *
   * The video with automatic subtitles
   */
  video: File;
};

/**
 * AutoSubtitleInput
 *
 * Input model for automatic subtitle generation and styling
 */
export type WorkflowUtilitiesAutoSubtitleInput = {
  /**
   * Font Weight
   *
   * Font weight (TikTok style typically uses bold or black)
   */
  font_weight?: "normal" | "bold" | "black";
  /**
   * Video Url
   *
   * URL of the video file to add automatic subtitles to
   *
   * Max file size: 95.4MB, Timeout: 30.0s
   */
  video_url: string;
  /**
   * Stroke Width
   *
   * Text stroke/outline width in pixels (0 for no stroke)
   */
  stroke_width?: number;
  /**
   * Font Color
   *
   * Subtitle text color for non-active words
   */
  font_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Font Size
   *
   * Font size for subtitles (TikTok style uses larger text)
   */
  font_size?: number;
  /**
   * Language
   *
   * Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra')
   */
  language?: string;
  /**
   * Y Offset
   *
   * Vertical offset in pixels (positive = move down, negative = move up)
   */
  y_offset?: number;
  /**
   * Background Opacity
   *
   * Background opacity (0.0 = fully transparent, 1.0 = fully opaque)
   */
  background_opacity?: number;
  /**
   * Stroke Color
   *
   * Text stroke/outline color
   */
  stroke_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Highlight Color
   *
   * Color for the currently speaking word (karaoke-style highlight)
   */
  highlight_color?:
    | "white"
    | "black"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta";
  /**
   * Enable Animation
   *
   * Enable animation effects for subtitles (bounce style entrance)
   */
  enable_animation?: boolean;
  /**
   * Font Name
   *
   * Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty')
   */
  font_name?: string;
  /**
   * Position
   *
   * Vertical position of subtitles
   */
  position?: "top" | "center" | "bottom";
  /**
   * Words Per Subtitle
   *
   * Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences.
   */
  words_per_subtitle?: number;
  /**
   * Background Color
   *
   * Background color behind text ('none' or 'transparent' for no background)
   */
  background_color?:
    | "black"
    | "white"
    | "red"
    | "green"
    | "blue"
    | "yellow"
    | "orange"
    | "purple"
    | "pink"
    | "brown"
    | "gray"
    | "cyan"
    | "magenta"
    | "none"
    | "transparent";
};

/**
 * FlashVSRPlusVideoOutput
 */
export type FlashvsrUpscaleVideoOutput = {
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed: number;
  /**
   * Video
   *
   * Upscaled video file after processing
   */
  video: File;
};

/**
 * FlashVSRPlusVideoInput
 *
 * Input fields common to FlashVSR+ image/video endpoints.
 */
export type FlashvsrUpscaleVideoInput = {
  /**
   * Video Url
   *
   * The input video to be upscaled
   */
  video_url: string;
  /**
   * Acceleration
   *
   * Acceleration mode for VAE decoding. Options: regular (best quality), high (balanced), full (fastest). More accerleation means longer duration videos can be processed too.
   */
  acceleration?: "regular" | "high" | "full";
  /**
   * Quality
   *
   * Quality level for tile blending (0-100). Controls overlap between tiles to prevent grid artifacts. Higher values provide better quality with more overlap. Recommended: 70-85 for high-res videos, 50-70 for faster processing.
   */
  quality?: number;
  /**
   * Output Format
   *
   * The format of the output video.
   */
  output_format?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Color Fix
   *
   * Color correction enabled.
   */
  color_fix?: boolean;
  /**
   * Output Write Mode
   *
   * The write mode of the output video.
   */
  output_write_mode?: "fast" | "balanced" | "small";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned inline and not stored in history.
   */
  sync_mode?: boolean;
  /**
   * Output Quality
   *
   * The quality of the output video.
   */
  output_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Upscale Factor
   *
   * Upscaling factor to be used.
   */
  upscale_factor?: number;
  /**
   * Preserve Audio
   *
   * Copy the original audio tracks into the upscaled video using FFmpeg when possible.
   */
  preserve_audio?: boolean;
  /**
   * Seed
   *
   * The random seed used for the generation process.
   */
  seed?: number;
};

/**
 * EdittoOutput
 */
export type EdittoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * ZIP archive of all video frames if requested.
   */
  frames_zip?: FileType2 | unknown;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  video: VideoFile;
};

/**
 * EdittoInput
 */
export type EdittoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL to the source video file. Required for inpainting.
   */
  video_url: string;
  /**
   * Acceleration
   *
   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.
   */
  acceleration?: "none" | "low" | "regular" | unknown;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between the original frames. A value of 0 means no interpolation.
   */
  num_interpolated_frames?: number;
  /**
   * Temporal Downsample Factor
   *
   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.
   */
  temporal_downsample_factor?: number;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.
   */
  frames_per_second?: number | unknown;
  /**
   * Match Input Number of Frames
   *
   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.
   */
  match_input_num_frames?: boolean;
  /**
   * Guidance Scale
   *
   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 241 (inclusive).
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Sampler
   *
   * Sampler to use for video generation.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "auto" | "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "1:1" | "9:16";
  /**
   * Return Frames Zip
   *
   * If true, also return a ZIP file containing all generated frames.
   */
  return_frames_zip?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Match Input Frames Per Second
   *
   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.
   */
  match_input_frames_per_second?: boolean;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Enable Auto Downsample
   *
   * If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.
   */
  enable_auto_downsample?: boolean;
};

/**
 * PointPromptBase
 */
export type PointPromptBase = {
  /**
   * Y
   *
   * Y Coordinate of the prompt
   */
  y?: number;
  /**
   * X
   *
   * X Coordinate of the prompt
   */
  x?: number;
  /**
   * Object Id
   *
   * Optional object identifier. Prompts sharing an object id refine the same object.
   */
  object_id?: number;
  /**
   * Label
   *
   * 1 for foreground, 0 for background
   */
  label?: 0 | 1;
};

/**
 * BoxPromptBase
 */
export type BoxPromptBase = {
  /**
   * Y Min
   *
   * Y Min Coordinate of the box
   */
  y_min?: number;
  /**
   * Object Id
   *
   * Optional object identifier. Boxes sharing an object id refine the same object.
   */
  object_id?: number;
  /**
   * X Max
   *
   * X Max Coordinate of the box
   */
  x_max?: number;
  /**
   * X Min
   *
   * X Min Coordinate of the box
   */
  x_min?: number;
  /**
   * Y Max
   *
   * Y Max Coordinate of the box
   */
  y_max?: number;
};

/**
 * SAM3VideoOutput
 */
export type Sam3VideoOutput = {
  /**
   * Boundingbox Frames Zip
   *
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
  /**
   * Video
   *
   * The segmented video.
   */
  video: File;
};

/**
 * SAM3VideoInput
 */
export type Sam3VideoInput = {
  /**
   * Prompt
   *
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').
   */
  prompt?: string;
  /**
   * Video Url
   *
   * The URL of the video to be segmented.
   */
  video_url: string;
  /**
   * Detection Threshold
   *
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise.
   */
  detection_threshold?: number;
  /**
   * Box Prompts
   *
   * List of box prompt coordinates (x_min, y_min, x_max, y_max).
   */
  box_prompts?: Array<BoxPromptBase>;
  /**
   * Point Prompts
   *
   * List of point prompts
   */
  point_prompts?: Array<PointPromptBase>;
  /**
   * Apply Mask
   *
   * Apply the mask on the video.
   */
  apply_mask?: boolean;
  /**
   * Text Prompt
   *
   * [DEPRECATED] Use 'prompt' instead. Kept for backward compatibility.
   *
   * @deprecated
   */
  text_prompt?: string;
};

/**
 * PointPrompt
 */
export type PointPrompt = {
  /**
   * Y
   *
   * Y Coordinate of the prompt
   */
  y?: number;
  /**
   * X
   *
   * X Coordinate of the prompt
   */
  x?: number;
  /**
   * Object Id
   *
   * Optional object identifier. Prompts sharing an object id refine the same object.
   */
  object_id?: number;
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number;
  /**
   * Label
   *
   * 1 for foreground, 0 for background
   */
  label?: 0 | 1;
};

/**
 * BoxPrompt
 */
export type BoxPrompt = {
  /**
   * Y Min
   *
   * Y Min Coordinate of the box
   */
  y_min?: number;
  /**
   * Object Id
   *
   * Optional object identifier. Boxes sharing an object id refine the same object.
   */
  object_id?: number;
  /**
   * Frame Index
   *
   * The frame index to interact with.
   */
  frame_index?: number;
  /**
   * X Max
   *
   * X Max Coordinate of the box
   */
  x_max?: number;
  /**
   * X Min
   *
   * X Min Coordinate of the box
   */
  x_min?: number;
  /**
   * Y Max
   *
   * Y Max Coordinate of the box
   */
  y_max?: number;
};

/**
 * SAM3VideoOutput
 */
export type Sam3VideoRleOutput = {
  /**
   * Boundingbox Frames Zip
   *
   * Zip file containing per-frame bounding box overlays.
   */
  boundingbox_frames_zip?: File;
  /**
   * Video
   *
   * The segmented video.
   */
  video: File;
};

/**
 * SAM3VideoRLEInput
 */
export type Sam3VideoRleInput = {
  /**
   * Prompt
   *
   * Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').
   */
  prompt?: string;
  /**
   * Video Url
   *
   * The URL of the video to be segmented.
   */
  video_url: string;
  /**
   * Detection Threshold
   *
   * Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail.
   */
  detection_threshold?: number;
  /**
   * Box Prompts
   *
   * List of box prompts with optional frame_index.
   */
  box_prompts?: Array<BoxPrompt>;
  /**
   * Boundingbox Zip
   *
   * Return per-frame bounding box overlays as a zip archive.
   */
  boundingbox_zip?: boolean;
  /**
   * Point Prompts
   *
   * List of point prompts with frame indices.
   */
  point_prompts?: Array<PointPrompt>;
  /**
   * Frame Index
   *
   * Frame index used for initial interaction when mask_url is provided.
   */
  frame_index?: number;
  /**
   * Mask Url
   *
   * The URL of the mask to be applied initially.
   */
  mask_url?: string;
  /**
   * Apply Mask
   *
   * Apply the mask on the video.
   */
  apply_mask?: boolean;
};

/**
 * LucyEditFastOutput
 */
export type LucyEditFastOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LucyEditFastInput
 */
export type LucyEditFastInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean;
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string;
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
};

/**
 * LTXRetakeVideoResponse
 */
export type Ltx2RetakeVideoOutput = {
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * LTXRetakeVideoRequest
 */
export type Ltx2RetakeVideoInput = {
  /**
   * Prompt
   *
   * The prompt to retake the video with
   */
  prompt: string;
  /**
   * Video URL
   *
   * The URL of the video to retake
   */
  video_url: string;
  /**
   * Start Time
   *
   * The start time of the video to retake in seconds
   */
  start_time?: number;
  /**
   * Duration
   *
   * The duration of the video to retake in seconds
   */
  duration?: number;
  /**
   * Retake Mode
   *
   * The retake mode to use for the retake
   */
  retake_mode?: "replace_audio" | "replace_video" | "replace_audio_and_video";
};

/**
 * GreenScreenRembgOutput
 */
export type VideoBackgroundRemovalGreenScreenOutput = {
  /**
   * Video
   */
  video: Array<FileType2>;
};

/**
 * GreenScreenRembgInput
 */
export type VideoBackgroundRemovalGreenScreenInput = {
  /**
   * Video Url
   */
  video_url: string;
  /**
   * Output Codec
   *
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
   */
  output_codec?: "vp9" | "h264";
  /**
   * Spill Suppression Strength
   *
   * Increase the value if green spots remain in the video, decrease if color changes are noticed on the extracted subject.
   */
  spill_suppression_strength?: number | unknown;
};

/**
 * OmniV2VReferenceOutput
 */
export type KlingVideoO1VideoToVideoReferenceOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * OmniV2VReferenceInput
 *
 * Input for video editing or video-as-reference generation.
 */
export type KlingVideoO1VideoToVideoReferenceInput = {
  /**
   * Prompt
   *
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Video Url
   *
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
  /**
   * Keep Audio
   *
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
  /**
   * Elements
   *
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * Image Urls
   *
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
};

/**
 * OmniVideoElementInput
 */
export type OmniVideoElementInput = {
  /**
   * Reference Image Urls
   *
   * Additional reference images from different angles. 1-3 images supported. At least one image is required.
   */
  reference_image_urls?: Array<string>;
  /**
   * Frontal Image Url
   *
   * The frontal image of the element (main view).
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  frontal_image_url: string;
};

/**
 * OmniV2VEditOutput
 */
export type KlingVideoO1VideoToVideoEditOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * OmniV2VEditInput
 *
 * Input for video editing or video-as-reference generation.
 */
export type KlingVideoO1VideoToVideoEditInput = {
  /**
   * Prompt
   *
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Video Url
   *
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string;
  /**
   * Elements
   *
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * Image Urls
   *
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Keep Audio
   *
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
};

/**
 * FastGeneralRembgOutput
 */
export type VideoBackgroundRemovalFastOutput = {
  /**
   * Video
   */
  video: Array<FileType2>;
};

/**
 * FastGeneralRembgInput
 */
export type VideoBackgroundRemovalFastInput = {
  /**
   * Video Url
   */
  video_url: string;
  /**
   * Subject Is Person
   *
   * Set to False if the subject is not a person.
   */
  subject_is_person?: boolean;
  /**
   * Output Codec
   *
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
   */
  output_codec?: "vp9" | "h264";
  /**
   * Refine Foreground Edges
   *
   * Improves the quality of the extracted object's edges.
   */
  refine_foreground_edges?: boolean;
};

/**
 * React1Output
 */
export type SyncLipsyncReact1Output = {
  /**
   * Video
   *
   * The generated video with synchronized lip and facial movements.
   */
  video: VideoFileType2;
};

/**
 * React1Input
 */
export type SyncLipsyncReact1Input = {
  /**
   * Emotion
   *
   * Emotion prompt for the generation. Currently supports single-word emotions only.
   */
  emotion: "happy" | "angry" | "sad" | "neutral" | "disgusted" | "surprised";
  /**
   * Video Url
   *
   * URL to the input video. Must be **15 seconds or shorter**.
   */
  video_url: string;
  /**
   * Lipsync Mode
   *
   * Lipsync mode when audio and video durations are out of sync.
   */
  lipsync_mode?: "cut_off" | "loop" | "bounce" | "silence" | "remap";
  /**
   * Audio Url
   *
   * URL to the input audio. Must be **15 seconds or shorter**.
   */
  audio_url: string;
  /**
   * Temperature
   *
   * Controls the expresiveness of the lipsync.
   */
  temperature?: number;
  /**
   * Model Mode
   *
   * Controls the edit region and movement scope for the model. Available options:
   * - `lips`: Only lipsync using react-1 (minimal facial changes).
   * - `face`: Lipsync + facial expressions without head movements.
   * - `head`: Lipsync + facial expressions + natural talking head movements.
   */
  model_mode?: "lips" | "face" | "head";
};

/**
 * Output
 *
 * Output from Wan Vision Enhancer
 */
export type WanVisionEnhancerOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Timings
   *
   * The timings of the different steps in the workflow.
   */
  timings: {
    [key: string]: number;
  };
  /**
   * The enhanced video file.
   */
  video: FileType2;
};

/**
 * Input
 *
 * Input parameters for Wan Vision Enhancer (Video-to-Video)
 */
export type WanVisionEnhancerInput = {
  /**
   * Prompt
   *
   * Optional prompt to prepend to the VLM-generated description. Leave empty to use only the auto-generated description from the video.
   */
  prompt?: string | unknown;
  /**
   * Video Url
   *
   * The URL of the video to enhance with Wan Video. Maximum 200MB file size. Videos longer than 500 frames will have only the first 500 frames processed (~8-21 seconds depending on fps).
   */
  video_url: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If not provided, a random seed will be used.
   */
  seed?: number | unknown;
  /**
   * Output Resolution
   *
   * Target output resolution for the enhanced video. 720p (native, fast) or 1080p (upscaled, slower). Processing is always done at 720p, then upscaled if 1080p selected.
   */
  target_resolution?: "720p" | "1080p";
  /**
   * Negative Prompt
   *
   * Negative prompt to avoid unwanted features.
   */
  negative_prompt?: string | unknown;
  /**
   * Creativity
   *
   * Controls how much the model enhances/changes the video. 0 = Minimal change (preserves original), 1 = Subtle enhancement (default), 2 = Medium enhancement, 3 = Strong enhancement, 4 = Maximum enhancement.
   */
  creativity?: number;
};

/**
 * OneToALLAnimationResponse
 */
export type OneToAllAnimation14bOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * OneToALLAnimationRequest
 */
export type OneToAllAnimation14bInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Image Guidance Scale
   *
   * The image guidance scale to use for the video generation.
   */
  image_guidance_scale?: number;
  /**
   * Pose Guidance Scale
   *
   * The pose guidance scale to use for the video generation.
   */
  pose_guidance_scale?: number;
  /**
   * Video Url
   *
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string;
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt: string;
};

/**
 * OneToALLAnimationResponse
 */
export type OneToAllAnimation13bOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * OneToALLAnimationRequest
 */
export type OneToAllAnimation13bInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Image Guidance Scale
   *
   * The image guidance scale to use for the video generation.
   */
  image_guidance_scale?: number;
  /**
   * Pose Guidance Scale
   *
   * The pose guidance scale to use for the video generation.
   */
  pose_guidance_scale?: number;
  /**
   * Video Url
   *
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string;
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt: string;
};

/**
 * SteadyDancerResponse
 *
 * Response model for SteadyDancer.
 */
export type SteadyDancerOutput = {
  /**
   * Num Frames
   *
   * The actual number of frames generated (aligned to 4k+1 pattern).
   */
  num_frames: number;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated dance animation video.
   */
  video: File;
};

/**
 * SteadyDancerRequest
 *
 * Request model for SteadyDancer human animation.
 */
export type SteadyDancerInput = {
  /**
   * Prompt
   *
   * Text prompt describing the desired animation.
   */
  prompt?: string;
  /**
   * Video Url
   *
   * URL of the driving pose video. The motion from this video will be transferred to the reference image.
   */
  video_url?: string;
  /**
   * Acceleration
   *
   * Acceleration levels.
   */
  acceleration?: "light" | "moderate" | "aggressive";
  /**
   * Pose Guidance Scale
   *
   * Pose guidance scale for pose control strength.
   */
  pose_guidance_scale?: number;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Pose Guidance End
   *
   * End ratio for pose guidance. Controls when pose guidance ends.
   */
  pose_guidance_end?: number;
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24. If not specified, uses the FPS from the input video.
   */
  frames_per_second?: number;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale for prompt adherence.
   */
  guidance_scale?: number;
  /**
   * Num Frames
   *
   * Number of frames to generate. If not specified, uses the frame count from the input video (capped at 241). Will be adjusted to nearest valid value (must satisfy 4k+1 pattern).
   */
  num_frames?: number;
  /**
   * Use Turbo
   *
   * If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized (num_inference_steps=6, guidance_scale=1.0) and uses the LightX2V distillation LoRA.
   */
  use_turbo?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', will be determined from the reference image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Pose Guidance Start
   *
   * Start ratio for pose guidance. Controls when pose guidance begins.
   */
  pose_guidance_start?: number;
  /**
   * Resolution
   *
   * Resolution of the generated video. 576p is default, 720p for higher quality. 480p is lower quality.
   */
  resolution?: "480p" | "576p" | "720p";
  /**
   * Image Url
   *
   * URL of the reference image to animate. This is the person/character whose appearance will be preserved.
   */
  image_url?: string;
  /**
   * Preserve Audio
   *
   * If enabled, copies audio from the input driving video to the output video.
   */
  preserve_audio?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
};

/**
 * OmniV2VEditOutput
 */
export type KlingVideoO1StandardVideoToVideoEditOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * OmniV2VEditInput
 *
 * Input for video editing or video-as-reference generation.
 */
export type KlingVideoO1StandardVideoToVideoEditInput = {
  /**
   * Prompt
   *
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Video Url
   *
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string;
  /**
   * Elements
   *
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * Image Urls
   *
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
  /**
   * Keep Audio
   *
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
};

/**
 * OmniV2VReferenceOutput
 */
export type KlingVideoO1StandardVideoToVideoReferenceOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * OmniV2VReferenceInput
 *
 * Input for video editing or video-as-reference generation.
 */
export type KlingVideoO1StandardVideoToVideoReferenceInput = {
  /**
   * Prompt
   *
   * Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Video Url
   *
   * Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB.
   *
   * Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s
   */
  video_url: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
  /**
   * Keep Audio
   *
   * Whether to keep the original audio from the video.
   */
  keep_audio?: boolean;
  /**
   * Elements
   *
   * Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * Image Urls
   *
   * Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.
   */
  image_urls?: Array<string>;
};

/**
 * Veo31VideoToVideoOutput
 */
export type Veo31ExtendVideoOutput = {
  /**
   * Video
   *
   * The extended video.
   */
  video: File;
};

/**
 * Veo31VideoToVideoInput
 *
 * Input for video extension/video-to-video generation.
 */
export type Veo31ExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing how the video should be extended
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "7s";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Video URL
   *
   * URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
   */
  video_url: string;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p";
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * Veo31VideoToVideoOutput
 */
export type Veo31FastExtendVideoOutput = {
  /**
   * Video
   *
   * The extended video.
   */
  video: File;
};

/**
 * Veo31VideoToVideoInput
 *
 * Input for video extension/video-to-video generation.
 */
export type Veo31FastExtendVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing how the video should be extended
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "7s";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Video URL
   *
   * URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.
   */
  video_url: string;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p";
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * ReferenceToVideoOutput
 *
 * Output for reference-to-video generation
 */
export type V26ReferenceToVideoOutput = {
  /**
   * Actual Prompt
   *
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * ReferenceToVideoInput
 *
 * Input for Wan 2.6 reference-to-video generation (R2V)
 */
export type V26ReferenceToVideoInput = {
  /**
   * Prompt
   *
   * Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution tier. R2V only supports 720p and 1080p (no 480p).
   */
  resolution?: "720p" | "1080p";
  /**
   * Video Urls
   *
   * Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects.
   */
  video_urls: Array<string>;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:3" | "3:4";
  /**
   * Duration
   *
   * Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s).
   */
  duration?: "5" | "10";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt rewriting using LLM.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Multi Shots
   *
   * When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True.
   */
  multi_shots?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};

/**
 * VideoOutput
 */
export type BriaVideoEraserErasePromptOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: Video | FileType2;
};

/**
 * EraseByPromptInputModel
 */
export type BriaVideoEraserErasePromptInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean;
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string;
  /**
   * Prompt
   *
   * Input prompt to detect object to erase
   */
  prompt: string;
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean;
};

/**
 * VideoOutput
 */
export type BriaVideoEraserEraseKeypointsOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: Video | FileType2;
};

/**
 * EraseByKeyPointsInputModel
 */
export type BriaVideoEraserEraseKeypointsInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean;
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string;
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * Keypoints
   *
   * Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}
   */
  keypoints: Array<string>;
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean;
};

/**
 * VideoOutput
 */
export type BriaVideoEraserEraseMaskOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: Video | FileType2;
};

/**
 * EraseInputModel
 */
export type BriaVideoEraserEraseMaskInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean;
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string;
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * Mask Video Url
   *
   * Input video to mask erase object from. duration must be less than 5s.
   */
  mask_video_url: string;
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean;
};

/**
 * CrystalVideoUpscaleOutput
 */
export type CrystalVideoUpscalerOutput = {
  /**
   * Video
   *
   * URL to the upscaled video
   */
  video: VideoFileType2;
};

/**
 * CrystalVideoUpscaleInput
 */
export type CrystalVideoUpscalerInput = {
  /**
   * Video Url
   *
   * URL to the input video.
   */
  video_url: string;
  /**
   * Scale Factor
   *
   * Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution.
   */
  scale_factor?: number;
};

/**
 * ScailResponse
 */
export type ScailOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * ScailRequest
 */
export type ScailInput = {
  /**
   * Prompt
   *
   * The prompt to guide video generation.
   */
  prompt: string;
  /**
   * Video Url
   *
   * The URL of the video to use as a reference for the video generation.
   */
  video_url: string;
  /**
   * Resolution
   *
   * Output resolution. Outputs 896x512 (landscape) or 512x896 (portrait) based on the input image aspect ratio.
   */
  resolution?: "512p";
  /**
   * Num Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number;
  /**
   * Multi Character
   *
   * Enable multi-character mode. Use when driving video has multiple people.
   */
  multi_character?: boolean;
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string;
};

/**
 * LucyRestyleOutput
 */
export type LucyRestyleOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LucyRestyleInput
 */
export type LucyRestyleInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the video to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the video directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean;
  /**
   * Video Url
   *
   * URL of the video to edit
   */
  video_url: string;
  /**
   * Resolution
   *
   * Resolution of the generated video
   */
  resolution?: "720p";
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * Seed
   *
   * Seed for video generation
   */
  seed?: number;
  /**
   * Enhance Prompt
   *
   * Whether to enhance the prompt for better results.
   */
  enhance_prompt?: boolean;
};

/**
 * MotionControlOutput
 *
 * Output model for motion control video generation.
 */
export type KlingVideoV26ProMotionControlOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * MotionControlRequest
 *
 * Request model for motion control video generation.
 */
export type KlingVideoV26ProMotionControlInput = {
  /**
   * Prompt
   */
  prompt?: string;
  /**
   * Video Url
   *
   * Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.
   */
  video_url: string;
  /**
   * Character Orientation
   *
   * Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).
   */
  character_orientation: "image" | "video";
  /**
   * Keep Original Sound
   *
   * Whether to keep the original sound from the reference video.
   */
  keep_original_sound?: boolean;
  /**
   * Image Url
   *
   * Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.
   */
  image_url: string;
};

/**
 * MotionControlOutput
 *
 * Output model for motion control video generation.
 */
export type KlingVideoV26StandardMotionControlOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * MotionControlRequest
 *
 * Request model for motion control video generation.
 */
export type KlingVideoV26StandardMotionControlInput = {
  /**
   * Prompt
   */
  prompt?: string;
  /**
   * Video Url
   *
   * Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.
   */
  video_url: string;
  /**
   * Character Orientation
   *
   * Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).
   */
  character_orientation: "image" | "video";
  /**
   * Keep Original Sound
   *
   * Whether to keep the original sound from the reference video.
   */
  keep_original_sound?: boolean;
  /**
   * Image Url
   *
   * Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.
   */
  image_url: string;
};

/**
 * TrajectoryParameters
 *
 * Camera trajectory parameters for re-camera operations.
 *
 * Each list represents interpolation values across frames:
 * - theta: Horizontal rotation angles (degrees)
 * - phi: Vertical rotation angles (degrees)
 * - radius: Camera distance scaling factors
 */
export type TrajectoryParameters = {
  /**
   * Theta
   *
   * Horizontal rotation angles (degrees) for each keyframe.
   */
  theta: Array<number>;
  /**
   * Radius
   *
   * Camera distance scaling factors for each keyframe.
   */
  radius: Array<number>;
  /**
   * Phi
   *
   * Vertical rotation angles (degrees) for each keyframe.
   */
  phi: Array<number>;
};

/**
 * LightXOutput
 */
export type LightxRecameraOutput = {
  /**
   * Viz Video
   *
   * Optional: visualization/debug video (if produced by the pipeline).
   */
  viz_video?: File;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Input Video
   *
   * Optional: normalized/processed input video (if produced by the pipeline).
   */
  input_video?: File;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LightXRecameraRequest
 *
 * Re-camera-only request (minimal schema).
 */
export type LightxRecameraInput = {
  /**
   * Prompt
   *
   * Optional text prompt. If omitted, Light-X will auto-caption the video.
   */
  prompt?: string;
  /**
   * Trajectory
   *
   * Camera trajectory parameters (required for recamera mode).
   */
  trajectory?: TrajectoryParameters;
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string;
  /**
   * Camera
   *
   * Camera control mode.
   */
  camera?: "traj" | "target";
  /**
   * Target Pose
   *
   * Target camera pose [theta, phi, radius, x, y] (required when camera='target').
   */
  target_pose?: Array<number>;
  /**
   * Mode
   *
   * Camera motion mode.
   */
  mode?: "gradual" | "bullet" | "direct" | "dolly-zoom";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};

/**
 * RelightParameters
 *
 * Relighting parameters for video relighting operations.
 *
 * Used with relight_condition_type 'ic' (intrinsic conditioning).
 */
export type RelightParameters = {
  /**
   * Relight Prompt
   *
   * Text prompt describing the desired lighting condition.
   */
  relight_prompt: string;
  /**
   * Bg Source
   *
   * Direction of the light source (used for IC-light).
   */
  bg_source?: "Left" | "Right" | "Top" | "Bottom";
  /**
   * Use Sky Mask
   *
   * Whether to use sky masking for outdoor scenes.
   */
  use_sky_mask?: boolean;
  /**
   * Cfg
   *
   * Classifier-free guidance scale for relighting.
   */
  cfg?: number;
};

/**
 * LightXOutput
 */
export type LightxRelightOutput = {
  /**
   * Viz Video
   *
   * Optional: visualization/debug video (if produced by the pipeline).
   */
  viz_video?: File;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Input Video
   *
   * Optional: normalized/processed input video (if produced by the pipeline).
   */
  input_video?: File;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LightXRelightRequest
 *
 * Relighting-only request (minimal schema).
 */
export type LightxRelightInput = {
  /**
   * Prompt
   *
   * Optional text prompt. If omitted, Light-X will auto-caption the video.
   */
  prompt?: string;
  /**
   * Video Url
   *
   * URL of the input video.
   */
  video_url: string;
  /**
   * Relight Parameters
   *
   * Relighting parameters (required for relight_condition_type='ic'). Not used for 'bg' (which expects a background image URL instead).
   */
  relight_parameters?: RelightParameters;
  /**
   * Ref Id
   *
   * Frame index to use as referencen to relight the video with reference.
   */
  ref_id?: number;
  /**
   * Relit Cond Img Url
   *
   * URL of conditioning image. Required for relight_condition_type='ref'/'hdr'. Also required for relight_condition_type='bg' (background image).
   */
  relit_cond_img_url?: string;
  /**
   * Relit Cond Type
   *
   * Relight condition type.
   */
  relit_cond_type?: "ic" | "ref" | "hdr" | "bg";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};

/**
 * VideoOutput
 */
export type VideoEraseMaskOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: Video | FileType2;
};

/**
 * EraseInputModel
 */
export type VideoEraseMaskInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean;
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string;
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * Mask Video Url
   *
   * Input video to mask erase object from. duration must be less than 5s.
   */
  mask_video_url: string;
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean;
};

/**
 * VideoOutput
 */
export type VideoErasePromptOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: Video | FileType2;
};

/**
 * EraseByPromptInputModel
 */
export type VideoErasePromptInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean;
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string;
  /**
   * Prompt
   *
   * Input prompt to detect object to erase
   */
  prompt: string;
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean;
};

/**
 * VideoOutput
 */
export type VideoEraseKeypointsOutput = {
  /**
   * Video
   *
   * Final video.
   */
  video: Video | FileType2;
};

/**
 * EraseByKeyPointsInputModel
 */
export type VideoEraseKeypointsInput = {
  /**
   * Preserve Audio
   *
   * If true, audio will be preserved in the output video.
   */
  preserve_audio?: boolean;
  /**
   * Video Url
   *
   * Input video to erase object from. duration must be less than 5s.
   */
  video_url: string;
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "gif"
    | "mov_h264"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h264"
    | "mkv_h265"
    | "mkv_vp9"
    | "mkv_mpeg4";
  /**
   * Keypoints
   *
   * Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}
   */
  keypoints: Array<string>;
  /**
   * Auto Trim
   *
   * auto trim the video, to working duration ( 5s )
   */
  auto_trim?: boolean;
};

/**
 * LTX2ExtendVideoOutput
 */
export type Ltx219bExtendVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2ExtendVideoInput
 */
export type Ltx219bExtendVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | unknown;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Extend Direction
   *
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.
   */
  extend_direction?: "forward" | "backward";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number;
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
};

/**
 * LTX2ExtendVideoOutput
 */
export type Ltx219bExtendVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRAExtendVideoInput
 */
export type Ltx219bExtendVideoLoraInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | unknown;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number;
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Extend Direction
   *
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.
   */
  extend_direction?: "forward" | "backward";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number;
};

/**
 * LoRAInput
 *
 * LoRA weight configuration.
 */
export type LoRaInput = {
  /**
   * Path
   *
   * URL, HuggingFace repo ID (owner/repo) to lora weights.
   */
  path: string;
  /**
   * Scale
   *
   * Scale factor for LoRA application (0.0 to 4.0).
   */
  scale?: number;
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.
   */
  weight_name?: string | unknown;
};

/**
 * LTX2ExtendVideoOutput
 */
export type Ltx219bDistilledExtendVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2DistilledExtendVideoInput
 */
export type Ltx219bDistilledExtendVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | unknown;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Extend Direction
   *
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.
   */
  extend_direction?: "forward" | "backward";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number;
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
};

/**
 * LTX2ExtendVideoOutput
 */
export type Ltx219bDistilledExtendVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRADistilledExtendVideoInput
 */
export type Ltx219bDistilledExtendVideoLoraInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Video URL
   *
   * The URL of the video to extend.
   */
  video_url: string;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the extended video.
   */
  end_image_url?: string | unknown;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Extend Direction
   *
   * Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.
   */
  extend_direction?: "forward" | "backward";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number;
  /**
   * Number of Context Frames
   *
   * The number of frames to use as context for the extension.
   */
  num_context_frames?: number;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
};

/**
 * LTX2VideoToVideoOutput
 */
export type Ltx219bVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2VideoToVideoInput
 */
export type Ltx219bVideoToVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number;
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number;
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * Audio URL
   *
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | unknown;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
};

/**
 * LTX2VideoToVideoOutput
 */
export type Ltx219bVideoToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRAVideoToVideoInput
 */
export type Ltx219bVideoToVideoLoraInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number;
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number;
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * Audio URL
   *
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | unknown;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
};

/**
 * LTX2VideoToVideoOutput
 */
export type Ltx219bDistilledVideoToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2DistilledVideoToVideoInput
 */
export type Ltx219bDistilledVideoToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string;
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number;
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number;
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * Audio URL
   *
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | unknown;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean;
};

/**
 * LTX2VideoToVideoOutput
 */
export type Ltx219bDistilledVideoToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRADistilledVideoToVideoInput
 */
export type Ltx219bDistilledVideoToVideoLoraInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Video URL
   *
   * The URL of the video to generate the video from.
   */
  video_url: string;
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * IC-LoRA Scale
   *
   * The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.
   */
  ic_lora_scale?: number;
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Video Strength
   *
   * Video conditioning strength. Lower values represent more freedom given to the model to change the video content.
   */
  video_strength?: number;
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Image URL
   *
   * An optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * Match Video Length
   *
   * When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.
   */
  match_video_length?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Preprocessor
   *
   * The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.
   */
  preprocessor?: "depth" | "canny" | "pose" | "none";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * IC-LoRA
   *
   * The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)
   */
  ic_lora?:
    | "match_preprocessor"
    | "canny"
    | "depth"
    | "pose"
    | "detailer"
    | "none";
  /**
   * Audio URL
   *
   * An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.
   */
  audio_url?: string | unknown;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.
   */
  audio_strength?: number;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Match Input FPS
   *
   * When true, match the output FPS to the input video's FPS instead of using the default target FPS.
   */
  match_input_fps?: boolean;
};

/**
 * FaceFusionVideoOutput
 *
 * FaceFusion output payload when video content is generated
 */
export type AiFaceSwapFaceswapvideoOutput = {
  /**
   * Warning
   *
   * Warning message if video was modified (e.g., truncated or FPS reduced)
   */
  warning?: string | unknown;
  video: Video;
  /**
   * Processing Time Ms
   *
   * Optional processing duration in milliseconds
   */
  processing_time_ms?: number | unknown;
};

/**
 * FaceSwapInputVideo
 *
 * Input schema for image  video face swap
 */
export type AiFaceSwapFaceswapvideoInput = {
  /**
   * Enable Occlusion Prevention
   *
   * Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more.
   */
  enable_occlusion_prevention?: boolean;
  /**
   * Source Face Url
   *
   * Source face image. Allowed items: bmp, jpeg, png, tiff, webp
   */
  source_face_url: string;
  /**
   * Target Video Url
   *
   * Target video URL (max 25 minutes, will be truncated if longer; FPS capped at 25). Allowed items: avi, m4v, mkv, mp4, mpeg, mov, mxf, webm, wmv
   */
  target_video_url: string;
};

/**
 * XAIVideoEditOutput
 */
export type GrokImagineVideoEditVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: VideoFileType2;
};

/**
 * XAIVideoEditInput
 */
export type GrokImagineVideoEditVideoInput = {
  /**
   * Prompt
   *
   * Text description of the desired edit.
   */
  prompt: string;
  /**
   * Video URL
   *
   * URL of the input video to edit. The video will be resized to a maximum area of 854x480 pixels and truncated to 8 seconds.
   */
  video_url: string;
  /**
   * Resolution
   *
   * Resolution of the output video.
   */
  resolution?: "auto" | "480p" | "720p";
};

/**
 * Output
 */
export type MmaudioV2Output = {
  /**
   * Video
   *
   * The generated video with the lip sync.
   */
  video: File;
};

/**
 * BaseInput
 */
export type MmaudioV2Input = {
  /**
   * Prompt
   *
   * The prompt to generate the audio for.
   */
  prompt: string;
  /**
   * Video Url
   *
   * The URL of the video to generate the audio for.
   */
  video_url: string;
  /**
   * Num Steps
   *
   * The number of steps to generate the audio for.
   */
  num_steps?: number;
  /**
   * Duration
   *
   * The duration of the audio to generate.
   */
  duration?: number;
  /**
   * Cfg Strength
   *
   * The strength of Classifier Free Guidance.
   */
  cfg_strength?: number;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Mask Away Clip
   *
   * Whether to mask away the clip.
   */
  mask_away_clip?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the audio for.
   */
  negative_prompt?: string;
};

/**
 * GeneralRembgOutput
 */
export type VideoBackgroundRemovalOutputType2 = {
  /**
   * Video
   */
  video: Array<FileType2>;
};

/**
 * OutputRemoveBackgroundModel
 */
export type VideoBackgroundRemovalOutput = {
  /**
   * Video
   *
   * Video with removed background and audio.
   */
  video: Video | FileType2;
};

/**
 * GeneralRembgInput
 */
export type VideoBackgroundRemovalInputType2 = {
  /**
   * Video Url
   */
  video_url: string;
  /**
   * Subject Is Person
   *
   * Set to False if the subject is not a person.
   */
  subject_is_person?: boolean;
  /**
   * Output Codec
   *
   * Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.
   */
  output_codec?: "vp9" | "h264";
  /**
   * Refine Foreground Edges
   *
   * Improves the quality of the extracted object's edges.
   */
  refine_foreground_edges?: boolean;
};

/**
 * InputRemoveBackgroundModel
 */
export type VideoBackgroundRemovalInput = {
  /**
   * Video Url
   *
   * Input video to remove background from. Size should be less than 14142x14142 and duration less than 30s.
   */
  video_url: string;
  /**
   * Output Container And Codec
   *
   * Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.
   */
  output_container_and_codec?:
    | "mp4_h265"
    | "mp4_h264"
    | "webm_vp9"
    | "mov_h265"
    | "mov_proresks"
    | "mkv_h265"
    | "mkv_h264"
    | "mkv_vp9"
    | "gif";
  /**
   * Background Color
   *
   * Background color. Options: Transparent, Black, White, Gray, Red, Green, Blue, Yellow, Cyan, Magenta, Orange.
   */
  background_color?:
    | "Transparent"
    | "Black"
    | "White"
    | "Gray"
    | "Red"
    | "Green"
    | "Blue"
    | "Yellow"
    | "Cyan"
    | "Magenta"
    | "Orange";
};

/**
 * AnimatediffLCMOutput
 */
export type AnimatediffSparsectrlLcmOutput = {
  /**
   * Seed
   *
   * The seed used to generate the video.
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file.
   */
  video: FileType3;
};

/**
 * File
 */
export type FileType3 = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size: number;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name: string;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type: string;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
};

/**
 * AnimatediffLCMInput
 */
export type AnimatediffSparsectrlLcmInput = {
  /**
   * Prompt
   *
   * The prompt to use for generating the image. Be as descriptive as possible for best results.
   */
  prompt: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable
   * Diffusion will output the same image every time.
   *
   */
  seed?: number;
  /**
   * Controlnet Type
   *
   * The type of controlnet to use for generating the video. The controlnet determines how the video will be animated.
   */
  controlnet_type?: "scribble" | "rgb";
  /**
   * Keyframe 2 Index
   *
   * The frame index of the third keyframe to use for the generation.
   */
  keyframe_2_index?: number;
  /**
   * Keyframe 0 Index
   *
   * The frame index of the first keyframe to use for the generation.
   */
  keyframe_0_index?: number;
  /**
   * Keyframe 1 Image Url
   *
   * The URL of the second keyframe to use for the generation.
   */
  keyframe_1_image_url?: string | null;
  /**
   * Keyframe 1 Index
   *
   * The frame index of the second keyframe to use for the generation.
   */
  keyframe_1_index?: number;
  /**
   * Classifier-Free Guidance scale (CFG)
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
   */
  guidance_scale?: number;
  /**
   * Number of inference steps
   *
   * Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image.
   */
  num_inference_steps?: number;
  /**
   * Keyframe 2 Image Url
   *
   * The URL of the third keyframe to use for the generation.
   */
  keyframe_2_image_url?: string | null;
  /**
   * Negative Prompt
   *
   *
   * The negative prompt to use. Use it to specify what you don't want.
   *
   */
  negative_prompt?: string;
  /**
   * Keyframe 0 Image Url
   *
   * The URL of the first keyframe to use for the generation.
   */
  keyframe_0_image_url?: string | null;
};

/**
 * VideoOutput
 */
export type MinimaxVideo01Output = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequest
 */
export type MinimaxVideo01Input = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
};

/**
 * AnimateDiffT2VOutput
 */
export type FastAnimatediffTurboTextToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file.
   */
  video: FileType2;
};

/**
 * AnimateDiffT2VTurboInput
 */
export type FastAnimatediffTurboTextToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use for generating the video. Be as descriptive as possible for best results.
   */
  prompt: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number;
  /**
   * Fps
   *
   * Number of frames per second to extract from the video.
   */
  fps?: number;
  /**
   * Video Size
   *
   * The size of the video to generate.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.
   */
  guidance_scale?: number;
  /**
   * Num Frames
   *
   * The number of frames to generate for the video.
   */
  num_frames?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform. 4-12 is recommended for turbo mode.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   *
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution).
   *
   */
  negative_prompt?: string;
  /**
   * Motions
   *
   * The motions to apply to the video.
   */
  motions?: Array<
    "zoom-out" | "zoom-in" | "pan-left" | "pan-right" | "tilt-up" | "tilt-down"
  >;
};

/**
 * AnimateDiffT2VOutput
 */
export type FastAnimatediffTextToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file.
   */
  video: FileType2;
};

/**
 * AnimateDiffT2VInput
 */
export type FastAnimatediffTextToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use for generating the video. Be as descriptive as possible for best results.
   */
  prompt: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number;
  /**
   * Fps
   *
   * Number of frames per second to extract from the video.
   */
  fps?: number;
  /**
   * Video Size
   *
   * The size of the video to generate.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related image to show you.
   *
   */
  guidance_scale?: number;
  /**
   * Num Frames
   *
   * The number of frames to generate for the video.
   */
  num_frames?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   *
   * The negative prompt to use. Use it to address details that you don't want
   * in the image. This could be colors, objects, scenery and even the small details
   * (e.g. moustache, blurry, low resolution).
   *
   */
  negative_prompt?: string;
  /**
   * Motions
   *
   * The motions to apply to the video.
   */
  motions?: Array<
    "zoom-out" | "zoom-in" | "pan-left" | "pan-right" | "tilt-up" | "tilt-down"
  >;
};

/**
 * Output
 */
export type T2vTurboOutput = {
  video: FileType2;
};

/**
 * Input
 */
export type T2vTurboInput = {
  /**
   * Prompt
   *
   * The prompt to generate images from
   */
  prompt: string;
  /**
   * Guidance Scale
   *
   * The guidance scale
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * The seed to use for the random number generator
   */
  seed?: number | unknown;
  /**
   * Export Fps
   *
   * The FPS of the exported video
   */
  export_fps?: number;
  /**
   * Num Frames
   *
   * The number of frames to generate
   */
  num_frames?: number;
  /**
   * Num Inference Steps
   *
   * The number of steps to sample
   */
  num_inference_steps?: number;
};

/**
 * FastSVDOutput
 */
export type FastSvdLcmTextToVideoOutput = {
  /**
   * Seed
   *
   *
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   *
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * FastSVDTextInput
 */
export type FastSvdLcmTextToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use as a starting point for the generation.
   */
  prompt: string;
  /**
   * Cond Aug
   *
   *
   * The conditoning augmentation determines the amount of noise that will be
   * added to the conditioning frame. The higher the number, the more noise
   * there will be, and the less the video will look like the initial image.
   * Increase it for more motion.
   *
   */
  cond_aug?: number;
  /**
   * Fps
   *
   *
   * The FPS of the generated video. The higher the number, the faster the video will
   * play. Total video length is 25 frames.
   *
   */
  fps?: number;
  /**
   * Motion Bucket Id
   *
   *
   * The motion bucket id determines the motion of the generated video. The
   * higher the number, the more motion there will be.
   *
   */
  motion_bucket_id?: number;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Steps
   *
   *
   * The number of steps to run the model for. The higher the number the better
   * the quality and longer it will take to generate.
   *
   */
  steps?: number;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number;
};

/**
 * FastSVDOutput
 */
export type FastSvdTextToVideoOutput = {
  /**
   * Seed
   *
   *
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   *
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * FastSVDTextInput
 */
export type FastSvdTextToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to use as a starting point for the generation.
   */
  prompt: string;
  /**
   * Cond Aug
   *
   *
   * The conditoning augmentation determines the amount of noise that will be
   * added to the conditioning frame. The higher the number, the more noise
   * there will be, and the less the video will look like the initial image.
   * Increase it for more motion.
   *
   */
  cond_aug?: number;
  /**
   * Deep Cache
   *
   *
   * Enabling [DeepCache](https://github.com/horseee/DeepCache) will make the execution
   * faster, but might sometimes degrade overall quality. The higher the setting, the
   * faster the execution will be, but the more quality might be lost.
   *
   */
  deep_cache?: "none" | "minimum" | "medium" | "high";
  /**
   * Fps
   *
   *
   * The FPS of the generated video. The higher the number, the faster the video will
   * play. Total video length is 25 frames.
   *
   */
  fps?: number;
  /**
   * Motion Bucket Id
   *
   *
   * The motion bucket id determines the motion of the generated video. The
   * higher the number, the more motion there will be.
   *
   */
  motion_bucket_id?: number;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Steps
   *
   *
   * The number of steps to run the model for. The higher the number the better
   * the quality and longer it will take to generate.
   *
   */
  steps?: number;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to use as a starting point for the generation.
   */
  negative_prompt?: string;
};

/**
 * Output
 */
export type LtxVideoOutput = {
  /**
   * Seed
   *
   * The seed used for random number generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * TextToVideoInput
 */
export type LtxVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * The seed to use for random number generation.
   */
  seed?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to take.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
};

/**
 * HunyuanT2VResponse
 */
export type HunyuanVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   */
  video: File;
};

/**
 * HunyuanVideoRequest
 */
export type HunyuanVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to run. Lower gets faster results, higher gets better results.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: "129" | "85";
  /**
   * Pro Mode
   *
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean;
};

/**
 * MochiT2VOutput
 */
export type MochiV1Output = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * MochiT2VInput
 */
export type MochiV1Input = {
  /**
   * Prompt
   *
   * The prompt to generate a video from.
   */
  prompt: string;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt for the video.
   */
  negative_prompt?: string;
};

/**
 * T2VOutput
 */
export type KlingVideoV15ProTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequest
 */
export type KlingVideoV15ProTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * CameraControl
 */
export type CameraControl = {
  /**
   * Movement Type
   *
   * The type of camera movement
   */
  movement_type: "horizontal" | "vertical" | "pan" | "tilt" | "roll" | "zoom";
  /**
   * Movement Value
   *
   * The value of the camera movement
   */
  movement_value: number;
};

/**
 * T2VOutput
 */
export type KlingVideoV1StandardTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * V1TextToVideoRequest
 */
export type KlingVideoV1StandardTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Advanced Camera Control
   *
   * Advanced Camera control parameters
   */
  advanced_camera_control?: CameraControl;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Camera Control
   *
   * Camera control parameters
   */
  camera_control?:
    | "down_back"
    | "forward_up"
    | "right_turn_forward"
    | "left_turn_forward";
};

/**
 * T2VLiveOutput
 */
export type MinimaxVideo01LiveOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoLiveRequest
 */
export type MinimaxVideo01LiveInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
};

/**
 * T2VOutput
 */
export type KlingVideoV16StandardTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequest
 */
export type KlingVideoV16StandardTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * Output
 */
export type Cogvideox5bOutput = {
  /**
   * Prompt
   *
   * The prompt used for generating the video.
   */
  prompt: string;
  /**
   * Timings
   */
  timings: {
    [key: string]: number;
  };
  /**
   * Seed
   *
   *
   * Seed of the generated video. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   */
  seed: number;
  /**
   * Video
   *
   * The URL to the generated video
   */
  video: File;
};

/**
 * BaseInput
 */
export type Cogvideox5bInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Use Rife
   *
   * Use RIFE for video interpolation
   */
  use_rife?: boolean;
  /**
   * Loras
   *
   *
   * The LoRAs to use for the image generation. We currently support one lora.
   *
   */
  loras?: Array<LoraWeight>;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you.
   *
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number;
  /**
   * Export Fps
   *
   * The target FPS of the video
   */
  export_fps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate video from
   */
  negative_prompt?: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
};

/**
 * Output
 */
export type TranspixarOutput = {
  /**
   * Prompt
   *
   * The prompt used for generating the video.
   */
  prompt: string;
  /**
   * Videos
   *
   * The URL to the generated video
   */
  videos: Array<FileType2>;
  /**
   * Timings
   */
  timings: {
    [key: string]: number;
  };
  /**
   * Seed
   *
   *
   * Seed of the generated video. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   */
  seed: number;
};

/**
 * BaseInput
 */
export type TranspixarInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you.
   *
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number;
  /**
   * Export Fps
   *
   * The target FPS of the video
   */
  export_fps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate video from
   */
  negative_prompt?: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number | unknown;
};

/**
 * HunyuanT2VResponse
 */
export type HunyuanVideoLoraOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   */
  video: File;
};

/**
 * HunyuanT2VRequest
 */
export type HunyuanVideoLoraInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Loras
   *
   *
   * The LoRAs to use for the image generation. You can use any number of LoRAs
   * and they will be merged together to generate the final image.
   *
   */
  loras?: Array<LoraWeight>;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: "129" | "85";
  /**
   * Pro Mode
   *
   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.
   */
  pro_mode?: boolean;
};

/**
 * Ray2T2VOutput
 */
export type LumaDreamMachineRay2Output = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * Ray2TextToVideoRequest
 */
export type LumaDreamMachineRay2Input = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Resolution
   *
   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * Loop
   *
   * Whether the video should loop (end of video is blended with the beginning)
   */
  loop?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video (9s costs 2x more)
   */
  duration?: "5s" | "9s";
};

/**
 * VideoOutput
 */
export type PixverseV35TextToVideoFastOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * FastTextToVideoRequest
 */
export type PixverseV35TextToVideoFastInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * VideoOutput
 */
export type PixverseV35TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequest
 */
export type PixverseV35TextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
   */
  duration?: "5" | "8";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * T2VDirectorOutput
 */
export type MinimaxVideo01DirectorOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoDirectorRequest
 */
export type MinimaxVideo01DirectorInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
};

/**
 * TextToVideoOutput
 */
export type Veo2Output = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoInput
 */
export type Veo2Input = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5s" | "6s" | "7s" | "8s";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Seed
   *
   * A seed to use for the video generation
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation
   */
  negative_prompt?: string;
  /**
   * Enhance Prompt
   *
   * Whether to enhance the video generation
   */
  enhance_prompt?: boolean;
};

/**
 * WanT2VResponse
 */
export type WanT2vOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanT2VRequest
 */
export type WanT2vInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Turbo Mode
   *
   * If true, the video will be generated faster with no noticeable degradation in the visual quality.
   */
  turbo_mode?: boolean;
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive).
   */
  num_frames?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * T2VOutput
 */
export type KlingVideoV16ProTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequest
 */
export type KlingVideoV16ProTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * TextToVideoOutput
 */
export type LtxVideoV095Output = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * TextToVideoInput
 */
export type LtxVideoV095Input = {
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using the model's own capabilities.
   */
  expand_prompt?: boolean;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
};

/**
 * VideoEffectsOutput
 */
export type KlingVideoV1StandardEffectsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * VideoEffectsRequest
 */
export type KlingVideoV1StandardEffectsInput = {
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Input Image Urls
   *
   * URL of images to be used for hug, kiss or heart_gesture video.
   */
  input_image_urls?: Array<string>;
  /**
   * Effect Scene
   *
   * The effect scene to use for the video generation
   */
  effect_scene:
    | "hug"
    | "kiss"
    | "heart_gesture"
    | "squish"
    | "expansion"
    | "fuzzyfuzzy"
    | "bloombloom"
    | "dizzydizzy"
    | "jelly_press"
    | "jelly_slice"
    | "jelly_squish"
    | "jelly_jiggle"
    | "pixelpixel"
    | "yearbook"
    | "instant_film"
    | "anime_figure"
    | "rocketrocket"
    | "fly_fly"
    | "disappear"
    | "lightning_power"
    | "bullet_time"
    | "bullet_time_360"
    | "media_interview"
    | "day_to_night"
    | "let's_ride"
    | "jumpdrop"
    | "swish_swish"
    | "running_man"
    | "jazz_jazz"
    | "swing_swing"
    | "skateskate"
    | "building_sweater"
    | "pure_white_wings"
    | "black_wings"
    | "golden_wing"
    | "pink_pink_wings"
    | "rampage_ape"
    | "a_list_look"
    | "countdown_teleport"
    | "firework_2026"
    | "instant_christmas"
    | "birthday_star"
    | "firework"
    | "celebration"
    | "tiger_hug_pro"
    | "pet_lion_pro"
    | "guardian_spirit"
    | "squeeze_scream"
    | "inner_voice"
    | "memory_alive"
    | "guess_what"
    | "eagle_snatch"
    | "hug_from_past"
    | "instant_kid"
    | "dollar_rain"
    | "cry_cry"
    | "building_collapse"
    | "mushroom"
    | "jesus_hug"
    | "shark_alert"
    | "lie_flat"
    | "polar_bear_hug"
    | "brown_bear_hug"
    | "office_escape_plow"
    | "watermelon_bomb"
    | "boss_coming"
    | "wig_out"
    | "car_explosion"
    | "tiger_hug"
    | "siblings"
    | "construction_worker"
    | "snatched"
    | "felt_felt"
    | "plushcut"
    | "drunk_dance"
    | "drunk_dance_pet"
    | "daoma_dance"
    | "bouncy_dance"
    | "smooth_sailing_dance"
    | "new_year_greeting"
    | "lion_dance"
    | "prosperity"
    | "great_success"
    | "golden_horse_fortune"
    | "red_packet_box"
    | "lucky_horse_year"
    | "lucky_red_packet"
    | "lucky_money_come"
    | "lion_dance_pet"
    | "dumpling_making_pet"
    | "fish_making_pet"
    | "pet_red_packet"
    | "lantern_glow"
    | "expression_challenge"
    | "overdrive"
    | "heart_gesture_dance"
    | "poping"
    | "martial_arts"
    | "running"
    | "nezha"
    | "motorcycle_dance"
    | "subject_3_dance"
    | "ghost_step_dance"
    | "phantom_jewel"
    | "zoom_out"
    | "cheers_2026"
    | "kiss_pro"
    | "fight_pro"
    | "hug_pro"
    | "heart_gesture_pro"
    | "dollar_rain_pro"
    | "pet_bee_pro"
    | "santa_random_surprise"
    | "magic_match_tree"
    | "happy_birthday"
    | "thumbs_up_pro"
    | "surprise_bouquet"
    | "bouquet_drop"
    | "3d_cartoon_1_pro"
    | "glamour_photo_shoot"
    | "box_of_joy"
    | "first_toast_of_the_year"
    | "my_santa_pic"
    | "santa_gift"
    | "steampunk_christmas"
    | "snowglobe"
    | "christmas_photo_shoot"
    | "ornament_crash"
    | "santa_express"
    | "particle_santa_surround"
    | "coronation_of_frost"
    | "spark_in_the_snow"
    | "scarlet_and_snow"
    | "cozy_toon_wrap"
    | "bullet_time_lite"
    | "magic_cloak"
    | "balloon_parade"
    | "jumping_ginger_joy"
    | "c4d_cartoon_pro"
    | "venomous_spider"
    | "throne_of_king"
    | "luminous_elf"
    | "woodland_elf"
    | "japanese_anime_1"
    | "american_comics"
    | "snowboarding"
    | "witch_transform"
    | "vampire_transform"
    | "pumpkin_head_transform"
    | "demon_transform"
    | "mummy_transform"
    | "zombie_transform"
    | "cute_pumpkin_transform"
    | "cute_ghost_transform"
    | "knock_knock_halloween"
    | "halloween_escape"
    | "baseball"
    | "trampoline"
    | "trampoline_night"
    | "pucker_up"
    | "feed_mooncake"
    | "flyer"
    | "dishwasher"
    | "pet_chinese_opera"
    | "magic_fireball"
    | "gallery_ring"
    | "pet_moto_rider"
    | "muscle_pet"
    | "pet_delivery"
    | "mythic_style"
    | "steampunk"
    | "3d_cartoon_2"
    | "pet_chef"
    | "santa_gifts"
    | "santa_hug"
    | "girlfriend"
    | "boyfriend"
    | "heart_gesture_1"
    | "pet_wizard"
    | "smoke_smoke"
    | "gun_shot"
    | "double_gun"
    | "pet_warrior"
    | "long_hair"
    | "pet_dance"
    | "wool_curly"
    | "pet_bee"
    | "marry_me"
    | "piggy_morph"
    | "ski_ski"
    | "magic_broom"
    | "splashsplash"
    | "surfsurf"
    | "fairy_wing"
    | "angel_wing"
    | "dark_wing"
    | "emoji";
};

/**
 * VideoEffectsOutput
 */
export type KlingVideoV15ProEffectsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * VideoEffectsRequest
 */
export type KlingVideoV15ProEffectsInput = {
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Input Image Urls
   *
   * URL of images to be used for hug, kiss or heart_gesture video.
   */
  input_image_urls?: Array<string>;
  /**
   * Effect Scene
   *
   * The effect scene to use for the video generation
   */
  effect_scene:
    | "hug"
    | "kiss"
    | "heart_gesture"
    | "squish"
    | "expansion"
    | "fuzzyfuzzy"
    | "bloombloom"
    | "dizzydizzy"
    | "jelly_press"
    | "jelly_slice"
    | "jelly_squish"
    | "jelly_jiggle"
    | "pixelpixel"
    | "yearbook"
    | "instant_film"
    | "anime_figure"
    | "rocketrocket"
    | "fly_fly"
    | "disappear"
    | "lightning_power"
    | "bullet_time"
    | "bullet_time_360"
    | "media_interview"
    | "day_to_night"
    | "let's_ride"
    | "jumpdrop"
    | "swish_swish"
    | "running_man"
    | "jazz_jazz"
    | "swing_swing"
    | "skateskate"
    | "building_sweater"
    | "pure_white_wings"
    | "black_wings"
    | "golden_wing"
    | "pink_pink_wings"
    | "rampage_ape"
    | "a_list_look"
    | "countdown_teleport"
    | "firework_2026"
    | "instant_christmas"
    | "birthday_star"
    | "firework"
    | "celebration"
    | "tiger_hug_pro"
    | "pet_lion_pro"
    | "guardian_spirit"
    | "squeeze_scream"
    | "inner_voice"
    | "memory_alive"
    | "guess_what"
    | "eagle_snatch"
    | "hug_from_past"
    | "instant_kid"
    | "dollar_rain"
    | "cry_cry"
    | "building_collapse"
    | "mushroom"
    | "jesus_hug"
    | "shark_alert"
    | "lie_flat"
    | "polar_bear_hug"
    | "brown_bear_hug"
    | "office_escape_plow"
    | "watermelon_bomb"
    | "boss_coming"
    | "wig_out"
    | "car_explosion"
    | "tiger_hug"
    | "siblings"
    | "construction_worker"
    | "snatched"
    | "felt_felt"
    | "plushcut"
    | "drunk_dance"
    | "drunk_dance_pet"
    | "daoma_dance"
    | "bouncy_dance"
    | "smooth_sailing_dance"
    | "new_year_greeting"
    | "lion_dance"
    | "prosperity"
    | "great_success"
    | "golden_horse_fortune"
    | "red_packet_box"
    | "lucky_horse_year"
    | "lucky_red_packet"
    | "lucky_money_come"
    | "lion_dance_pet"
    | "dumpling_making_pet"
    | "fish_making_pet"
    | "pet_red_packet"
    | "lantern_glow"
    | "expression_challenge"
    | "overdrive"
    | "heart_gesture_dance"
    | "poping"
    | "martial_arts"
    | "running"
    | "nezha"
    | "motorcycle_dance"
    | "subject_3_dance"
    | "ghost_step_dance"
    | "phantom_jewel"
    | "zoom_out"
    | "cheers_2026"
    | "kiss_pro"
    | "fight_pro"
    | "hug_pro"
    | "heart_gesture_pro"
    | "dollar_rain_pro"
    | "pet_bee_pro"
    | "santa_random_surprise"
    | "magic_match_tree"
    | "happy_birthday"
    | "thumbs_up_pro"
    | "surprise_bouquet"
    | "bouquet_drop"
    | "3d_cartoon_1_pro"
    | "glamour_photo_shoot"
    | "box_of_joy"
    | "first_toast_of_the_year"
    | "my_santa_pic"
    | "santa_gift"
    | "steampunk_christmas"
    | "snowglobe"
    | "christmas_photo_shoot"
    | "ornament_crash"
    | "santa_express"
    | "particle_santa_surround"
    | "coronation_of_frost"
    | "spark_in_the_snow"
    | "scarlet_and_snow"
    | "cozy_toon_wrap"
    | "bullet_time_lite"
    | "magic_cloak"
    | "balloon_parade"
    | "jumping_ginger_joy"
    | "c4d_cartoon_pro"
    | "venomous_spider"
    | "throne_of_king"
    | "luminous_elf"
    | "woodland_elf"
    | "japanese_anime_1"
    | "american_comics"
    | "snowboarding"
    | "witch_transform"
    | "vampire_transform"
    | "pumpkin_head_transform"
    | "demon_transform"
    | "mummy_transform"
    | "zombie_transform"
    | "cute_pumpkin_transform"
    | "cute_ghost_transform"
    | "knock_knock_halloween"
    | "halloween_escape"
    | "baseball"
    | "trampoline"
    | "trampoline_night"
    | "pucker_up"
    | "feed_mooncake"
    | "flyer"
    | "dishwasher"
    | "pet_chinese_opera"
    | "magic_fireball"
    | "gallery_ring"
    | "pet_moto_rider"
    | "muscle_pet"
    | "pet_delivery"
    | "mythic_style"
    | "steampunk"
    | "3d_cartoon_2"
    | "pet_chef"
    | "santa_gifts"
    | "santa_hug"
    | "girlfriend"
    | "boyfriend"
    | "heart_gesture_1"
    | "pet_wizard"
    | "smoke_smoke"
    | "gun_shot"
    | "double_gun"
    | "pet_warrior"
    | "long_hair"
    | "pet_dance"
    | "wool_curly"
    | "pet_bee"
    | "marry_me"
    | "piggy_morph"
    | "ski_ski"
    | "magic_broom"
    | "splashsplash"
    | "surfsurf"
    | "fairy_wing"
    | "angel_wing"
    | "dark_wing"
    | "emoji";
};

/**
 * VideoEffectsOutput
 */
export type KlingVideoV16StandardEffectsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * VideoEffectsRequest
 */
export type KlingVideoV16StandardEffectsInput = {
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Input Image Urls
   *
   * URL of images to be used for hug, kiss or heart_gesture video.
   */
  input_image_urls?: Array<string>;
  /**
   * Effect Scene
   *
   * The effect scene to use for the video generation
   */
  effect_scene:
    | "hug"
    | "kiss"
    | "heart_gesture"
    | "squish"
    | "expansion"
    | "fuzzyfuzzy"
    | "bloombloom"
    | "dizzydizzy"
    | "jelly_press"
    | "jelly_slice"
    | "jelly_squish"
    | "jelly_jiggle"
    | "pixelpixel"
    | "yearbook"
    | "instant_film"
    | "anime_figure"
    | "rocketrocket"
    | "fly_fly"
    | "disappear"
    | "lightning_power"
    | "bullet_time"
    | "bullet_time_360"
    | "media_interview"
    | "day_to_night"
    | "let's_ride"
    | "jumpdrop"
    | "swish_swish"
    | "running_man"
    | "jazz_jazz"
    | "swing_swing"
    | "skateskate"
    | "building_sweater"
    | "pure_white_wings"
    | "black_wings"
    | "golden_wing"
    | "pink_pink_wings"
    | "rampage_ape"
    | "a_list_look"
    | "countdown_teleport"
    | "firework_2026"
    | "instant_christmas"
    | "birthday_star"
    | "firework"
    | "celebration"
    | "tiger_hug_pro"
    | "pet_lion_pro"
    | "guardian_spirit"
    | "squeeze_scream"
    | "inner_voice"
    | "memory_alive"
    | "guess_what"
    | "eagle_snatch"
    | "hug_from_past"
    | "instant_kid"
    | "dollar_rain"
    | "cry_cry"
    | "building_collapse"
    | "mushroom"
    | "jesus_hug"
    | "shark_alert"
    | "lie_flat"
    | "polar_bear_hug"
    | "brown_bear_hug"
    | "office_escape_plow"
    | "watermelon_bomb"
    | "boss_coming"
    | "wig_out"
    | "car_explosion"
    | "tiger_hug"
    | "siblings"
    | "construction_worker"
    | "snatched"
    | "felt_felt"
    | "plushcut"
    | "drunk_dance"
    | "drunk_dance_pet"
    | "daoma_dance"
    | "bouncy_dance"
    | "smooth_sailing_dance"
    | "new_year_greeting"
    | "lion_dance"
    | "prosperity"
    | "great_success"
    | "golden_horse_fortune"
    | "red_packet_box"
    | "lucky_horse_year"
    | "lucky_red_packet"
    | "lucky_money_come"
    | "lion_dance_pet"
    | "dumpling_making_pet"
    | "fish_making_pet"
    | "pet_red_packet"
    | "lantern_glow"
    | "expression_challenge"
    | "overdrive"
    | "heart_gesture_dance"
    | "poping"
    | "martial_arts"
    | "running"
    | "nezha"
    | "motorcycle_dance"
    | "subject_3_dance"
    | "ghost_step_dance"
    | "phantom_jewel"
    | "zoom_out"
    | "cheers_2026"
    | "kiss_pro"
    | "fight_pro"
    | "hug_pro"
    | "heart_gesture_pro"
    | "dollar_rain_pro"
    | "pet_bee_pro"
    | "santa_random_surprise"
    | "magic_match_tree"
    | "happy_birthday"
    | "thumbs_up_pro"
    | "surprise_bouquet"
    | "bouquet_drop"
    | "3d_cartoon_1_pro"
    | "glamour_photo_shoot"
    | "box_of_joy"
    | "first_toast_of_the_year"
    | "my_santa_pic"
    | "santa_gift"
    | "steampunk_christmas"
    | "snowglobe"
    | "christmas_photo_shoot"
    | "ornament_crash"
    | "santa_express"
    | "particle_santa_surround"
    | "coronation_of_frost"
    | "spark_in_the_snow"
    | "scarlet_and_snow"
    | "cozy_toon_wrap"
    | "bullet_time_lite"
    | "magic_cloak"
    | "balloon_parade"
    | "jumping_ginger_joy"
    | "c4d_cartoon_pro"
    | "venomous_spider"
    | "throne_of_king"
    | "luminous_elf"
    | "woodland_elf"
    | "japanese_anime_1"
    | "american_comics"
    | "snowboarding"
    | "witch_transform"
    | "vampire_transform"
    | "pumpkin_head_transform"
    | "demon_transform"
    | "mummy_transform"
    | "zombie_transform"
    | "cute_pumpkin_transform"
    | "cute_ghost_transform"
    | "knock_knock_halloween"
    | "halloween_escape"
    | "baseball"
    | "trampoline"
    | "trampoline_night"
    | "pucker_up"
    | "feed_mooncake"
    | "flyer"
    | "dishwasher"
    | "pet_chinese_opera"
    | "magic_fireball"
    | "gallery_ring"
    | "pet_moto_rider"
    | "muscle_pet"
    | "pet_delivery"
    | "mythic_style"
    | "steampunk"
    | "3d_cartoon_2"
    | "pet_chef"
    | "santa_gifts"
    | "santa_hug"
    | "girlfriend"
    | "boyfriend"
    | "heart_gesture_1"
    | "pet_wizard"
    | "smoke_smoke"
    | "gun_shot"
    | "double_gun"
    | "pet_warrior"
    | "long_hair"
    | "pet_dance"
    | "wool_curly"
    | "pet_bee"
    | "marry_me"
    | "piggy_morph"
    | "ski_ski"
    | "magic_broom"
    | "splashsplash"
    | "surfsurf"
    | "fairy_wing"
    | "angel_wing"
    | "dark_wing"
    | "emoji";
};

/**
 * VideoEffectsOutput
 */
export type KlingVideoV16ProEffectsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * VideoEffectsRequest
 */
export type KlingVideoV16ProEffectsInput = {
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Input Image Urls
   *
   * URL of images to be used for hug, kiss or heart_gesture video.
   */
  input_image_urls?: Array<string>;
  /**
   * Effect Scene
   *
   * The effect scene to use for the video generation
   */
  effect_scene:
    | "hug"
    | "kiss"
    | "heart_gesture"
    | "squish"
    | "expansion"
    | "fuzzyfuzzy"
    | "bloombloom"
    | "dizzydizzy"
    | "jelly_press"
    | "jelly_slice"
    | "jelly_squish"
    | "jelly_jiggle"
    | "pixelpixel"
    | "yearbook"
    | "instant_film"
    | "anime_figure"
    | "rocketrocket"
    | "fly_fly"
    | "disappear"
    | "lightning_power"
    | "bullet_time"
    | "bullet_time_360"
    | "media_interview"
    | "day_to_night"
    | "let's_ride"
    | "jumpdrop"
    | "swish_swish"
    | "running_man"
    | "jazz_jazz"
    | "swing_swing"
    | "skateskate"
    | "building_sweater"
    | "pure_white_wings"
    | "black_wings"
    | "golden_wing"
    | "pink_pink_wings"
    | "rampage_ape"
    | "a_list_look"
    | "countdown_teleport"
    | "firework_2026"
    | "instant_christmas"
    | "birthday_star"
    | "firework"
    | "celebration"
    | "tiger_hug_pro"
    | "pet_lion_pro"
    | "guardian_spirit"
    | "squeeze_scream"
    | "inner_voice"
    | "memory_alive"
    | "guess_what"
    | "eagle_snatch"
    | "hug_from_past"
    | "instant_kid"
    | "dollar_rain"
    | "cry_cry"
    | "building_collapse"
    | "mushroom"
    | "jesus_hug"
    | "shark_alert"
    | "lie_flat"
    | "polar_bear_hug"
    | "brown_bear_hug"
    | "office_escape_plow"
    | "watermelon_bomb"
    | "boss_coming"
    | "wig_out"
    | "car_explosion"
    | "tiger_hug"
    | "siblings"
    | "construction_worker"
    | "snatched"
    | "felt_felt"
    | "plushcut"
    | "drunk_dance"
    | "drunk_dance_pet"
    | "daoma_dance"
    | "bouncy_dance"
    | "smooth_sailing_dance"
    | "new_year_greeting"
    | "lion_dance"
    | "prosperity"
    | "great_success"
    | "golden_horse_fortune"
    | "red_packet_box"
    | "lucky_horse_year"
    | "lucky_red_packet"
    | "lucky_money_come"
    | "lion_dance_pet"
    | "dumpling_making_pet"
    | "fish_making_pet"
    | "pet_red_packet"
    | "lantern_glow"
    | "expression_challenge"
    | "overdrive"
    | "heart_gesture_dance"
    | "poping"
    | "martial_arts"
    | "running"
    | "nezha"
    | "motorcycle_dance"
    | "subject_3_dance"
    | "ghost_step_dance"
    | "phantom_jewel"
    | "zoom_out"
    | "cheers_2026"
    | "kiss_pro"
    | "fight_pro"
    | "hug_pro"
    | "heart_gesture_pro"
    | "dollar_rain_pro"
    | "pet_bee_pro"
    | "santa_random_surprise"
    | "magic_match_tree"
    | "happy_birthday"
    | "thumbs_up_pro"
    | "surprise_bouquet"
    | "bouquet_drop"
    | "3d_cartoon_1_pro"
    | "glamour_photo_shoot"
    | "box_of_joy"
    | "first_toast_of_the_year"
    | "my_santa_pic"
    | "santa_gift"
    | "steampunk_christmas"
    | "snowglobe"
    | "christmas_photo_shoot"
    | "ornament_crash"
    | "santa_express"
    | "particle_santa_surround"
    | "coronation_of_frost"
    | "spark_in_the_snow"
    | "scarlet_and_snow"
    | "cozy_toon_wrap"
    | "bullet_time_lite"
    | "magic_cloak"
    | "balloon_parade"
    | "jumping_ginger_joy"
    | "c4d_cartoon_pro"
    | "venomous_spider"
    | "throne_of_king"
    | "luminous_elf"
    | "woodland_elf"
    | "japanese_anime_1"
    | "american_comics"
    | "snowboarding"
    | "witch_transform"
    | "vampire_transform"
    | "pumpkin_head_transform"
    | "demon_transform"
    | "mummy_transform"
    | "zombie_transform"
    | "cute_pumpkin_transform"
    | "cute_ghost_transform"
    | "knock_knock_halloween"
    | "halloween_escape"
    | "baseball"
    | "trampoline"
    | "trampoline_night"
    | "pucker_up"
    | "feed_mooncake"
    | "flyer"
    | "dishwasher"
    | "pet_chinese_opera"
    | "magic_fireball"
    | "gallery_ring"
    | "pet_moto_rider"
    | "muscle_pet"
    | "pet_delivery"
    | "mythic_style"
    | "steampunk"
    | "3d_cartoon_2"
    | "pet_chef"
    | "santa_gifts"
    | "santa_hug"
    | "girlfriend"
    | "boyfriend"
    | "heart_gesture_1"
    | "pet_wizard"
    | "smoke_smoke"
    | "gun_shot"
    | "double_gun"
    | "pet_warrior"
    | "long_hair"
    | "pet_dance"
    | "wool_curly"
    | "pet_bee"
    | "marry_me"
    | "piggy_morph"
    | "ski_ski"
    | "magic_broom"
    | "splashsplash"
    | "surfsurf"
    | "fairy_wing"
    | "angel_wing"
    | "dark_wing"
    | "emoji";
};

/**
 * WanProT2VResponse
 */
export type WanProTextToVideoOutput = {
  video: FileType2;
};

/**
 * WanProT2VRequest
 */
export type WanProTextToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video
   */
  prompt: string;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker
   */
  enable_safety_checker?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
};

/**
 * TurboTextToVideoOutput
 *
 * Output from text-to-video generation
 */
export type PikaV2TurboTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoTurboInput
 *
 * Base request for text-to-video generation
 */
export type PikaV2TurboTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: number;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the model
   */
  negative_prompt?: string;
};

/**
 * Pika22TextToVideoOutput
 *
 * Output model for Pika 2.2 text-to-video generation
 */
export type PikaV22TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * Pika22TextToVideoRequest
 *
 * Request model for Pika 2.2 text-to-video generation
 */
export type PikaV22TextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "1080p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: 5 | 10;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the model
   */
  negative_prompt?: string;
};

/**
 * TextToVideoV21Output
 *
 * Output from text-to-video generation
 */
export type PikaV21TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideov21Input
 *
 * Base request for text-to-video generation
 */
export type PikaV21TextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: number;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the model
   */
  negative_prompt?: string;
};

/**
 * Ray2T2VOutput
 */
export type LumaDreamMachineRay2FlashOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * Ray2TextToVideoRequest
 */
export type LumaDreamMachineRay2FlashInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Resolution
   *
   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * Loop
   *
   * Whether the video should loop (end of video is blended with the beginning)
   */
  loop?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video (9s costs 2x more)
   */
  duration?: "5s" | "9s";
};

/**
 * WanT2VResponse
 */
export type WanT2vLoraOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanLoRARequest
 */
export type WanT2vLoraInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p,580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Reverse Video
   *
   * If true, the video will be reversed.
   */
  reverse_video?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Loras
   *
   * LoRA weights to be used in the inference.
   */
  loras?: Array<LoraWeightType2>;
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number;
  /**
   * Turbo Mode
   *
   * If true, the video will be generated faster with no noticeable degradation in the visual quality.
   */
  turbo_mode?: boolean;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive).
   */
  num_frames?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * LoraWeight
 */
export type LoraWeightType2 = {
  /**
   * Path
   *
   * URL or the path to the LoRA weights.
   */
  path: string;
  /**
   * Scale
   *
   *
   * The scale of the LoRA weight. This is used to scale the LoRA weight
   * before merging it with the base model.
   *
   */
  scale?: number;
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Used only if `path` is a Hugging Face repository, and required only if you have more than 1 safetensors file in the repo.
   */
  weight_name?: string;
};

/**
 * LipsyncOutput
 */
export type KlingVideoLipsyncTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LipsyncT2VRequest
 */
export type KlingVideoLipsyncTextToVideoInput = {
  /**
   * Text
   *
   * Text content for lip-sync video generation. Max 120 characters.
   */
  text: string;
  /**
   * Video Url
   *
   * The URL of the video to generate the lip sync for. Supports .mp4/.mov, 100MB, 2-60s, 720p/1080p only, width/height 7201920px. If validation fails, an error is returned.
   */
  video_url: string;
  /**
   * Voice Id
   *
   * Voice ID to use for speech synthesis
   */
  voice_id:
    | "genshin_vindi2"
    | "zhinen_xuesheng"
    | "AOT"
    | "ai_shatang"
    | "genshin_klee2"
    | "genshin_kirara"
    | "ai_kaiya"
    | "oversea_male1"
    | "ai_chenjiahao_712"
    | "girlfriend_4_speech02"
    | "chat1_female_new-3"
    | "chat_0407_5-1"
    | "cartoon-boy-07"
    | "uk_boy1"
    | "cartoon-girl-01"
    | "PeppaPig_platform"
    | "ai_huangzhong_712"
    | "ai_huangyaoshi_712"
    | "ai_laoguowang_712"
    | "chengshu_jiejie"
    | "you_pingjing"
    | "calm_story1"
    | "uk_man2"
    | "laopopo_speech02"
    | "heainainai_speech02"
    | "reader_en_m-v1"
    | "commercial_lady_en_f-v1"
    | "tiyuxi_xuedi"
    | "tiexin_nanyou"
    | "girlfriend_1_speech02"
    | "girlfriend_2_speech02"
    | "zhuxi_speech02"
    | "uk_oldman3"
    | "dongbeilaotie_speech02"
    | "chongqingxiaohuo_speech02"
    | "chuanmeizi_speech02"
    | "chaoshandashu_speech02"
    | "ai_taiwan_man2_speech02"
    | "xianzhanggui_speech02"
    | "tianjinjiejie_speech02"
    | "diyinnansang_DB_CN_M_04-v2"
    | "yizhipiannan-v1"
    | "guanxiaofang-v2"
    | "tianmeixuemei-v1"
    | "daopianyansang-v1"
    | "mengwa-v1";
  /**
   * Voice Language
   *
   * The voice language corresponding to the Voice ID
   */
  voice_language?: "zh" | "en";
  /**
   * Voice Speed
   *
   * Speech rate for Text to Video generation
   */
  voice_speed?: number;
};

/**
 * LipsyncA2VOutput
 */
export type KlingVideoLipsyncAudioToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * LipsyncA2VRequest
 */
export type KlingVideoLipsyncAudioToVideoInput = {
  /**
   * Video Url
   *
   * The URL of the video to generate the lip sync for. Supports .mp4/.mov, 100MB, 210s, 720p/1080p only, width/height 7201920px.
   */
  video_url: string;
  /**
   * Audio Url
   *
   * The URL of the audio to generate the lip sync for. Minimum duration is 2s and maximum duration is 60s. Maximum file size is 5MB.
   */
  audio_url: string;
};

/**
 * VideoOutputV4
 */
export type PixverseV4TextToVideoFastOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * FastTextToVideoRequest
 */
export type PixverseV4TextToVideoFastInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * VideoOutputV4
 */
export type PixverseV4TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequest
 */
export type PixverseV4TextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
   */
  duration?: "5" | "8";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * MagiResponse
 */
export type MagiDistilledOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MagiTextToVideoRequest
 */
export type MagiDistilledInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number;
};

/**
 * MagiResponse
 */
export type MagiOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MagiTextToVideoRequest
 */
export type MagiInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32 | 64;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number;
};

/**
 * Q1TextToVideoOutput
 */
export type ViduQ1TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video using the Q1 model
   */
  video: File;
};

/**
 * Q1TextToVideoRequest
 */
export type ViduQ1TextToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the output video
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Style
   *
   * The style of output video
   */
  style?: "general" | "anime";
  /**
   * Seed
   *
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};

/**
 * VideoOutputV4
 */
export type PixverseV45TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequest
 */
export type PixverseV45TextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
   */
  duration?: "5" | "8";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * VideoOutputV4
 */
export type PixverseV45TextToVideoFastOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * FastTextToVideoRequest
 */
export type PixverseV45TextToVideoFastInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * TextToVideoOutput
 */
export type LtxVideo13bDistilledOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * DistilledTextToVideoInput
 *
 * Distilled model input
 */
export type LtxVideo13bDistilledInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9, 1:1 or 9:16).
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9";
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * TextToVideoOutput
 */
export type LtxVideo13bDevOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * TextToVideoInput
 */
export type LtxVideo13bDevInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9, 1:1 or 9:16).
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9";
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * TextToVideoV21MasterOutput
 */
export type KlingVideoV21MasterTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoV21MasterRequest
 */
export type KlingVideoV21MasterTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * SeedanceVideoOutput
 */
export type BytedanceSeedanceV1LiteTextToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SeedanceTextToVideoInput
 */
export type BytedanceSeedanceV1LiteTextToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution - 480p for faster generation, 720p for higher quality
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "9:21";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Seed
   *
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * Camera Fixed
   *
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
};

/**
 * SeedanceProT2VVideoOutput
 */
export type BytedanceSeedanceV1ProTextToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SeedanceProTextToVideoInput
 */
export type BytedanceSeedanceV1ProTextToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Seed
   *
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * Camera Fixed
   *
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
};

/**
 * TextToVideoHailuo02Output
 */
export type MinimaxHailuo02ProTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ProTextToVideoHailuo02Input
 */
export type MinimaxHailuo02ProTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
};

/**
 * TextToVideoOutput
 */
export type Ltxv13B098DistilledOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * DistilledTextToVideoInput
 *
 * Distilled model input
 */
export type Ltxv13B098DistilledInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Temporal AdaIN Factor
   *
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
   */
  temporal_adain_factor?: number;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Second Pass Number of Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Enable Detail Pass
   *
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9";
  /**
   * Tone Map Compression Ratio
   *
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * WanT2VResponse
 */
export type WanV22A14bTextToVideoOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanT2VRequest
 */
export type WanV22A14bTextToVideoInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: "none" | "regular";
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number;
  /**
   * Guidance Scale (1st Stage)
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Guidance Scale (2nd Stage)
   *
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
   */
  guidance_scale_2?: number;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean;
};

/**
 * WanSmallT2VResponse
 */
export type WanV225bTextToVideoOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanSmallT2VRequest
 */
export type WanV225bTextToVideoInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (580p or 720p).
   */
  resolution?: "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean;
};

/**
 * WanTurboT2VResponse
 */
export type WanV22A14bTextToVideoTurboOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanTurboT2VRequest
 */
export type WanV22A14bTextToVideoTurboInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: "none" | "regular";
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
};

/**
 * WanSmallFastVideoT2VResponse
 */
export type WanV225bTextToVideoFastWanOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanSmallFastVideoT2VRequest
 */
export type WanV225bTextToVideoFastWanInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (580p or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * WanSmallT2VResponse
 */
export type WanV225bTextToVideoDistillOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanDistillT2VRequest
 */
export type WanV225bTextToVideoDistillInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Guidance Scale (1st Stage)
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (580p or 720p).
   */
  resolution?: "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean;
};

/**
 * WanT2VResponse
 */
export type WanV22A14bTextToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanLoRAT2VRequest
 */
export type WanV22A14bTextToVideoLoraInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: "none" | "regular";
  /**
   * Reverse Video
   *
   * If true, the video will be reversed.
   */
  reverse_video?: boolean;
  /**
   * Loras
   *
   * LoRA weights to be used in the inference.
   */
  loras?: Array<LoRaWeightType2>;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number;
  /**
   * Guidance Scale (1st Stage)
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9 or 9:16).
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Guidance Scale (2nd Stage)
   *
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
   */
  guidance_scale_2?: number;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean;
};

/**
 * LoRAWeight
 */
export type LoRaWeightType2 = {
  /**
   * Path
   *
   * URL or the path to the LoRA weights.
   */
  path: string;
  /**
   * Scale
   *
   *
   * The scale of the LoRA weight. This is used to scale the LoRA weight
   * before merging it with the base model.
   *
   */
  scale?: number;
  /**
   * Transformer
   *
   * Specifies the transformer to load the lora weight into. 'high' loads into the high-noise transformer, 'low' loads it into the low-noise transformer, while 'both' loads the LoRA into both transformers.
   */
  transformer?: "high" | "low" | "both";
  /**
   * Weight Name
   *
   * Name of the LoRA weight. Used only if `path` is a Hugging Face repository, and required only if you have more than 1 safetensors file in the repo.
   */
  weight_name?: string;
};

/**
 * MareyOutput
 */
export type MareyT2vOutput = {
  video: FileType2;
};

/**
 * MareyInputT2V
 */
export type MareyT2vInput = {
  /**
   * Prompt
   *
   * The prompt to generate a video from
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "5s" | "10s";
  /**
   * Dimensions
   *
   * The dimensions of the generated video in width x height format.
   */
  dimensions?: "1920x1080" | "1152x1152" | "1536x1152" | "1152x1536";
  /**
   * Guidance Scale
   *
   * Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely.
   */
  guidance_scale?: number | unknown;
  /**
   * Seed
   *
   * Seed for random number generation. Use -1 for random seed each run.
   */
  seed?: number | unknown;
  /**
   * Negative Prompt
   *
   * Negative prompt used to guide the model away from undesirable features.
   */
  negative_prompt?: string | unknown;
};

/**
 * AvatarSingleTextResponse
 */
export type InfinitalkSingleTextOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * InfiniTalkSingleTextRequest
 */
export type InfinitalkSingleTextInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: "480p" | "720p";
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Text Input
   *
   * The text input to guide video generation.
   */
  text_input: string;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Voice
   *
   * The voice to use for speech generation
   */
  voice:
    | "Aria"
    | "Roger"
    | "Sarah"
    | "Laura"
    | "Charlie"
    | "George"
    | "Callum"
    | "River"
    | "Liam"
    | "Charlotte"
    | "Alice"
    | "Matilda"
    | "Will"
    | "Jessica"
    | "Eric"
    | "Chris"
    | "Brian"
    | "Daniel"
    | "Lily"
    | "Bill";
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 41 to 721.
   */
  num_frames?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};

/**
 * VideoOutputV5
 */
export type PixverseV5TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequest
 */
export type PixverseV5TextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
   */
  duration?: "5" | "8";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * AvatarsAppOutput
 */
export type AvatarsTextToVideoOutputType2 = {
  video: FileType2;
};

/**
 * InferenceResult
 */
export type AvatarsTextToVideoOutput = {
  /**
   * Moderation Transcription
   */
  moderation_transcription?: string | unknown;
  /**
   * Moderation Error
   */
  moderation_error?: string | unknown;
  /**
   * Moderation Flagged
   */
  moderation_flagged?: boolean;
  video?: Video | unknown;
};

/**
 * Text2VideoInput
 */
export type AvatarsTextToVideoInputType2 = {
  /**
   * Text
   */
  text: string;
  /**
   * Avatar Id
   *
   * The avatar to use for the video
   */
  avatar_id:
    | "emily_vertical_primary"
    | "emily_vertical_secondary"
    | "marcus_vertical_primary"
    | "marcus_vertical_secondary"
    | "mira_vertical_primary"
    | "mira_vertical_secondary"
    | "jasmine_vertical_primary"
    | "jasmine_vertical_secondary"
    | "jasmine_vertical_walking"
    | "aisha_vertical_walking"
    | "elena_vertical_primary"
    | "elena_vertical_secondary"
    | "any_male_vertical_primary"
    | "any_female_vertical_primary"
    | "any_male_vertical_secondary"
    | "any_female_vertical_secondary"
    | "any_female_vertical_walking"
    | "emily_primary"
    | "emily_side"
    | "marcus_primary"
    | "marcus_side"
    | "aisha_walking"
    | "elena_primary"
    | "elena_side"
    | "any_male_primary"
    | "any_female_primary"
    | "any_male_side"
    | "any_female_side";
};

/**
 * TextToVideoRequest
 */
export type AvatarsTextToVideoInput = {
  /**
   * Text
   */
  text: string;
  /**
   * Voice
   */
  voice:
    | "Rachel"
    | "Clyde"
    | "Roger"
    | "Sarah"
    | "Laura"
    | "Thomas"
    | "Charlie"
    | "George"
    | "Callum"
    | "River"
    | "Harry"
    | "Liam"
    | "Alice"
    | "Matilda"
    | "Will"
    | "Jessica"
    | "Lilly"
    | "Bill"
    | "Oxley"
    | "Luna";
  /**
   * Remove Background
   *
   * Enabling the remove background feature will result in a 50% increase in the price.
   */
  remove_background?: boolean;
  /**
   * Avatar
   */
  avatar:
    | "Mia outdoor (UGC)"
    | "Lara (Masterclass)"
    | "Ines (UGC)"
    | "Maria (Masterclass)"
    | "Emma (UGC)"
    | "Sienna (Masterclass)"
    | "Elena (UGC)"
    | "Jasmine (Masterclass)"
    | "Amara (Masterclass)"
    | "Ryan podcast (UGC)"
    | "Tyler (Masterclass)"
    | "Jayse (Masterclass)"
    | "Paul (Masterclass)"
    | "Matteo (UGC)"
    | "Daniel car (UGC)"
    | "Dario (Masterclass)"
    | "Viva (Masterclass)"
    | "Chen (Masterclass)"
    | "Alex (Masterclass)"
    | "Vanessa (UGC)"
    | "Laurent (UGC)"
    | "Noemie car (UGC)"
    | "Brandon (UGC)"
    | "Byron (Masterclass)"
    | "Calista (Masterclass)"
    | "Milo (Masterclass)"
    | "Fabien (Masterclass)"
    | "Rose (UGC)";
};

/**
 * VideoOutput
 *
 * Base output for video generation
 */
export type Wan25PreviewTextToVideoOutput = {
  /**
   * Actual Prompt
   *
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * TextToVideoInput
 *
 * Input for text-to-video generation
 */
export type Wan25PreviewTextToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt for video generation. Supports Chinese and English, max 800 characters.
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the generated video in seconds. Choose between 5 or 10 seconds.
   */
  duration?: "5" | "10";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Resolution
   *
   * Video resolution tier
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Audio Url
   *
   *
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds),
   * the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   *
   */
  audio_url?: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};

/**
 * OviT2VResponse
 */
export type OviOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video?: FileType2 | unknown;
};

/**
 * OviT2VRequest
 */
export type OviInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video in W:H format. One of (512x992, 992x512, 960x512, 512x960, 720x720, or 448x1120).
   */
  resolution?:
    | "512x992"
    | "992x512"
    | "960x512"
    | "512x960"
    | "720x720"
    | "448x1120"
    | "1120x448";
  /**
   * Num Inference Steps
   *
   * The number of inference steps.
   */
  num_inference_steps?: number;
  /**
   * Audio Negative Prompt
   *
   * Negative prompt for audio generation.
   */
  audio_negative_prompt?: string;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
};

/**
 * TextToVideoOutput
 */
export type Sora2TextToVideoOutput = {
  /**
   * Spritesheet
   *
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
  /**
   * Thumbnail
   *
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Video ID
   *
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Video
   *
   * The generated video
   */
  video: VideoFileType2;
};

/**
 * TextToVideoInput
 */
export type Sora2TextToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the generated video in seconds
   */
  duration?: 4 | 8 | 12;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "720p";
  /**
   * Model
   *
   * The model to use for the generation. When the default model is selected, the latest snapshot of the model will be used - otherwise, select a specific snapshot of the model.
   */
  model?: "sora-2" | "sora-2-2025-12-08" | "sora-2-2025-10-06";
  /**
   * Delete Video
   *
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
   */
  delete_video?: boolean;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "9:16" | "16:9";
};

/**
 * ProTextToVideoOutput
 */
export type Sora2TextToVideoProOutput = {
  /**
   * Spritesheet
   *
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
  /**
   * Thumbnail
   *
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Video ID
   *
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Video
   *
   * The generated video
   */
  video: VideoFileType2;
};

/**
 * ProTextToVideoInput
 */
export type Sora2TextToVideoProInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the generated video in seconds
   */
  duration?: 4 | 8 | 12;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Delete Video
   *
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
   */
  delete_video?: boolean;
};

/**
 * Veo31TextToVideoOutput
 */
export type Veo31Output = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo31TextToVideoInput
 */
export type Veo31Input = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * Veo31TextToVideoOutput
 */
export type Veo31FastOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo31TextToVideoInput
 */
export type Veo31FastInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * KandinskyT2VResponse
 */
export type Kandinsky5TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video?: File;
};

/**
 * KandinskyT2VRequest
 */
export type Kandinsky5TextToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768).
   */
  resolution?: "768x512";
  /**
   * Duration
   *
   * The length of the video to generate (5s or 10s)
   */
  duration?: "5s" | "10s";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).
   */
  aspect_ratio?: "3:2" | "1:1" | "2:3";
  /**
   * Num Inference Steps
   *
   * The number of inference steps.
   */
  num_inference_steps?: number;
};

/**
 * KandinskyT2VResponse
 */
export type Kandinsky5TextToVideoDistillOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video?: File;
};

/**
 * KandinskyT2VDistillRequest
 */
export type Kandinsky5TextToVideoDistillInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Duration
   *
   * The length of the video to generate (5s or 10s)
   */
  duration?: "5s" | "10s";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).
   */
  aspect_ratio?: "3:2" | "1:1" | "2:3";
  /**
   * Resolution
   *
   * Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768).
   */
  resolution?: "768x512";
};

/**
 * WanAlphaResponse
 */
export type WanAlphaOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Image
   *
   * The generated image file.
   */
  image?: VideoFileType2;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Mask
   *
   * The generated mask file.
   */
  mask?: VideoFileType2;
  /**
   * Video
   *
   * The generated video file.
   */
  video?: VideoFileType2;
};

/**
 * WanAlphaRequest
 */
export type WanAlphaInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * Shift
   *
   * The shift of the generated video.
   */
  shift?: number;
  /**
   * Mask Clamp Upper
   *
   * The upper bound of the mask clamping.
   */
  mask_clamp_upper?: number;
  /**
   * FPS
   *
   * The frame rate of the generated video.
   */
  fps?: number;
  /**
   * Mask Clamp Lower
   *
   * The lower bound of the mask clamping.
   */
  mask_clamp_lower?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Mask Binarization Threshold
   *
   * The threshold for mask binarization. When binarize_mask is True, this threshold will be used to binarize the mask. This will also be used for transparency when the output type is `.webm`.
   */
  mask_binarization_threshold?: number;
  /**
   * Sampler
   *
   * The sampler to use.
   */
  sampler?: "unipc" | "dpm++" | "euler";
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "240p" | "360p" | "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Binarize Mask
   *
   * Whether to binarize the mask.
   */
  binarize_mask?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
};

/**
 * VideoToVideoOutput
 */
export type KreaWan14bTextToVideoOutput = {
  video: FileType2;
};

/**
 * TextToVideoInput
 */
export type KreaWan14bTextToVideoInput = {
  /**
   * Prompt
   *
   * Prompt for the video-to-video generation.
   */
  prompt: string;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be a multiple of 12 plus 6, for example 6, 18, 30, 42, etc.
   */
  num_frames?: number;
  /**
   * Seed
   *
   * Seed for the video-to-video generation.
   */
  seed?: number | unknown;
};

/**
 * Q2TextToVideoOutput
 */
export type ViduQ2TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video from text using the Q2 model
   */
  video: File;
};

/**
 * Q2TextToVideoRequest
 */
export type ViduQ2TextToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 3000 characters
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the output video
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Resolution
   *
   * Output video resolution
   */
  resolution?: "360p" | "520p" | "720p" | "1080p";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: 2 | 3 | 4 | 5 | 6 | 7 | 8;
  /**
   * Bgm
   *
   * Whether to add background music to the video (only for 4-second videos)
   */
  bgm?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};

/**
 * SeedanceFastT2VVideoOutput
 */
export type BytedanceSeedanceV1ProFastTextToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SeedanceProFastTextToVideoInput
 */
export type BytedanceSeedanceV1ProFastTextToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Seed
   *
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
  /**
   * Camera Fixed
   *
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
};

/**
 * ProTextToVideoHailuo23Output
 */
export type MinimaxHailuo23ProTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ProTextToVideoHailuo23Input
 */
export type MinimaxHailuo23ProTextToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
};

/**
 * StandardTextToVideoHailuo23Output
 */
export type MinimaxHailuo23StandardTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * StandardTextToVideoHailuo23Input
 */
export type MinimaxHailuo23StandardTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the video in seconds.
   */
  duration?: "6" | "10";
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
};

/**
 * LongCatVideoResponse
 */
export type LongcatVideoDistilledTextToVideo480pOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LongCatVideoRequest
 */
export type LongcatVideoDistilledTextToVideo480pInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * FPS
   *
   * The frame rate of the generated video.
   */
  fps?: number;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * LongCatVideoResponse
 */
export type LongcatVideoDistilledTextToVideo720pOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LongCat720PVideoRequest
 */
export type LongcatVideoDistilledTextToVideo720pInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * FPS
   *
   * The frame rate of the generated video.
   */
  fps?: number;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Number of Refinement Inference Steps
   *
   * The number of inference steps to use for refinement.
   */
  num_refine_inference_steps?: number;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * LongCatVideoResponse
 */
export type LongcatVideoTextToVideo480pOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LongCatCFGVideoRequest
 */
export type LongcatVideoTextToVideo480pInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * The acceleration level to use for the video generation.
   */
  acceleration?: "none" | "regular";
  /**
   * FPS
   *
   * The frame rate of the generated video.
   */
  fps?: number;
  /**
   * Guidance Scale
   *
   * The guidance scale to use for the video generation.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to use for the video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number;
};

/**
 * LongCatVideoResponse
 */
export type LongcatVideoTextToVideo720pOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LongCat720PCFGVideoRequest
 */
export type LongcatVideoTextToVideo720pInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * The acceleration level to use for the video generation.
   */
  acceleration?: "none" | "regular";
  /**
   * FPS
   *
   * The frame rate of the generated video.
   */
  fps?: number;
  /**
   * Number of Refinement Inference Steps
   *
   * The number of inference steps to use for refinement.
   */
  num_refine_inference_steps?: number;
  /**
   * Guidance Scale
   *
   * The guidance scale to use for the video generation.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to use for the video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
};

/**
 * SanaVideoOutput
 */
export type SanaVideoOutput = {
  /**
   * Seed
   *
   * The random seed used for the generation process
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SanaVideoInput
 */
export type SanaVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video to generate
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the output video
   */
  resolution?: "480p";
  /**
   * Fps
   *
   * Frames per second for the output video
   */
  fps?: number;
  /**
   * Motion Score
   *
   * Motion intensity score (higher = more motion)
   */
  motion_score?: number;
  /**
   * Guidance Scale
   *
   * Guidance scale for generation (higher = more prompt adherence)
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * Number of denoising steps
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * Random seed for reproducible generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt describing what to avoid in the generation
   */
  negative_prompt?: string;
  /**
   * Num Frames
   *
   * Number of frames to generate
   */
  num_frames?: number;
};

/**
 * GenerationOutput
 *
 * Output model for text-to-video generation
 */
export type InfinityStarTextToVideoOutput = {
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * GenerationInput
 *
 * Input model for text-to-video generation
 */
export type InfinityStarTextToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for generating the video
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated output
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * Enhance Prompt
   *
   * Whether to use an LLM to enhance the prompt.
   */
  enhance_prompt?: boolean;
  /**
   * Use Apg
   *
   * Whether to use APG
   */
  use_apg?: boolean;
  /**
   * Guidance Scale
   *
   * Guidance scale for generation
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. Leave empty for random generation.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to guide what to avoid in generation
   */
  negative_prompt?: string;
  /**
   * Tau Video
   *
   * Tau value for video scale
   */
  tau_video?: number;
};

/**
 * HunyuanVideo15Response
 */
export type HunyuanVideoV15TextToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * HunyuanVideo15T2VRequest
 */
export type HunyuanVideoV15TextToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video.
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video.
   */
  resolution?: "480p";
  /**
   * Enable Prompt Expansion
   *
   * Enable prompt expansion to enhance the input prompt.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility.
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to guide what not to generate.
   */
  negative_prompt?: string;
  /**
   * Num Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
};

/**
 * LTXVTextToVideoResponse
 */
export type Ltx2TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * LTXVTextToVideoRequest
 */
export type Ltx2TextToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "1080p" | "1440p" | "2160p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the generated video
   */
  generate_audio?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: 6 | 8 | 10;
  /**
   * Frames per Second
   *
   * The frames per second of the generated video
   */
  fps?: 25 | 50;
};

/**
 * LTXVTextToVideoResponse
 */
export type Ltx2TextToVideoFastOutput = {
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * LTXVTextToVideoFastRequest
 */
export type Ltx2TextToVideoFastInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "1080p" | "1440p" | "2160p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the generated video
   */
  generate_audio?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video in seconds. The fast model supports 6-20 seconds. Note: Durations longer than 10 seconds (12, 14, 16, 18, 20) are only supported with 25 FPS and 1080p resolution.
   */
  duration?: 6 | 8 | 10 | 12 | 14 | 16 | 18 | 20;
  /**
   * Frames per Second
   *
   * The frames per second of the generated video
   */
  fps?: 25 | 50;
};

/**
 * VideoOutputV5_5
 */
export type PixverseV55TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequestV5_5
 */
export type PixverseV55TextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Thinking Type
   *
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * Generate Multi Clip Switch
   *
   * Enable multi-clip generation with dynamic camera changes
   */
  generate_multi_clip_switch?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds
   */
  duration?: "5" | "8" | "10";
  /**
   * Generate Audio Switch
   *
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * TextToVideoV26ProOutput
 */
export type KlingVideoV26ProTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoV26ProRequest
 */
export type KlingVideoV26ProTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Generate Audio
   *
   * Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.
   */
  generate_audio?: boolean;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * FabricOneTextOutput
 */
export type Fabric10TextOutput = {
  video: FileType2;
};

/**
 * FabricOneTextInput
 */
export type Fabric10TextInput = {
  /**
   * Text
   */
  text: string;
  /**
   * Resolution
   *
   * Resolution
   */
  resolution: "720p" | "480p";
  /**
   * Voice Description
   *
   * Optional additional voice description. The primary voice description is auto-generated from the image. You can use simple descriptors like 'British accent' or 'Confident' or provide a detailed description like 'Confident male voice, mid-20s, with notes of...'
   */
  voice_description?: string | unknown;
  /**
   * Image Url
   */
  image_url: string;
};

/**
 * TextToVideoOutput
 *
 * Output for text-to-video generation
 */
export type V26TextToVideoOutput = {
  /**
   * Actual Prompt
   *
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * TextToVideoInput
 *
 * Input for Wan 2.6 text-to-video generation
 */
export type V26TextToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt for video generation. Supports Chinese and English, max 800 characters. For multi-shot videos, use format: 'Overall description. First shot [0-3s] content. Second shot [3-5s] content.'
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the generated video in seconds. Choose between 5, 10, or 15 seconds.
   */
  duration?: "5" | "10" | "15";
  /**
   * Resolution
   *
   * Video resolution tier. Wan 2.6 T2V only supports 720p and 1080p (no 480p).
   */
  resolution?: "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video. Wan 2.6 supports additional ratios.
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:3" | "3:4";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Audio Url
   *
   *
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds),
   * the audio is truncated to the first N seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   *
   */
  audio_url?: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Multi Shots
   *
   * When true, enables intelligent multi-shot segmentation for coherent narrative videos. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.
   */
  multi_shots?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};

/**
 * SeedanceProv15T2VVideoOutput
 */
export type BytedanceSeedanceV15ProTextToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SeedanceProv15TextToVideoInput
 */
export type BytedanceSeedanceV15ProTextToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video
   */
  generate_audio?: boolean;
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera Fixed
   *
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Seed
   *
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
};

/**
 * KandinskyT2VResponse
 */
export type Kandinsky5ProTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video?: File;
};

/**
 * KandinskyT2VRequest
 */
export type Kandinsky5ProTextToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution: 512p or 1024p.
   */
  resolution?: "512P" | "1024P";
  /**
   * Acceleration
   *
   * Acceleration level for faster generation.
   */
  acceleration?: "none" | "regular";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).
   */
  aspect_ratio?: "3:2" | "1:1" | "2:3";
  /**
   * Num Inference Steps
   *
   * The number of inference steps.
   */
  num_inference_steps?: number;
  /**
   * Duration
   *
   * The length of the video to generate (5s or 10s)
   */
  duration?: "5s";
};

/**
 * LTX2TextToVideoOutput
 */
export type Ltx219bTextToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2TextToVideoInput
 */
export type Ltx219bTextToVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
};

/**
 * LTX2TextToVideoOutput
 */
export type Ltx219bTextToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRATextToVideoInput
 */
export type Ltx219bTextToVideoLoraInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
};

/**
 * LTX2TextToVideoOutput
 */
export type Ltx219bDistilledTextToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2DistilledTextToVideoInput
 */
export type Ltx219bDistilledTextToVideoInput = {
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
};

/**
 * LTX2TextToVideoOutput
 */
export type Ltx219bDistilledTextToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRADistilledTextToVideoInput
 */
export type Ltx219bDistilledTextToVideoLoraInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
};

/**
 * VideoOutputV5_5
 */
export type PixverseV56TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoRequestV5_6
 */
export type PixverseV56TextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Thinking Type
   *
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds
   */
  duration?: "5" | "8" | "10";
  /**
   * Generate Audio Switch
   *
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * XAITextToVideoOutput
 */
export type GrokImagineVideoTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: VideoFileType2;
};

/**
 * XAITextToVideoInput
 */
export type GrokImagineVideoTextToVideoInput = {
  /**
   * Prompt
   *
   * Text description of the desired video.
   */
  prompt: string;
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: number;
  /**
   * Resolution
   *
   * Resolution of the output video.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "4:3" | "3:2" | "1:1" | "2:3" | "3:4" | "9:16";
};

/**
 * Q3TextToVideoOutput
 */
export type ViduQ3TextToVideoOutput = {
  /**
   * Video
   *
   * The generated video from text using the Q3 model
   */
  video: File;
};

/**
 * Q3TextToVideoRequest
 */
export type ViduQ3TextToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 2000 characters
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the output video
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "1:1";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: number;
  /**
   * Resolution
   *
   * Output video resolution
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Audio
   *
   * Whether to use direct audio-video generation. When true, outputs video with sound.
   */
  audio?: boolean;
};

/**
 * TextToVideoV2MasterOutput
 */
export type KlingVideoV2MasterTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoV2MasterRequest
 */
export type KlingVideoV2MasterTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * Veo3TextToVideoOutput
 */
export type Veo3Output = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo3TextToVideoInput
 */
export type Veo3Input = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p";
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * TextToVideoHailuo02Output
 */
export type MinimaxHailuo02StandardTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * StandardTextToVideoHailuo02Input
 */
export type MinimaxHailuo02StandardTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution.
   */
  duration?: "6" | "10";
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
};

/**
 * Veo3TextToVideoOutput
 */
export type Veo3FastOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo3TextToVideoInput
 */
export type Veo3FastInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p";
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * TextToVideoV25ProOutput
 */
export type KlingVideoV25TurboProTextToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TextToVideoV25ProRequest
 */
export type KlingVideoV25TurboProTextToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * FastSVDOutput
 */
export type FastSvdLcmOutput = {
  /**
   * Seed
   *
   *
   * Seed of the generated Image. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   *
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * FastSVDImageInput
 */
export type FastSvdLcmInput = {
  /**
   * Motion Bucket Id
   *
   *
   * The motion bucket id determines the motion of the generated video. The
   * higher the number, the more motion there will be.
   *
   */
  motion_bucket_id?: number;
  /**
   * Fps
   *
   *
   * The FPS of the generated video. The higher the number, the faster the video will
   * play. Total video length is 25 frames.
   *
   */
  fps?: number;
  /**
   * Steps
   *
   *
   * The number of steps to run the model for. The higher the number the better
   * the quality and longer it will take to generate.
   *
   */
  steps?: number;
  /**
   * Cond Aug
   *
   *
   * The conditoning augmentation determines the amount of noise that will be
   * added to the conditioning frame. The higher the number, the more noise
   * there will be, and the less the video will look like the initial image.
   * Increase it for more motion.
   *
   */
  cond_aug?: number;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number;
  /**
   * Image Url
   *
   * The URL of the image to use as a starting point for the generation.
   */
  image_url: string;
};

/**
 * SadTalkerOutput
 */
export type SadtalkerOutput = {
  /**
   * Video
   *
   * URL of the generated video
   */
  video: File;
};

/**
 * SadTalkerInput
 */
export type SadtalkerInput = {
  /**
   * Pose Style
   *
   * The style of the pose
   */
  pose_style?: number;
  /**
   * Source Image Url
   *
   * URL of the source image
   */
  source_image_url: string;
  /**
   * Driven Audio Url
   *
   * URL of the driven audio
   */
  driven_audio_url: string;
  /**
   * Face Enhancer
   *
   * The type of face enhancer to use
   */
  face_enhancer?: "gfpgan";
  /**
   * Expression Scale
   *
   * The scale of the expression
   */
  expression_scale?: number;
  /**
   * Face Model Resolution
   *
   * The resolution of the face model
   */
  face_model_resolution?: "256" | "512";
  /**
   * Still Mode
   *
   * Whether to use still mode. Fewer head motion, works with preprocess `full`.
   */
  still_mode?: boolean;
  /**
   * Preprocess
   *
   * The type of preprocessing to use
   */
  preprocess?: "crop" | "extcrop" | "resize" | "full" | "extfull";
};

/**
 * MuseTalkOutput
 */
export type MusetalkOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MuseTalkInput
 */
export type MusetalkInput = {
  /**
   * Source Video Url
   *
   * URL of the source video
   */
  source_video_url: string;
  /**
   * Audio Url
   *
   * URL of the audio
   */
  audio_url: string;
};

/**
 * LivePortraitOutput
 */
export type LivePortraitOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LivePortraitInput
 */
export type LivePortraitInput = {
  /**
   * Smile
   *
   * Amount to smile
   */
  smile?: number;
  /**
   * Video Url
   *
   * URL of the video to drive the lip syncing.
   */
  video_url: string;
  /**
   * Eyebrow
   *
   * Amount to raise or lower eyebrows
   */
  eyebrow?: number;
  /**
   * Flag Stitching
   *
   * Whether to enable stitching. Recommended to set to True.
   */
  flag_stitching?: boolean;
  /**
   * Wink
   *
   * Amount to wink
   */
  wink?: number;
  /**
   * Rotate Pitch
   *
   * Amount to rotate the face in pitch
   */
  rotate_pitch?: number;
  /**
   * Blink
   *
   * Amount to blink the eyes
   */
  blink?: number;
  /**
   * Scale
   *
   * Scaling factor for the face crop.
   */
  scale?: number;
  /**
   * Eee
   *
   * Amount to shape mouth in 'eee' position
   */
  eee?: number;
  /**
   * Flag Pasteback
   *
   * Whether to paste-back/stitch the animated face cropping from the face-cropping space to the original image space.
   */
  flag_pasteback?: boolean;
  /**
   * Pupil Y
   *
   * Amount to move pupils vertically
   */
  pupil_y?: number;
  /**
   * Rotate Yaw
   *
   * Amount to rotate the face in yaw
   */
  rotate_yaw?: number;
  /**
   * Flag Do Rot
   *
   * Whether to conduct the rotation when flag_do_crop is True.
   */
  flag_do_rot?: boolean;
  /**
   * Woo
   *
   * Amount to shape mouth in 'woo' position
   */
  woo?: number;
  /**
   * Aaa
   *
   * Amount to open mouth in 'aaa' shape
   */
  aaa?: number;
  /**
   * Image Url
   *
   * URL of the image to be animated
   */
  image_url: string;
  /**
   * Flag Relative
   *
   * Whether to use relative motion.
   */
  flag_relative?: boolean;
  /**
   * Flag Eye Retargeting
   *
   * Whether to enable eye retargeting.
   */
  flag_eye_retargeting?: boolean;
  /**
   * Flag Lip Zero
   *
   * Whether to set the lip to closed state before animation. Only takes effect when flag_eye_retargeting and flag_lip_retargeting are False.
   */
  flag_lip_zero?: boolean;
  /**
   * Batch Size
   *
   * Batch size for the model. The larger the batch size, the faster the model will run, but the more memory it will consume.
   */
  batch_size?: number;
  /**
   * Rotate Roll
   *
   * Amount to rotate the face in roll
   */
  rotate_roll?: number;
  /**
   * Pupil X
   *
   * Amount to move pupils horizontally
   */
  pupil_x?: number;
  /**
   * Vy Ratio
   *
   * Vertical offset ratio for face crop. Positive values move up, negative values move down.
   */
  vy_ratio?: number;
  /**
   * Dsize
   *
   * Size of the output image.
   */
  dsize?: number;
  /**
   * Enable Safety Checker
   *
   *
   * Whether to enable the safety checker. If enabled, the model will check if the input image contains a face before processing it.
   * The safety checker will process the input image
   *
   */
  enable_safety_checker?: boolean;
  /**
   * Vx Ratio
   *
   * Horizontal offset ratio for face crop.
   */
  vx_ratio?: number;
  /**
   * Flag Lip Retargeting
   *
   * Whether to enable lip retargeting.
   */
  flag_lip_retargeting?: boolean;
  /**
   * Flag Do Crop
   *
   * Whether to crop the source portrait to the face-cropping space.
   */
  flag_do_crop?: boolean;
};

/**
 * Frame
 */
export type Frame = {
  /**
   * URL
   *
   * URL of the frame
   */
  url: string;
};

/**
 * AMTInterpolationOutput
 */
export type AmtInterpolationFrameInterpolationOutput = {
  /**
   * Video
   *
   * Generated video
   */
  video: File;
};

/**
 * AMTFrameInterpolationInput
 */
export type AmtInterpolationFrameInterpolationInput = {
  /**
   * Frames
   *
   * Frames to interpolate
   */
  frames: Array<Frame>;
  /**
   * Recursive Interpolation Passes
   *
   * Number of recursive interpolation passes
   */
  recursive_interpolation_passes?: number;
  /**
   * Output FPS
   *
   * Output frames per second
   */
  output_fps?: number;
};

/**
 * VideoOutput
 */
export type StableVideoOutput = {
  /**
   * Seed
   *
   * Seed for random number generator
   */
  seed: number;
  /**
   * Video
   *
   * Generated video
   */
  video: File;
};

/**
 * ImageInput
 */
export type StableVideoInput = {
  /**
   * Motion Bucket Id
   *
   *
   * The motion bucket id determines the motion of the generated video. The
   * higher the number, the more motion there will be.
   *
   */
  motion_bucket_id?: number;
  /**
   * Fps
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Cond Aug
   *
   *
   * The conditoning augmentation determines the amount of noise that will be
   * added to the conditioning frame. The higher the number, the more noise
   * there will be, and the less the video will look like the initial image.
   * Increase it for more motion.
   *
   */
  cond_aug?: number;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of Stable Diffusion
   * will output the same image every time.
   *
   */
  seed?: number;
  /**
   * Image Url
   *
   * The URL of the image to use as a starting point for the generation.
   */
  image_url: string;
};

/**
 * KlingV1I2VOutput
 */
export type KlingVideoV1StandardImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * V1ImageToVideoRequest
 */
export type KlingVideoV1StandardImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt for the video
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * URL of the image to be used for the video
   */
  image_url: string;
  /**
   * Static Mask Url
   *
   * URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)
   */
  static_mask_url?: string;
  /**
   * Dynamic Masks
   *
   * List of dynamic masks
   */
  dynamic_masks?: Array<DynamicMask>;
  /**
   * Tail Image Url
   *
   * URL of the image to be used for the end of the video
   */
  tail_image_url?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * Trajectory
 */
export type Trajectory = {
  /**
   * Y
   *
   * Y coordinate of the motion trajectory
   */
  y: number;
  /**
   * X
   *
   * X coordinate of the motion trajectory
   */
  x: number;
};

/**
 * DynamicMask
 */
export type DynamicMask = {
  /**
   * Trajectories
   *
   * List of trajectories
   */
  trajectories?: Array<Trajectory>;
  /**
   * Mask Url
   *
   * URL of the image for Dynamic Brush Application Area (Mask image created by users using the motion brush)
   */
  mask_url: string;
};

/**
 * I2VOutput
 */
export type KlingVideoV15ProImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * KlingV15ProImageToVideoRequest
 */
export type KlingVideoV15ProImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Image Url
   */
  image_url: string;
  /**
   * Static Mask Url
   *
   * URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)
   */
  static_mask_url?: string;
  /**
   * Dynamic Masks
   *
   * List of dynamic masks
   */
  dynamic_masks?: Array<DynamicMask>;
  /**
   * Tail Image Url
   *
   * URL of the image to be used for the end of the video
   */
  tail_image_url?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * Output
 */
export type Cogvideox5bImageToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generating the video.
   */
  prompt: string;
  /**
   * Timings
   */
  timings: {
    [key: string]: number;
  };
  /**
   * Seed
   *
   *
   * Seed of the generated video. It will be the same value of the one passed in the
   * input or the randomly generated that was used in case none was passed.
   *
   */
  seed: number;
  /**
   * Video
   *
   * The URL to the generated video
   */
  video: File;
};

/**
 * ImageToVideoInput
 */
export type Cogvideox5bImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Use Rife
   *
   * Use RIFE for video interpolation
   */
  use_rife?: boolean;
  /**
   * Image URL
   *
   * The URL to the image to generate the video from.
   */
  image_url: string;
  /**
   * Loras
   *
   *
   * The LoRAs to use for the image generation. We currently support one lora.
   *
   */
  loras?: Array<LoraWeight>;
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance scale (CFG)
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt when looking for a related video to show you.
   *
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to perform.
   */
  num_inference_steps?: number;
  /**
   * Export Fps
   *
   * The target FPS of the video
   */
  export_fps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate video from
   */
  negative_prompt?: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
};

/**
 * Output
 */
export type LtxVideoImageToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for random number generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * ImageToVideoInput
 */
export type LtxVideoImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * The seed to use for random number generation.
   */
  seed?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to take.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Image URL
   *
   * The URL of the image to generate the video from.
   */
  image_url: string;
};

/**
 * I2VLiveOutput
 */
export type MinimaxVideo01LiveImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequest
 */
export type MinimaxVideo01LiveImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * SadTalkerOutput
 */
export type SadtalkerReferenceOutput = {
  /**
   * Video
   *
   * URL of the generated video
   */
  video: File;
};

/**
 * SadTalkerRefVideoInput
 */
export type SadtalkerReferenceInput = {
  /**
   * Pose Style
   *
   * The style of the pose
   */
  pose_style?: number;
  /**
   * Source Image Url
   *
   * URL of the source image
   */
  source_image_url: string;
  /**
   * Reference Pose Video Url
   *
   * URL of the reference video
   */
  reference_pose_video_url: string;
  /**
   * Driven Audio Url
   *
   * URL of the driven audio
   */
  driven_audio_url: string;
  /**
   * Face Enhancer
   *
   * The type of face enhancer to use
   */
  face_enhancer?: "gfpgan";
  /**
   * Expression Scale
   *
   * The scale of the expression
   */
  expression_scale?: number;
  /**
   * Face Model Resolution
   *
   * The resolution of the face model
   */
  face_model_resolution?: "256" | "512";
  /**
   * Still Mode
   *
   * Whether to use still mode. Fewer head motion, works with preprocess `full`.
   */
  still_mode?: boolean;
  /**
   * Preprocess
   *
   * The type of preprocessing to use
   */
  preprocess?: "crop" | "extcrop" | "resize" | "full" | "extfull";
};

/**
 * I2VOutput
 */
export type KlingVideoV16StandardImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequest
 */
export type KlingVideoV16StandardImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Image Url
   */
  image_url: string;
};

/**
 * SubjectReferenceOutput
 */
export type MinimaxVideo01SubjectReferenceOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * SubjectReferenceRequest
 */
export type MinimaxVideo01SubjectReferenceInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Subject Reference Image Url
   *
   * URL of the subject reference image to use for consistent subject appearance
   */
  subject_reference_image_url: string;
};

/**
 * I2VOutput
 */
export type PixverseV35ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequest
 */
export type PixverseV35ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
   */
  duration?: "5" | "8";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * I2VOutput
 */
export type PixverseV35ImageToVideoFastOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * FastImageToVideoRequest
 */
export type PixverseV35ImageToVideoFastInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * Output
 */
export type HunyuanVideoImg2VidLoraOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * Input
 */
export type HunyuanVideoImg2VidLoraInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * Image URL
   *
   * The URL to the image to generate the video from. The image must be 960x544 or it will get cropped and resized to that size.
   */
  image_url: string;
};

/**
 * Ray2I2VOutput
 */
export type LumaDreamMachineRay2ImageToVideoOutput = {
  /**
   * Video
   *
   * URL of the generated video
   */
  video: File;
};

/**
 * Ray2ImageToVideoRequest
 */
export type LumaDreamMachineRay2ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Resolution
   *
   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * Loop
   *
   * Whether the video should loop (end of video is blended with the beginning)
   */
  loop?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video
   */
  duration?: "5s" | "9s";
  /**
   * Image Url
   *
   * Initial image to start the video from. Can be used together with end_image_url.
   */
  image_url?: string;
  /**
   * End Image Url
   *
   * Final image to end the video with. Can be used together with image_url.
   */
  end_image_url?: string;
};

/**
 * SkyreelsI2VResponse
 */
export type SkyreelsI2vOutput = {
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   */
  video: File;
};

/**
 * SkyreelsI2VRequest
 */
export type SkyreelsI2vInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the output video
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Image Url
   *
   * URL of the image input.
   */
  image_url: string;
  /**
   * Guidance Scale
   *
   * Guidance scale for generation (between 1.0 and 20.0)
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * Random seed for generation. If not provided, a random seed will be used.
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * Number of denoising steps (between 1 and 50). Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to guide generation away from certain attributes.
   */
  negative_prompt?: string;
};

/**
 * I2VDirectorOutput
 */
export type MinimaxVideo01DirectorImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoDirectorRequest
 */
export type MinimaxVideo01DirectorImageToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * HunyuanI2VResponse
 */
export type HunyuanVideoImageToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  video: File;
};

/**
 * HunyuanVideoRequest
 */
export type HunyuanVideoImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video to generate.
   */
  resolution?: "720p";
  /**
   * Image Url
   *
   * URL of the image input.
   */
  image_url: string;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: "129";
  /**
   * I2V Stability
   *
   * Turning on I2V Stability reduces hallucination but also reduces motion.
   */
  i2v_stability?: boolean;
};

/**
 * WanI2VResponse
 */
export type WanI2vLoraOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanLoRAI2VRequest
 */
export type WanI2vLoraInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Reverse Video
   *
   * If true, the video will be reversed.
   */
  reverse_video?: boolean;
  /**
   * Loras
   *
   * LoRA weights to be used in the inference.
   */
  loras?: Array<LoraWeightType2>;
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number;
  /**
   * Turbo Mode
   *
   * If true, the video will be generated faster with no noticeable degradation in the visual quality.
   */
  turbo_mode?: boolean;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the output video.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: "480p" | "720p";
  /**
   * Image Url
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Guide Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guide_scale?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
};

/**
 * TemplateToVideoOutput
 */
export type ViduTemplateToVideoOutput = {
  /**
   * Video
   *
   * The generated video using a predefined template
   */
  video: File;
};

/**
 * TemplateToVideoRequest
 */
export type ViduTemplateToVideoInput = {
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the output video
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Template
   *
   * AI video template to use. Pricing varies by template: Standard templates (hug, kiss, love_pose, etc.) cost 4 credits ($0.20), Premium templates (lunar_newyear, dynasty_dress, dreamy_wedding, etc.) cost 6 credits ($0.30), and Advanced templates (live_photo) cost 10 credits ($0.50).
   */
  template?:
    | "dreamy_wedding"
    | "romantic_lift"
    | "sweet_proposal"
    | "couple_arrival"
    | "cupid_arrow"
    | "pet_lovers"
    | "lunar_newyear"
    | "hug"
    | "kiss"
    | "dynasty_dress"
    | "wish_sender"
    | "love_pose"
    | "hair_swap"
    | "youth_rewind"
    | "morphlab"
    | "live_photo"
    | "emotionlab"
    | "live_memory"
    | "interaction"
    | "christmas"
    | "pet_finger"
    | "eat_mushrooms"
    | "beast_chase_library"
    | "beast_chase_supermarket"
    | "petal_scattered"
    | "emoji_figure"
    | "hair_color_change"
    | "multiple_people_kissing"
    | "beast_chase_amazon"
    | "beast_chase_mountain"
    | "balloonman_explodes_pro"
    | "get_thinner"
    | "jump2pool"
    | "bodyshake"
    | "jiggle_up"
    | "shake_it_dance"
    | "subject_3"
    | "pubg_winner_hit"
    | "shake_it_down"
    | "blueprint_supreme"
    | "hip_twist"
    | "motor_dance"
    | "rat_dance"
    | "kwok_dance"
    | "leg_sweep_dance"
    | "heeseung_march"
    | "shake_to_max"
    | "dame_un_grrr"
    | "i_know"
    | "lit_bounce"
    | "wave_dance"
    | "chill_dance"
    | "hip_flicking"
    | "sakura_season"
    | "zongzi_wrap"
    | "zongzi_drop"
    | "dragonboat_shot"
    | "rain_kiss"
    | "child_memory"
    | "couple_drop"
    | "couple_walk"
    | "flower_receive"
    | "love_drop"
    | "cheek_kiss"
    | "carry_me"
    | "blow_kiss"
    | "love_fall"
    | "french_kiss_8s"
    | "workday_feels"
    | "love_story"
    | "bloom_magic"
    | "ghibli"
    | "minecraft"
    | "box_me"
    | "claw_me"
    | "clayshot"
    | "manga_meme"
    | "quad_meme"
    | "pixel_me"
    | "clayshot_duo"
    | "irasutoya"
    | "american_comic"
    | "simpsons_comic"
    | "yayoi_kusama_style"
    | "pop_art"
    | "jojo_style"
    | "slice_therapy"
    | "balloon_flyaway"
    | "flying"
    | "paperman"
    | "pinch"
    | "bloom_doorobear"
    | "gender_swap"
    | "nap_me"
    | "sexy_me"
    | "spin360"
    | "smooth_shift"
    | "paper_fall"
    | "jump_to_cloud"
    | "pilot"
    | "sweet_dreams"
    | "soul_depart"
    | "punch_hit"
    | "watermelon_hit"
    | "split_stance_pet"
    | "make_face"
    | "break_glass"
    | "split_stance_human"
    | "covered_liquid_metal"
    | "fluffy_plunge"
    | "pet_belly_dance"
    | "water_float"
    | "relax_cut"
    | "head_to_balloon"
    | "cloning"
    | "across_the_universe_jungle"
    | "clothes_spinning_remnant"
    | "across_the_universe_jurassic"
    | "across_the_universe_moon"
    | "fisheye_pet"
    | "hitchcock_zoom"
    | "cute_bangs"
    | "earth_zoom_out"
    | "fisheye_human"
    | "drive_yacht"
    | "virtual_singer"
    | "earth_zoom_in"
    | "aliens_coming"
    | "drive_ferrari"
    | "bjd_style"
    | "virtual_fitting"
    | "orbit"
    | "zoom_in"
    | "ai_outfit"
    | "spin180"
    | "orbit_dolly"
    | "orbit_dolly_fast"
    | "auto_spin"
    | "walk_forward"
    | "outfit_show"
    | "zoom_in_fast"
    | "zoom_out_image"
    | "zoom_out_startend"
    | "muscling"
    | "captain_america"
    | "hulk"
    | "cap_walk"
    | "hulk_dive"
    | "exotic_princess"
    | "beast_companion"
    | "cartoon_doll"
    | "golden_epoch"
    | "oscar_gala"
    | "fashion_stride"
    | "star_carpet"
    | "flame_carpet"
    | "frost_carpet"
    | "mecha_x"
    | "style_me"
    | "tap_me"
    | "saber_warrior"
    | "pet2human"
    | "graduation"
    | "fishermen"
    | "happy_birthday"
    | "fairy_me"
    | "ladudu_me"
    | "ladudu_me_random"
    | "squid_game"
    | "superman"
    | "grow_wings"
    | "clevage"
    | "fly_with_doraemon"
    | "creatice_product_down"
    | "pole_dance"
    | "hug_from_behind"
    | "creatice_product_up_cybercity"
    | "creatice_product_up_bluecircuit"
    | "creatice_product_up"
    | "run_fast"
    | "background_explosion";
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Input Image Urls
   *
   * URLs of the images to use with the template. Number of images required varies by template: 'dynasty_dress' and 'shop_frame' accept 1-2 images, 'wish_sender' requires exactly 3 images, all other templates accept only 1 image.
   */
  input_image_urls: Array<string>;
};

/**
 * StartEndToVideoOutput
 */
export type ViduStartEndToVideoOutput = {
  /**
   * Video
   *
   * The generated transition video between start and end frames
   */
  video: File;
};

/**
 * StartEndToVideoRequest
 */
export type ViduStartEndToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * End Image Url
   *
   * URL of the image to use as the last frame
   */
  end_image_url: string;
  /**
   * Start Image Url
   *
   * URL of the image to use as the first frame
   */
  start_image_url: string;
};

/**
 * ReferenceToVideoOutput
 */
export type ViduReferenceToVideoOutput = {
  /**
   * Video
   *
   * The generated video with consistent subjects from reference images
   */
  video: File;
};

/**
 * ReferenceToVideoRequest
 */
export type ViduReferenceToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the output video
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Reference Image Urls
   *
   * URLs of the reference images to use for consistent subject appearance
   */
  reference_image_urls: Array<string>;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};

/**
 * VideoOutput
 */
export type ViduImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequest
 */
export type ViduImageToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * TurboImageToVideoOutput
 *
 * Output model for all video generation endpoints
 */
export type PikaV2TurboImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoTurboInput
 *
 * Base request for image-to-video generation
 */
export type PikaV2TurboImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: number;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the model
   */
  negative_prompt?: string;
  /**
   * Image Url
   */
  image_url: string;
};

/**
 * Pika22PikascenesOutput
 *
 * Output model for Pika 2.2 Pikascenes generation
 */
export type PikaV22PikascenesOutput = {
  /**
   * Video
   *
   * The generated video combining multiple images
   */
  video: File;
};

/**
 * Pika22PikascenesRequest
 *
 * Request model for Pika 2.2 Pikascenes (collection-to-video) generation
 */
export type PikaV22PikascenesInput = {
  /**
   * Prompt
   *
   * Text prompt describing the desired video
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1" | "4:5" | "5:4" | "3:2" | "2:3";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: 5 | 10;
  /**
   * Ingredients Mode
   *
   * Mode for integrating multiple images. Precise mode is more accurate, creative mode is more creative.
   */
  ingredients_mode?: "precise" | "creative";
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Image Urls
   *
   * URLs of images to combine into a video
   */
  image_urls: Array<string>;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the model
   */
  negative_prompt?: string;
};

/**
 * Pika22ImageToVideoOutput
 *
 * Output model for Pika 2.2 image-to-video generation
 */
export type PikaV22ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * Pika22ImageToVideoRequest
 *
 * Request model for Pika 2.2 image-to-video generation
 */
export type PikaV22ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: 5 | 10;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the model
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * ImageToVideoV21Output
 *
 * Output from image-to-video generation
 */
export type PikaV21ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideov21Input
 *
 * Base request for image-to-video generation
 */
export type PikaV21ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: number;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the model
   */
  negative_prompt?: string;
  /**
   * Image Url
   */
  image_url: string;
};

/**
 * PikaffectsOutput
 *
 * Output from Pikaffects generation
 */
export type PikaV15PikaffectsOutput = {
  /**
   * Video
   *
   * The generated video with applied effect
   */
  video: File;
};

/**
 * PikaffectsRequest
 *
 * Request model for Pikaffects endpoint
 */
export type PikaV15PikaffectsInput = {
  /**
   * Pikaffect
   *
   * The Pikaffect to apply
   */
  pikaffect:
    | "Cake-ify"
    | "Crumble"
    | "Crush"
    | "Decapitate"
    | "Deflate"
    | "Dissolve"
    | "Explode"
    | "Eye-pop"
    | "Inflate"
    | "Levitate"
    | "Melt"
    | "Peel"
    | "Poke"
    | "Squish"
    | "Ta-da"
    | "Tear";
  /**
   * Prompt
   *
   * Text prompt to guide the effect
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to guide the model
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * URL of the input image
   */
  image_url: string;
};

/**
 * Ray2I2VOutput
 */
export type LumaDreamMachineRay2FlashImageToVideoOutput = {
  /**
   * Video
   *
   * URL of the generated video
   */
  video: File;
};

/**
 * Ray2ImageToVideoRequest
 */
export type LumaDreamMachineRay2FlashImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "9:16" | "4:3" | "3:4" | "21:9" | "9:21";
  /**
   * Resolution
   *
   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * Loop
   *
   * Whether the video should loop (end of video is blended with the beginning)
   */
  loop?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video
   */
  duration?: "5s" | "9s";
  /**
   * Image Url
   *
   * Initial image to start the video from. Can be used together with end_image_url.
   */
  image_url?: string;
  /**
   * End Image Url
   *
   * Final image to end the video with. Can be used together with image_url.
   */
  end_image_url?: string;
};

/**
 * TransitionOutput
 */
export type PixverseV35TransitionOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TransitionRequest
 */
export type PixverseV35TransitionInput = {
  /**
   * First Image Url
   *
   * URL of the image to use as the first frame
   */
  first_image_url: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Prompt
   *
   * The prompt for the transition
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "8";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * End Image Url
   *
   * URL of the image to use as the last frame
   */
  end_image_url?: string;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * EffectOutput
 */
export type PixverseV35EffectsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * EffectInput
 */
export type PixverseV35EffectsInput = {
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "8";
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Effect
   *
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Image Url
   *
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string;
};

/**
 * I2VOutputV4
 */
export type PixverseV4ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequestV4
 */
export type PixverseV4ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
   */
  duration?: "5" | "8";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Camera Movement
   *
   * The type of camera movement to apply to the video
   */
  camera_movement?:
    | "horizontal_left"
    | "horizontal_right"
    | "vertical_up"
    | "vertical_down"
    | "zoom_in"
    | "zoom_out"
    | "crane_up"
    | "quickly_zoom_in"
    | "quickly_zoom_out"
    | "smooth_zoom_in"
    | "camera_rotation"
    | "robo_arm"
    | "super_dolly_out"
    | "whip_pan"
    | "hitchcock"
    | "left_follow"
    | "right_follow"
    | "pan_left"
    | "pan_right"
    | "fix_bg";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * I2VOutputV4
 */
export type PixverseV4ImageToVideoFastOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * FastImageToVideoRequestV4
 */
export type PixverseV4ImageToVideoFastInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Camera Movement
   *
   * The type of camera movement to apply to the video
   */
  camera_movement?:
    | "horizontal_left"
    | "horizontal_right"
    | "vertical_up"
    | "vertical_down"
    | "zoom_in"
    | "zoom_out"
    | "crane_up"
    | "quickly_zoom_in"
    | "quickly_zoom_out"
    | "smooth_zoom_in"
    | "camera_rotation"
    | "robo_arm"
    | "super_dolly_out"
    | "whip_pan"
    | "hitchcock"
    | "left_follow"
    | "right_follow"
    | "pan_left"
    | "pan_right"
    | "fix_bg";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * FramePackResponse
 */
export type FramepackOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  video: FileType2;
};

/**
 * FramePackRequest
 */
export type FramepackInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation (max 500 characters).
   */
  prompt: string;
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.
   */
  resolution?: "720p" | "480p";
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image Url
   *
   * URL of the image input.
   */
  image_url: string;
  /**
   * Guidance Scale
   *
   * Guidance scale for the generation.
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number | unknown;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * CFG Scale
   *
   * Classifier-Free Guidance scale for the generation.
   */
  cfg_scale?: number;
};

/**
 * WanFLF2VResponse
 */
export type WanFlf2vOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanFLF2VRequest
 */
export type WanFlf2vInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: "none" | "regular";
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Start Image Url
   *
   * URL of the starting image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  start_image_url: string;
  /**
   * End Image Url
   *
   * URL of the ending image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  end_image_url: string;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Guide Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guide_scale?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
};

/**
 * FramePackFLF2VResponse
 */
export type FramepackFlf2vOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  video: FileType2;
};

/**
 * FramePackF2LFRequest
 */
export type FramepackFlf2vInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation (max 500 characters).
   */
  prompt: string;
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.
   */
  resolution?: "720p" | "480p";
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Image Url
   *
   * URL of the image input.
   */
  image_url: string;
  /**
   * Strength of last frame
   *
   * Determines the influence of the final frame on the generated video. Higher values result in the output being more heavily influenced by the last frame.
   */
  strength?: number;
  /**
   * Guidance Scale
   *
   * Guidance scale for the generation.
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number | unknown;
  /**
   * End Image Url
   *
   * URL of the end image input.
   */
  end_image_url: string;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * CFG Scale
   *
   * Classifier-Free Guidance scale for the generation.
   */
  cfg_scale?: number;
};

/**
 * MagiImageToVideoResponse
 */
export type MagiDistilledImageToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MagiImageToVideoRequest
 */
export type MagiDistilledImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Image Url
   *
   * URL of the input image to represent the first frame of the video. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number;
};

/**
 * EffectOutput
 */
export type PixverseV4EffectsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * EffectInput
 */
export type PixverseV4EffectsInput = {
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "8";
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Effect
   *
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Image Url
   *
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string;
};

/**
 * MagiImageToVideoResponse
 */
export type MagiImageToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MagiImageToVideoRequest
 */
export type MagiImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Image Url
   *
   * URL of the input image to represent the first frame of the video. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: 4 | 8 | 16 | 32 | 64;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit.
   */
  num_frames?: number;
};

/**
 * Q1ImageToVideoOutput
 */
export type ViduQ1ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video using the Q1 model from a single image
   */
  video: File;
};

/**
 * Q1ImageToVideoRequest
 */
export type ViduQ1ImageToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * Seed
   *
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * Q1StartEndToVideoOutput
 */
export type ViduQ1StartEndToVideoOutput = {
  /**
   * Video
   *
   * The generated transition video between start and end frames using the Q1 model
   */
  video: File;
};

/**
 * Q1StartEndToVideoRequest
 */
export type ViduQ1StartEndToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * Seed
   *
   * Seed for the random number generator
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * End Image Url
   *
   * URL of the image to use as the last frame
   */
  end_image_url: string;
  /**
   * Start Image Url
   *
   * URL of the image to use as the first frame
   */
  start_image_url: string;
};

/**
 * FramePackF1Response
 */
export type FramepackF1Output = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  video: FileType2;
};

/**
 * FramePackF1Request
 */
export type FramepackF1Input = {
  /**
   * Prompt
   *
   * Text prompt for video generation (max 500 characters).
   */
  prompt: string;
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.
   */
  resolution?: "720p" | "480p";
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image Url
   *
   * URL of the image input.
   */
  image_url: string;
  /**
   * Guidance Scale
   *
   * Guidance scale for the generation.
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number | unknown;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * CFG Scale
   *
   * Classifier-Free Guidance scale for the generation.
   */
  cfg_scale?: number;
};

/**
 * HunyuanCustomResponse
 */
export type HunyuanCustomOutput = {
  /**
   * Seed
   *
   * The seed used for generating the video.
   */
  seed: number;
  /**
   * Video
   */
  video: File;
};

/**
 * HunyuanCustomRequest
 */
export type HunyuanCustomInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation (max 500 characters).
   */
  prompt: string;
  /**
   * Aspect Ratio (W:H)
   *
   * The aspect ratio of the video to generate.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.
   */
  resolution?: "512p" | "720p";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image Url
   *
   * URL of the image input.
   */
  image_url: string;
  /**
   * Frames per second
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed to use for generating the video.
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps to run. Lower gets faster results, higher gets better results.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * CFG Scale
   *
   * Classifier-Free Guidance scale for the generation.
   */
  cfg_scale?: number;
};

/**
 * EffectOutput
 */
export type PixverseV45EffectsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * EffectInput
 */
export type PixverseV45EffectsInput = {
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "8";
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Effect
   *
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Image Url
   *
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string;
};

/**
 * I2VOutputV4
 */
export type PixverseV45ImageToVideoFastOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * FastImageToVideoRequestV4
 */
export type PixverseV45ImageToVideoFastInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Camera Movement
   *
   * The type of camera movement to apply to the video
   */
  camera_movement?:
    | "horizontal_left"
    | "horizontal_right"
    | "vertical_up"
    | "vertical_down"
    | "zoom_in"
    | "zoom_out"
    | "crane_up"
    | "quickly_zoom_in"
    | "quickly_zoom_out"
    | "smooth_zoom_in"
    | "camera_rotation"
    | "robo_arm"
    | "super_dolly_out"
    | "whip_pan"
    | "hitchcock"
    | "left_follow"
    | "right_follow"
    | "pan_left"
    | "pan_right"
    | "fix_bg";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * TransitionOutput
 */
export type PixverseV45TransitionOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TransitionRequest
 */
export type PixverseV45TransitionInput = {
  /**
   * First Image Url
   *
   * URL of the image to use as the first frame
   */
  first_image_url: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Prompt
   *
   * The prompt for the transition
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "8";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * End Image Url
   *
   * URL of the image to use as the last frame
   */
  end_image_url?: string;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * ImageToVideoOutput
 */
export type LtxVideoLoraImageToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * ImageToVideoInput
 *
 * Request model for image-to-video generation.
 */
export type LtxVideoLoraImageToVideoInput = {
  /**
   * Number Of Steps
   *
   * The number of inference steps to use.
   */
  number_of_steps?: number;
  /**
   * Resolution
   *
   * The resolution of the video.
   */
  resolution?: "480p" | "720p";
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16" | "auto";
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using the LLM.
   */
  expand_prompt?: boolean;
  /**
   * Number Of Frames
   *
   * The number of frames in the video.
   */
  number_of_frames?: number;
  /**
   * Image Url
   *
   * The URL of the image to use as input.
   */
  image_url: string;
  /**
   * Loras
   *
   * The LoRA weights to use for generation.
   */
  loras?: Array<LoRaWeightType3>;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Seed
   *
   * The seed to use for generation.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to use.
   */
  negative_prompt?: string;
};

/**
 * ImageToVideoOutput
 */
export type LtxVideo13bDevImageToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * ImageToVideoInput
 */
export type LtxVideo13bDevImageToVideoInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Image Url
   *
   * Image URL for Image-to-Video task
   */
  image_url: string;
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number;
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * ImageToVideoOutput
 */
export type LtxVideo13bDistilledImageToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * DistilledImageToVideoInput
 *
 * Distilled model input
 */
export type LtxVideo13bDistilledImageToVideoInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * First Pass Num Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Second Pass Num Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p).
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Image Url
   *
   * Image URL for Image-to-Video task
   */
  image_url: string;
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number;
  /**
   * First Pass Skip Final Steps
   *
   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.
   */
  first_pass_skip_final_steps?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * ElementsOutput
 */
export type KlingVideoV16ProElementsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * MultiImageToVideoRequest
 */
export type KlingVideoV16ProElementsInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Input Image Urls
   *
   * List of image URLs to use for video generation. Supports up to 4 images.
   */
  input_image_urls: Array<string>;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
};

/**
 * ElementsOutput
 */
export type KlingVideoV16StandardElementsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * MultiImageToVideoRequest
 */
export type KlingVideoV16StandardElementsInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Input Image Urls
   *
   * List of image URLs to use for video generation. Supports up to 4 images.
   */
  input_image_urls: Array<string>;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
};

/**
 * Output
 */
export type HunyuanPortraitOutput = {
  /**
   * Video
   *
   * The generated video with the portrait animation.
   */
  video: File;
};

/**
 * Input
 */
export type HunyuanPortraitInput = {
  /**
   * Video Url
   *
   * The URL of the driving video.
   */
  video_url: string;
  /**
   * Seed
   *
   * Random seed for generation. If None, a random seed will be used.
   */
  seed?: number;
  /**
   * Use Arcface
   *
   * Whether to use ArcFace for face recognition.
   */
  use_arcface?: boolean;
  /**
   * Image Url
   *
   * The URL of the source image.
   */
  image_url: string;
};

/**
 * ImageToVideoV21ProOutput
 */
export type KlingVideoV21ProImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoV21ProRequest
 */
export type KlingVideoV21ProImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
  /**
   * Tail Image Url
   *
   * URL of the image to be used for the end of the video
   */
  tail_image_url?: string;
  /**
   * Image Url
   *
   * URL of the image to be used for the video
   */
  image_url: string;
};

/**
 * Output
 */
export type HunyuanAvatarOutput = {
  /**
   * Video
   *
   * The generated video with the avatar animation.
   */
  video: File;
};

/**
 * Input
 */
export type HunyuanAvatarInput = {
  /**
   * Text
   *
   * Text prompt describing the scene.
   */
  text?: string;
  /**
   * Image Url
   *
   * The URL of the reference image.
   */
  image_url: string;
  /**
   * Turbo Mode
   *
   * If true, the video will be generated faster with no noticeable degradation in the visual quality.
   */
  turbo_mode?: boolean;
  /**
   * Audio Url
   *
   * The URL of the audio file.
   */
  audio_url: string;
  /**
   * Seed
   *
   * Random seed for generation.
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Num Frames
   *
   * Number of video frames to generate at 25 FPS. If greater than the input audio length, it will capped to the length of the input audio.
   */
  num_frames?: number;
};

/**
 * SeedanceVideoOutput
 */
export type BytedanceSeedanceV1LiteImageToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SeedanceImageToVideoInput
 */
export type BytedanceSeedanceV1LiteImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution - 480p for faster generation, 720p for higher quality
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Image Url
   *
   * The URL of the image used to generate video
   */
  image_url: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera Fixed
   *
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * End Image Url
   *
   * The URL of the image the video ends with. Defaults to None.
   */
  end_image_url?: string;
  /**
   * Seed
   *
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
};

/**
 * ImageToVideoHailuo02Output
 */
export type MinimaxHailuo02ProImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ProImageToVideoHailuo02Input
 */
export type MinimaxHailuo02ProImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * End Image Url
   *
   * Optional URL of the image to use as the last frame of the video
   */
  end_image_url?: string;
  /**
   * Image Url
   */
  image_url: string;
};

/**
 * AvatarMultiAudioResponse
 */
export type AiAvatarMultiOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * AvatarMultiAudioPersonRequest
 */
export type AiAvatarMultiInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: "480p" | "720p";
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * First Audio URL
   *
   * The URL of the Person 1 audio file.
   */
  first_audio_url: string;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Second Audio URL
   *
   * The URL of the Person 2 audio file.
   */
  second_audio_url?: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Use Only First Audio
   *
   * Whether to use only the first audio file.
   */
  use_only_first_audio?: boolean;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number;
};

/**
 * AvatarMultiTextResponse
 */
export type AiAvatarMultiTextOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * AvatarMultiTextRequest
 */
export type AiAvatarMultiTextInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Second Text Input
   *
   * The text input to guide video generation.
   */
  second_text_input: string;
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: "480p" | "720p";
  /**
   * First Text Input
   *
   * The text input to guide video generation.
   */
  first_text_input: string;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Voice2
   *
   * The second person's voice to use for speech generation
   */
  voice2?:
    | "Aria"
    | "Roger"
    | "Sarah"
    | "Laura"
    | "Charlie"
    | "George"
    | "Callum"
    | "River"
    | "Liam"
    | "Charlotte"
    | "Alice"
    | "Matilda"
    | "Will"
    | "Jessica"
    | "Eric"
    | "Chris"
    | "Brian"
    | "Daniel"
    | "Lily"
    | "Bill";
  /**
   * Voice1
   *
   * The first person's voice to use for speech generation
   */
  voice1?:
    | "Aria"
    | "Roger"
    | "Sarah"
    | "Laura"
    | "Charlie"
    | "George"
    | "Callum"
    | "River"
    | "Liam"
    | "Charlotte"
    | "Alice"
    | "Matilda"
    | "Will"
    | "Jessica"
    | "Eric"
    | "Chris"
    | "Brian"
    | "Daniel"
    | "Lily"
    | "Bill";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number;
};

/**
 * AvatarSingleAudioResponse
 */
export type AiAvatarOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * AvatarSingleAudioRequest
 */
export type AiAvatarInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: "480p" | "720p";
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Audio URL
   *
   * The URL of the audio file.
   */
  audio_url: string;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};

/**
 * AvatarSingleTextResponse
 */
export type AiAvatarSingleTextOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * AvatarSingleTextRequest
 */
export type AiAvatarSingleTextInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the video to generate. Must be either 480p or 720p.
   */
  resolution?: "480p" | "720p";
  /**
   * Acceleration
   *
   * The acceleration level to use for generation.
   */
  acceleration?: "none" | "regular" | "high";
  /**
   * Text Input
   *
   * The text input to guide video generation.
   */
  text_input: string;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Voice
   *
   * The voice to use for speech generation
   */
  voice:
    | "Aria"
    | "Roger"
    | "Sarah"
    | "Laura"
    | "Charlie"
    | "George"
    | "Callum"
    | "River"
    | "Liam"
    | "Charlotte"
    | "Alice"
    | "Matilda"
    | "Will"
    | "Jessica"
    | "Eric"
    | "Chris"
    | "Brian"
    | "Daniel"
    | "Lily"
    | "Bill";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number;
};

/**
 * Q1ReferenceToVideoOutput
 */
export type ViduQ1ReferenceToVideoOutput = {
  /**
   * Video
   *
   * The generated video with consistent subjects from reference images using the Q1 model
   */
  video: File;
};

/**
 * Q1ReferenceToVideoRequest
 */
export type ViduQ1ReferenceToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 1500 characters
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the output video
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Bgm
   *
   * Whether to add background music to the generated video
   */
  bgm?: boolean;
  /**
   * Reference Image Urls
   *
   * URLs of the reference images to use for consistent subject appearance. Q1 model supports up to 7 reference images.
   */
  reference_image_urls: Array<string>;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};

/**
 * Veo3ImageToVideoOutput
 */
export type Veo3FastImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo3ImageToVideoInput
 */
export type Veo3FastImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing how the image should be animated
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Image URL
   *
   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
   */
  image_url: string;
};

/**
 * ImageToVideoOutput
 */
export type Ltxv13B098DistilledImageToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * DistilledImageToVideoInput
 *
 * Distilled model input
 */
export type Ltxv13B098DistilledImageToVideoInput = {
  /**
   * Second Pass Skip Initial Steps
   *
   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.
   */
  second_pass_skip_initial_steps?: number;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps during the first pass.
   */
  first_pass_num_inference_steps?: number;
  /**
   * Frame Rate
   *
   * The frame rate of the video.
   */
  frame_rate?: number;
  /**
   * Reverse Video
   *
   * Whether to reverse the video.
   */
  reverse_video?: boolean;
  /**
   * Prompt
   *
   * Text prompt to guide generation
   */
  prompt: string;
  /**
   * Expand Prompt
   *
   * Whether to expand the prompt using a language model.
   */
  expand_prompt?: boolean;
  /**
   * Temporal AdaIN Factor
   *
   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.
   */
  temporal_adain_factor?: number;
  /**
   * Loras
   *
   * LoRA weights to use for generation
   */
  loras?: Array<LoRaWeight>;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames in the video.
   */
  num_frames?: number;
  /**
   * Second Pass Number of Inference Steps
   *
   * Number of inference steps during the second pass.
   */
  second_pass_num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for generation
   */
  negative_prompt?: string;
  /**
   * Enable Detail Pass
   *
   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.
   */
  enable_detail_pass?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "9:16" | "1:1" | "16:9" | "auto";
  /**
   * Tone Map Compression Ratio
   *
   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.
   */
  tone_map_compression_ratio?: number;
  /**
   * Image URL
   *
   * Image URL for Image-to-Video task
   */
  image_url: string;
  /**
   * Constant Rate Factor
   *
   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.
   */
  constant_rate_factor?: number;
  /**
   * Seed
   *
   * Random seed for generation
   */
  seed?: number;
};

/**
 * OmniHumanOutput
 */
export type BytedanceOmnihumanOutput = {
  /**
   * Duration
   *
   * Duration of audio input/video output as used for billing.
   */
  duration: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * OmniHumanInput
 */
export type BytedanceOmnihumanInput = {
  /**
   * Audio Url
   *
   * The URL of the audio file to generate the video. Audio must be under 30s long.
   */
  audio_url: string;
  /**
   * Image Url
   *
   * The URL of the image used to generate the video
   */
  image_url: string;
};

/**
 * WanI2VResponse
 */
export type WanV22A14bImageToVideoOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanI2VRequest
 */
export type WanV22A14bImageToVideoInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: "none" | "regular";
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number;
  /**
   * End Image URL
   *
   * URL of the end image.
   */
  end_image_url?: string;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Guidance Scale (1st Stage)
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Guidance Scale (2nd Stage)
   *
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
   */
  guidance_scale_2?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean;
};

/**
 * WanSmallI2VResponse
 */
export type WanV225bImageToVideoOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanSmallI2VRequest
 */
export type WanV225bImageToVideoInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (580p or 720p).
   */
  resolution?: "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean;
};

/**
 * WanTurboI2VResponse
 */
export type WanV22A14bImageToVideoTurboOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanTurboI2VRequest
 */
export type WanV22A14bImageToVideoTurboInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: "none" | "regular";
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * End Image URL
   *
   * URL of the end image.
   */
  end_image_url?: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
};

/**
 * Veo3ImageToVideoOutput
 */
export type Veo3ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo3ImageToVideoInput
 */
export type Veo3ImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing how the image should be animated
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Image URL
   *
   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
   */
  image_url: string;
};

/**
 * ImageToVideoHailuo02FastOutput
 */
export type MinimaxHailuo02FastImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * FastImageToVideoHailuo02Input
 */
export type MinimaxHailuo02FastImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution.
   */
  duration?: "6" | "10";
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Image Url
   */
  image_url: string;
};

/**
 * WanI2VResponse
 */
export type WanV22A14bImageToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanLoRAI2VRequest
 */
export type WanV22A14bImageToVideoLoraInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: "none" | "regular";
  /**
   * Number of Interpolated Frames
   *
   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.
   */
  num_interpolated_frames?: number;
  /**
   * Reverse Video
   *
   * If true, the video will be reversed.
   */
  reverse_video?: boolean;
  /**
   * Loras
   *
   * LoRA weights to be used in the inference.
   */
  loras?: Array<LoRaWeightType2>;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 17 to 161 (inclusive).
   */
  num_frames?: number;
  /**
   * End Image URL
   *
   * URL of the end image.
   */
  end_image_url?: string;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Guidance Scale (1st Stage)
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Guidance Scale (2nd Stage)
   *
   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.
   */
  guidance_scale_2?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Interpolator Model
   *
   * The model to use for frame interpolation. If None, no interpolation is applied.
   */
  interpolator_model?: "none" | "film" | "rife";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Adjust FPS for Interpolation
   *
   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.
   */
  adjust_fps_for_interpolation?: boolean;
};

export type BytedanceVideoStylizeOutput = unknown;

/**
 * StylizeInput
 */
export type BytedanceVideoStylizeInput = {
  /**
   * Style
   *
   * The style for your character in the video. Please use a short description.
   */
  style: string;
  /**
   * Image Url
   *
   * URL of the image to make the stylized video from.
   */
  image_url: string;
};

/**
 * MareyOutput
 */
export type MareyI2vOutput = {
  video: FileType2;
};

/**
 * MareyInputI2V
 */
export type MareyI2vInput = {
  /**
   * Prompt
   *
   * The prompt to generate a video from
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "5s" | "10s";
  /**
   * Image Url
   *
   * The URL of the image to use as the first frame of the video.
   */
  image_url: string;
  /**
   * Dimensions
   *
   * The dimensions of the generated video in width x height format.
   */
  dimensions?:
    | "1920x1080"
    | "1080x1920"
    | "1152x1152"
    | "1536x1152"
    | "1152x1536";
  /**
   * Guidance Scale
   *
   * Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely.
   */
  guidance_scale?: number | unknown;
  /**
   * Seed
   *
   * Seed for random number generation. Use -1 for random seed each run.
   */
  seed?: number | unknown;
  /**
   * Negative Prompt
   *
   * Negative prompt used to guide the model away from undesirable features.
   */
  negative_prompt?: string | unknown;
};

/**
 * I2VOutputV5
 */
export type PixverseV5ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequestV5
 */
export type PixverseV5ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
   */
  duration?: "5" | "8";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * EffectOutput
 */
export type PixverseV5EffectsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * EffectInput
 */
export type PixverseV5EffectsInput = {
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "8";
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Effect
   *
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Image Url
   *
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string;
};

/**
 * TransitionOutputV5
 */
export type PixverseV5TransitionOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TransitionRequest
 */
export type PixverseV5TransitionInput = {
  /**
   * First Image Url
   *
   * URL of the image to use as the first frame
   */
  first_image_url: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Prompt
   *
   * The prompt for the transition
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "8";
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * End Image Url
   *
   * URL of the image to use as the last frame
   */
  end_image_url?: string;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * ProcessOutput
 */
export type DecartLucy5bImageToVideoOutput = {
  /**
   * Video
   *
   * The generated MP4 video with H.264 encoding
   */
  video: File;
};

/**
 * ProcessRequest
 */
export type DecartLucy5bImageToVideoInput = {
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Resolution
   *
   * Resolution of the generated video
   */
  resolution?: "720p";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * A coordinate point with x and y values for motion tracking
 */
export type TrackPoint = {
  /**
   * X coordinate
   */
  x: number;
  /**
   * Y coordinate
   */
  y: number;
};

/**
 * WanATIResponse
 */
export type WanAtiOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanATIRequest
 */
export type WanAtiInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Image URL
   *
   * URL of the input image.
   */
  image_url: string;
  /**
   * Track
   *
   * Motion tracks to guide video generation. Each track is a sequence of points defining a motion trajectory. Multiple tracks can control different elements or objects in the video. Expected format: array of tracks, where each track is an array of points with 'x' and 'y' coordinates (up to 121 points per track). Points will be automatically padded to 121 if fewer are provided. Coordinates should be within the image dimensions.
   */
  track: Array<Array<TrackPoint>>;
  /**
   * Guidance Scale (1st Stage)
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};

/**
 * SeedanceReferenceToVideoOutput
 */
export type BytedanceSeedanceV1LiteReferenceToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SeedanceReferenceToVideoInput
 */
export type BytedanceSeedanceV1LiteReferenceToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution - 480p for faster generation, 720p for higher quality
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera Fixed
   *
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Reference Image Urls
   *
   * Reference images to generate the video with.
   */
  reference_image_urls: Array<string>;
  /**
   * Seed
   *
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
};

/**
 * Lucy14BOutput
 */
export type Lucy14bImageToVideoOutput = {
  /**
   * Video
   *
   * The generated MP4 video with H.264 encoding
   */
  video: File;
};

/**
 * Lucy14BImageToVideoInput
 */
export type Lucy14bImageToVideoInput = {
  /**
   * Sync Mode
   *
   *
   * If set to true, the function will wait for the image to be generated
   * and uploaded before returning the response. This will increase the
   * latency of the function but it allows you to get the image directly
   * in the response without going through the CDN.
   *
   */
  sync_mode?: boolean;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?: "9:16" | "16:9";
  /**
   * Prompt
   *
   * Text description of the desired video content
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video
   */
  resolution?: "720p";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * AIAvatarOutput
 */
export type KlingVideoV1ProAiAvatarOutput = {
  /**
   * Duration
   *
   * Duration of the output video in seconds.
   */
  duration: number;
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * AIAvatarInput
 */
export type KlingVideoV1ProAiAvatarInput = {
  /**
   * Prompt
   *
   * The prompt to use for the video generation.
   */
  prompt?: string;
  /**
   * Audio Url
   *
   * The URL of the audio file.
   */
  audio_url: string;
  /**
   * Image Url
   *
   * The URL of the image to use as your avatar
   */
  image_url: string;
};

/**
 * AIAvatarOutput
 */
export type KlingVideoV1StandardAiAvatarOutput = {
  /**
   * Duration
   *
   * Duration of the output video in seconds.
   */
  duration: number;
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * AIAvatarInput
 */
export type KlingVideoV1StandardAiAvatarInput = {
  /**
   * Prompt
   *
   * The prompt to use for the video generation.
   */
  prompt?: string;
  /**
   * Audio Url
   *
   * The URL of the audio file.
   */
  audio_url: string;
  /**
   * Image Url
   *
   * The URL of the image to use as your avatar
   */
  image_url: string;
};

/**
 * FabricOneOutput
 */
export type Fabric10Output = {
  video: FileType2;
};

/**
 * FabricOneLipsyncInput
 */
export type Fabric10Input = {
  /**
   * Resolution
   *
   * Resolution
   */
  resolution: "720p" | "480p";
  /**
   * Audio Url
   */
  audio_url: string;
  /**
   * Image Url
   */
  image_url: string;
};

/**
 * OmniHumanv15Output
 */
export type BytedanceOmnihumanV15Output = {
  /**
   * Duration
   *
   * Duration of audio input/video output as used for billing.
   */
  duration: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * OmniHumanv15Input
 */
export type BytedanceOmnihumanV15Input = {
  /**
   * Turbo Mode
   *
   * Generate a video at a faster rate with a slight quality trade-off.
   */
  turbo_mode?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video. Defaults to 1080p. 720p generation is faster and higher in quality. 1080p generation is limited to 30s audio and 720p generation is limited to 60s audio.
   */
  resolution?: "720p" | "1080p";
  /**
   * Prompt
   *
   * The text prompt used to guide the video generation.
   */
  prompt?: string;
  /**
   * Audio Url
   *
   * The URL of the audio file to generate the video. Audio must be under 30s long for 1080p generation and under 60s long for 720p generation.
   */
  audio_url: string;
  /**
   * Image Url
   *
   * The URL of the image used to generate the video
   */
  image_url: string;
};

/**
 * FabricOneOutput
 */
export type Fabric10FastOutput = {
  video: FileType2;
};

/**
 * FabricOneLipsyncInput
 */
export type Fabric10FastInput = {
  /**
   * Resolution
   *
   * Resolution
   */
  resolution: "720p" | "480p";
  /**
   * Audio Url
   */
  audio_url: string;
  /**
   * Image Url
   */
  image_url: string;
};

/**
 * OviI2VResponse
 */
export type OviImageToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * The generated video file.
   */
  video?: FileType2 | unknown;
};

/**
 * OviI2VRequest
 */
export type OviImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Num Inference Steps
   *
   * The number of inference steps.
   */
  num_inference_steps?: number;
  /**
   * Audio Negative Prompt
   *
   * Negative prompt for audio generation.
   */
  audio_negative_prompt?: string;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * The image URL to guide video generation.
   */
  image_url: string;
};

/**
 * ImageToVideoOutput
 */
export type Sora2ImageToVideoOutput = {
  /**
   * Spritesheet
   *
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
  /**
   * Thumbnail
   *
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Video ID
   *
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Video
   *
   * The generated video
   */
  video: VideoFileType2;
};

/**
 * ImageToVideoInput
 */
export type Sora2ImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the generated video in seconds
   */
  duration?: 4 | 8 | 12;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "auto" | "720p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "auto" | "9:16" | "16:9";
  /**
   * Image URL
   *
   * The URL of the image to use as the first frame
   */
  image_url: string;
  /**
   * Model
   *
   * The model to use for the generation. When the default model is selected, the latest snapshot of the model will be used - otherwise, select a specific snapshot of the model.
   */
  model?: "sora-2" | "sora-2-2025-12-08" | "sora-2-2025-10-06";
  /**
   * Delete Video
   *
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
   */
  delete_video?: boolean;
};

/**
 * ProImageToVideoOutput
 */
export type Sora2ImageToVideoProOutput = {
  /**
   * Spritesheet
   *
   * Spritesheet image for the video
   */
  spritesheet?: ImageFile;
  /**
   * Thumbnail
   *
   * Thumbnail image for the video
   */
  thumbnail?: ImageFile;
  /**
   * Video ID
   *
   * The ID of the generated video
   */
  video_id: string;
  /**
   * Video
   *
   * The generated video
   */
  video: VideoFileType2;
};

/**
 * ProImageToVideoInput
 */
export type Sora2ImageToVideoProInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the generated video in seconds
   */
  duration?: 4 | 8 | 12;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "auto" | "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "auto" | "9:16" | "16:9";
  /**
   * Delete Video
   *
   * Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.
   */
  delete_video?: boolean;
  /**
   * Image URL
   *
   * The URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * Veo31ImageToVideoOutput
 */
export type Veo31ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo31ImageToVideoInput
 */
export type Veo31ImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video. Only 16:9 and 9:16 are supported.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Image URL
   *
   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
   */
  image_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * Veo31ImageToVideoOutput
 */
export type Veo31FastImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo31ImageToVideoInput
 */
export type Veo31FastImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video. Only 16:9 and 9:16 are supported.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Image URL
   *
   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.
   */
  image_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * Veo31ReferenceToVideoOutput
 */
export type Veo31ReferenceToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo31ReferenceToVideoInput
 */
export type Veo31ReferenceToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "8s";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Image Urls
   *
   * URLs of the reference images to use for consistent subject appearance
   */
  image_urls: Array<string>;
};

/**
 * Veo31FirstLastFrameToVideoOutput
 */
export type Veo31FirstLastFrameToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo31FirstLastFrameToVideoInput
 */
export type Veo31FirstLastFrameToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * First Frame URL
   *
   * URL of the first frame of the video
   */
  first_frame_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Last Frame URL
   *
   * URL of the last frame of the video
   */
  last_frame_url: string;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * Veo31FirstLastFrameToVideoOutput
 */
export type Veo31FastFirstLastFrameToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * Veo31FirstLastFrameToVideoInput
 */
export type Veo31FastFirstLastFrameToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the video you want to generate
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video.
   */
  duration?: "4s" | "6s" | "8s";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * Auto Fix
   *
   * Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.
   */
  auto_fix?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "720p" | "1080p" | "4k";
  /**
   * First Frame URL
   *
   * URL of the first frame of the video
   */
  first_frame_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Last Frame URL
   *
   * URL of the last frame of the video
   */
  last_frame_url: string;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * ImageToVideoV25StandardOutput
 */
export type KlingVideoV25TurboStandardImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoV25StandardRequest
 */
export type KlingVideoV25TurboStandardImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * URL of the image to be used for the video
   */
  image_url: string;
};

/**
 * Q2ImageToVideoOutput
 */
export type ViduQ2ImageToVideoProOutput = {
  /**
   * Video
   *
   * The generated video from image using the Q2 model
   */
  video: File;
};

/**
 * Q2ImageToVideoRequest
 */
export type ViduQ2ImageToVideoProInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 3000 characters
   */
  prompt: string;
  /**
   * Resolution
   *
   * Output video resolution
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: 2 | 3 | 4 | 5 | 6 | 7 | 8;
  /**
   * Image Url
   *
   * URL of the image to use as the starting frame
   */
  image_url: string;
  /**
   * Bgm
   *
   * Whether to add background music to the video (only for 4-second videos)
   */
  bgm?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * End Image Url
   *
   * URL of the image to use as the ending frame. When provided, generates a transition video between start and end frames.
   */
  end_image_url?: string;
};

/**
 * Q2ImageToVideoOutput
 */
export type ViduQ2ImageToVideoTurboOutput = {
  /**
   * Video
   *
   * The generated video from image using the Q2 model
   */
  video: File;
};

/**
 * Q2ImageToVideoRequest
 */
export type ViduQ2ImageToVideoTurboInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 3000 characters
   */
  prompt: string;
  /**
   * Resolution
   *
   * Output video resolution
   */
  resolution?: "720p" | "1080p";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: 2 | 3 | 4 | 5 | 6 | 7 | 8;
  /**
   * Image Url
   *
   * URL of the image to use as the starting frame
   */
  image_url: string;
  /**
   * Bgm
   *
   * Whether to add background music to the video (only for 4-second videos)
   */
  bgm?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
  /**
   * End Image Url
   *
   * URL of the image to use as the ending frame. When provided, generates a transition video between start and end frames.
   */
  end_image_url?: string;
};

/**
 * SeedanceFastI2VVideoOutput
 */
export type BytedanceSeedanceV1ProFastImageToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SeedanceProFastImageToVideoInput
 */
export type BytedanceSeedanceV1ProFastImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Image Url
   *
   * The URL of the image used to generate video
   */
  image_url: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera Fixed
   *
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * Seed
   *
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
};

/**
 * ProFastImageToVideoHailuo23Output
 */
export type MinimaxHailuo23FastProImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ProFastImageToVideoHailuo23Input
 */
export type MinimaxHailuo23FastProImageToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * StandardImageToVideoHailuo23Output
 */
export type MinimaxHailuo23StandardImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * StandardImageToVideoHailuo23Input
 */
export type MinimaxHailuo23StandardImageToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the video in seconds.
   */
  duration?: "6" | "10";
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * StandardFastImageToVideoHailuo23Output
 */
export type MinimaxHailuo23FastStandardImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * StandardFastImageToVideoHailuo23Input
 */
export type MinimaxHailuo23FastStandardImageToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the video in seconds.
   */
  duration?: "6" | "10";
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * LongCatImageToVideoResponse
 */
export type LongcatVideoDistilledImageToVideo480pOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LongCatImageToVideoRequest
 */
export type LongcatVideoDistilledImageToVideo480pInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt?: string;
  /**
   * FPS
   *
   * The frame rate of the generated video.
   */
  fps?: number;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Image URL
   *
   * The URL of the image to generate a video from.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * LongCatImageToVideoResponse
 */
export type LongcatVideoDistilledImageToVideo720pOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LongCat720PImageToVideoRequest
 */
export type LongcatVideoDistilledImageToVideo720pInput = {
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt?: string;
  /**
   * FPS
   *
   * The frame rate of the generated video.
   */
  fps?: number;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Number of Refinement Inference Steps
   *
   * The number of inference steps to use for refinement.
   */
  num_refine_inference_steps?: number;
  /**
   * Image URL
   *
   * The URL of the image to generate a video from.
   */
  image_url: string;
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
};

/**
 * LongCatImageToVideoResponse
 */
export type LongcatVideoImageToVideo480pOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LongCatCFGImageToVideoRequest
 */
export type LongcatVideoImageToVideo480pInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt?: string;
  /**
   * Acceleration
   *
   * The acceleration level to use for the video generation.
   */
  acceleration?: "none" | "regular";
  /**
   * FPS
   *
   * The frame rate of the generated video.
   */
  fps?: number;
  /**
   * Guidance Scale
   *
   * The guidance scale to use for the video generation.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to use for the video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Image URL
   *
   * The URL of the image to generate a video from.
   */
  image_url: string;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
};

/**
 * LongCatImageToVideoResponse
 */
export type LongcatVideoImageToVideo720pOutput = {
  /**
   * Prompt
   *
   * The prompt used for generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * LongCat720PCFGImageToVideoRequest
 */
export type LongcatVideoImageToVideo720pInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt?: string;
  /**
   * Acceleration
   *
   * The acceleration level to use for the video generation.
   */
  acceleration?: "none" | "regular";
  /**
   * FPS
   *
   * The frame rate of the generated video.
   */
  fps?: number;
  /**
   * Number of Refinement Inference Steps
   *
   * The number of inference steps to use for refinement.
   */
  num_refine_inference_steps?: number;
  /**
   * Guidance Scale
   *
   * The guidance scale to use for the video generation.
   */
  guidance_scale?: number;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to use for the video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Image URL
   *
   * The URL of the image to generate a video from.
   */
  image_url: string;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
};

/**
 * KeyframeTransition
 *
 * Configuration for a transition between two keyframes
 */
export type KeyframeTransition = {
  /**
   * Prompt
   *
   * Specific prompt for this transition. Overrides the global prompt if provided.
   */
  prompt?: string;
  /**
   * Duration
   *
   * Duration of this transition in seconds
   */
  duration?: number;
};

/**
 * Pika22KeyframesToVideoOutput
 *
 * Output model for Pika 2.2 keyframes-to-video generation
 */
export type PikaV22PikaframesOutput = {
  /**
   * Video
   *
   * The generated video with transitions between keyframes
   */
  video: File;
};

/**
 * Pika22KeyframesToVideoRequest
 */
export type PikaV22PikaframesInput = {
  /**
   * Prompt
   *
   * Default prompt for all transitions. Individual transition prompts override this.
   */
  prompt?: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "720p" | "1080p";
  /**
   * Transitions
   *
   * Configuration for each transition. Length must be len(image_urls) - 1. Total duration of all transitions must not exceed 25 seconds. If not provided, uses default 5-second transitions with the global prompt.
   */
  transitions?: Array<KeyframeTransition>;
  /**
   * Seed
   *
   * The seed for the random number generator
   */
  seed?: number;
  /**
   * Image Urls
   *
   * URLs of keyframe images (2-5 images) to create transitions between
   */
  image_urls: Array<string>;
  /**
   * Negative Prompt
   *
   * A negative prompt to guide the model
   */
  negative_prompt?: string;
};

/**
 * SwapOutput
 */
export type PixverseSwapOutput = {
  /**
   * Video
   *
   * The generated swapped video
   */
  video: File;
};

/**
 * SwapRequest
 */
export type PixverseSwapInput = {
  /**
   * Original Sound Switch
   *
   * Whether to keep the original audio
   */
  original_sound_switch?: boolean;
  /**
   * Video Url
   *
   * URL of the external video to swap
   */
  video_url: string;
  /**
   * Keyframe Id
   *
   * The keyframe ID (from 1 to the last frame position)
   */
  keyframe_id?: number;
  /**
   * Mode
   *
   * The swap mode to use
   */
  mode?: "person" | "object" | "background";
  /**
   * Resolution
   *
   * The output resolution (1080p not supported)
   */
  resolution?: "360p" | "540p" | "720p";
  /**
   * Image Url
   *
   * URL of the target image for swapping
   */
  image_url: string;
};

/**
 * LynxOutput
 */
export type LynxOutput = {
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * LynxInput
 */
export type LynxInput = {
  /**
   * Prompt
   *
   * Text prompt to guide video generation
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p)
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video (16:9, 9:16, or 1:1)
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Guidance Scale 2
   *
   * Image guidance scale. Controls how closely the generated video follows the reference image. Higher values increase adherence to the reference image but may decrease quality.
   */
  guidance_scale_2?: number;
  /**
   * Strength
   *
   * Reference image scale. Controls the influence of the reference image on the generated video.
   */
  strength?: number;
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 30.
   */
  frames_per_second?: number;
  /**
   * Image Url
   *
   * The URL of the subject image to be used for video generation
   */
  image_url: string;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Frames
   *
   * Number of frames in the generated video. Must be between 9 to 100.
   */
  num_frames?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to guide what should not appear in the generated video
   */
  negative_prompt?: string;
  /**
   * Ip Scale
   *
   * Identity preservation scale. Controls how closely the generated video preserves the subject's identity from the reference image.
   */
  ip_scale?: number;
};

/**
 * LTXVImageToVideoResponse
 */
export type Ltx2ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * LTXVImageToVideoRequest
 */
export type Ltx2ImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "1080p" | "1440p" | "2160p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the generated video
   */
  generate_audio?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: 6 | 8 | 10;
  /**
   * Frames per Second
   *
   * The frames per second of the generated video
   */
  fps?: 25 | 50;
  /**
   * Image URL
   *
   * URL of the image to generate the video from. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
   */
  image_url: string;
};

/**
 * LTXVImageToVideoResponse
 */
export type Ltx2ImageToVideoFastOutput = {
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * LTXVImageToVideoFastRequest
 */
export type Ltx2ImageToVideoFastInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "1080p" | "1440p" | "2160p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the generated video
   */
  generate_audio?: boolean;
  /**
   * Duration
   *
   * The duration of the generated video in seconds. The fast model supports 6-20 seconds. Note: Durations longer than 10 seconds (12, 14, 16, 18, 20) are only supported with 25 FPS and 1080p resolution.
   */
  duration?: 6 | 8 | 10 | 12 | 14 | 16 | 18 | 20;
  /**
   * Frames per Second
   *
   * The frames per second of the generated video
   */
  fps?: 25 | 50;
  /**
   * Image URL
   *
   * URL of the image to generate the video from. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats.
   */
  image_url: string;
};

/**
 * OmniVideoReferenceToVideoOutput
 */
export type KlingVideoO1ReferenceToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * OmniVideoReferenceToVideoInput
 *
 * Input for start-frame video generation with optional reference images and elements.
 */
export type KlingVideoO1ReferenceToVideoInput = {
  /**
   * Prompt
   *
   * Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame.
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
  /**
   * Elements
   *
   * Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image).
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * Image Urls
   *
   * Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image).
   */
  image_urls?: Array<string>;
};

/**
 * OmniVideoImageToVideoOutput
 *
 * Output for Kling Omni Video generation.
 */
export type KlingVideoO1ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * OmniVideoImageToVideoInput
 */
export type KlingVideoO1ImageToVideoInput = {
  /**
   * Prompt
   *
   * Use @Image1 to reference the start frame, @Image2 to reference the end frame.
   */
  prompt: string;
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
  /**
   * Start Image Url
   *
   * Image to use as the first frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  start_image_url: string;
  /**
   * End Image Url
   *
   * Image to use as the last frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  end_image_url?: string;
};

/**
 * I2VOutputV5_5
 */
export type PixverseV55ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequestV5_5
 */
export type PixverseV55ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds
   */
  duration?: "5" | "8" | "10";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Thinking Type
   *
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * Generate Multi Clip Switch
   *
   * Enable multi-clip generation with dynamic camera changes
   */
  generate_multi_clip_switch?: boolean;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
  /**
   * Generate Audio Switch
   *
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * TransitionOutputV5_5
 */
export type PixverseV55TransitionOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TransitionRequestV5_5
 */
export type PixverseV55TransitionInput = {
  /**
   * First Image Url
   *
   * URL of the image to use as the first frame
   */
  first_image_url: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Thinking Type
   *
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * Prompt
   *
   * The prompt for the transition
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds
   */
  duration?: "5" | "8" | "10";
  /**
   * Generate Audio Switch
   *
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * End Image Url
   *
   * URL of the image to use as the last frame
   */
  end_image_url?: string;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * EffectOutput
 */
export type PixverseV55EffectsOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * EffectInputV5_5
 */
export type PixverseV55EffectsInput = {
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "8" | "10";
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Thinking Type
   *
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * Effect
   *
   * The effect to apply to the video
   */
  effect:
    | "Kiss Me AI"
    | "Kiss"
    | "Muscle Surge"
    | "Warmth of Jesus"
    | "Anything, Robot"
    | "The Tiger Touch"
    | "Hug"
    | "Holy Wings"
    | "Microwave"
    | "Zombie Mode"
    | "Squid Game"
    | "Baby Face"
    | "Black Myth: Wukong"
    | "Long Hair Magic"
    | "Leggy Run"
    | "Fin-tastic Mermaid"
    | "Punch Face"
    | "Creepy Devil Smile"
    | "Thunder God"
    | "Eye Zoom Challenge"
    | "Who's Arrested?"
    | "Baby Arrived"
    | "Werewolf Rage"
    | "Bald Swipe"
    | "BOOM DROP"
    | "Huge Cutie"
    | "Liquid Metal"
    | "Sharksnap!"
    | "Dust Me Away"
    | "3D Figurine Factor"
    | "Bikini Up"
    | "My Girlfriends"
    | "My Boyfriends"
    | "Subject 3 Fever"
    | "Earth Zoom"
    | "Pole Dance"
    | "Vroom Dance"
    | "GhostFace Terror"
    | "Dragon Evoker"
    | "Skeletal Bae"
    | "Summoning succubus"
    | "Halloween Voodoo Doll"
    | "3D Naked-Eye AD"
    | "Package Explosion"
    | "Dishes Served"
    | "Ocean ad"
    | "Supermarket AD"
    | "Tree doll"
    | "Come Feel My Abs"
    | "The Bicep Flex"
    | "London Elite Vibe"
    | "Flora Nymph Gown"
    | "Christmas Costume"
    | "It's Snowy"
    | "Reindeer Cruiser"
    | "Snow Globe Maker"
    | "Pet Christmas Outfit"
    | "Adopt a Polar Pal"
    | "Cat Christmas Box"
    | "Starlight Gift Box"
    | "Xmas Poster"
    | "Pet Christmas Tree"
    | "City Santa Hat"
    | "Stocking Sweetie"
    | "Christmas Night"
    | "Xmas Front Page Karma"
    | "Grinch's Xmas Hijack"
    | "Giant Product"
    | "Truck Fashion Shoot"
    | "Beach AD"
    | "Shoal Surround"
    | "Mechanical Assembly"
    | "Lighting AD"
    | "Billboard AD"
    | "Product close-up"
    | "Parachute Delivery"
    | "Dreamlike Cloud"
    | "Macaron Machine"
    | "Poster AD"
    | "Truck AD"
    | "Graffiti AD"
    | "3D Figurine Factory"
    | "The Exclusive First Class"
    | "Art Zoom Challenge"
    | "I Quit"
    | "Hitchcock Dolly Zoom"
    | "Smell the Lens"
    | "I believe I can fly"
    | "Strikout Dance"
    | "Pixel World"
    | "Mint in Box"
    | "Hands up, Hand"
    | "Flora Nymph Go"
    | "Somber Embrace"
    | "Beam me up"
    | "Suit Swagger";
  /**
   * Image Url
   *
   * Optional URL of the image to use as the first frame. If not provided, generates from text
   */
  image_url: string;
};

/**
 * ImageToVideoV26ProOutput
 */
export type KlingVideoV26ProImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoV26ProRequest
 */
export type KlingVideoV26ProImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Voice Ids
   *
   * List of voice IDs to use for voice control. Reference voices in the prompt using <<<voice_1>>>, <<<voice_2>>>. Maximum 2 voices allowed. When provided and referenced in prompt, enables voice control billing.
   */
  voice_ids?: Array<string>;
  /**
   * Generate Audio
   *
   * Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.
   */
  generate_audio?: boolean;
  /**
   * Start Image Url
   *
   * URL of the image to be used for the video
   */
  start_image_url: string;
  /**
   * End Image Url
   *
   * URL of the image to be used for the end of the video
   */
  end_image_url?: string;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
};

/**
 * AIAvatarOutput
 */
export type KlingVideoAiAvatarV2StandardOutput = {
  /**
   * Duration
   *
   * Duration of the output video in seconds.
   */
  duration: number;
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * AIAvatarInput
 */
export type KlingVideoAiAvatarV2StandardInput = {
  /**
   * Prompt
   *
   * The prompt to use for the video generation.
   */
  prompt?: string;
  /**
   * Audio Url
   *
   * The URL of the audio file.
   */
  audio_url: string;
  /**
   * Image Url
   *
   * The URL of the image to use as your avatar
   */
  image_url: string;
};

/**
 * AIAvatarOutput
 */
export type KlingVideoAiAvatarV2ProOutput = {
  /**
   * Duration
   *
   * Duration of the output video in seconds.
   */
  duration: number;
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * AIAvatarInput
 */
export type KlingVideoAiAvatarV2ProInput = {
  /**
   * Prompt
   *
   * The prompt to use for the video generation.
   */
  prompt?: string;
  /**
   * Audio Url
   *
   * The URL of the audio file.
   */
  audio_url: string;
  /**
   * Image Url
   *
   * The URL of the image to use as your avatar
   */
  image_url: string;
};

/**
 * AuroraOutputModel
 */
export type CreatifyAuroraOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: VideoFileType2;
};

/**
 * AuroraInputModel
 */
export type CreatifyAuroraInput = {
  /**
   * Prompt
   *
   * A text prompt to guide the video generation process.
   */
  prompt?: string;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "480p" | "720p";
  /**
   * Guidance Scale
   *
   * Guidance scale to be used for text prompt adherence.
   */
  guidance_scale?: number;
  /**
   * Audio Guidance Scale
   *
   * Guidance scale to be used for audio adherence.
   */
  audio_guidance_scale?: number;
  /**
   * Audio Url
   *
   * The URL of the audio file to be used for video generation.
   */
  audio_url: string;
  /**
   * Image Url
   *
   * The URL of the image file to be used for video generation.
   */
  image_url: string;
};

/**
 * OmniVideoImageToVideoOutput
 *
 * Output for Kling Omni Video generation.
 */
export type KlingVideoO1StandardImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * OmniVideoImageToVideoInput
 */
export type KlingVideoO1StandardImageToVideoInput = {
  /**
   * Prompt
   *
   * Use @Image1 to reference the start frame, @Image2 to reference the end frame.
   */
  prompt: string;
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
  /**
   * Start Image Url
   *
   * Image to use as the first frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  start_image_url: string;
  /**
   * End Image Url
   *
   * Image to use as the last frame of the video.
   *
   * Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s
   */
  end_image_url?: string;
};

/**
 * OmniVideoReferenceToVideoOutput
 */
export type KlingVideoO1StandardReferenceToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: File;
};

/**
 * OmniVideoReferenceToVideoInput
 *
 * Input for start-frame video generation with optional reference images and elements.
 */
export type KlingVideoO1StandardReferenceToVideoInput = {
  /**
   * Prompt
   *
   * Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame.
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10";
  /**
   * Elements
   *
   * Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image).
   */
  elements?: Array<OmniVideoElementInput>;
  /**
   * Image Urls
   *
   * Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image).
   */
  image_urls?: Array<string>;
};

/**
 * ImageToVideoOutput
 *
 * Output for image-to-video generation
 */
export type V26ImageToVideoOutput = {
  /**
   * Actual Prompt
   *
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * ImageToVideoInput
 *
 * Input for Wan 2.6 image-to-video generation
 */
export type V26ImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the desired video motion. Max 800 characters.
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds.
   */
  duration?: "5" | "10" | "15";
  /**
   * Resolution
   *
   * Video resolution. Valid values: 720p, 1080p
   */
  resolution?: "720p" | "1080p";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Image URL
   *
   * URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680.
   */
  image_url: string;
  /**
   * Audio Url
   *
   *
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds),
   * the audio is truncated to the first N seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   *
   */
  audio_url?: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Multi Shots
   *
   * When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.
   */
  multi_shots?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt rewriting using LLM.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * HunyuanVideo15Response
 */
export type HunyuanVideoV15ImageToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * HunyuanVideo15I2VRequest
 */
export type HunyuanVideoV15ImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video.
   */
  aspect_ratio?: "16:9" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the video.
   */
  resolution?: "480p";
  /**
   * Image Url
   *
   * URL of the reference image for image-to-video generation.
   */
  image_url: string;
  /**
   * Enable Prompt Expansion
   *
   * Enable prompt expansion to enhance the input prompt.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility.
   */
  seed?: number;
  /**
   * Num Inference Steps
   *
   * The number of inference steps.
   */
  num_inference_steps?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to guide what not to generate.
   */
  negative_prompt?: string;
  /**
   * Num Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
};

/**
 * LiveAvatarResponse
 */
export type LiveAvatarOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated avatar video file with synchronized audio.
   */
  video: VideoFileType2;
};

/**
 * LiveAvatarRequest
 */
export type LiveAvatarInput = {
  /**
   * Frames per Clip
   *
   * Number of frames per clip. Must be a multiple of 4. Higher values = smoother but slower generation.
   */
  frames_per_clip?: number;
  /**
   * Prompt
   *
   * A text prompt describing the scene and character. Helps guide the video generation style and context.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * Acceleration level for faster video decoding
   */
  acceleration?: "none" | "light" | "regular" | "high";
  /**
   * Reference Image URL
   *
   * The URL of the reference image for avatar generation. The character in this image will be animated.
   */
  image_url: string;
  /**
   * Number of Clips
   *
   * Number of video clips to generate. Each clip is approximately 3 seconds. Set higher for longer videos.
   */
  num_clips?: number;
  /**
   * Audio URL
   *
   * The URL of the driving audio file (WAV or MP3). The avatar will be animated to match this audio.
   */
  audio_url: string;
  /**
   * Seed
   *
   * Random seed for reproducible generation.
   */
  seed?: number;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values follow the prompt more closely.
   */
  guidance_scale?: number;
  /**
   * Enable Safety Checker
   *
   * Enable safety checker for content moderation.
   */
  enable_safety_checker?: boolean;
};

/**
 * SeedanceProv15I2VVideoOutput
 */
export type BytedanceSeedanceV15ProImageToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SeedanceProv15ImageToVideoInput
 */
export type BytedanceSeedanceV15ProImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video
   */
  generate_audio?: boolean;
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Image Url
   *
   * The URL of the image used to generate video
   */
  image_url: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera Fixed
   *
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * End Image Url
   *
   * The URL of the image the video ends with. Defaults to None.
   */
  end_image_url?: string;
  /**
   * Seed
   *
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
};

/**
 * KandinskyI2VResponse
 */
export type Kandinsky5ProImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video?: File;
};

/**
 * KandinskyI2VRequest
 */
export type Kandinsky5ProImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution: 512p or 1024p.
   */
  resolution?: "512P" | "1024P";
  /**
   * Acceleration
   *
   * Acceleration level for faster generation.
   */
  acceleration?: "none" | "regular";
  /**
   * Duration
   *
   * Video duration.
   */
  duration?: "5s";
  /**
   * Num Inference Steps
   */
  num_inference_steps?: number;
  /**
   * Image Url
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string;
};

/**
 * Schema referenced but not defined by fal.ai (missing from source OpenAPI spec)
 */
export type TrajectoryPoint = {
  [key: string]: unknown;
};

/**
 * WanMoveOutput
 */
export type WanMoveOutput = {
  /**
   * Seed
   *
   * Random seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * Generated Video File
   */
  video: VideoFileType2;
};

/**
 * WANMoveInput
 */
export type WanMoveInput = {
  /**
   * Prompt
   *
   * Text prompt to guide the video generation.
   */
  prompt: string;
  /**
   * Trajectories
   *
   * A list of trajectories. Each trajectory list means the movement of one object.
   */
  trajectories: Array<Array<TrajectoryPoint>>;
  /**
   * Image Url
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to guide the video generation.
   */
  negative_prompt?: string;
};

/**
 * LTX2ImageToVideoOutput
 */
export type Ltx219bImageToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2ImageToVideoInput
 */
export type Ltx219bImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image URL
   *
   * The URL of the image to generate the video from.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Interpolation Direction
   *
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
};

/**
 * LTX2ImageToVideoOutput
 */
export type Ltx219bImageToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRAImageToVideoInput
 */
export type Ltx219bImageToVideoLoraInput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image URL
   *
   * The URL of the image to generate the video from.
   */
  image_url: string;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Interpolation Direction
   *
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.
   */
  interpolation_direction?: "forward" | "backward";
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
};

/**
 * LTX2ImageToVideoOutput
 */
export type Ltx219bDistilledImageToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2DistilledImageToVideoInput
 */
export type Ltx219bDistilledImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image URL
   *
   * The URL of the image to generate the video from.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Interpolation Direction
   *
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.
   */
  interpolation_direction?: "forward" | "backward";
};

/**
 * LTX2ImageToVideoOutput
 */
export type Ltx219bDistilledImageToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRADistilledImageToVideoInput
 */
export type Ltx219bDistilledImageToVideoLoraInput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Generate Audio
   *
   * Whether to generate audio for the video.
   */
  generate_audio?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image URL
   *
   * The URL of the image to generate the video from.
   */
  image_url: string;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Interpolation Direction
   *
   * The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.
   */
  interpolation_direction?: "forward" | "backward";
};

/**
 * ImageToVideoOutput
 *
 * Output for image-to-video generation
 */
export type V26ImageToVideoFlashOutput = {
  /**
   * Actual Prompt
   *
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * ImageToVideoInput
 *
 * Input for Wan 2.6 image-to-video generation
 */
export type V26ImageToVideoFlashInput = {
  /**
   * Prompt
   *
   * The text prompt describing the desired video motion. Max 800 characters.
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds.
   */
  duration?: "5" | "10" | "15";
  /**
   * Resolution
   *
   * Video resolution. Valid values: 720p, 1080p
   */
  resolution?: "720p" | "1080p";
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Image URL
   *
   * URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680.
   */
  image_url: string;
  /**
   * Audio Url
   *
   *
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds),
   * the audio is truncated to the first N seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   *
   */
  audio_url?: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Multi Shots
   *
   * When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.
   */
  multi_shots?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt rewriting using LLM.
   */
  enable_prompt_expansion?: boolean;
};

/**
 * Q2ProReferenceToVideoOutput
 */
export type ViduQ2ReferenceToVideoProOutput = {
  /**
   * Video
   *
   * The generated video with video/image references using the Q2 Pro model
   */
  video: File;
};

/**
 * Q2ProReferenceToVideoRequest
 */
export type ViduQ2ReferenceToVideoProInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 2000 characters
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the output video (e.g., auto, 16:9, 9:16, 1:1, or any W:H)
   */
  aspect_ratio?: string;
  /**
   * Resolution
   *
   * Output video resolution
   */
  resolution?: "540p" | "720p" | "1080p";
  /**
   * Duration
   *
   * Duration of the video in seconds (0 for automatic duration)
   */
  duration?: number;
  /**
   * Reference Video Urls
   *
   * URLs of the reference videos for video editing or motion reference. Supports up to 2 videos.
   */
  reference_video_urls?: Array<string>;
  /**
   * Bgm
   *
   * Whether to add background music to the generated video
   */
  bgm?: boolean;
  /**
   * Reference Image Urls
   *
   * URLs of the reference images for subject appearance. If videos are provided, up to 4 images are allowed; otherwise up to 7 images.
   */
  reference_image_urls?: Array<string>;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Movement Amplitude
   *
   * The movement amplitude of objects in the frame
   */
  movement_amplitude?: "auto" | "small" | "medium" | "large";
};

/**
 * I2VOutputV5_5
 */
export type PixverseV56ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequestV5_6
 */
export type PixverseV56ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds
   */
  duration?: "5" | "8" | "10";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Thinking Type
   *
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
  /**
   * Generate Audio Switch
   *
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * TransitionOutputV5_5
 */
export type PixverseV56TransitionOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * TransitionRequestV5_6
 */
export type PixverseV56TransitionInput = {
  /**
   * First Image Url
   *
   * URL of the image to use as the first frame
   */
  first_image_url: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "16:9" | "4:3" | "1:1" | "3:4" | "9:16";
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Thinking Type
   *
   * Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision
   */
  thinking_type?: "enabled" | "disabled" | "auto";
  /**
   * Prompt
   *
   * The prompt for the transition
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds
   */
  duration?: "5" | "8" | "10";
  /**
   * Generate Audio Switch
   *
   * Enable audio generation (BGM, SFX, dialogue)
   */
  generate_audio_switch?: boolean;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * End Image Url
   *
   * URL of the image to use as the last frame
   */
  end_image_url?: string;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * XAIImageToVideoOutput
 */
export type GrokImagineVideoImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video.
   */
  video: VideoFileType2;
};

/**
 * XAIImageToVideoInput
 */
export type GrokImagineVideoImageToVideoInput = {
  /**
   * Prompt
   *
   * Text description of desired changes or motion in the video.
   */
  prompt: string;
  /**
   * Duration
   *
   * Video duration in seconds.
   */
  duration?: number;
  /**
   * Resolution
   *
   * Resolution of the output video.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video.
   */
  aspect_ratio?:
    | "auto"
    | "16:9"
    | "4:3"
    | "3:2"
    | "1:1"
    | "2:3"
    | "3:4"
    | "9:16";
  /**
   * Image URL
   *
   * URL of the input image for video generation.
   */
  image_url: string;
};

/**
 * Q3ImageToVideoOutput
 */
export type ViduQ3ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video from image using the Q3 model
   */
  video: File;
};

/**
 * Q3ImageToVideoRequest
 */
export type ViduQ3ImageToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation, max 2000 characters
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: number;
  /**
   * Resolution
   *
   * Output video resolution
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Audio
   *
   * Whether to use direct audio-video generation. When true, outputs video with sound.
   */
  audio?: boolean;
  /**
   * Image Url
   *
   * URL or base64 image to use as the starting frame
   */
  image_url: string;
};

/**
 * WanI2VResponse
 */
export type WanI2vOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanI2VRequest
 */
export type WanI2vInput = {
  /**
   * Prompt
   *
   * The text prompt to guide video generation.
   */
  prompt: string;
  /**
   * Shift
   *
   * Shift parameter for video generation.
   */
  shift?: number;
  /**
   * Acceleration
   *
   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.
   */
  acceleration?: "none" | "regular";
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video. Must be between 5 to 24.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Num Frames
   *
   * Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.
   */
  num_frames?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.
   */
  resolution?: "480p" | "720p";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.
   */
  aspect_ratio?: "auto" | "16:9" | "9:16" | "1:1";
  /**
   * Image Url
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Guide Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guide_scale?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
};

/**
 * ImageToVideoV2MasterOutput
 */
export type KlingVideoV2MasterImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoV2MasterRequest
 */
export type KlingVideoV2MasterImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * URL of the image to be used for the video
   */
  image_url: string;
};

/**
 * I2VOutputV4
 */
export type PixverseV45ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequestV4
 */
export type PixverseV45ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Resolution
   *
   * The resolution of the generated video
   */
  resolution?: "360p" | "540p" | "720p" | "1080p";
  /**
   * Duration
   *
   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds
   */
  duration?: "5" | "8";
  /**
   * Style
   *
   * The style of the generated video
   */
  style?: "anime" | "3d_animation" | "clay" | "comic" | "cyberpunk";
  /**
   * Camera Movement
   *
   * The type of camera movement to apply to the video
   */
  camera_movement?:
    | "horizontal_left"
    | "horizontal_right"
    | "vertical_up"
    | "vertical_down"
    | "zoom_in"
    | "zoom_out"
    | "crane_up"
    | "quickly_zoom_in"
    | "quickly_zoom_out"
    | "smooth_zoom_in"
    | "camera_rotation"
    | "robo_arm"
    | "super_dolly_out"
    | "whip_pan"
    | "hitchcock"
    | "left_follow"
    | "right_follow"
    | "pan_left"
    | "pan_right"
    | "fix_bg";
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
  /**
   * Seed
   *
   *
   * The same seed and the same prompt given to the same version of the model
   * will output the same video every time.
   *
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt to be used for the generation
   */
  negative_prompt?: string;
};

/**
 * ImageToVideoV21StandardOutput
 */
export type KlingVideoV21StandardImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoV21StandardRequest
 */
export type KlingVideoV21StandardImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * URL of the image to be used for the video
   */
  image_url: string;
};

/**
 * ImageToVideoV21MasterOutput
 */
export type KlingVideoV21MasterImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoV21MasterRequest
 */
export type KlingVideoV21MasterImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Image Url
   *
   * URL of the image to be used for the video
   */
  image_url: string;
};

/**
 * SeedanceProI2VVideoOutput
 */
export type BytedanceSeedanceV1ProImageToVideoOutput = {
  /**
   * Seed
   *
   * Seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * Generated video file
   */
  video: File;
};

/**
 * SeedanceProImageToVideoInput
 */
export type BytedanceSeedanceV1ProImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt used to generate the video
   */
  prompt: string;
  /**
   * Resolution
   *
   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "21:9" | "16:9" | "4:3" | "1:1" | "3:4" | "9:16" | "auto";
  /**
   * Duration
   *
   * Duration of the video in seconds
   */
  duration?: "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" | "10" | "11" | "12";
  /**
   * Image Url
   *
   * The URL of the image used to generate video
   */
  image_url: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera Fixed
   *
   * Whether to fix the camera position
   */
  camera_fixed?: boolean;
  /**
   * End Image Url
   *
   * The URL of the image the video ends with. Defaults to None.
   */
  end_image_url?: string;
  /**
   * Seed
   *
   * Random seed to control video generation. Use -1 for random.
   */
  seed?: number;
};

/**
 * ImageToVideoHailuo02Output
 */
export type MinimaxHailuo02StandardImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * StandardImageToVideoHailuo02Input
 */
export type MinimaxHailuo02StandardImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution.
   */
  duration?: "6" | "10";
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Resolution
   *
   * The resolution of the generated video.
   */
  resolution?: "512P" | "768P";
  /**
   * End Image Url
   *
   * Optional URL of the image to use as the last frame of the video
   */
  end_image_url?: string;
  /**
   * Image Url
   */
  image_url: string;
};

/**
 * ImageToVideoV25ProOutput
 */
export type KlingVideoV25TurboProImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoV25ProRequest
 */
export type KlingVideoV25TurboProImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
  /**
   * Tail Image Url
   *
   * URL of the image to be used for the end of the video
   */
  tail_image_url?: string;
  /**
   * Image Url
   *
   * URL of the image to be used for the video
   */
  image_url: string;
};

/**
 * VideoOutput
 *
 * Base output for video generation
 */
export type Wan25PreviewImageToVideoOutput = {
  /**
   * Actual Prompt
   *
   * The actual prompt used if prompt rewriting was enabled
   */
  actual_prompt?: string;
  /**
   * Seed
   *
   * The seed used for generation
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file
   */
  video: VideoFileType2;
};

/**
 * ImageToVideoInput
 *
 * Input for image-to-video generation
 */
export type Wan25PreviewImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing the desired video motion. Max 800 characters.
   */
  prompt: string;
  /**
   * Duration
   *
   * Duration of the generated video in seconds. Choose between 5 or 10 seconds.
   */
  duration?: "5" | "10";
  /**
   * Resolution
   *
   * Video resolution. Valid values: 480p, 720p, 1080p
   */
  resolution?: "480p" | "720p" | "1080p";
  /**
   * Image URL
   *
   * URL of the image to use as the first frame. Must be publicly accessible or base64 data URI.
   *
   * Max file size: 25.0MB, Min width: 360px, Min height: 360px, Max width: 2000px, Max height: 2000px, Timeout: 20.0s
   */
  image_url: string;
  /**
   * Audio Url
   *
   *
   * URL of the audio to use as the background music. Must be publicly accessible.
   * Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds),
   * the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If
   * the audio is shorter than the video, the remaining part of the video will be silent.
   * For example, if the audio is 3 seconds long and the video duration is 5 seconds, the
   * first 3 seconds of the output video will have sound, and the last 2 seconds will be silent.
   * - Format: WAV, MP3.
   * - Duration: 3 to 30 s.
   * - File size: Up to 15 MB.
   *
   */
  audio_url?: string;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt rewriting using LLM.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Negative Prompt
   *
   * Negative prompt to describe content to avoid. Max 500 characters.
   */
  negative_prompt?: string;
  /**
   * Enable Safety Checker
   *
   * If set to true, the safety checker will be enabled.
   */
  enable_safety_checker?: boolean;
};

/**
 * ProImageToVideoHailuo23Output
 */
export type MinimaxHailuo23ProImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ProImageToVideoHailuo23Input
 */
export type MinimaxHailuo23ProImageToVideoInput = {
  /**
   * Prompt
   *
   * Text prompt for video generation
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * VideoOutput
 */
export type MinimaxVideo01ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoRequest
 */
export type MinimaxVideo01ImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Prompt Optimizer
   *
   * Whether to use the model's prompt optimizer
   */
  prompt_optimizer?: boolean;
  /**
   * Image Url
   *
   * URL of the image to use as the first frame
   */
  image_url: string;
};

/**
 * I2VOutput
 */
export type KlingVideoV16ProImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ProImageToVideoRequest
 */
export type KlingVideoV16ProImageToVideoInput = {
  /**
   * Prompt
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video frame
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5" | "10";
  /**
   * Tail Image Url
   *
   * URL of the image to be used for the end of the video
   */
  tail_image_url?: string;
  /**
   * Image Url
   */
  image_url: string;
  /**
   * Negative Prompt
   */
  negative_prompt?: string;
  /**
   * Cfg Scale
   *
   *
   * The CFG (Classifier Free Guidance) scale is a measure of how close you want
   * the model to stick to your prompt.
   *
   */
  cfg_scale?: number;
};

/**
 * WanProI2VResponse
 */
export type WanProImageToVideoOutput = {
  video: FileType2;
};

/**
 * WanProI2VRequest
 */
export type WanProImageToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to generate the video
   */
  prompt: string;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker
   */
  enable_safety_checker?: boolean;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number | unknown;
  /**
   * Image Url
   *
   * The URL of the image to generate the video from
   */
  image_url: string;
};

/**
 * ImageToVideoOutput
 */
export type Veo2ImageToVideoOutput = {
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * ImageToVideoInput
 */
export type Veo2ImageToVideoInput = {
  /**
   * Prompt
   *
   * The text prompt describing how the image should be animated
   */
  prompt: string;
  /**
   * Duration
   *
   * The duration of the generated video in seconds
   */
  duration?: "5s" | "6s" | "7s" | "8s";
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the generated video
   */
  aspect_ratio?: "auto" | "auto_prefer_portrait" | "16:9" | "9:16";
  /**
   * Image Url
   *
   * URL of the input image to animate. Should be 720p or higher resolution.
   */
  image_url: string;
};

/**
 * WanEffectsOutput
 */
export type WanEffectsOutput = {
  /**
   * Seed
   */
  seed: number;
  /**
   * Video
   *
   * The generated video
   */
  video: File;
};

/**
 * BaseInput
 */
export type WanEffectsInput = {
  /**
   * Effect Type
   *
   * The type of effect to apply to the video.
   */
  effect_type?:
    | "squish"
    | "muscle"
    | "inflate"
    | "crush"
    | "rotate"
    | "gun-shooting"
    | "deflate"
    | "cakeify"
    | "hulk"
    | "baby"
    | "bride"
    | "classy"
    | "puppy"
    | "snow-white"
    | "disney-princess"
    | "mona-lisa"
    | "painting"
    | "pirate-captain"
    | "princess"
    | "jungle"
    | "samurai"
    | "vip"
    | "warrior"
    | "zen"
    | "assassin"
    | "timelapse"
    | "tsunami"
    | "fire"
    | "zoom-call"
    | "doom-fps"
    | "fus-ro-dah"
    | "hug-jesus"
    | "robot-face-reveal"
    | "super-saiyan"
    | "jumpscare"
    | "laughing"
    | "cartoon-jaw-drop"
    | "crying"
    | "kissing"
    | "angry-face"
    | "selfie-younger-self"
    | "animeify"
    | "blast";
  /**
   * Aspect Ratio
   *
   * Aspect ratio of the output video.
   */
  aspect_ratio?: "16:9" | "9:16" | "1:1";
  /**
   * Subject
   *
   * The subject to insert into the predefined prompt template for the selected effect.
   */
  subject: string;
  /**
   * Lora Scale
   *
   * The scale of the LoRA weight. Used to adjust effect intensity.
   */
  lora_scale?: number;
  /**
   * Image URL
   *
   * URL of the input image.
   */
  image_url: string;
  /**
   * Turbo Mode
   *
   * Whether to use turbo mode. If True, the video will be generated faster but with lower quality.
   */
  turbo_mode?: boolean;
  /**
   * Frames Per Second
   *
   * Frames per second of the generated video.
   */
  frames_per_second?: number;
  /**
   * Num Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
  /**
   * Num Frames
   *
   * Number of frames to generate.
   */
  num_frames?: number;
};

/**
 * EchoMimicResponse
 */
export type EchomimicV3Output = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * EchoMimicRequest
 */
export type EchomimicV3Input = {
  /**
   * Prompt
   *
   * The prompt to use for the video generation.
   */
  prompt: string;
  /**
   * Audio URL
   *
   * The URL of the audio to use as a reference for the video generation.
   */
  audio_url: string;
  /**
   * Image URL
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string;
  /**
   * Guidance Scale
   *
   * The guidance scale to use for the video generation.
   */
  guidance_scale?: number;
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale to use for the video generation.
   */
  audio_guidance_scale?: number;
  /**
   * Number of frames per generation
   *
   * The number of frames to generate at once.
   */
  num_frames_per_generation?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to use for the video generation.
   */
  negative_prompt?: string;
  /**
   * Seed
   *
   * The seed to use for the video generation.
   */
  seed?: number;
};

/**
 * StableAvatarResponse
 */
export type StableAvatarOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * StableAvatarRequest
 */
export type StableAvatarInput = {
  /**
   * Prompt
   *
   * The prompt to use for the video generation.
   */
  prompt: string;
  /**
   * Aspect Ratio
   *
   * The aspect ratio of the video to generate. If 'auto', the aspect ratio will be determined by the reference image.
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16" | "auto";
  /**
   * Perturbation
   *
   * The amount of perturbation to use for the video generation. 0.0 means no perturbation, 1.0 means full perturbation.
   */
  perturbation?: number;
  /**
   * Image URL
   *
   * The URL of the image to use as a reference for the video generation.
   */
  image_url: string;
  /**
   * Guidance Scale
   *
   * The guidance scale to use for the video generation.
   */
  guidance_scale?: number;
  /**
   * Seed
   *
   * The seed to use for the video generation.
   */
  seed?: number;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use for the video generation.
   */
  num_inference_steps?: number;
  /**
   * Audio URL
   *
   * The URL of the audio to use as a reference for the video generation.
   */
  audio_url: string;
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale to use for the video generation.
   */
  audio_guidance_scale?: number;
};

/**
 * WanS2VResponse
 */
export type WanV2214bSpeechToVideoOutput = {
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * WanS2VRequest
 */
export type WanV2214bSpeechToVideoInput = {
  /**
   * Shift
   *
   * Shift value for the video. Must be between 1.0 and 10.0.
   */
  shift?: number;
  /**
   * Prompt
   *
   * The text prompt used for video generation.
   */
  prompt: string;
  /**
   * Frames per Second
   *
   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.
   */
  frames_per_second?: number;
  /**
   * Enable Safety Checker
   *
   * If set to true, input data will be checked for safety before processing.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * Number of frames to generate. Must be between 40 to 120, (must be multiple of 4).
   */
  num_frames?: number;
  /**
   * Guidance Scale
   *
   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.
   */
  guidance_scale?: number;
  /**
   * Negative Prompt
   *
   * Negative prompt for video generation.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Resolution
   *
   * Resolution of the generated video (480p, 580p, or 720p).
   */
  resolution?: "480p" | "580p" | "720p";
  /**
   * Enable Output Safety Checker
   *
   * If set to true, output video will be checked for safety after generation.
   */
  enable_output_safety_checker?: boolean;
  /**
   * Image URL
   *
   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.
   */
  image_url: string;
  /**
   * Video Quality
   *
   * The quality of the output video. Higher quality means better visual quality but larger file size.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Audio URL
   *
   * The URL of the audio file.
   */
  audio_url: string;
  /**
   * Number of Inference Steps
   *
   * Number of inference steps for sampling. Higher values give better quality but take longer.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * Random seed for reproducibility. If None, a random seed is chosen.
   */
  seed?: number;
};

/**
 * AvatarsAppOutput
 */
export type AvatarsAudioToVideoOutputType2 = {
  video: FileType2;
};

/**
 * InferenceResult
 */
export type AvatarsAudioToVideoOutput = {
  /**
   * Moderation Transcription
   */
  moderation_transcription?: string | unknown;
  /**
   * Moderation Error
   */
  moderation_error?: string | unknown;
  /**
   * Moderation Flagged
   */
  moderation_flagged?: boolean;
  video?: Video | unknown;
};

/**
 * Audio2VideoInput
 */
export type AvatarsAudioToVideoInputType2 = {
  /**
   * Audio Url
   */
  audio_url: string;
  /**
   * Avatar Id
   *
   * The avatar to use for the video
   */
  avatar_id:
    | "emily_vertical_primary"
    | "emily_vertical_secondary"
    | "marcus_vertical_primary"
    | "marcus_vertical_secondary"
    | "mira_vertical_primary"
    | "mira_vertical_secondary"
    | "jasmine_vertical_primary"
    | "jasmine_vertical_secondary"
    | "jasmine_vertical_walking"
    | "aisha_vertical_walking"
    | "elena_vertical_primary"
    | "elena_vertical_secondary"
    | "any_male_vertical_primary"
    | "any_female_vertical_primary"
    | "any_male_vertical_secondary"
    | "any_female_vertical_secondary"
    | "any_female_vertical_walking"
    | "emily_primary"
    | "emily_side"
    | "marcus_primary"
    | "marcus_side"
    | "aisha_walking"
    | "elena_primary"
    | "elena_side"
    | "any_male_primary"
    | "any_female_primary"
    | "any_male_side"
    | "any_female_side";
};

/**
 * InferenceRequest
 */
export type AvatarsAudioToVideoInput = {
  /**
   * Avatar
   */
  avatar:
    | "Mia outdoor (UGC)"
    | "Lara (Masterclass)"
    | "Ines (UGC)"
    | "Maria (Masterclass)"
    | "Emma (UGC)"
    | "Sienna (Masterclass)"
    | "Elena (UGC)"
    | "Jasmine (Masterclass)"
    | "Amara (Masterclass)"
    | "Ryan podcast (UGC)"
    | "Tyler (Masterclass)"
    | "Jayse (Masterclass)"
    | "Paul (Masterclass)"
    | "Matteo (UGC)"
    | "Daniel car (UGC)"
    | "Dario (Masterclass)"
    | "Viva (Masterclass)"
    | "Chen (Masterclass)"
    | "Alex (Masterclass)"
    | "Vanessa (UGC)"
    | "Laurent (UGC)"
    | "Noemie car (UGC)"
    | "Brandon (UGC)"
    | "Byron (Masterclass)"
    | "Calista (Masterclass)"
    | "Milo (Masterclass)"
    | "Fabien (Masterclass)"
    | "Rose (UGC)";
  /**
   * Remove Background
   *
   * Enabling the remove background feature will result in a 50% increase in the price.
   */
  remove_background?: boolean;
  /**
   * Audio Url
   */
  audio_url: string;
};

/**
 * AudioToVideoResponse
 *
 * Response model for audio-to-video generation (no reference image).
 */
export type LongcatSingleAvatarAudioToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * AudioToVideoRequest
 *
 * Request model for audio-to-video generation.
 */
export type LongcatSingleAvatarAudioToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt?: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.
   */
  resolution?: "480p" | "720p";
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale. Higher values may lead to exaggerated mouth movements.
   */
  audio_guidance_scale?: number;
  /**
   * Number of Segments
   *
   * Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.
   */
  num_segments?: number;
  /**
   * Audio URL
   *
   * The URL of the audio file to drive the avatar.
   */
  audio_url: string;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to avoid in the video generation.
   */
  negative_prompt?: string;
  /**
   * Text Guidance Scale
   *
   * The text guidance scale for classifier-free guidance.
   */
  text_guidance_scale?: number;
};

/**
 * ImageAudioToVideoResponse
 *
 * Response model for image+audio to video generation.
 */
export type LongcatSingleAvatarImageAudioToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * ImageAudioToVideoRequest
 *
 * Request model for image+audio to video generation.
 */
export type LongcatSingleAvatarImageAudioToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt: string;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.
   */
  resolution?: "480p" | "720p";
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale. Higher values may lead to exaggerated mouth movements.
   */
  audio_guidance_scale?: number;
  /**
   * Number of Segments
   *
   * Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.
   */
  num_segments?: number;
  /**
   * Image URL
   *
   * The URL of the image to animate.
   */
  image_url: string;
  /**
   * Audio URL
   *
   * The URL of the audio file to drive the avatar.
   */
  audio_url: string;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Negative Prompt
   *
   * The negative prompt to avoid in the video generation.
   */
  negative_prompt?: string;
  /**
   * Text Guidance Scale
   *
   * The text guidance scale for classifier-free guidance.
   */
  text_guidance_scale?: number;
};

/**
 * BoundingBox
 */
export type BoundingBox = {
  /**
   * Y
   *
   * Y-coordinate of the top-left corner
   */
  y: number;
  /**
   * X
   *
   * X-coordinate of the top-left corner
   */
  x: number;
  /**
   * H
   *
   * Height of the bounding box
   */
  h: number;
  /**
   * W
   *
   * Width of the bounding box
   */
  w: number;
  /**
   * Label
   *
   * Label of the bounding box
   */
  label: string;
};

/**
 * MultiSpeakerImageAudioToVideoResponse
 *
 * Response model for multi-speaker image+audio to video generation.
 */
export type LongcatMultiAvatarImageAudioToVideoOutput = {
  /**
   * Seed
   *
   * The seed used for generation.
   */
  seed: number;
  /**
   * Video
   *
   * The generated video file.
   */
  video: File;
};

/**
 * MultiSpeakerImageAudioToVideoRequest
 *
 * Request model for multi-speaker image+audio to video generation.
 */
export type LongcatMultiAvatarImageAudioToVideoInput = {
  /**
   * Prompt
   *
   * The prompt to guide the video generation.
   */
  prompt?: string;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * Audio URL Person 2
   *
   * The URL of the audio file for person 2 (right side).
   */
  audio_url_person2?: string;
  /**
   * Enable Safety Checker
   *
   * Whether to enable safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Bbox Person1
   *
   * Bounding box for person 1. If not provided, defaults to left half of image.
   */
  bbox_person1?: BoundingBox;
  /**
   * Negative Prompt
   *
   * The negative prompt to avoid in the video generation.
   */
  negative_prompt?: string;
  /**
   * Text Guidance Scale
   *
   * The text guidance scale for classifier-free guidance.
   */
  text_guidance_scale?: number;
  /**
   * Resolution
   *
   * Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.
   */
  resolution?: "480p" | "720p";
  /**
   * Audio Type
   *
   * How to combine the two audio tracks. 'para' (parallel) plays both simultaneously, 'add' (sequential) plays person 1 first then person 2.
   */
  audio_type?: "para" | "add";
  /**
   * Image URL
   *
   * The URL of the image containing two speakers.
   */
  image_url: string;
  /**
   * Audio URL Person 1
   *
   * The URL of the audio file for person 1 (left side).
   */
  audio_url_person1?: string;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number;
  /**
   * Audio Guidance Scale
   *
   * The audio guidance scale. Higher values may lead to exaggerated mouth movements.
   */
  audio_guidance_scale?: number;
  /**
   * Bbox Person2
   *
   * Bounding box for person 2. If not provided, defaults to right half of image.
   */
  bbox_person2?: BoundingBox;
  /**
   * Number of Segments
   *
   * Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.
   */
  num_segments?: number;
};

/**
 * DubbingVideoOutput
 */
export type ElevenlabsDubbingOutput = {
  /**
   * Target Lang
   *
   * The target language of the dubbed content
   */
  target_lang: string;
  video: FileType2;
};

/**
 * DubbingRequest
 */
export type ElevenlabsDubbingInput = {
  /**
   * Video Url
   *
   * URL of the video file to dub. Either audio_url or video_url must be provided. If both are provided, video_url takes priority.
   */
  video_url?: string | unknown;
  /**
   * Highest Resolution
   *
   * Whether to use the highest resolution for dubbing.
   */
  highest_resolution?: boolean;
  /**
   * Audio Url
   *
   * URL of the audio file to dub. Either audio_url or video_url must be provided.
   */
  audio_url?: string | unknown;
  /**
   * Target Lang
   *
   * Target language code for dubbing (ISO 639-1)
   */
  target_lang: string;
  /**
   * Num Speakers
   *
   * Number of speakers in the audio. If not provided, will be auto-detected.
   */
  num_speakers?: number | unknown;
  /**
   * Source Lang
   *
   * Source language code. If not provided, will be auto-detected.
   */
  source_lang?: string | unknown;
};

/**
 * LTX2AudioToVideoOutput
 */
export type Ltx219bAudioToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2AudioToVideoInput
 */
export type Ltx219bAudioToVideoInput = {
  /**
   * Match Audio Length
   *
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
   */
  match_audio_length?: boolean;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Preprocess Audio
   *
   * Whether to preprocess the audio before using it as conditioning.
   */
  preprocess_audio?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image URL
   *
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
   */
  audio_strength?: number;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Audio URL
   *
   * The URL of the audio to generate the video from.
   */
  audio_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
};

/**
 * LTX2AudioToVideoOutput
 */
export type Ltx219bDistilledAudioToVideoOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2DistilledAudioToVideoInput
 */
export type Ltx219bDistilledAudioToVideoInput = {
  /**
   * Match Audio Length
   *
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
   */
  match_audio_length?: boolean;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Preprocess Audio
   *
   * Whether to preprocess the audio before using it as conditioning.
   */
  preprocess_audio?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image URL
   *
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
   */
  audio_strength?: number;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Audio URL
   *
   * The URL of the audio to generate the video from.
   */
  audio_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
};

/**
 * LTX2AudioToVideoOutput
 */
export type Ltx219bAudioToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRAAudioToVideoInput
 */
export type Ltx219bAudioToVideoLoraInput = {
  /**
   * Match Audio Length
   *
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
   */
  match_audio_length?: boolean;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * Number of Inference Steps
   *
   * The number of inference steps to use.
   */
  num_inference_steps?: number;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Guidance Scale
   *
   * The guidance scale to use.
   */
  guidance_scale?: number;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Preprocess Audio
   *
   * Whether to preprocess the audio before using it as conditioning.
   */
  preprocess_audio?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image URL
   *
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
   */
  audio_strength?: number;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Audio URL
   *
   * The URL of the audio to generate the video from.
   */
  audio_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
};

/**
 * LTX2AudioToVideoOutput
 */
export type Ltx219bDistilledAudioToVideoLoraOutput = {
  /**
   * Prompt
   *
   * The prompt used for the generation.
   */
  prompt: string;
  /**
   * Seed
   *
   * The seed used for the random number generator.
   */
  seed: number;
  video: VideoFile;
};

/**
 * LTX2LoRADistilledAudioToVideoInput
 */
export type Ltx219bDistilledAudioToVideoLoraInput = {
  /**
   * Match Audio Length
   *
   * When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.
   */
  match_audio_length?: boolean;
  /**
   * Prompt
   *
   * The prompt to generate the video from.
   */
  prompt: string;
  /**
   * Acceleration
   *
   * The acceleration level to use.
   */
  acceleration?: "none" | "regular" | "high" | "full";
  /**
   * Use Multi-Scale
   *
   * Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.
   */
  use_multiscale?: boolean;
  /**
   * FPS
   *
   * The frames per second of the generated video.
   */
  fps?: number;
  /**
   * LoRAs
   *
   * The LoRAs to use for the generation.
   */
  loras: Array<LoRaInput>;
  /**
   * Camera LoRA
   *
   * The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora?:
    | "dolly_in"
    | "dolly_out"
    | "dolly_left"
    | "dolly_right"
    | "jib_up"
    | "jib_down"
    | "static"
    | "none";
  /**
   * Video Size
   *
   * The size of the generated video. Use 'auto' to match the input image dimensions if provided.
   */
  video_size?:
    | ImageSize
    | "auto"
    | "square_hd"
    | "square"
    | "portrait_4_3"
    | "portrait_16_9"
    | "landscape_4_3"
    | "landscape_16_9";
  /**
   * Enable Safety Checker
   *
   * Whether to enable the safety checker.
   */
  enable_safety_checker?: boolean;
  /**
   * Camera LoRA Scale
   *
   * The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.
   */
  camera_lora_scale?: number;
  /**
   * Image Strength
   *
   * The strength of the image to use for the video generation.
   */
  image_strength?: number;
  /**
   * Preprocess Audio
   *
   * Whether to preprocess the audio before using it as conditioning.
   */
  preprocess_audio?: boolean;
  /**
   * Negative Prompt
   *
   * The negative prompt to generate the video from.
   */
  negative_prompt?: string;
  /**
   * Video Write Mode
   *
   * The write mode of the generated video.
   */
  video_write_mode?: "fast" | "balanced" | "small";
  /**
   * Video Output Type
   *
   * The output type of the generated video.
   */
  video_output_type?:
    | "X264 (.mp4)"
    | "VP9 (.webm)"
    | "PRORES4444 (.mov)"
    | "GIF (.gif)";
  /**
   * End Image URL
   *
   * The URL of the image to use as the end of the video.
   */
  end_image_url?: string | unknown;
  /**
   * Number of Frames
   *
   * The number of frames to generate.
   */
  num_frames?: number;
  /**
   * Image URL
   *
   * Optional URL of an image to use as the first frame of the video.
   */
  image_url?: string | unknown;
  /**
   * Sync Mode
   *
   * If `True`, the media will be returned as a data URI and the output data won't be available in the request history.
   */
  sync_mode?: boolean;
  /**
   * Video Quality
   *
   * The quality of the generated video.
   */
  video_quality?: "low" | "medium" | "high" | "maximum";
  /**
   * Enable Prompt Expansion
   *
   * Whether to enable prompt expansion.
   */
  enable_prompt_expansion?: boolean;
  /**
   * Audio Strength
   *
   * Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.
   */
  audio_strength?: number;
  /**
   * End Image Strength
   *
   * The strength of the end image to use for the video generation.
   */
  end_image_strength?: number;
  /**
   * Audio URL
   *
   * The URL of the audio to generate the video from.
   */
  audio_url: string;
  /**
   * Seed
   *
   * The seed for the random number generator.
   */
  seed?: number | unknown;
};

export type QueueStatus = {
  status: "IN_QUEUE" | "IN_PROGRESS" | "COMPLETED";
  /**
   * The request id.
   */
  request_id: string;
  /**
   * The response url.
   */
  response_url?: string;
  /**
   * The status url.
   */
  status_url?: string;
  /**
   * The cancel url.
   */
  cancel_url?: string;
  /**
   * The logs.
   */
  logs?: {
    [key: string]: unknown;
  };
  /**
   * The metrics.
   */
  metrics?: {
    [key: string]: unknown;
  };
  /**
   * The queue position.
   */
  queue_position?: number;
};

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-2-19b/distilled/audio-to-video/lora/requests/{request_id}/status";
  };

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-2-19b/distilled/audio-to-video/lora/requests/{request_id}/cancel";
  };

export type PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledAudioToVideoLoraData = {
  body: Ltx219bDistilledAudioToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/audio-to-video/lora";
};

export type PostFalAiLtx219bDistilledAudioToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledAudioToVideoLoraResponse =
  PostFalAiLtx219bDistilledAudioToVideoLoraResponses[keyof PostFalAiLtx219bDistilledAudioToVideoLoraResponses];

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/audio-to-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: Ltx219bDistilledAudioToVideoLoraOutput;
  };

export type GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledAudioToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/audio-to-video/lora/requests/{request_id}/status";
};

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/audio-to-video/lora/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bAudioToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bAudioToVideoLoraData = {
  body: Ltx219bAudioToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/audio-to-video/lora";
};

export type PostFalAiLtx219bAudioToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bAudioToVideoLoraResponse =
  PostFalAiLtx219bAudioToVideoLoraResponses[keyof PostFalAiLtx219bAudioToVideoLoraResponses];

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/audio-to-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bAudioToVideoLoraOutput;
};

export type GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bAudioToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-2-19b/distilled/audio-to-video/requests/{request_id}/status";
  };

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-2-19b/distilled/audio-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledAudioToVideoData = {
  body: Ltx219bDistilledAudioToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/audio-to-video";
};

export type PostFalAiLtx219bDistilledAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledAudioToVideoResponse =
  PostFalAiLtx219bDistilledAudioToVideoResponses[keyof PostFalAiLtx219bDistilledAudioToVideoResponses];

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/audio-to-video/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bDistilledAudioToVideoOutput;
};

export type GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledAudioToVideoRequestsByRequestIdResponses];

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/audio-to-video/requests/{request_id}/status";
};

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bAudioToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/audio-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bAudioToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bAudioToVideoData = {
  body: Ltx219bAudioToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/audio-to-video";
};

export type PostFalAiLtx219bAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bAudioToVideoResponse =
  PostFalAiLtx219bAudioToVideoResponses[keyof PostFalAiLtx219bAudioToVideoResponses];

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/audio-to-video/requests/{request_id}";
};

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bAudioToVideoOutput;
};

export type GetFalAiLtx219bAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bAudioToVideoRequestsByRequestIdResponses];

export type GetFalAiElevenlabsDubbingRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/elevenlabs/dubbing/requests/{request_id}/status";
};

export type GetFalAiElevenlabsDubbingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiElevenlabsDubbingRequestsByRequestIdStatusResponse =
  GetFalAiElevenlabsDubbingRequestsByRequestIdStatusResponses[keyof GetFalAiElevenlabsDubbingRequestsByRequestIdStatusResponses];

export type PutFalAiElevenlabsDubbingRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/elevenlabs/dubbing/requests/{request_id}/cancel";
};

export type PutFalAiElevenlabsDubbingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiElevenlabsDubbingRequestsByRequestIdCancelResponse =
  PutFalAiElevenlabsDubbingRequestsByRequestIdCancelResponses[keyof PutFalAiElevenlabsDubbingRequestsByRequestIdCancelResponses];

export type PostFalAiElevenlabsDubbingData = {
  body: ElevenlabsDubbingInput;
  path?: never;
  query?: never;
  url: "/fal-ai/elevenlabs/dubbing";
};

export type PostFalAiElevenlabsDubbingResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiElevenlabsDubbingResponse =
  PostFalAiElevenlabsDubbingResponses[keyof PostFalAiElevenlabsDubbingResponses];

export type GetFalAiElevenlabsDubbingRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/elevenlabs/dubbing/requests/{request_id}";
};

export type GetFalAiElevenlabsDubbingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ElevenlabsDubbingOutput;
};

export type GetFalAiElevenlabsDubbingRequestsByRequestIdResponse =
  GetFalAiElevenlabsDubbingRequestsByRequestIdResponses[keyof GetFalAiElevenlabsDubbingRequestsByRequestIdResponses];

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/longcat-multi-avatar/image-audio-to-video/requests/{request_id}/status";
  };

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-multi-avatar/image-audio-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatMultiAvatarImageAudioToVideoData = {
  body: LongcatMultiAvatarImageAudioToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-multi-avatar/image-audio-to-video";
};

export type PostFalAiLongcatMultiAvatarImageAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatMultiAvatarImageAudioToVideoResponse =
  PostFalAiLongcatMultiAvatarImageAudioToVideoResponses[keyof PostFalAiLongcatMultiAvatarImageAudioToVideoResponses];

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-multi-avatar/image-audio-to-video/requests/{request_id}";
  };

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LongcatMultiAvatarImageAudioToVideoOutput;
  };

export type GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLongcatMultiAvatarImageAudioToVideoRequestsByRequestIdResponses];

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/longcat-single-avatar/image-audio-to-video/requests/{request_id}/status";
  };

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-single-avatar/image-audio-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatSingleAvatarImageAudioToVideoData = {
  body: LongcatSingleAvatarImageAudioToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-single-avatar/image-audio-to-video";
};

export type PostFalAiLongcatSingleAvatarImageAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatSingleAvatarImageAudioToVideoResponse =
  PostFalAiLongcatSingleAvatarImageAudioToVideoResponses[keyof PostFalAiLongcatSingleAvatarImageAudioToVideoResponses];

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-single-avatar/image-audio-to-video/requests/{request_id}";
  };

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LongcatSingleAvatarImageAudioToVideoOutput;
  };

export type GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLongcatSingleAvatarImageAudioToVideoRequestsByRequestIdResponses];

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/longcat-single-avatar/audio-to-video/requests/{request_id}/status";
  };

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-single-avatar/audio-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatSingleAvatarAudioToVideoData = {
  body: LongcatSingleAvatarAudioToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-single-avatar/audio-to-video";
};

export type PostFalAiLongcatSingleAvatarAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatSingleAvatarAudioToVideoResponse =
  PostFalAiLongcatSingleAvatarAudioToVideoResponses[keyof PostFalAiLongcatSingleAvatarAudioToVideoResponses];

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/longcat-single-avatar/audio-to-video/requests/{request_id}";
};

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LongcatSingleAvatarAudioToVideoOutput;
  };

export type GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdResponse =
  GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiLongcatSingleAvatarAudioToVideoRequestsByRequestIdResponses];

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/argil/avatars/audio-to-video/requests/{request_id}/status";
};

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusResponse =
  GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusResponses[keyof GetArgilAvatarsAudioToVideoRequestsByRequestIdStatusResponses];

export type PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/argil/avatars/audio-to-video/requests/{request_id}/cancel";
};

export type PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelResponse =
  PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelResponses[keyof PutArgilAvatarsAudioToVideoRequestsByRequestIdCancelResponses];

export type PostArgilAvatarsAudioToVideoData = {
  body: AvatarsAudioToVideoInput;
  path?: never;
  query?: never;
  url: "/argil/avatars/audio-to-video";
};

export type PostArgilAvatarsAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostArgilAvatarsAudioToVideoResponse =
  PostArgilAvatarsAudioToVideoResponses[keyof PostArgilAvatarsAudioToVideoResponses];

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/argil/avatars/audio-to-video/requests/{request_id}";
};

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AvatarsAudioToVideoOutput;
};

export type GetArgilAvatarsAudioToVideoRequestsByRequestIdResponse =
  GetArgilAvatarsAudioToVideoRequestsByRequestIdResponses[keyof GetArgilAvatarsAudioToVideoRequestsByRequestIdResponses];

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-14b/speech-to-video/requests/{request_id}/status";
};

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-14b/speech-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanV2214bSpeechToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWanV2214bSpeechToVideoData = {
  body: WanV2214bSpeechToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-14b/speech-to-video";
};

export type PostFalAiWanV2214bSpeechToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV2214bSpeechToVideoResponse =
  PostFalAiWanV2214bSpeechToVideoResponses[keyof PostFalAiWanV2214bSpeechToVideoResponses];

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-14b/speech-to-video/requests/{request_id}";
};

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV2214bSpeechToVideoOutput;
};

export type GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdResponse =
  GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdResponses[keyof GetFalAiWanV2214bSpeechToVideoRequestsByRequestIdResponses];

export type GetFalAiStableAvatarRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/stable-avatar/requests/{request_id}/status";
};

export type GetFalAiStableAvatarRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiStableAvatarRequestsByRequestIdStatusResponse =
  GetFalAiStableAvatarRequestsByRequestIdStatusResponses[keyof GetFalAiStableAvatarRequestsByRequestIdStatusResponses];

export type PutFalAiStableAvatarRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/stable-avatar/requests/{request_id}/cancel";
};

export type PutFalAiStableAvatarRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiStableAvatarRequestsByRequestIdCancelResponse =
  PutFalAiStableAvatarRequestsByRequestIdCancelResponses[keyof PutFalAiStableAvatarRequestsByRequestIdCancelResponses];

export type PostFalAiStableAvatarData = {
  body: StableAvatarInput;
  path?: never;
  query?: never;
  url: "/fal-ai/stable-avatar";
};

export type PostFalAiStableAvatarResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiStableAvatarResponse =
  PostFalAiStableAvatarResponses[keyof PostFalAiStableAvatarResponses];

export type GetFalAiStableAvatarRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/stable-avatar/requests/{request_id}";
};

export type GetFalAiStableAvatarRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: StableAvatarOutput;
};

export type GetFalAiStableAvatarRequestsByRequestIdResponse =
  GetFalAiStableAvatarRequestsByRequestIdResponses[keyof GetFalAiStableAvatarRequestsByRequestIdResponses];

export type GetFalAiEchomimicV3RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/echomimic-v3/requests/{request_id}/status";
};

export type GetFalAiEchomimicV3RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiEchomimicV3RequestsByRequestIdStatusResponse =
  GetFalAiEchomimicV3RequestsByRequestIdStatusResponses[keyof GetFalAiEchomimicV3RequestsByRequestIdStatusResponses];

export type PutFalAiEchomimicV3RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/echomimic-v3/requests/{request_id}/cancel";
};

export type PutFalAiEchomimicV3RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiEchomimicV3RequestsByRequestIdCancelResponse =
  PutFalAiEchomimicV3RequestsByRequestIdCancelResponses[keyof PutFalAiEchomimicV3RequestsByRequestIdCancelResponses];

export type PostFalAiEchomimicV3Data = {
  body: EchomimicV3Input;
  path?: never;
  query?: never;
  url: "/fal-ai/echomimic-v3";
};

export type PostFalAiEchomimicV3Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiEchomimicV3Response =
  PostFalAiEchomimicV3Responses[keyof PostFalAiEchomimicV3Responses];

export type GetFalAiEchomimicV3RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/echomimic-v3/requests/{request_id}";
};

export type GetFalAiEchomimicV3RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: EchomimicV3Output;
};

export type GetFalAiEchomimicV3RequestsByRequestIdResponse =
  GetFalAiEchomimicV3RequestsByRequestIdResponses[keyof GetFalAiEchomimicV3RequestsByRequestIdResponses];

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/veed/avatars/audio-to-video/requests/{request_id}/status";
};

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusResponse =
  GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusResponses[keyof GetVeedAvatarsAudioToVideoRequestsByRequestIdStatusResponses];

export type PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/avatars/audio-to-video/requests/{request_id}/cancel";
};

export type PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelResponse =
  PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelResponses[keyof PutVeedAvatarsAudioToVideoRequestsByRequestIdCancelResponses];

export type PostVeedAvatarsAudioToVideoData = {
  body: AvatarsAudioToVideoInputType2;
  path?: never;
  query?: never;
  url: "/veed/avatars/audio-to-video";
};

export type PostVeedAvatarsAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostVeedAvatarsAudioToVideoResponse =
  PostVeedAvatarsAudioToVideoResponses[keyof PostVeedAvatarsAudioToVideoResponses];

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/avatars/audio-to-video/requests/{request_id}";
};

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AvatarsAudioToVideoOutputType2;
};

export type GetVeedAvatarsAudioToVideoRequestsByRequestIdResponse =
  GetVeedAvatarsAudioToVideoRequestsByRequestIdResponses[keyof GetVeedAvatarsAudioToVideoRequestsByRequestIdResponses];

export type GetFalAiWanEffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-effects/requests/{request_id}/status";
};

export type GetFalAiWanEffectsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanEffectsRequestsByRequestIdStatusResponse =
  GetFalAiWanEffectsRequestsByRequestIdStatusResponses[keyof GetFalAiWanEffectsRequestsByRequestIdStatusResponses];

export type PutFalAiWanEffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-effects/requests/{request_id}/cancel";
};

export type PutFalAiWanEffectsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanEffectsRequestsByRequestIdCancelResponse =
  PutFalAiWanEffectsRequestsByRequestIdCancelResponses[keyof PutFalAiWanEffectsRequestsByRequestIdCancelResponses];

export type PostFalAiWanEffectsData = {
  body: WanEffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-effects";
};

export type PostFalAiWanEffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanEffectsResponse =
  PostFalAiWanEffectsResponses[keyof PostFalAiWanEffectsResponses];

export type GetFalAiWanEffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-effects/requests/{request_id}";
};

export type GetFalAiWanEffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanEffectsOutput;
};

export type GetFalAiWanEffectsRequestsByRequestIdResponse =
  GetFalAiWanEffectsRequestsByRequestIdResponses[keyof GetFalAiWanEffectsRequestsByRequestIdResponses];

export type GetFalAiVeo2ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo2/image-to-video/requests/{request_id}/status";
};

export type GetFalAiVeo2ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo2ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo2ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo2ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo2ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo2/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiVeo2ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo2ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo2ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo2ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo2ImageToVideoData = {
  body: Veo2ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo2/image-to-video";
};

export type PostFalAiVeo2ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo2ImageToVideoResponse =
  PostFalAiVeo2ImageToVideoResponses[keyof PostFalAiVeo2ImageToVideoResponses];

export type GetFalAiVeo2ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo2/image-to-video/requests/{request_id}";
};

export type GetFalAiVeo2ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo2ImageToVideoOutput;
};

export type GetFalAiVeo2ImageToVideoRequestsByRequestIdResponse =
  GetFalAiVeo2ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiVeo2ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiWanProImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-pro/image-to-video/requests/{request_id}/status";
};

export type GetFalAiWanProImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWanProImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-pro/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWanProImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWanProImageToVideoData = {
  body: WanProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-pro/image-to-video";
};

export type PostFalAiWanProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanProImageToVideoResponse =
  PostFalAiWanProImageToVideoResponses[keyof PostFalAiWanProImageToVideoResponses];

export type GetFalAiWanProImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-pro/image-to-video/requests/{request_id}";
};

export type GetFalAiWanProImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanProImageToVideoOutput;
};

export type GetFalAiWanProImageToVideoRequestsByRequestIdResponse =
  GetFalAiWanProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiWanProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v1.6/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v1.6/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV16ProImageToVideoData = {
  body: KlingVideoV16ProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/image-to-video";
};

export type PostFalAiKlingVideoV16ProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV16ProImageToVideoResponse =
  PostFalAiKlingVideoV16ProImageToVideoResponses[keyof PostFalAiKlingVideoV16ProImageToVideoResponses];

export type GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV16ProImageToVideoOutput;
};

export type GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV16ProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/minimax/video-01/image-to-video/requests/{request_id}/status";
};

export type GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxVideo01ImageToVideoData = {
  body: MinimaxVideo01ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/video-01/image-to-video";
};

export type PostFalAiMinimaxVideo01ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxVideo01ImageToVideoResponse =
  PostFalAiMinimaxVideo01ImageToVideoResponses[keyof PostFalAiMinimaxVideo01ImageToVideoResponses];

export type GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01/image-to-video/requests/{request_id}";
};

export type GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MinimaxVideo01ImageToVideoOutput;
};

export type GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxVideo01ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-2.3/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo23ProImageToVideoData = {
  body: MinimaxHailuo23ProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-2.3/pro/image-to-video";
};

export type PostFalAiMinimaxHailuo23ProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo23ProImageToVideoResponse =
  PostFalAiMinimaxHailuo23ProImageToVideoResponses[keyof PostFalAiMinimaxHailuo23ProImageToVideoResponses];

export type GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/hailuo-2.3/pro/image-to-video/requests/{request_id}";
};

export type GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo23ProImageToVideoOutput;
  };

export type GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo23ProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiWan25PreviewImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-25-preview/image-to-video/requests/{request_id}/status";
};

export type GetFalAiWan25PreviewImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWan25PreviewImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWan25PreviewImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWan25PreviewImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWan25PreviewImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-25-preview/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWan25PreviewImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWan25PreviewImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWan25PreviewImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWan25PreviewImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWan25PreviewImageToVideoData = {
  body: Wan25PreviewImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-25-preview/image-to-video";
};

export type PostFalAiWan25PreviewImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWan25PreviewImageToVideoResponse =
  PostFalAiWan25PreviewImageToVideoResponses[keyof PostFalAiWan25PreviewImageToVideoResponses];

export type GetFalAiWan25PreviewImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-25-preview/image-to-video/requests/{request_id}";
};

export type GetFalAiWan25PreviewImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Wan25PreviewImageToVideoOutput;
};

export type GetFalAiWan25PreviewImageToVideoRequestsByRequestIdResponse =
  GetFalAiWan25PreviewImageToVideoRequestsByRequestIdResponses[keyof GetFalAiWan25PreviewImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.5-turbo/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.5-turbo/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV25TurboProImageToVideoData = {
  body: KlingVideoV25TurboProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.5-turbo/pro/image-to-video";
};

export type PostFalAiKlingVideoV25TurboProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV25TurboProImageToVideoResponse =
  PostFalAiKlingVideoV25TurboProImageToVideoResponses[keyof PostFalAiKlingVideoV25TurboProImageToVideoResponses];

export type GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.5-turbo/pro/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV25TurboProImageToVideoOutput;
  };

export type GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV25TurboProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-02/standard/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-02/standard/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo02StandardImageToVideoData = {
  body: MinimaxHailuo02StandardImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-02/standard/image-to-video";
};

export type PostFalAiMinimaxHailuo02StandardImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo02StandardImageToVideoResponse =
  PostFalAiMinimaxHailuo02StandardImageToVideoResponses[keyof PostFalAiMinimaxHailuo02StandardImageToVideoResponses];

export type GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-02/standard/image-to-video/requests/{request_id}";
  };

export type GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo02StandardImageToVideoOutput;
  };

export type GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo02StandardImageToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance/seedance/v1/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceSeedanceV1ProImageToVideoData = {
  body: BytedanceSeedanceV1ProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1/pro/image-to-video";
};

export type PostFalAiBytedanceSeedanceV1ProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceSeedanceV1ProImageToVideoResponse =
  PostFalAiBytedanceSeedanceV1ProImageToVideoResponses[keyof PostFalAiBytedanceSeedanceV1ProImageToVideoResponses];

export type GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/pro/image-to-video/requests/{request_id}";
  };

export type GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceSeedanceV1ProImageToVideoOutput;
  };

export type GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceSeedanceV1ProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.1/master/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.1/master/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV21MasterImageToVideoData = {
  body: KlingVideoV21MasterImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.1/master/image-to-video";
};

export type PostFalAiKlingVideoV21MasterImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV21MasterImageToVideoResponse =
  PostFalAiKlingVideoV21MasterImageToVideoResponses[keyof PostFalAiKlingVideoV21MasterImageToVideoResponses];

export type GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.1/master/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV21MasterImageToVideoOutput;
  };

export type GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV21MasterImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.1/standard/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.1/standard/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV21StandardImageToVideoData = {
  body: KlingVideoV21StandardImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.1/standard/image-to-video";
};

export type PostFalAiKlingVideoV21StandardImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV21StandardImageToVideoResponse =
  PostFalAiKlingVideoV21StandardImageToVideoResponses[keyof PostFalAiKlingVideoV21StandardImageToVideoResponses];

export type GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.1/standard/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV21StandardImageToVideoOutput;
  };

export type GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV21StandardImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV45ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4.5/image-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV45ImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV45ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV45ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV45ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV45ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV45ImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV45ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV45ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV45ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV45ImageToVideoData = {
  body: PixverseV45ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4.5/image-to-video";
};

export type PostFalAiPixverseV45ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV45ImageToVideoResponse =
  PostFalAiPixverseV45ImageToVideoResponses[keyof PostFalAiPixverseV45ImageToVideoResponses];

export type GetFalAiPixverseV45ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/image-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV45ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV45ImageToVideoOutput;
};

export type GetFalAiPixverseV45ImageToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV45ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV45ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2/master/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2/master/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV2MasterImageToVideoData = {
  body: KlingVideoV2MasterImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2/master/image-to-video";
};

export type PostFalAiKlingVideoV2MasterImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV2MasterImageToVideoResponse =
  PostFalAiKlingVideoV2MasterImageToVideoResponses[keyof PostFalAiKlingVideoV2MasterImageToVideoResponses];

export type GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2/master/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV2MasterImageToVideoOutput;
  };

export type GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV2MasterImageToVideoRequestsByRequestIdResponses];

export type GetFalAiWanI2vRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-i2v/requests/{request_id}/status";
};

export type GetFalAiWanI2vRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanI2vRequestsByRequestIdStatusResponse =
  GetFalAiWanI2vRequestsByRequestIdStatusResponses[keyof GetFalAiWanI2vRequestsByRequestIdStatusResponses];

export type PutFalAiWanI2vRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-i2v/requests/{request_id}/cancel";
};

export type PutFalAiWanI2vRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanI2vRequestsByRequestIdCancelResponse =
  PutFalAiWanI2vRequestsByRequestIdCancelResponses[keyof PutFalAiWanI2vRequestsByRequestIdCancelResponses];

export type PostFalAiWanI2vData = {
  body: WanI2vInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-i2v";
};

export type PostFalAiWanI2vResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanI2vResponse =
  PostFalAiWanI2vResponses[keyof PostFalAiWanI2vResponses];

export type GetFalAiWanI2vRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-i2v/requests/{request_id}";
};

export type GetFalAiWanI2vRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanI2vOutput;
};

export type GetFalAiWanI2vRequestsByRequestIdResponse =
  GetFalAiWanI2vRequestsByRequestIdResponses[keyof GetFalAiWanI2vRequestsByRequestIdResponses];

export type GetFalAiViduQ3ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q3/image-to-video/requests/{request_id}/status";
};

export type GetFalAiViduQ3ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduQ3ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduQ3ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ3ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ3ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q3/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduQ3ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduQ3ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduQ3ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ3ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ3ImageToVideoData = {
  body: ViduQ3ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q3/image-to-video";
};

export type PostFalAiViduQ3ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ3ImageToVideoResponse =
  PostFalAiViduQ3ImageToVideoResponses[keyof PostFalAiViduQ3ImageToVideoResponses];

export type GetFalAiViduQ3ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q3/image-to-video/requests/{request_id}";
};

export type GetFalAiViduQ3ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ3ImageToVideoOutput;
};

export type GetFalAiViduQ3ImageToVideoRequestsByRequestIdResponse =
  GetFalAiViduQ3ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiViduQ3ImageToVideoRequestsByRequestIdResponses];

export type GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/xai/grok-imagine-video/image-to-video/requests/{request_id}/status";
};

export type GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdStatusResponse =
  GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdStatusResponses[keyof GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdStatusResponses];

export type PutXaiGrokImagineVideoImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/xai/grok-imagine-video/image-to-video/requests/{request_id}/cancel";
};

export type PutXaiGrokImagineVideoImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutXaiGrokImagineVideoImageToVideoRequestsByRequestIdCancelResponse =
  PutXaiGrokImagineVideoImageToVideoRequestsByRequestIdCancelResponses[keyof PutXaiGrokImagineVideoImageToVideoRequestsByRequestIdCancelResponses];

export type PostXaiGrokImagineVideoImageToVideoData = {
  body: GrokImagineVideoImageToVideoInput;
  path?: never;
  query?: never;
  url: "/xai/grok-imagine-video/image-to-video";
};

export type PostXaiGrokImagineVideoImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostXaiGrokImagineVideoImageToVideoResponse =
  PostXaiGrokImagineVideoImageToVideoResponses[keyof PostXaiGrokImagineVideoImageToVideoResponses];

export type GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/xai/grok-imagine-video/image-to-video/requests/{request_id}";
};

export type GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: GrokImagineVideoImageToVideoOutput;
};

export type GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdResponse =
  GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdResponses[keyof GetXaiGrokImagineVideoImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV56TransitionRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5.6/transition/requests/{request_id}/status";
};

export type GetFalAiPixverseV56TransitionRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV56TransitionRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV56TransitionRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV56TransitionRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV56TransitionRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.6/transition/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV56TransitionRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV56TransitionRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV56TransitionRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV56TransitionRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV56TransitionData = {
  body: PixverseV56TransitionInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5.6/transition";
};

export type PostFalAiPixverseV56TransitionResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV56TransitionResponse =
  PostFalAiPixverseV56TransitionResponses[keyof PostFalAiPixverseV56TransitionResponses];

export type GetFalAiPixverseV56TransitionRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.6/transition/requests/{request_id}";
};

export type GetFalAiPixverseV56TransitionRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV56TransitionOutput;
};

export type GetFalAiPixverseV56TransitionRequestsByRequestIdResponse =
  GetFalAiPixverseV56TransitionRequestsByRequestIdResponses[keyof GetFalAiPixverseV56TransitionRequestsByRequestIdResponses];

export type GetFalAiPixverseV56ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5.6/image-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV56ImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV56ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV56ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV56ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV56ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.6/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV56ImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV56ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV56ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV56ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV56ImageToVideoData = {
  body: PixverseV56ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5.6/image-to-video";
};

export type PostFalAiPixverseV56ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV56ImageToVideoResponse =
  PostFalAiPixverseV56ImageToVideoResponses[keyof PostFalAiPixverseV56ImageToVideoResponses];

export type GetFalAiPixverseV56ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.6/image-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV56ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV56ImageToVideoOutput;
};

export type GetFalAiPixverseV56ImageToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV56ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV56ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q2/reference-to-video/pro/requests/{request_id}/status";
};

export type GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdStatusResponse =
  GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ2ReferenceToVideoProRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/reference-to-video/pro/requests/{request_id}/cancel";
};

export type PutFalAiViduQ2ReferenceToVideoProRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiViduQ2ReferenceToVideoProRequestsByRequestIdCancelResponse =
  PutFalAiViduQ2ReferenceToVideoProRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ2ReferenceToVideoProRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ2ReferenceToVideoProData = {
  body: ViduQ2ReferenceToVideoProInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q2/reference-to-video/pro";
};

export type PostFalAiViduQ2ReferenceToVideoProResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ2ReferenceToVideoProResponse =
  PostFalAiViduQ2ReferenceToVideoProResponses[keyof PostFalAiViduQ2ReferenceToVideoProResponses];

export type GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/reference-to-video/pro/requests/{request_id}";
};

export type GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ2ReferenceToVideoProOutput;
};

export type GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdResponse =
  GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdResponses[keyof GetFalAiViduQ2ReferenceToVideoProRequestsByRequestIdResponses];

export type GetWanV26ImageToVideoFlashRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/wan/v2.6/image-to-video/flash/requests/{request_id}/status";
};

export type GetWanV26ImageToVideoFlashRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetWanV26ImageToVideoFlashRequestsByRequestIdStatusResponse =
  GetWanV26ImageToVideoFlashRequestsByRequestIdStatusResponses[keyof GetWanV26ImageToVideoFlashRequestsByRequestIdStatusResponses];

export type PutWanV26ImageToVideoFlashRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/wan/v2.6/image-to-video/flash/requests/{request_id}/cancel";
};

export type PutWanV26ImageToVideoFlashRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutWanV26ImageToVideoFlashRequestsByRequestIdCancelResponse =
  PutWanV26ImageToVideoFlashRequestsByRequestIdCancelResponses[keyof PutWanV26ImageToVideoFlashRequestsByRequestIdCancelResponses];

export type PostWanV26ImageToVideoFlashData = {
  body: V26ImageToVideoFlashInput;
  path?: never;
  query?: never;
  url: "/wan/v2.6/image-to-video/flash";
};

export type PostWanV26ImageToVideoFlashResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostWanV26ImageToVideoFlashResponse =
  PostWanV26ImageToVideoFlashResponses[keyof PostWanV26ImageToVideoFlashResponses];

export type GetWanV26ImageToVideoFlashRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/wan/v2.6/image-to-video/flash/requests/{request_id}";
};

export type GetWanV26ImageToVideoFlashRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: V26ImageToVideoFlashOutput;
};

export type GetWanV26ImageToVideoFlashRequestsByRequestIdResponse =
  GetWanV26ImageToVideoFlashRequestsByRequestIdResponses[keyof GetWanV26ImageToVideoFlashRequestsByRequestIdResponses];

export type GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-2-19b/distilled/image-to-video/lora/requests/{request_id}/status";
  };

export type GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-2-19b/distilled/image-to-video/lora/requests/{request_id}/cancel";
  };

export type PutFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledImageToVideoLoraData = {
  body: Ltx219bDistilledImageToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/image-to-video/lora";
};

export type PostFalAiLtx219bDistilledImageToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledImageToVideoLoraResponse =
  PostFalAiLtx219bDistilledImageToVideoLoraResponses[keyof PostFalAiLtx219bDistilledImageToVideoLoraResponses];

export type GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/image-to-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: Ltx219bDistilledImageToVideoLoraOutput;
  };

export type GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledImageToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-2-19b/distilled/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-2-19b/distilled/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLtx219bDistilledImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledImageToVideoData = {
  body: Ltx219bDistilledImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/image-to-video";
};

export type PostFalAiLtx219bDistilledImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledImageToVideoResponse =
  PostFalAiLtx219bDistilledImageToVideoResponses[keyof PostFalAiLtx219bDistilledImageToVideoResponses];

export type GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/image-to-video/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bDistilledImageToVideoOutput;
};

export type GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledImageToVideoRequestsByRequestIdResponses];

export type GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/image-to-video/lora/requests/{request_id}/status";
};

export type GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bImageToVideoLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/image-to-video/lora/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bImageToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bImageToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bImageToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bImageToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bImageToVideoLoraData = {
  body: Ltx219bImageToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/image-to-video/lora";
};

export type PostFalAiLtx219bImageToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bImageToVideoLoraResponse =
  PostFalAiLtx219bImageToVideoLoraResponses[keyof PostFalAiLtx219bImageToVideoLoraResponses];

export type GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/image-to-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bImageToVideoLoraOutput;
};

export type GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bImageToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/image-to-video/requests/{request_id}/status";
};

export type GetFalAiLtx219bImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx219bImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx219bImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bImageToVideoData = {
  body: Ltx219bImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/image-to-video";
};

export type PostFalAiLtx219bImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bImageToVideoResponse =
  PostFalAiLtx219bImageToVideoResponses[keyof PostFalAiLtx219bImageToVideoResponses];

export type GetFalAiLtx219bImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/image-to-video/requests/{request_id}";
};

export type GetFalAiLtx219bImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bImageToVideoOutput;
};

export type GetFalAiLtx219bImageToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bImageToVideoRequestsByRequestIdResponses];

export type GetFalAiWanMoveRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-move/requests/{request_id}/status";
};

export type GetFalAiWanMoveRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanMoveRequestsByRequestIdStatusResponse =
  GetFalAiWanMoveRequestsByRequestIdStatusResponses[keyof GetFalAiWanMoveRequestsByRequestIdStatusResponses];

export type PutFalAiWanMoveRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-move/requests/{request_id}/cancel";
};

export type PutFalAiWanMoveRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanMoveRequestsByRequestIdCancelResponse =
  PutFalAiWanMoveRequestsByRequestIdCancelResponses[keyof PutFalAiWanMoveRequestsByRequestIdCancelResponses];

export type PostFalAiWanMoveData = {
  body: WanMoveInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-move";
};

export type PostFalAiWanMoveResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanMoveResponse =
  PostFalAiWanMoveResponses[keyof PostFalAiWanMoveResponses];

export type GetFalAiWanMoveRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-move/requests/{request_id}";
};

export type GetFalAiWanMoveRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanMoveOutput;
};

export type GetFalAiWanMoveRequestsByRequestIdResponse =
  GetFalAiWanMoveRequestsByRequestIdResponses[keyof GetFalAiWanMoveRequestsByRequestIdResponses];

export type GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kandinsky5-pro/image-to-video/requests/{request_id}/status";
};

export type GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKandinsky5ProImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kandinsky5-pro/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiKandinsky5ProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKandinsky5ProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKandinsky5ProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKandinsky5ProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKandinsky5ProImageToVideoData = {
  body: Kandinsky5ProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kandinsky5-pro/image-to-video";
};

export type PostFalAiKandinsky5ProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKandinsky5ProImageToVideoResponse =
  PostFalAiKandinsky5ProImageToVideoResponses[keyof PostFalAiKandinsky5ProImageToVideoResponses];

export type GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kandinsky5-pro/image-to-video/requests/{request_id}";
};

export type GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Kandinsky5ProImageToVideoOutput;
};

export type GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdResponse =
  GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKandinsky5ProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance/seedance/v1.5/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1.5/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceSeedanceV15ProImageToVideoData = {
  body: BytedanceSeedanceV15ProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1.5/pro/image-to-video";
};

export type PostFalAiBytedanceSeedanceV15ProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceSeedanceV15ProImageToVideoResponse =
  PostFalAiBytedanceSeedanceV15ProImageToVideoResponses[keyof PostFalAiBytedanceSeedanceV15ProImageToVideoResponses];

export type GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1.5/pro/image-to-video/requests/{request_id}";
  };

export type GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceSeedanceV15ProImageToVideoOutput;
  };

export type GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceSeedanceV15ProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiLiveAvatarRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/live-avatar/requests/{request_id}/status";
};

export type GetFalAiLiveAvatarRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLiveAvatarRequestsByRequestIdStatusResponse =
  GetFalAiLiveAvatarRequestsByRequestIdStatusResponses[keyof GetFalAiLiveAvatarRequestsByRequestIdStatusResponses];

export type PutFalAiLiveAvatarRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/live-avatar/requests/{request_id}/cancel";
};

export type PutFalAiLiveAvatarRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLiveAvatarRequestsByRequestIdCancelResponse =
  PutFalAiLiveAvatarRequestsByRequestIdCancelResponses[keyof PutFalAiLiveAvatarRequestsByRequestIdCancelResponses];

export type PostFalAiLiveAvatarData = {
  body: LiveAvatarInput;
  path?: never;
  query?: never;
  url: "/fal-ai/live-avatar";
};

export type PostFalAiLiveAvatarResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLiveAvatarResponse =
  PostFalAiLiveAvatarResponses[keyof PostFalAiLiveAvatarResponses];

export type GetFalAiLiveAvatarRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/live-avatar/requests/{request_id}";
};

export type GetFalAiLiveAvatarRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LiveAvatarOutput;
};

export type GetFalAiLiveAvatarRequestsByRequestIdResponse =
  GetFalAiLiveAvatarRequestsByRequestIdResponses[keyof GetFalAiLiveAvatarRequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-video-v1.5/image-to-video/requests/{request_id}/status";
};

export type GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-v1.5/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoV15ImageToVideoData = {
  body: HunyuanVideoV15ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video-v1.5/image-to-video";
};

export type PostFalAiHunyuanVideoV15ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoV15ImageToVideoResponse =
  PostFalAiHunyuanVideoV15ImageToVideoResponses[keyof PostFalAiHunyuanVideoV15ImageToVideoResponses];

export type GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-v1.5/image-to-video/requests/{request_id}";
};

export type GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoV15ImageToVideoOutput;
};

export type GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoV15ImageToVideoRequestsByRequestIdResponses];

export type GetWanV26ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/wan/v2.6/image-to-video/requests/{request_id}/status";
};

export type GetWanV26ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetWanV26ImageToVideoRequestsByRequestIdStatusResponse =
  GetWanV26ImageToVideoRequestsByRequestIdStatusResponses[keyof GetWanV26ImageToVideoRequestsByRequestIdStatusResponses];

export type PutWanV26ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/wan/v2.6/image-to-video/requests/{request_id}/cancel";
};

export type PutWanV26ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutWanV26ImageToVideoRequestsByRequestIdCancelResponse =
  PutWanV26ImageToVideoRequestsByRequestIdCancelResponses[keyof PutWanV26ImageToVideoRequestsByRequestIdCancelResponses];

export type PostWanV26ImageToVideoData = {
  body: V26ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/wan/v2.6/image-to-video";
};

export type PostWanV26ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostWanV26ImageToVideoResponse =
  PostWanV26ImageToVideoResponses[keyof PostWanV26ImageToVideoResponses];

export type GetWanV26ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/wan/v2.6/image-to-video/requests/{request_id}";
};

export type GetWanV26ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: V26ImageToVideoOutput;
};

export type GetWanV26ImageToVideoRequestsByRequestIdResponse =
  GetWanV26ImageToVideoRequestsByRequestIdResponses[keyof GetWanV26ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/o1/standard/reference-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/standard/reference-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoO1StandardReferenceToVideoData = {
  body: KlingVideoO1StandardReferenceToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/o1/standard/reference-to-video";
};

export type PostFalAiKlingVideoO1StandardReferenceToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoO1StandardReferenceToVideoResponse =
  PostFalAiKlingVideoO1StandardReferenceToVideoResponses[keyof PostFalAiKlingVideoO1StandardReferenceToVideoResponses];

export type GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/standard/reference-to-video/requests/{request_id}";
  };

export type GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoO1StandardReferenceToVideoOutput;
  };

export type GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1StandardReferenceToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/o1/standard/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/standard/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoO1StandardImageToVideoData = {
  body: KlingVideoO1StandardImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/o1/standard/image-to-video";
};

export type PostFalAiKlingVideoO1StandardImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoO1StandardImageToVideoResponse =
  PostFalAiKlingVideoO1StandardImageToVideoResponses[keyof PostFalAiKlingVideoO1StandardImageToVideoResponses];

export type GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/o1/standard/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoO1StandardImageToVideoOutput;
  };

export type GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1StandardImageToVideoRequestsByRequestIdResponses];

export type GetFalAiCreatifyAuroraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/creatify/aurora/requests/{request_id}/status";
};

export type GetFalAiCreatifyAuroraRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiCreatifyAuroraRequestsByRequestIdStatusResponse =
  GetFalAiCreatifyAuroraRequestsByRequestIdStatusResponses[keyof GetFalAiCreatifyAuroraRequestsByRequestIdStatusResponses];

export type PutFalAiCreatifyAuroraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/creatify/aurora/requests/{request_id}/cancel";
};

export type PutFalAiCreatifyAuroraRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiCreatifyAuroraRequestsByRequestIdCancelResponse =
  PutFalAiCreatifyAuroraRequestsByRequestIdCancelResponses[keyof PutFalAiCreatifyAuroraRequestsByRequestIdCancelResponses];

export type PostFalAiCreatifyAuroraData = {
  body: CreatifyAuroraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/creatify/aurora";
};

export type PostFalAiCreatifyAuroraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiCreatifyAuroraResponse =
  PostFalAiCreatifyAuroraResponses[keyof PostFalAiCreatifyAuroraResponses];

export type GetFalAiCreatifyAuroraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/creatify/aurora/requests/{request_id}";
};

export type GetFalAiCreatifyAuroraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: CreatifyAuroraOutput;
};

export type GetFalAiCreatifyAuroraRequestsByRequestIdResponse =
  GetFalAiCreatifyAuroraRequestsByRequestIdResponses[keyof GetFalAiCreatifyAuroraRequestsByRequestIdResponses];

export type GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/ai-avatar/v2/pro/requests/{request_id}/status";
};

export type GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/ai-avatar/v2/pro/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoAiAvatarV2ProData = {
  body: KlingVideoAiAvatarV2ProInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/ai-avatar/v2/pro";
};

export type PostFalAiKlingVideoAiAvatarV2ProResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoAiAvatarV2ProResponse =
  PostFalAiKlingVideoAiAvatarV2ProResponses[keyof PostFalAiKlingVideoAiAvatarV2ProResponses];

export type GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/ai-avatar/v2/pro/requests/{request_id}";
};

export type GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoAiAvatarV2ProOutput;
};

export type GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdResponse =
  GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdResponses[keyof GetFalAiKlingVideoAiAvatarV2ProRequestsByRequestIdResponses];

export type GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/ai-avatar/v2/standard/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/ai-avatar/v2/standard/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoAiAvatarV2StandardData = {
  body: KlingVideoAiAvatarV2StandardInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/ai-avatar/v2/standard";
};

export type PostFalAiKlingVideoAiAvatarV2StandardResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoAiAvatarV2StandardResponse =
  PostFalAiKlingVideoAiAvatarV2StandardResponses[keyof PostFalAiKlingVideoAiAvatarV2StandardResponses];

export type GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/ai-avatar/v2/standard/requests/{request_id}";
};

export type GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoAiAvatarV2StandardOutput;
};

export type GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdResponse =
  GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdResponses[keyof GetFalAiKlingVideoAiAvatarV2StandardRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.6/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.6/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV26ProImageToVideoData = {
  body: KlingVideoV26ProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.6/pro/image-to-video";
};

export type PostFalAiKlingVideoV26ProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV26ProImageToVideoResponse =
  PostFalAiKlingVideoV26ProImageToVideoResponses[keyof PostFalAiKlingVideoV26ProImageToVideoResponses];

export type GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.6/pro/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV26ProImageToVideoOutput;
};

export type GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV26ProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV55EffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5.5/effects/requests/{request_id}/status";
};

export type GetFalAiPixverseV55EffectsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV55EffectsRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV55EffectsRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV55EffectsRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV55EffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.5/effects/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV55EffectsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV55EffectsRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV55EffectsRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV55EffectsRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV55EffectsData = {
  body: PixverseV55EffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5.5/effects";
};

export type PostFalAiPixverseV55EffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV55EffectsResponse =
  PostFalAiPixverseV55EffectsResponses[keyof PostFalAiPixverseV55EffectsResponses];

export type GetFalAiPixverseV55EffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.5/effects/requests/{request_id}";
};

export type GetFalAiPixverseV55EffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV55EffectsOutput;
};

export type GetFalAiPixverseV55EffectsRequestsByRequestIdResponse =
  GetFalAiPixverseV55EffectsRequestsByRequestIdResponses[keyof GetFalAiPixverseV55EffectsRequestsByRequestIdResponses];

export type GetFalAiPixverseV55TransitionRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5.5/transition/requests/{request_id}/status";
};

export type GetFalAiPixverseV55TransitionRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV55TransitionRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV55TransitionRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV55TransitionRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV55TransitionRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.5/transition/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV55TransitionRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV55TransitionRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV55TransitionRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV55TransitionRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV55TransitionData = {
  body: PixverseV55TransitionInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5.5/transition";
};

export type PostFalAiPixverseV55TransitionResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV55TransitionResponse =
  PostFalAiPixverseV55TransitionResponses[keyof PostFalAiPixverseV55TransitionResponses];

export type GetFalAiPixverseV55TransitionRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.5/transition/requests/{request_id}";
};

export type GetFalAiPixverseV55TransitionRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV55TransitionOutput;
};

export type GetFalAiPixverseV55TransitionRequestsByRequestIdResponse =
  GetFalAiPixverseV55TransitionRequestsByRequestIdResponses[keyof GetFalAiPixverseV55TransitionRequestsByRequestIdResponses];

export type GetFalAiPixverseV55ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5.5/image-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV55ImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV55ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV55ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV55ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV55ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.5/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV55ImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV55ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV55ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV55ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV55ImageToVideoData = {
  body: PixverseV55ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5.5/image-to-video";
};

export type PostFalAiPixverseV55ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV55ImageToVideoResponse =
  PostFalAiPixverseV55ImageToVideoResponses[keyof PostFalAiPixverseV55ImageToVideoResponses];

export type GetFalAiPixverseV55ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.5/image-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV55ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV55ImageToVideoOutput;
};

export type GetFalAiPixverseV55ImageToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV55ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV55ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/o1/image-to-video/requests/{request_id}/status";
};

export type GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoO1ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/o1/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoO1ImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoO1ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoO1ImageToVideoData = {
  body: KlingVideoO1ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/o1/image-to-video";
};

export type PostFalAiKlingVideoO1ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoO1ImageToVideoResponse =
  PostFalAiKlingVideoO1ImageToVideoResponses[keyof PostFalAiKlingVideoO1ImageToVideoResponses];

export type GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/o1/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoO1ImageToVideoOutput;
};

export type GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/o1/reference-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/reference-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoO1ReferenceToVideoData = {
  body: KlingVideoO1ReferenceToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/o1/reference-to-video";
};

export type PostFalAiKlingVideoO1ReferenceToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoO1ReferenceToVideoResponse =
  PostFalAiKlingVideoO1ReferenceToVideoResponses[keyof PostFalAiKlingVideoO1ReferenceToVideoResponses];

export type GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/o1/reference-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoO1ReferenceToVideoOutput;
};

export type GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1ReferenceToVideoRequestsByRequestIdResponses];

export type GetFalAiLtx2ImageToVideoFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2/image-to-video/fast/requests/{request_id}/status";
};

export type GetFalAiLtx2ImageToVideoFastRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx2ImageToVideoFastRequestsByRequestIdStatusResponse =
  GetFalAiLtx2ImageToVideoFastRequestsByRequestIdStatusResponses[keyof GetFalAiLtx2ImageToVideoFastRequestsByRequestIdStatusResponses];

export type PutFalAiLtx2ImageToVideoFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/image-to-video/fast/requests/{request_id}/cancel";
};

export type PutFalAiLtx2ImageToVideoFastRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx2ImageToVideoFastRequestsByRequestIdCancelResponse =
  PutFalAiLtx2ImageToVideoFastRequestsByRequestIdCancelResponses[keyof PutFalAiLtx2ImageToVideoFastRequestsByRequestIdCancelResponses];

export type PostFalAiLtx2ImageToVideoFastData = {
  body: Ltx2ImageToVideoFastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2/image-to-video/fast";
};

export type PostFalAiLtx2ImageToVideoFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx2ImageToVideoFastResponse =
  PostFalAiLtx2ImageToVideoFastResponses[keyof PostFalAiLtx2ImageToVideoFastResponses];

export type GetFalAiLtx2ImageToVideoFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/image-to-video/fast/requests/{request_id}";
};

export type GetFalAiLtx2ImageToVideoFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx2ImageToVideoFastOutput;
};

export type GetFalAiLtx2ImageToVideoFastRequestsByRequestIdResponse =
  GetFalAiLtx2ImageToVideoFastRequestsByRequestIdResponses[keyof GetFalAiLtx2ImageToVideoFastRequestsByRequestIdResponses];

export type GetFalAiLtx2ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2/image-to-video/requests/{request_id}/status";
};

export type GetFalAiLtx2ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx2ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx2ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx2ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx2ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx2ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx2ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx2ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx2ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx2ImageToVideoData = {
  body: Ltx2ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2/image-to-video";
};

export type PostFalAiLtx2ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx2ImageToVideoResponse =
  PostFalAiLtx2ImageToVideoResponses[keyof PostFalAiLtx2ImageToVideoResponses];

export type GetFalAiLtx2ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/image-to-video/requests/{request_id}";
};

export type GetFalAiLtx2ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx2ImageToVideoOutput;
};

export type GetFalAiLtx2ImageToVideoRequestsByRequestIdResponse =
  GetFalAiLtx2ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx2ImageToVideoRequestsByRequestIdResponses];

export type GetBytedanceLynxRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/bytedance/lynx/requests/{request_id}/status";
};

export type GetBytedanceLynxRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetBytedanceLynxRequestsByRequestIdStatusResponse =
  GetBytedanceLynxRequestsByRequestIdStatusResponses[keyof GetBytedanceLynxRequestsByRequestIdStatusResponses];

export type PutBytedanceLynxRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bytedance/lynx/requests/{request_id}/cancel";
};

export type PutBytedanceLynxRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutBytedanceLynxRequestsByRequestIdCancelResponse =
  PutBytedanceLynxRequestsByRequestIdCancelResponses[keyof PutBytedanceLynxRequestsByRequestIdCancelResponses];

export type PostBytedanceLynxData = {
  body: LynxInput;
  path?: never;
  query?: never;
  url: "/bytedance/lynx";
};

export type PostBytedanceLynxResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostBytedanceLynxResponse =
  PostBytedanceLynxResponses[keyof PostBytedanceLynxResponses];

export type GetBytedanceLynxRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bytedance/lynx/requests/{request_id}";
};

export type GetBytedanceLynxRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LynxOutput;
};

export type GetBytedanceLynxRequestsByRequestIdResponse =
  GetBytedanceLynxRequestsByRequestIdResponses[keyof GetBytedanceLynxRequestsByRequestIdResponses];

export type GetFalAiPixverseSwapRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/swap/requests/{request_id}/status";
};

export type GetFalAiPixverseSwapRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseSwapRequestsByRequestIdStatusResponse =
  GetFalAiPixverseSwapRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseSwapRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseSwapRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/swap/requests/{request_id}/cancel";
};

export type PutFalAiPixverseSwapRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseSwapRequestsByRequestIdCancelResponse =
  PutFalAiPixverseSwapRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseSwapRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseSwapData = {
  body: PixverseSwapInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/swap";
};

export type PostFalAiPixverseSwapResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseSwapResponse =
  PostFalAiPixverseSwapResponses[keyof PostFalAiPixverseSwapResponses];

export type GetFalAiPixverseSwapRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/swap/requests/{request_id}";
};

export type GetFalAiPixverseSwapRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseSwapOutput;
};

export type GetFalAiPixverseSwapRequestsByRequestIdResponse =
  GetFalAiPixverseSwapRequestsByRequestIdResponses[keyof GetFalAiPixverseSwapRequestsByRequestIdResponses];

export type GetFalAiPikaV22PikaframesRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v2.2/pikaframes/requests/{request_id}/status";
};

export type GetFalAiPikaV22PikaframesRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPikaV22PikaframesRequestsByRequestIdStatusResponse =
  GetFalAiPikaV22PikaframesRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV22PikaframesRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV22PikaframesRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.2/pikaframes/requests/{request_id}/cancel";
};

export type PutFalAiPikaV22PikaframesRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPikaV22PikaframesRequestsByRequestIdCancelResponse =
  PutFalAiPikaV22PikaframesRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV22PikaframesRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV22PikaframesData = {
  body: PikaV22PikaframesInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v2.2/pikaframes";
};

export type PostFalAiPikaV22PikaframesResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV22PikaframesResponse =
  PostFalAiPikaV22PikaframesResponses[keyof PostFalAiPikaV22PikaframesResponses];

export type GetFalAiPikaV22PikaframesRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.2/pikaframes/requests/{request_id}";
};

export type GetFalAiPikaV22PikaframesRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV22PikaframesOutput;
};

export type GetFalAiPikaV22PikaframesRequestsByRequestIdResponse =
  GetFalAiPikaV22PikaframesRequestsByRequestIdResponses[keyof GetFalAiPikaV22PikaframesRequestsByRequestIdResponses];

export type GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/longcat-video/image-to-video/720p/requests/{request_id}/status";
  };

export type GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdStatusResponse =
  GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatVideoImageToVideo720pRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/image-to-video/720p/requests/{request_id}/cancel";
  };

export type PutFalAiLongcatVideoImageToVideo720pRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatVideoImageToVideo720pRequestsByRequestIdCancelResponse =
  PutFalAiLongcatVideoImageToVideo720pRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatVideoImageToVideo720pRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatVideoImageToVideo720pData = {
  body: LongcatVideoImageToVideo720pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-video/image-to-video/720p";
};

export type PostFalAiLongcatVideoImageToVideo720pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatVideoImageToVideo720pResponse =
  PostFalAiLongcatVideoImageToVideo720pResponses[keyof PostFalAiLongcatVideoImageToVideo720pResponses];

export type GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/longcat-video/image-to-video/720p/requests/{request_id}";
};

export type GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LongcatVideoImageToVideo720pOutput;
};

export type GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdResponse =
  GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdResponses[keyof GetFalAiLongcatVideoImageToVideo720pRequestsByRequestIdResponses];

export type GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/longcat-video/image-to-video/480p/requests/{request_id}/status";
  };

export type GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdStatusResponse =
  GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatVideoImageToVideo480pRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/image-to-video/480p/requests/{request_id}/cancel";
  };

export type PutFalAiLongcatVideoImageToVideo480pRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatVideoImageToVideo480pRequestsByRequestIdCancelResponse =
  PutFalAiLongcatVideoImageToVideo480pRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatVideoImageToVideo480pRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatVideoImageToVideo480pData = {
  body: LongcatVideoImageToVideo480pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-video/image-to-video/480p";
};

export type PostFalAiLongcatVideoImageToVideo480pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatVideoImageToVideo480pResponse =
  PostFalAiLongcatVideoImageToVideo480pResponses[keyof PostFalAiLongcatVideoImageToVideo480pResponses];

export type GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/longcat-video/image-to-video/480p/requests/{request_id}";
};

export type GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LongcatVideoImageToVideo480pOutput;
};

export type GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdResponse =
  GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdResponses[keyof GetFalAiLongcatVideoImageToVideo480pRequestsByRequestIdResponses];

export type GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/longcat-video/distilled/image-to-video/720p/requests/{request_id}/status";
  };

export type GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdStatusResponse =
  GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/distilled/image-to-video/720p/requests/{request_id}/cancel";
  };

export type PutFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdCancelResponse =
  PutFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatVideoDistilledImageToVideo720pData = {
  body: LongcatVideoDistilledImageToVideo720pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-video/distilled/image-to-video/720p";
};

export type PostFalAiLongcatVideoDistilledImageToVideo720pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatVideoDistilledImageToVideo720pResponse =
  PostFalAiLongcatVideoDistilledImageToVideo720pResponses[keyof PostFalAiLongcatVideoDistilledImageToVideo720pResponses];

export type GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/distilled/image-to-video/720p/requests/{request_id}";
  };

export type GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LongcatVideoDistilledImageToVideo720pOutput;
  };

export type GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdResponse =
  GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdResponses[keyof GetFalAiLongcatVideoDistilledImageToVideo720pRequestsByRequestIdResponses];

export type GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/longcat-video/distilled/image-to-video/480p/requests/{request_id}/status";
  };

export type GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdStatusResponse =
  GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/distilled/image-to-video/480p/requests/{request_id}/cancel";
  };

export type PutFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdCancelResponse =
  PutFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatVideoDistilledImageToVideo480pData = {
  body: LongcatVideoDistilledImageToVideo480pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-video/distilled/image-to-video/480p";
};

export type PostFalAiLongcatVideoDistilledImageToVideo480pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatVideoDistilledImageToVideo480pResponse =
  PostFalAiLongcatVideoDistilledImageToVideo480pResponses[keyof PostFalAiLongcatVideoDistilledImageToVideo480pResponses];

export type GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/distilled/image-to-video/480p/requests/{request_id}";
  };

export type GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LongcatVideoDistilledImageToVideo480pOutput;
  };

export type GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdResponse =
  GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdResponses[keyof GetFalAiLongcatVideoDistilledImageToVideo480pRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-2.3-fast/standard/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3-fast/standard/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo23FastStandardImageToVideoData = {
  body: MinimaxHailuo23FastStandardImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-2.3-fast/standard/image-to-video";
};

export type PostFalAiMinimaxHailuo23FastStandardImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo23FastStandardImageToVideoResponse =
  PostFalAiMinimaxHailuo23FastStandardImageToVideoResponses[keyof PostFalAiMinimaxHailuo23FastStandardImageToVideoResponses];

export type GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3-fast/standard/image-to-video/requests/{request_id}";
  };

export type GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo23FastStandardImageToVideoOutput;
  };

export type GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo23FastStandardImageToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-2.3/standard/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3/standard/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo23StandardImageToVideoData = {
  body: MinimaxHailuo23StandardImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-2.3/standard/image-to-video";
};

export type PostFalAiMinimaxHailuo23StandardImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo23StandardImageToVideoResponse =
  PostFalAiMinimaxHailuo23StandardImageToVideoResponses[keyof PostFalAiMinimaxHailuo23StandardImageToVideoResponses];

export type GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3/standard/image-to-video/requests/{request_id}";
  };

export type GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo23StandardImageToVideoOutput;
  };

export type GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo23StandardImageToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-2.3-fast/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3-fast/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo23FastProImageToVideoData = {
  body: MinimaxHailuo23FastProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-2.3-fast/pro/image-to-video";
};

export type PostFalAiMinimaxHailuo23FastProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo23FastProImageToVideoResponse =
  PostFalAiMinimaxHailuo23FastProImageToVideoResponses[keyof PostFalAiMinimaxHailuo23FastProImageToVideoResponses];

export type GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3-fast/pro/image-to-video/requests/{request_id}";
  };

export type GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo23FastProImageToVideoOutput;
  };

export type GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo23FastProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance/seedance/v1/pro/fast/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/pro/fast/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceSeedanceV1ProFastImageToVideoData = {
  body: BytedanceSeedanceV1ProFastImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1/pro/fast/image-to-video";
};

export type PostFalAiBytedanceSeedanceV1ProFastImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceSeedanceV1ProFastImageToVideoResponse =
  PostFalAiBytedanceSeedanceV1ProFastImageToVideoResponses[keyof PostFalAiBytedanceSeedanceV1ProFastImageToVideoResponses];

export type GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/pro/fast/image-to-video/requests/{request_id}";
  };

export type GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceSeedanceV1ProFastImageToVideoOutput;
  };

export type GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceSeedanceV1ProFastImageToVideoRequestsByRequestIdResponses];

export type GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q2/image-to-video/turbo/requests/{request_id}/status";
};

export type GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdStatusResponse =
  GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ2ImageToVideoTurboRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/image-to-video/turbo/requests/{request_id}/cancel";
};

export type PutFalAiViduQ2ImageToVideoTurboRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiViduQ2ImageToVideoTurboRequestsByRequestIdCancelResponse =
  PutFalAiViduQ2ImageToVideoTurboRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ2ImageToVideoTurboRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ2ImageToVideoTurboData = {
  body: ViduQ2ImageToVideoTurboInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q2/image-to-video/turbo";
};

export type PostFalAiViduQ2ImageToVideoTurboResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ2ImageToVideoTurboResponse =
  PostFalAiViduQ2ImageToVideoTurboResponses[keyof PostFalAiViduQ2ImageToVideoTurboResponses];

export type GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/image-to-video/turbo/requests/{request_id}";
};

export type GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ2ImageToVideoTurboOutput;
};

export type GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdResponse =
  GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdResponses[keyof GetFalAiViduQ2ImageToVideoTurboRequestsByRequestIdResponses];

export type GetFalAiViduQ2ImageToVideoProRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q2/image-to-video/pro/requests/{request_id}/status";
};

export type GetFalAiViduQ2ImageToVideoProRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduQ2ImageToVideoProRequestsByRequestIdStatusResponse =
  GetFalAiViduQ2ImageToVideoProRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ2ImageToVideoProRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ2ImageToVideoProRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/image-to-video/pro/requests/{request_id}/cancel";
};

export type PutFalAiViduQ2ImageToVideoProRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduQ2ImageToVideoProRequestsByRequestIdCancelResponse =
  PutFalAiViduQ2ImageToVideoProRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ2ImageToVideoProRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ2ImageToVideoProData = {
  body: ViduQ2ImageToVideoProInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q2/image-to-video/pro";
};

export type PostFalAiViduQ2ImageToVideoProResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ2ImageToVideoProResponse =
  PostFalAiViduQ2ImageToVideoProResponses[keyof PostFalAiViduQ2ImageToVideoProResponses];

export type GetFalAiViduQ2ImageToVideoProRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/image-to-video/pro/requests/{request_id}";
};

export type GetFalAiViduQ2ImageToVideoProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ2ImageToVideoProOutput;
};

export type GetFalAiViduQ2ImageToVideoProRequestsByRequestIdResponse =
  GetFalAiViduQ2ImageToVideoProRequestsByRequestIdResponses[keyof GetFalAiViduQ2ImageToVideoProRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.5-turbo/standard/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.5-turbo/standard/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV25TurboStandardImageToVideoData = {
  body: KlingVideoV25TurboStandardImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.5-turbo/standard/image-to-video";
};

export type PostFalAiKlingVideoV25TurboStandardImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV25TurboStandardImageToVideoResponse =
  PostFalAiKlingVideoV25TurboStandardImageToVideoResponses[keyof PostFalAiKlingVideoV25TurboStandardImageToVideoResponses];

export type GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.5-turbo/standard/image-to-video/requests/{request_id}";
  };

export type GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV25TurboStandardImageToVideoOutput;
  };

export type GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV25TurboStandardImageToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/veo3.1/fast/first-last-frame-to-video/requests/{request_id}/status";
  };

export type GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/veo3.1/fast/first-last-frame-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo31FastFirstLastFrameToVideoData = {
  body: Veo31FastFirstLastFrameToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3.1/fast/first-last-frame-to-video";
};

export type PostFalAiVeo31FastFirstLastFrameToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo31FastFirstLastFrameToVideoResponse =
  PostFalAiVeo31FastFirstLastFrameToVideoResponses[keyof PostFalAiVeo31FastFirstLastFrameToVideoResponses];

export type GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/fast/first-last-frame-to-video/requests/{request_id}";
};

export type GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: Veo31FastFirstLastFrameToVideoOutput;
  };

export type GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdResponse =
  GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdResponses[keyof GetFalAiVeo31FastFirstLastFrameToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3.1/first-last-frame-to-video/requests/{request_id}/status";
};

export type GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/first-last-frame-to-video/requests/{request_id}/cancel";
};

export type PutFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo31FirstLastFrameToVideoData = {
  body: Veo31FirstLastFrameToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3.1/first-last-frame-to-video";
};

export type PostFalAiVeo31FirstLastFrameToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo31FirstLastFrameToVideoResponse =
  PostFalAiVeo31FirstLastFrameToVideoResponses[keyof PostFalAiVeo31FirstLastFrameToVideoResponses];

export type GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/first-last-frame-to-video/requests/{request_id}";
};

export type GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo31FirstLastFrameToVideoOutput;
};

export type GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdResponse =
  GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdResponses[keyof GetFalAiVeo31FirstLastFrameToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo31ReferenceToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3.1/reference-to-video/requests/{request_id}/status";
};

export type GetFalAiVeo31ReferenceToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo31ReferenceToVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo31ReferenceToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31ReferenceToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo31ReferenceToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/reference-to-video/requests/{request_id}/cancel";
};

export type PutFalAiVeo31ReferenceToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo31ReferenceToVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo31ReferenceToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31ReferenceToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo31ReferenceToVideoData = {
  body: Veo31ReferenceToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3.1/reference-to-video";
};

export type PostFalAiVeo31ReferenceToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo31ReferenceToVideoResponse =
  PostFalAiVeo31ReferenceToVideoResponses[keyof PostFalAiVeo31ReferenceToVideoResponses];

export type GetFalAiVeo31ReferenceToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/reference-to-video/requests/{request_id}";
};

export type GetFalAiVeo31ReferenceToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo31ReferenceToVideoOutput;
};

export type GetFalAiVeo31ReferenceToVideoRequestsByRequestIdResponse =
  GetFalAiVeo31ReferenceToVideoRequestsByRequestIdResponses[keyof GetFalAiVeo31ReferenceToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo31FastImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3.1/fast/image-to-video/requests/{request_id}/status";
};

export type GetFalAiVeo31FastImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo31FastImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo31FastImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31FastImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo31FastImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/fast/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiVeo31FastImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo31FastImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo31FastImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31FastImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo31FastImageToVideoData = {
  body: Veo31FastImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3.1/fast/image-to-video";
};

export type PostFalAiVeo31FastImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo31FastImageToVideoResponse =
  PostFalAiVeo31FastImageToVideoResponses[keyof PostFalAiVeo31FastImageToVideoResponses];

export type GetFalAiVeo31FastImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/fast/image-to-video/requests/{request_id}";
};

export type GetFalAiVeo31FastImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo31FastImageToVideoOutput;
};

export type GetFalAiVeo31FastImageToVideoRequestsByRequestIdResponse =
  GetFalAiVeo31FastImageToVideoRequestsByRequestIdResponses[keyof GetFalAiVeo31FastImageToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo31ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3.1/image-to-video/requests/{request_id}/status";
};

export type GetFalAiVeo31ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo31ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo31ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo31ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiVeo31ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo31ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo31ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo31ImageToVideoData = {
  body: Veo31ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3.1/image-to-video";
};

export type PostFalAiVeo31ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo31ImageToVideoResponse =
  PostFalAiVeo31ImageToVideoResponses[keyof PostFalAiVeo31ImageToVideoResponses];

export type GetFalAiVeo31ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/image-to-video/requests/{request_id}";
};

export type GetFalAiVeo31ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo31ImageToVideoOutput;
};

export type GetFalAiVeo31ImageToVideoRequestsByRequestIdResponse =
  GetFalAiVeo31ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiVeo31ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiSora2ImageToVideoProRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sora-2/image-to-video/pro/requests/{request_id}/status";
};

export type GetFalAiSora2ImageToVideoProRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSora2ImageToVideoProRequestsByRequestIdStatusResponse =
  GetFalAiSora2ImageToVideoProRequestsByRequestIdStatusResponses[keyof GetFalAiSora2ImageToVideoProRequestsByRequestIdStatusResponses];

export type PutFalAiSora2ImageToVideoProRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/image-to-video/pro/requests/{request_id}/cancel";
};

export type PutFalAiSora2ImageToVideoProRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSora2ImageToVideoProRequestsByRequestIdCancelResponse =
  PutFalAiSora2ImageToVideoProRequestsByRequestIdCancelResponses[keyof PutFalAiSora2ImageToVideoProRequestsByRequestIdCancelResponses];

export type PostFalAiSora2ImageToVideoProData = {
  body: Sora2ImageToVideoProInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sora-2/image-to-video/pro";
};

export type PostFalAiSora2ImageToVideoProResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSora2ImageToVideoProResponse =
  PostFalAiSora2ImageToVideoProResponses[keyof PostFalAiSora2ImageToVideoProResponses];

export type GetFalAiSora2ImageToVideoProRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/image-to-video/pro/requests/{request_id}";
};

export type GetFalAiSora2ImageToVideoProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Sora2ImageToVideoProOutput;
};

export type GetFalAiSora2ImageToVideoProRequestsByRequestIdResponse =
  GetFalAiSora2ImageToVideoProRequestsByRequestIdResponses[keyof GetFalAiSora2ImageToVideoProRequestsByRequestIdResponses];

export type GetFalAiSora2ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sora-2/image-to-video/requests/{request_id}/status";
};

export type GetFalAiSora2ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSora2ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiSora2ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiSora2ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiSora2ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiSora2ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSora2ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiSora2ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiSora2ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiSora2ImageToVideoData = {
  body: Sora2ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sora-2/image-to-video";
};

export type PostFalAiSora2ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSora2ImageToVideoResponse =
  PostFalAiSora2ImageToVideoResponses[keyof PostFalAiSora2ImageToVideoResponses];

export type GetFalAiSora2ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/image-to-video/requests/{request_id}";
};

export type GetFalAiSora2ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Sora2ImageToVideoOutput;
};

export type GetFalAiSora2ImageToVideoRequestsByRequestIdResponse =
  GetFalAiSora2ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiSora2ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiOviImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ovi/image-to-video/requests/{request_id}/status";
};

export type GetFalAiOviImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiOviImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiOviImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiOviImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiOviImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ovi/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiOviImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiOviImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiOviImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiOviImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiOviImageToVideoData = {
  body: OviImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ovi/image-to-video";
};

export type PostFalAiOviImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiOviImageToVideoResponse =
  PostFalAiOviImageToVideoResponses[keyof PostFalAiOviImageToVideoResponses];

export type GetFalAiOviImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ovi/image-to-video/requests/{request_id}";
};

export type GetFalAiOviImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: OviImageToVideoOutput;
};

export type GetFalAiOviImageToVideoRequestsByRequestIdResponse =
  GetFalAiOviImageToVideoRequestsByRequestIdResponses[keyof GetFalAiOviImageToVideoRequestsByRequestIdResponses];

export type GetVeedFabric10FastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/veed/fabric-1.0/fast/requests/{request_id}/status";
};

export type GetVeedFabric10FastRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetVeedFabric10FastRequestsByRequestIdStatusResponse =
  GetVeedFabric10FastRequestsByRequestIdStatusResponses[keyof GetVeedFabric10FastRequestsByRequestIdStatusResponses];

export type PutVeedFabric10FastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/fabric-1.0/fast/requests/{request_id}/cancel";
};

export type PutVeedFabric10FastRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutVeedFabric10FastRequestsByRequestIdCancelResponse =
  PutVeedFabric10FastRequestsByRequestIdCancelResponses[keyof PutVeedFabric10FastRequestsByRequestIdCancelResponses];

export type PostVeedFabric10FastData = {
  body: Fabric10FastInput;
  path?: never;
  query?: never;
  url: "/veed/fabric-1.0/fast";
};

export type PostVeedFabric10FastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostVeedFabric10FastResponse =
  PostVeedFabric10FastResponses[keyof PostVeedFabric10FastResponses];

export type GetVeedFabric10FastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/fabric-1.0/fast/requests/{request_id}";
};

export type GetVeedFabric10FastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Fabric10FastOutput;
};

export type GetVeedFabric10FastRequestsByRequestIdResponse =
  GetVeedFabric10FastRequestsByRequestIdResponses[keyof GetVeedFabric10FastRequestsByRequestIdResponses];

export type GetFalAiBytedanceOmnihumanV15RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/bytedance/omnihuman/v1.5/requests/{request_id}/status";
};

export type GetFalAiBytedanceOmnihumanV15RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiBytedanceOmnihumanV15RequestsByRequestIdStatusResponse =
  GetFalAiBytedanceOmnihumanV15RequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceOmnihumanV15RequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceOmnihumanV15RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/bytedance/omnihuman/v1.5/requests/{request_id}/cancel";
};

export type PutFalAiBytedanceOmnihumanV15RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiBytedanceOmnihumanV15RequestsByRequestIdCancelResponse =
  PutFalAiBytedanceOmnihumanV15RequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceOmnihumanV15RequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceOmnihumanV15Data = {
  body: BytedanceOmnihumanV15Input;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/omnihuman/v1.5";
};

export type PostFalAiBytedanceOmnihumanV15Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceOmnihumanV15Response =
  PostFalAiBytedanceOmnihumanV15Responses[keyof PostFalAiBytedanceOmnihumanV15Responses];

export type GetFalAiBytedanceOmnihumanV15RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/bytedance/omnihuman/v1.5/requests/{request_id}";
};

export type GetFalAiBytedanceOmnihumanV15RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: BytedanceOmnihumanV15Output;
};

export type GetFalAiBytedanceOmnihumanV15RequestsByRequestIdResponse =
  GetFalAiBytedanceOmnihumanV15RequestsByRequestIdResponses[keyof GetFalAiBytedanceOmnihumanV15RequestsByRequestIdResponses];

export type GetVeedFabric10RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/veed/fabric-1.0/requests/{request_id}/status";
};

export type GetVeedFabric10RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetVeedFabric10RequestsByRequestIdStatusResponse =
  GetVeedFabric10RequestsByRequestIdStatusResponses[keyof GetVeedFabric10RequestsByRequestIdStatusResponses];

export type PutVeedFabric10RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/fabric-1.0/requests/{request_id}/cancel";
};

export type PutVeedFabric10RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutVeedFabric10RequestsByRequestIdCancelResponse =
  PutVeedFabric10RequestsByRequestIdCancelResponses[keyof PutVeedFabric10RequestsByRequestIdCancelResponses];

export type PostVeedFabric10Data = {
  body: Fabric10Input;
  path?: never;
  query?: never;
  url: "/veed/fabric-1.0";
};

export type PostVeedFabric10Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostVeedFabric10Response =
  PostVeedFabric10Responses[keyof PostVeedFabric10Responses];

export type GetVeedFabric10RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/fabric-1.0/requests/{request_id}";
};

export type GetVeedFabric10RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Fabric10Output;
};

export type GetVeedFabric10RequestsByRequestIdResponse =
  GetVeedFabric10RequestsByRequestIdResponses[keyof GetVeedFabric10RequestsByRequestIdResponses];

export type GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v1/standard/ai-avatar/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v1/standard/ai-avatar/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV1StandardAiAvatarData = {
  body: KlingVideoV1StandardAiAvatarInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1/standard/ai-avatar";
};

export type PostFalAiKlingVideoV1StandardAiAvatarResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV1StandardAiAvatarResponse =
  PostFalAiKlingVideoV1StandardAiAvatarResponses[keyof PostFalAiKlingVideoV1StandardAiAvatarResponses];

export type GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1/standard/ai-avatar/requests/{request_id}";
};

export type GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV1StandardAiAvatarOutput;
};

export type GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdResponse =
  GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV1StandardAiAvatarRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/v1/pro/ai-avatar/requests/{request_id}/status";
};

export type GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1/pro/ai-avatar/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV1ProAiAvatarData = {
  body: KlingVideoV1ProAiAvatarInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1/pro/ai-avatar";
};

export type PostFalAiKlingVideoV1ProAiAvatarResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV1ProAiAvatarResponse =
  PostFalAiKlingVideoV1ProAiAvatarResponses[keyof PostFalAiKlingVideoV1ProAiAvatarResponses];

export type GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1/pro/ai-avatar/requests/{request_id}";
};

export type GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV1ProAiAvatarOutput;
};

export type GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdResponse =
  GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV1ProAiAvatarRequestsByRequestIdResponses];

export type GetDecartLucy14bImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/decart/lucy-14b/image-to-video/requests/{request_id}/status";
};

export type GetDecartLucy14bImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetDecartLucy14bImageToVideoRequestsByRequestIdStatusResponse =
  GetDecartLucy14bImageToVideoRequestsByRequestIdStatusResponses[keyof GetDecartLucy14bImageToVideoRequestsByRequestIdStatusResponses];

export type PutDecartLucy14bImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-14b/image-to-video/requests/{request_id}/cancel";
};

export type PutDecartLucy14bImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutDecartLucy14bImageToVideoRequestsByRequestIdCancelResponse =
  PutDecartLucy14bImageToVideoRequestsByRequestIdCancelResponses[keyof PutDecartLucy14bImageToVideoRequestsByRequestIdCancelResponses];

export type PostDecartLucy14bImageToVideoData = {
  body: Lucy14bImageToVideoInput;
  path?: never;
  query?: never;
  url: "/decart/lucy-14b/image-to-video";
};

export type PostDecartLucy14bImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostDecartLucy14bImageToVideoResponse =
  PostDecartLucy14bImageToVideoResponses[keyof PostDecartLucy14bImageToVideoResponses];

export type GetDecartLucy14bImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-14b/image-to-video/requests/{request_id}";
};

export type GetDecartLucy14bImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Lucy14bImageToVideoOutput;
};

export type GetDecartLucy14bImageToVideoRequestsByRequestIdResponse =
  GetDecartLucy14bImageToVideoRequestsByRequestIdResponses[keyof GetDecartLucy14bImageToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance/seedance/v1/lite/reference-to-video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/lite/reference-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceSeedanceV1LiteReferenceToVideoData = {
  body: BytedanceSeedanceV1LiteReferenceToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1/lite/reference-to-video";
};

export type PostFalAiBytedanceSeedanceV1LiteReferenceToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceSeedanceV1LiteReferenceToVideoResponse =
  PostFalAiBytedanceSeedanceV1LiteReferenceToVideoResponses[keyof PostFalAiBytedanceSeedanceV1LiteReferenceToVideoResponses];

export type GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/lite/reference-to-video/requests/{request_id}";
  };

export type GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceSeedanceV1LiteReferenceToVideoOutput;
  };

export type GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceSeedanceV1LiteReferenceToVideoRequestsByRequestIdResponses];

export type GetFalAiWanAtiRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-ati/requests/{request_id}/status";
};

export type GetFalAiWanAtiRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanAtiRequestsByRequestIdStatusResponse =
  GetFalAiWanAtiRequestsByRequestIdStatusResponses[keyof GetFalAiWanAtiRequestsByRequestIdStatusResponses];

export type PutFalAiWanAtiRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-ati/requests/{request_id}/cancel";
};

export type PutFalAiWanAtiRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanAtiRequestsByRequestIdCancelResponse =
  PutFalAiWanAtiRequestsByRequestIdCancelResponses[keyof PutFalAiWanAtiRequestsByRequestIdCancelResponses];

export type PostFalAiWanAtiData = {
  body: WanAtiInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-ati";
};

export type PostFalAiWanAtiResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanAtiResponse =
  PostFalAiWanAtiResponses[keyof PostFalAiWanAtiResponses];

export type GetFalAiWanAtiRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-ati/requests/{request_id}";
};

export type GetFalAiWanAtiRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanAtiOutput;
};

export type GetFalAiWanAtiRequestsByRequestIdResponse =
  GetFalAiWanAtiRequestsByRequestIdResponses[keyof GetFalAiWanAtiRequestsByRequestIdResponses];

export type GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/decart/lucy-5b/image-to-video/requests/{request_id}/status";
};

export type GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiDecartLucy5bImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/decart/lucy-5b/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiDecartLucy5bImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiDecartLucy5bImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiDecartLucy5bImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiDecartLucy5bImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiDecartLucy5bImageToVideoData = {
  body: DecartLucy5bImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/decart/lucy-5b/image-to-video";
};

export type PostFalAiDecartLucy5bImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiDecartLucy5bImageToVideoResponse =
  PostFalAiDecartLucy5bImageToVideoResponses[keyof PostFalAiDecartLucy5bImageToVideoResponses];

export type GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/decart/lucy-5b/image-to-video/requests/{request_id}";
};

export type GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: DecartLucy5bImageToVideoOutput;
};

export type GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdResponse =
  GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdResponses[keyof GetFalAiDecartLucy5bImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV5TransitionRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5/transition/requests/{request_id}/status";
};

export type GetFalAiPixverseV5TransitionRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV5TransitionRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV5TransitionRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV5TransitionRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV5TransitionRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5/transition/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV5TransitionRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV5TransitionRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV5TransitionRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV5TransitionRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV5TransitionData = {
  body: PixverseV5TransitionInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5/transition";
};

export type PostFalAiPixverseV5TransitionResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV5TransitionResponse =
  PostFalAiPixverseV5TransitionResponses[keyof PostFalAiPixverseV5TransitionResponses];

export type GetFalAiPixverseV5TransitionRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5/transition/requests/{request_id}";
};

export type GetFalAiPixverseV5TransitionRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV5TransitionOutput;
};

export type GetFalAiPixverseV5TransitionRequestsByRequestIdResponse =
  GetFalAiPixverseV5TransitionRequestsByRequestIdResponses[keyof GetFalAiPixverseV5TransitionRequestsByRequestIdResponses];

export type GetFalAiPixverseV5EffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5/effects/requests/{request_id}/status";
};

export type GetFalAiPixverseV5EffectsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV5EffectsRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV5EffectsRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV5EffectsRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV5EffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5/effects/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV5EffectsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV5EffectsRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV5EffectsRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV5EffectsRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV5EffectsData = {
  body: PixverseV5EffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5/effects";
};

export type PostFalAiPixverseV5EffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV5EffectsResponse =
  PostFalAiPixverseV5EffectsResponses[keyof PostFalAiPixverseV5EffectsResponses];

export type GetFalAiPixverseV5EffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5/effects/requests/{request_id}";
};

export type GetFalAiPixverseV5EffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV5EffectsOutput;
};

export type GetFalAiPixverseV5EffectsRequestsByRequestIdResponse =
  GetFalAiPixverseV5EffectsRequestsByRequestIdResponses[keyof GetFalAiPixverseV5EffectsRequestsByRequestIdResponses];

export type GetFalAiPixverseV5ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5/image-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV5ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV5ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV5ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV5ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV5ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV5ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV5ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV5ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV5ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV5ImageToVideoData = {
  body: PixverseV5ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5/image-to-video";
};

export type PostFalAiPixverseV5ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV5ImageToVideoResponse =
  PostFalAiPixverseV5ImageToVideoResponses[keyof PostFalAiPixverseV5ImageToVideoResponses];

export type GetFalAiPixverseV5ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5/image-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV5ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV5ImageToVideoOutput;
};

export type GetFalAiPixverseV5ImageToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV5ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV5ImageToVideoRequestsByRequestIdResponses];

export type GetMoonvalleyMareyI2vRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/moonvalley/marey/i2v/requests/{request_id}/status";
};

export type GetMoonvalleyMareyI2vRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetMoonvalleyMareyI2vRequestsByRequestIdStatusResponse =
  GetMoonvalleyMareyI2vRequestsByRequestIdStatusResponses[keyof GetMoonvalleyMareyI2vRequestsByRequestIdStatusResponses];

export type PutMoonvalleyMareyI2vRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/moonvalley/marey/i2v/requests/{request_id}/cancel";
};

export type PutMoonvalleyMareyI2vRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutMoonvalleyMareyI2vRequestsByRequestIdCancelResponse =
  PutMoonvalleyMareyI2vRequestsByRequestIdCancelResponses[keyof PutMoonvalleyMareyI2vRequestsByRequestIdCancelResponses];

export type PostMoonvalleyMareyI2vData = {
  body: MareyI2vInput;
  path?: never;
  query?: never;
  url: "/moonvalley/marey/i2v";
};

export type PostMoonvalleyMareyI2vResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostMoonvalleyMareyI2vResponse =
  PostMoonvalleyMareyI2vResponses[keyof PostMoonvalleyMareyI2vResponses];

export type GetMoonvalleyMareyI2vRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/moonvalley/marey/i2v/requests/{request_id}";
};

export type GetMoonvalleyMareyI2vRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MareyI2vOutput;
};

export type GetMoonvalleyMareyI2vRequestsByRequestIdResponse =
  GetMoonvalleyMareyI2vRequestsByRequestIdResponses[keyof GetMoonvalleyMareyI2vRequestsByRequestIdResponses];

export type GetFalAiBytedanceVideoStylizeRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/bytedance/video-stylize/requests/{request_id}/status";
};

export type GetFalAiBytedanceVideoStylizeRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiBytedanceVideoStylizeRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceVideoStylizeRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceVideoStylizeRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceVideoStylizeRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/bytedance/video-stylize/requests/{request_id}/cancel";
};

export type PutFalAiBytedanceVideoStylizeRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiBytedanceVideoStylizeRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceVideoStylizeRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceVideoStylizeRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceVideoStylizeData = {
  body: BytedanceVideoStylizeInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/video-stylize";
};

export type PostFalAiBytedanceVideoStylizeResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceVideoStylizeResponse =
  PostFalAiBytedanceVideoStylizeResponses[keyof PostFalAiBytedanceVideoStylizeResponses];

export type GetFalAiBytedanceVideoStylizeRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/bytedance/video-stylize/requests/{request_id}";
};

export type GetFalAiBytedanceVideoStylizeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: BytedanceVideoStylizeOutput;
};

export type GetFalAiBytedanceVideoStylizeRequestsByRequestIdResponse =
  GetFalAiBytedanceVideoStylizeRequestsByRequestIdResponses[keyof GetFalAiBytedanceVideoStylizeRequestsByRequestIdResponses];

export type GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/lora/requests/{request_id}/status";
};

export type GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/lora/requests/{request_id}/cancel";
};

export type PutFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiWanV22A14bImageToVideoLoraData = {
  body: WanV22A14bImageToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/lora";
};

export type PostFalAiWanV22A14bImageToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV22A14bImageToVideoLoraResponse =
  PostFalAiWanV22A14bImageToVideoLoraResponses[keyof PostFalAiWanV22A14bImageToVideoLoraResponses];

export type GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/lora/requests/{request_id}";
};

export type GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV22A14bImageToVideoLoraOutput;
};

export type GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdResponse =
  GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiWanV22A14bImageToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-02-fast/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-02-fast/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo02FastImageToVideoData = {
  body: MinimaxHailuo02FastImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-02-fast/image-to-video";
};

export type PostFalAiMinimaxHailuo02FastImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo02FastImageToVideoResponse =
  PostFalAiMinimaxHailuo02FastImageToVideoResponses[keyof PostFalAiMinimaxHailuo02FastImageToVideoResponses];

export type GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/hailuo-02-fast/image-to-video/requests/{request_id}";
};

export type GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo02FastImageToVideoOutput;
  };

export type GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo02FastImageToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo3ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3/image-to-video/requests/{request_id}/status";
};

export type GetFalAiVeo3ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo3ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo3ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo3ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo3ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiVeo3ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo3ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo3ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo3ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo3ImageToVideoData = {
  body: Veo3ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3/image-to-video";
};

export type PostFalAiVeo3ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo3ImageToVideoResponse =
  PostFalAiVeo3ImageToVideoResponses[keyof PostFalAiVeo3ImageToVideoResponses];

export type GetFalAiVeo3ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3/image-to-video/requests/{request_id}";
};

export type GetFalAiVeo3ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo3ImageToVideoOutput;
};

export type GetFalAiVeo3ImageToVideoRequestsByRequestIdResponse =
  GetFalAiVeo3ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiVeo3ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/turbo/requests/{request_id}/status";
};

export type GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdStatusResponse =
  GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdStatusResponses[keyof GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdStatusResponses];

export type PutFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/turbo/requests/{request_id}/cancel";
};

export type PutFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdCancelResponse =
  PutFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdCancelResponses[keyof PutFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdCancelResponses];

export type PostFalAiWanV22A14bImageToVideoTurboData = {
  body: WanV22A14bImageToVideoTurboInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/turbo";
};

export type PostFalAiWanV22A14bImageToVideoTurboResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV22A14bImageToVideoTurboResponse =
  PostFalAiWanV22A14bImageToVideoTurboResponses[keyof PostFalAiWanV22A14bImageToVideoTurboResponses];

export type GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/turbo/requests/{request_id}";
};

export type GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV22A14bImageToVideoTurboOutput;
};

export type GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdResponse =
  GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdResponses[keyof GetFalAiWanV22A14bImageToVideoTurboRequestsByRequestIdResponses];

export type GetFalAiWanV225bImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-5b/image-to-video/requests/{request_id}/status";
};

export type GetFalAiWanV225bImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanV225bImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanV225bImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanV225bImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWanV225bImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWanV225bImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanV225bImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanV225bImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanV225bImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWanV225bImageToVideoData = {
  body: WanV225bImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/image-to-video";
};

export type PostFalAiWanV225bImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV225bImageToVideoResponse =
  PostFalAiWanV225bImageToVideoResponses[keyof PostFalAiWanV225bImageToVideoResponses];

export type GetFalAiWanV225bImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/image-to-video/requests/{request_id}";
};

export type GetFalAiWanV225bImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV225bImageToVideoOutput;
};

export type GetFalAiWanV225bImageToVideoRequestsByRequestIdResponse =
  GetFalAiWanV225bImageToVideoRequestsByRequestIdResponses[keyof GetFalAiWanV225bImageToVideoRequestsByRequestIdResponses];

export type GetFalAiWanV22A14bImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/requests/{request_id}/status";
};

export type GetFalAiWanV22A14bImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanV22A14bImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanV22A14bImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanV22A14bImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWanV22A14bImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWanV22A14bImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanV22A14bImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanV22A14bImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanV22A14bImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWanV22A14bImageToVideoData = {
  body: WanV22A14bImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/image-to-video";
};

export type PostFalAiWanV22A14bImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV22A14bImageToVideoResponse =
  PostFalAiWanV22A14bImageToVideoResponses[keyof PostFalAiWanV22A14bImageToVideoResponses];

export type GetFalAiWanV22A14bImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/image-to-video/requests/{request_id}";
};

export type GetFalAiWanV22A14bImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV22A14bImageToVideoOutput;
};

export type GetFalAiWanV22A14bImageToVideoRequestsByRequestIdResponse =
  GetFalAiWanV22A14bImageToVideoRequestsByRequestIdResponses[keyof GetFalAiWanV22A14bImageToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceOmnihumanRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/bytedance/omnihuman/requests/{request_id}/status";
};

export type GetFalAiBytedanceOmnihumanRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiBytedanceOmnihumanRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceOmnihumanRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceOmnihumanRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceOmnihumanRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/bytedance/omnihuman/requests/{request_id}/cancel";
};

export type PutFalAiBytedanceOmnihumanRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiBytedanceOmnihumanRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceOmnihumanRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceOmnihumanRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceOmnihumanData = {
  body: BytedanceOmnihumanInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/omnihuman";
};

export type PostFalAiBytedanceOmnihumanResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceOmnihumanResponse =
  PostFalAiBytedanceOmnihumanResponses[keyof PostFalAiBytedanceOmnihumanResponses];

export type GetFalAiBytedanceOmnihumanRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/bytedance/omnihuman/requests/{request_id}";
};

export type GetFalAiBytedanceOmnihumanRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: BytedanceOmnihumanOutput;
};

export type GetFalAiBytedanceOmnihumanRequestsByRequestIdResponse =
  GetFalAiBytedanceOmnihumanRequestsByRequestIdResponses[keyof GetFalAiBytedanceOmnihumanRequestsByRequestIdResponses];

export type GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltxv-13b-098-distilled/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltxv-13b-098-distilled/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtxv13B098DistilledImageToVideoData = {
  body: Ltxv13B098DistilledImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltxv-13b-098-distilled/image-to-video";
};

export type PostFalAiLtxv13B098DistilledImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxv13B098DistilledImageToVideoResponse =
  PostFalAiLtxv13B098DistilledImageToVideoResponses[keyof PostFalAiLtxv13B098DistilledImageToVideoResponses];

export type GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltxv-13b-098-distilled/image-to-video/requests/{request_id}";
};

export type GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: Ltxv13B098DistilledImageToVideoOutput;
  };

export type GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdResponse =
  GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLtxv13B098DistilledImageToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo3FastImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3/fast/image-to-video/requests/{request_id}/status";
};

export type GetFalAiVeo3FastImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo3FastImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo3FastImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo3FastImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo3FastImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3/fast/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiVeo3FastImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo3FastImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo3FastImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo3FastImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo3FastImageToVideoData = {
  body: Veo3FastImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3/fast/image-to-video";
};

export type PostFalAiVeo3FastImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo3FastImageToVideoResponse =
  PostFalAiVeo3FastImageToVideoResponses[keyof PostFalAiVeo3FastImageToVideoResponses];

export type GetFalAiVeo3FastImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3/fast/image-to-video/requests/{request_id}";
};

export type GetFalAiVeo3FastImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo3FastImageToVideoOutput;
};

export type GetFalAiVeo3FastImageToVideoRequestsByRequestIdResponse =
  GetFalAiVeo3FastImageToVideoRequestsByRequestIdResponses[keyof GetFalAiVeo3FastImageToVideoRequestsByRequestIdResponses];

export type GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q1/reference-to-video/requests/{request_id}/status";
};

export type GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ1ReferenceToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q1/reference-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduQ1ReferenceToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduQ1ReferenceToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduQ1ReferenceToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ1ReferenceToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ1ReferenceToVideoData = {
  body: ViduQ1ReferenceToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q1/reference-to-video";
};

export type PostFalAiViduQ1ReferenceToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ1ReferenceToVideoResponse =
  PostFalAiViduQ1ReferenceToVideoResponses[keyof PostFalAiViduQ1ReferenceToVideoResponses];

export type GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q1/reference-to-video/requests/{request_id}";
};

export type GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ1ReferenceToVideoOutput;
};

export type GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdResponse =
  GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdResponses[keyof GetFalAiViduQ1ReferenceToVideoRequestsByRequestIdResponses];

export type GetFalAiAiAvatarSingleTextRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ai-avatar/single-text/requests/{request_id}/status";
};

export type GetFalAiAiAvatarSingleTextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiAiAvatarSingleTextRequestsByRequestIdStatusResponse =
  GetFalAiAiAvatarSingleTextRequestsByRequestIdStatusResponses[keyof GetFalAiAiAvatarSingleTextRequestsByRequestIdStatusResponses];

export type PutFalAiAiAvatarSingleTextRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ai-avatar/single-text/requests/{request_id}/cancel";
};

export type PutFalAiAiAvatarSingleTextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiAiAvatarSingleTextRequestsByRequestIdCancelResponse =
  PutFalAiAiAvatarSingleTextRequestsByRequestIdCancelResponses[keyof PutFalAiAiAvatarSingleTextRequestsByRequestIdCancelResponses];

export type PostFalAiAiAvatarSingleTextData = {
  body: AiAvatarSingleTextInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ai-avatar/single-text";
};

export type PostFalAiAiAvatarSingleTextResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiAiAvatarSingleTextResponse =
  PostFalAiAiAvatarSingleTextResponses[keyof PostFalAiAiAvatarSingleTextResponses];

export type GetFalAiAiAvatarSingleTextRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ai-avatar/single-text/requests/{request_id}";
};

export type GetFalAiAiAvatarSingleTextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AiAvatarSingleTextOutput;
};

export type GetFalAiAiAvatarSingleTextRequestsByRequestIdResponse =
  GetFalAiAiAvatarSingleTextRequestsByRequestIdResponses[keyof GetFalAiAiAvatarSingleTextRequestsByRequestIdResponses];

export type GetFalAiAiAvatarRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ai-avatar/requests/{request_id}/status";
};

export type GetFalAiAiAvatarRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiAiAvatarRequestsByRequestIdStatusResponse =
  GetFalAiAiAvatarRequestsByRequestIdStatusResponses[keyof GetFalAiAiAvatarRequestsByRequestIdStatusResponses];

export type PutFalAiAiAvatarRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ai-avatar/requests/{request_id}/cancel";
};

export type PutFalAiAiAvatarRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiAiAvatarRequestsByRequestIdCancelResponse =
  PutFalAiAiAvatarRequestsByRequestIdCancelResponses[keyof PutFalAiAiAvatarRequestsByRequestIdCancelResponses];

export type PostFalAiAiAvatarData = {
  body: AiAvatarInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ai-avatar";
};

export type PostFalAiAiAvatarResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiAiAvatarResponse =
  PostFalAiAiAvatarResponses[keyof PostFalAiAiAvatarResponses];

export type GetFalAiAiAvatarRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ai-avatar/requests/{request_id}";
};

export type GetFalAiAiAvatarRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AiAvatarOutput;
};

export type GetFalAiAiAvatarRequestsByRequestIdResponse =
  GetFalAiAiAvatarRequestsByRequestIdResponses[keyof GetFalAiAiAvatarRequestsByRequestIdResponses];

export type GetFalAiAiAvatarMultiTextRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ai-avatar/multi-text/requests/{request_id}/status";
};

export type GetFalAiAiAvatarMultiTextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiAiAvatarMultiTextRequestsByRequestIdStatusResponse =
  GetFalAiAiAvatarMultiTextRequestsByRequestIdStatusResponses[keyof GetFalAiAiAvatarMultiTextRequestsByRequestIdStatusResponses];

export type PutFalAiAiAvatarMultiTextRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ai-avatar/multi-text/requests/{request_id}/cancel";
};

export type PutFalAiAiAvatarMultiTextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiAiAvatarMultiTextRequestsByRequestIdCancelResponse =
  PutFalAiAiAvatarMultiTextRequestsByRequestIdCancelResponses[keyof PutFalAiAiAvatarMultiTextRequestsByRequestIdCancelResponses];

export type PostFalAiAiAvatarMultiTextData = {
  body: AiAvatarMultiTextInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ai-avatar/multi-text";
};

export type PostFalAiAiAvatarMultiTextResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiAiAvatarMultiTextResponse =
  PostFalAiAiAvatarMultiTextResponses[keyof PostFalAiAiAvatarMultiTextResponses];

export type GetFalAiAiAvatarMultiTextRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ai-avatar/multi-text/requests/{request_id}";
};

export type GetFalAiAiAvatarMultiTextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AiAvatarMultiTextOutput;
};

export type GetFalAiAiAvatarMultiTextRequestsByRequestIdResponse =
  GetFalAiAiAvatarMultiTextRequestsByRequestIdResponses[keyof GetFalAiAiAvatarMultiTextRequestsByRequestIdResponses];

export type GetFalAiAiAvatarMultiRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ai-avatar/multi/requests/{request_id}/status";
};

export type GetFalAiAiAvatarMultiRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiAiAvatarMultiRequestsByRequestIdStatusResponse =
  GetFalAiAiAvatarMultiRequestsByRequestIdStatusResponses[keyof GetFalAiAiAvatarMultiRequestsByRequestIdStatusResponses];

export type PutFalAiAiAvatarMultiRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ai-avatar/multi/requests/{request_id}/cancel";
};

export type PutFalAiAiAvatarMultiRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiAiAvatarMultiRequestsByRequestIdCancelResponse =
  PutFalAiAiAvatarMultiRequestsByRequestIdCancelResponses[keyof PutFalAiAiAvatarMultiRequestsByRequestIdCancelResponses];

export type PostFalAiAiAvatarMultiData = {
  body: AiAvatarMultiInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ai-avatar/multi";
};

export type PostFalAiAiAvatarMultiResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiAiAvatarMultiResponse =
  PostFalAiAiAvatarMultiResponses[keyof PostFalAiAiAvatarMultiResponses];

export type GetFalAiAiAvatarMultiRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ai-avatar/multi/requests/{request_id}";
};

export type GetFalAiAiAvatarMultiRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AiAvatarMultiOutput;
};

export type GetFalAiAiAvatarMultiRequestsByRequestIdResponse =
  GetFalAiAiAvatarMultiRequestsByRequestIdResponses[keyof GetFalAiAiAvatarMultiRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-02/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-02/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo02ProImageToVideoData = {
  body: MinimaxHailuo02ProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-02/pro/image-to-video";
};

export type PostFalAiMinimaxHailuo02ProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo02ProImageToVideoResponse =
  PostFalAiMinimaxHailuo02ProImageToVideoResponses[keyof PostFalAiMinimaxHailuo02ProImageToVideoResponses];

export type GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/hailuo-02/pro/image-to-video/requests/{request_id}";
};

export type GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo02ProImageToVideoOutput;
  };

export type GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo02ProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance/seedance/v1/lite/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/lite/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceSeedanceV1LiteImageToVideoData = {
  body: BytedanceSeedanceV1LiteImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1/lite/image-to-video";
};

export type PostFalAiBytedanceSeedanceV1LiteImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceSeedanceV1LiteImageToVideoResponse =
  PostFalAiBytedanceSeedanceV1LiteImageToVideoResponses[keyof PostFalAiBytedanceSeedanceV1LiteImageToVideoResponses];

export type GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/lite/image-to-video/requests/{request_id}";
  };

export type GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceSeedanceV1LiteImageToVideoOutput;
  };

export type GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceSeedanceV1LiteImageToVideoRequestsByRequestIdResponses];

export type GetFalAiHunyuanAvatarRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-avatar/requests/{request_id}/status";
};

export type GetFalAiHunyuanAvatarRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiHunyuanAvatarRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanAvatarRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanAvatarRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanAvatarRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-avatar/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanAvatarRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiHunyuanAvatarRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanAvatarRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanAvatarRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanAvatarData = {
  body: HunyuanAvatarInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-avatar";
};

export type PostFalAiHunyuanAvatarResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanAvatarResponse =
  PostFalAiHunyuanAvatarResponses[keyof PostFalAiHunyuanAvatarResponses];

export type GetFalAiHunyuanAvatarRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-avatar/requests/{request_id}";
};

export type GetFalAiHunyuanAvatarRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanAvatarOutput;
};

export type GetFalAiHunyuanAvatarRequestsByRequestIdResponse =
  GetFalAiHunyuanAvatarRequestsByRequestIdResponses[keyof GetFalAiHunyuanAvatarRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.1/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.1/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV21ProImageToVideoData = {
  body: KlingVideoV21ProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.1/pro/image-to-video";
};

export type PostFalAiKlingVideoV21ProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV21ProImageToVideoResponse =
  PostFalAiKlingVideoV21ProImageToVideoResponses[keyof PostFalAiKlingVideoV21ProImageToVideoResponses];

export type GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.1/pro/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV21ProImageToVideoOutput;
};

export type GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV21ProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiHunyuanPortraitRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-portrait/requests/{request_id}/status";
};

export type GetFalAiHunyuanPortraitRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiHunyuanPortraitRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanPortraitRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanPortraitRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanPortraitRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-portrait/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanPortraitRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiHunyuanPortraitRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanPortraitRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanPortraitRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanPortraitData = {
  body: HunyuanPortraitInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-portrait";
};

export type PostFalAiHunyuanPortraitResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanPortraitResponse =
  PostFalAiHunyuanPortraitResponses[keyof PostFalAiHunyuanPortraitResponses];

export type GetFalAiHunyuanPortraitRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-portrait/requests/{request_id}";
};

export type GetFalAiHunyuanPortraitRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanPortraitOutput;
};

export type GetFalAiHunyuanPortraitRequestsByRequestIdResponse =
  GetFalAiHunyuanPortraitRequestsByRequestIdResponses[keyof GetFalAiHunyuanPortraitRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v1.6/standard/elements/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV16StandardElementsRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v1.6/standard/elements/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV16StandardElementsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV16StandardElementsRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV16StandardElementsRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV16StandardElementsRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV16StandardElementsData = {
  body: KlingVideoV16StandardElementsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.6/standard/elements";
};

export type PostFalAiKlingVideoV16StandardElementsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV16StandardElementsResponse =
  PostFalAiKlingVideoV16StandardElementsResponses[keyof PostFalAiKlingVideoV16StandardElementsResponses];

export type GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/standard/elements/requests/{request_id}";
};

export type GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV16StandardElementsOutput;
  };

export type GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdResponse =
  GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV16StandardElementsRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV16ProElementsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/v1.6/pro/elements/requests/{request_id}/status";
};

export type GetFalAiKlingVideoV16ProElementsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV16ProElementsRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV16ProElementsRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV16ProElementsRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV16ProElementsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/elements/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoV16ProElementsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV16ProElementsRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV16ProElementsRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV16ProElementsRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV16ProElementsData = {
  body: KlingVideoV16ProElementsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/elements";
};

export type PostFalAiKlingVideoV16ProElementsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV16ProElementsResponse =
  PostFalAiKlingVideoV16ProElementsResponses[keyof PostFalAiKlingVideoV16ProElementsResponses];

export type GetFalAiKlingVideoV16ProElementsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/elements/requests/{request_id}";
};

export type GetFalAiKlingVideoV16ProElementsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV16ProElementsOutput;
};

export type GetFalAiKlingVideoV16ProElementsRequestsByRequestIdResponse =
  GetFalAiKlingVideoV16ProElementsRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV16ProElementsRequestsByRequestIdResponses];

export type GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-video-13b-distilled/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-video-13b-distilled/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideo13bDistilledImageToVideoData = {
  body: LtxVideo13bDistilledImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-13b-distilled/image-to-video";
};

export type PostFalAiLtxVideo13bDistilledImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideo13bDistilledImageToVideoResponse =
  PostFalAiLtxVideo13bDistilledImageToVideoResponses[keyof PostFalAiLtxVideo13bDistilledImageToVideoResponses];

export type GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-distilled/image-to-video/requests/{request_id}";
};

export type GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LtxVideo13bDistilledImageToVideoOutput;
  };

export type GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDistilledImageToVideoRequestsByRequestIdResponses];

export type GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video-13b-dev/image-to-video/requests/{request_id}/status";
};

export type GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideo13bDevImageToVideoData = {
  body: LtxVideo13bDevImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/image-to-video";
};

export type PostFalAiLtxVideo13bDevImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideo13bDevImageToVideoResponse =
  PostFalAiLtxVideo13bDevImageToVideoResponses[keyof PostFalAiLtxVideo13bDevImageToVideoResponses];

export type GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/image-to-video/requests/{request_id}";
};

export type GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideo13bDevImageToVideoOutput;
};

export type GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDevImageToVideoRequestsByRequestIdResponses];

export type GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video-lora/image-to-video/requests/{request_id}/status";
};

export type GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideoLoraImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-lora/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideoLoraImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxVideoLoraImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoLoraImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoLoraImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideoLoraImageToVideoData = {
  body: LtxVideoLoraImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-lora/image-to-video";
};

export type PostFalAiLtxVideoLoraImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideoLoraImageToVideoResponse =
  PostFalAiLtxVideoLoraImageToVideoResponses[keyof PostFalAiLtxVideoLoraImageToVideoResponses];

export type GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-lora/image-to-video/requests/{request_id}";
};

export type GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideoLoraImageToVideoOutput;
};

export type GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdResponse =
  GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLtxVideoLoraImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV45TransitionRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4.5/transition/requests/{request_id}/status";
};

export type GetFalAiPixverseV45TransitionRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV45TransitionRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV45TransitionRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV45TransitionRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV45TransitionRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/transition/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV45TransitionRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV45TransitionRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV45TransitionRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV45TransitionRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV45TransitionData = {
  body: PixverseV45TransitionInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4.5/transition";
};

export type PostFalAiPixverseV45TransitionResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV45TransitionResponse =
  PostFalAiPixverseV45TransitionResponses[keyof PostFalAiPixverseV45TransitionResponses];

export type GetFalAiPixverseV45TransitionRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/transition/requests/{request_id}";
};

export type GetFalAiPixverseV45TransitionRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV45TransitionOutput;
};

export type GetFalAiPixverseV45TransitionRequestsByRequestIdResponse =
  GetFalAiPixverseV45TransitionRequestsByRequestIdResponses[keyof GetFalAiPixverseV45TransitionRequestsByRequestIdResponses];

export type GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4.5/image-to-video/fast/requests/{request_id}/status";
};

export type GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV45ImageToVideoFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/image-to-video/fast/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV45ImageToVideoFastRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV45ImageToVideoFastRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV45ImageToVideoFastRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV45ImageToVideoFastRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV45ImageToVideoFastData = {
  body: PixverseV45ImageToVideoFastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4.5/image-to-video/fast";
};

export type PostFalAiPixverseV45ImageToVideoFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV45ImageToVideoFastResponse =
  PostFalAiPixverseV45ImageToVideoFastResponses[keyof PostFalAiPixverseV45ImageToVideoFastResponses];

export type GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/image-to-video/fast/requests/{request_id}";
};

export type GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV45ImageToVideoFastOutput;
};

export type GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdResponse =
  GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdResponses[keyof GetFalAiPixverseV45ImageToVideoFastRequestsByRequestIdResponses];

export type GetFalAiPixverseV45EffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4.5/effects/requests/{request_id}/status";
};

export type GetFalAiPixverseV45EffectsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV45EffectsRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV45EffectsRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV45EffectsRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV45EffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/effects/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV45EffectsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV45EffectsRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV45EffectsRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV45EffectsRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV45EffectsData = {
  body: PixverseV45EffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4.5/effects";
};

export type PostFalAiPixverseV45EffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV45EffectsResponse =
  PostFalAiPixverseV45EffectsResponses[keyof PostFalAiPixverseV45EffectsResponses];

export type GetFalAiPixverseV45EffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/effects/requests/{request_id}";
};

export type GetFalAiPixverseV45EffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV45EffectsOutput;
};

export type GetFalAiPixverseV45EffectsRequestsByRequestIdResponse =
  GetFalAiPixverseV45EffectsRequestsByRequestIdResponses[keyof GetFalAiPixverseV45EffectsRequestsByRequestIdResponses];

export type GetFalAiHunyuanCustomRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-custom/requests/{request_id}/status";
};

export type GetFalAiHunyuanCustomRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiHunyuanCustomRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanCustomRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanCustomRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanCustomRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-custom/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanCustomRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiHunyuanCustomRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanCustomRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanCustomRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanCustomData = {
  body: HunyuanCustomInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-custom";
};

export type PostFalAiHunyuanCustomResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanCustomResponse =
  PostFalAiHunyuanCustomResponses[keyof PostFalAiHunyuanCustomResponses];

export type GetFalAiHunyuanCustomRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-custom/requests/{request_id}";
};

export type GetFalAiHunyuanCustomRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanCustomOutput;
};

export type GetFalAiHunyuanCustomRequestsByRequestIdResponse =
  GetFalAiHunyuanCustomRequestsByRequestIdResponses[keyof GetFalAiHunyuanCustomRequestsByRequestIdResponses];

export type GetFalAiFramepackF1RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/framepack/f1/requests/{request_id}/status";
};

export type GetFalAiFramepackF1RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFramepackF1RequestsByRequestIdStatusResponse =
  GetFalAiFramepackF1RequestsByRequestIdStatusResponses[keyof GetFalAiFramepackF1RequestsByRequestIdStatusResponses];

export type PutFalAiFramepackF1RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/framepack/f1/requests/{request_id}/cancel";
};

export type PutFalAiFramepackF1RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFramepackF1RequestsByRequestIdCancelResponse =
  PutFalAiFramepackF1RequestsByRequestIdCancelResponses[keyof PutFalAiFramepackF1RequestsByRequestIdCancelResponses];

export type PostFalAiFramepackF1Data = {
  body: FramepackF1Input;
  path?: never;
  query?: never;
  url: "/fal-ai/framepack/f1";
};

export type PostFalAiFramepackF1Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFramepackF1Response =
  PostFalAiFramepackF1Responses[keyof PostFalAiFramepackF1Responses];

export type GetFalAiFramepackF1RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/framepack/f1/requests/{request_id}";
};

export type GetFalAiFramepackF1RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FramepackF1Output;
};

export type GetFalAiFramepackF1RequestsByRequestIdResponse =
  GetFalAiFramepackF1RequestsByRequestIdResponses[keyof GetFalAiFramepackF1RequestsByRequestIdResponses];

export type GetFalAiViduQ1StartEndToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q1/start-end-to-video/requests/{request_id}/status";
};

export type GetFalAiViduQ1StartEndToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduQ1StartEndToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduQ1StartEndToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ1StartEndToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ1StartEndToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q1/start-end-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduQ1StartEndToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduQ1StartEndToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduQ1StartEndToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ1StartEndToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ1StartEndToVideoData = {
  body: ViduQ1StartEndToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q1/start-end-to-video";
};

export type PostFalAiViduQ1StartEndToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ1StartEndToVideoResponse =
  PostFalAiViduQ1StartEndToVideoResponses[keyof PostFalAiViduQ1StartEndToVideoResponses];

export type GetFalAiViduQ1StartEndToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q1/start-end-to-video/requests/{request_id}";
};

export type GetFalAiViduQ1StartEndToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ1StartEndToVideoOutput;
};

export type GetFalAiViduQ1StartEndToVideoRequestsByRequestIdResponse =
  GetFalAiViduQ1StartEndToVideoRequestsByRequestIdResponses[keyof GetFalAiViduQ1StartEndToVideoRequestsByRequestIdResponses];

export type GetFalAiViduQ1ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q1/image-to-video/requests/{request_id}/status";
};

export type GetFalAiViduQ1ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduQ1ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduQ1ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ1ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ1ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q1/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduQ1ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduQ1ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduQ1ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ1ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ1ImageToVideoData = {
  body: ViduQ1ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q1/image-to-video";
};

export type PostFalAiViduQ1ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ1ImageToVideoResponse =
  PostFalAiViduQ1ImageToVideoResponses[keyof PostFalAiViduQ1ImageToVideoResponses];

export type GetFalAiViduQ1ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q1/image-to-video/requests/{request_id}";
};

export type GetFalAiViduQ1ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ1ImageToVideoOutput;
};

export type GetFalAiViduQ1ImageToVideoRequestsByRequestIdResponse =
  GetFalAiViduQ1ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiViduQ1ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiMagiImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/magi/image-to-video/requests/{request_id}/status";
};

export type GetFalAiMagiImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMagiImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMagiImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMagiImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMagiImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiMagiImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMagiImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMagiImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMagiImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMagiImageToVideoData = {
  body: MagiImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/magi/image-to-video";
};

export type PostFalAiMagiImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMagiImageToVideoResponse =
  PostFalAiMagiImageToVideoResponses[keyof PostFalAiMagiImageToVideoResponses];

export type GetFalAiMagiImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi/image-to-video/requests/{request_id}";
};

export type GetFalAiMagiImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MagiImageToVideoOutput;
};

export type GetFalAiMagiImageToVideoRequestsByRequestIdResponse =
  GetFalAiMagiImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMagiImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV4EffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4/effects/requests/{request_id}/status";
};

export type GetFalAiPixverseV4EffectsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV4EffectsRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV4EffectsRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV4EffectsRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV4EffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/effects/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV4EffectsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV4EffectsRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV4EffectsRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV4EffectsRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV4EffectsData = {
  body: PixverseV4EffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4/effects";
};

export type PostFalAiPixverseV4EffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV4EffectsResponse =
  PostFalAiPixverseV4EffectsResponses[keyof PostFalAiPixverseV4EffectsResponses];

export type GetFalAiPixverseV4EffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/effects/requests/{request_id}";
};

export type GetFalAiPixverseV4EffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV4EffectsOutput;
};

export type GetFalAiPixverseV4EffectsRequestsByRequestIdResponse =
  GetFalAiPixverseV4EffectsRequestsByRequestIdResponses[keyof GetFalAiPixverseV4EffectsRequestsByRequestIdResponses];

export type GetFalAiMagiDistilledImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/magi-distilled/image-to-video/requests/{request_id}/status";
};

export type GetFalAiMagiDistilledImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMagiDistilledImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMagiDistilledImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMagiDistilledImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMagiDistilledImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi-distilled/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiMagiDistilledImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMagiDistilledImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMagiDistilledImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMagiDistilledImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMagiDistilledImageToVideoData = {
  body: MagiDistilledImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/magi-distilled/image-to-video";
};

export type PostFalAiMagiDistilledImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMagiDistilledImageToVideoResponse =
  PostFalAiMagiDistilledImageToVideoResponses[keyof PostFalAiMagiDistilledImageToVideoResponses];

export type GetFalAiMagiDistilledImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi-distilled/image-to-video/requests/{request_id}";
};

export type GetFalAiMagiDistilledImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MagiDistilledImageToVideoOutput;
};

export type GetFalAiMagiDistilledImageToVideoRequestsByRequestIdResponse =
  GetFalAiMagiDistilledImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMagiDistilledImageToVideoRequestsByRequestIdResponses];

export type GetFalAiFramepackFlf2vRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/framepack/flf2v/requests/{request_id}/status";
};

export type GetFalAiFramepackFlf2vRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFramepackFlf2vRequestsByRequestIdStatusResponse =
  GetFalAiFramepackFlf2vRequestsByRequestIdStatusResponses[keyof GetFalAiFramepackFlf2vRequestsByRequestIdStatusResponses];

export type PutFalAiFramepackFlf2vRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/framepack/flf2v/requests/{request_id}/cancel";
};

export type PutFalAiFramepackFlf2vRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFramepackFlf2vRequestsByRequestIdCancelResponse =
  PutFalAiFramepackFlf2vRequestsByRequestIdCancelResponses[keyof PutFalAiFramepackFlf2vRequestsByRequestIdCancelResponses];

export type PostFalAiFramepackFlf2vData = {
  body: FramepackFlf2vInput;
  path?: never;
  query?: never;
  url: "/fal-ai/framepack/flf2v";
};

export type PostFalAiFramepackFlf2vResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFramepackFlf2vResponse =
  PostFalAiFramepackFlf2vResponses[keyof PostFalAiFramepackFlf2vResponses];

export type GetFalAiFramepackFlf2vRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/framepack/flf2v/requests/{request_id}";
};

export type GetFalAiFramepackFlf2vRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FramepackFlf2vOutput;
};

export type GetFalAiFramepackFlf2vRequestsByRequestIdResponse =
  GetFalAiFramepackFlf2vRequestsByRequestIdResponses[keyof GetFalAiFramepackFlf2vRequestsByRequestIdResponses];

export type GetFalAiWanFlf2vRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-flf2v/requests/{request_id}/status";
};

export type GetFalAiWanFlf2vRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanFlf2vRequestsByRequestIdStatusResponse =
  GetFalAiWanFlf2vRequestsByRequestIdStatusResponses[keyof GetFalAiWanFlf2vRequestsByRequestIdStatusResponses];

export type PutFalAiWanFlf2vRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-flf2v/requests/{request_id}/cancel";
};

export type PutFalAiWanFlf2vRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanFlf2vRequestsByRequestIdCancelResponse =
  PutFalAiWanFlf2vRequestsByRequestIdCancelResponses[keyof PutFalAiWanFlf2vRequestsByRequestIdCancelResponses];

export type PostFalAiWanFlf2vData = {
  body: WanFlf2vInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-flf2v";
};

export type PostFalAiWanFlf2vResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanFlf2vResponse =
  PostFalAiWanFlf2vResponses[keyof PostFalAiWanFlf2vResponses];

export type GetFalAiWanFlf2vRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-flf2v/requests/{request_id}";
};

export type GetFalAiWanFlf2vRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanFlf2vOutput;
};

export type GetFalAiWanFlf2vRequestsByRequestIdResponse =
  GetFalAiWanFlf2vRequestsByRequestIdResponses[keyof GetFalAiWanFlf2vRequestsByRequestIdResponses];

export type GetFalAiFramepackRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/framepack/requests/{request_id}/status";
};

export type GetFalAiFramepackRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFramepackRequestsByRequestIdStatusResponse =
  GetFalAiFramepackRequestsByRequestIdStatusResponses[keyof GetFalAiFramepackRequestsByRequestIdStatusResponses];

export type PutFalAiFramepackRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/framepack/requests/{request_id}/cancel";
};

export type PutFalAiFramepackRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFramepackRequestsByRequestIdCancelResponse =
  PutFalAiFramepackRequestsByRequestIdCancelResponses[keyof PutFalAiFramepackRequestsByRequestIdCancelResponses];

export type PostFalAiFramepackData = {
  body: FramepackInput;
  path?: never;
  query?: never;
  url: "/fal-ai/framepack";
};

export type PostFalAiFramepackResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFramepackResponse =
  PostFalAiFramepackResponses[keyof PostFalAiFramepackResponses];

export type GetFalAiFramepackRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/framepack/requests/{request_id}";
};

export type GetFalAiFramepackRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FramepackOutput;
};

export type GetFalAiFramepackRequestsByRequestIdResponse =
  GetFalAiFramepackRequestsByRequestIdResponses[keyof GetFalAiFramepackRequestsByRequestIdResponses];

export type GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4/image-to-video/fast/requests/{request_id}/status";
};

export type GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV4ImageToVideoFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/image-to-video/fast/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV4ImageToVideoFastRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV4ImageToVideoFastRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV4ImageToVideoFastRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV4ImageToVideoFastRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV4ImageToVideoFastData = {
  body: PixverseV4ImageToVideoFastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4/image-to-video/fast";
};

export type PostFalAiPixverseV4ImageToVideoFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV4ImageToVideoFastResponse =
  PostFalAiPixverseV4ImageToVideoFastResponses[keyof PostFalAiPixverseV4ImageToVideoFastResponses];

export type GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/image-to-video/fast/requests/{request_id}";
};

export type GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV4ImageToVideoFastOutput;
};

export type GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdResponse =
  GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdResponses[keyof GetFalAiPixverseV4ImageToVideoFastRequestsByRequestIdResponses];

export type GetFalAiPixverseV4ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4/image-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV4ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV4ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV4ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV4ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV4ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV4ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV4ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV4ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV4ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV4ImageToVideoData = {
  body: PixverseV4ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4/image-to-video";
};

export type PostFalAiPixverseV4ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV4ImageToVideoResponse =
  PostFalAiPixverseV4ImageToVideoResponses[keyof PostFalAiPixverseV4ImageToVideoResponses];

export type GetFalAiPixverseV4ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/image-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV4ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV4ImageToVideoOutput;
};

export type GetFalAiPixverseV4ImageToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV4ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV4ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV35EffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v3.5/effects/requests/{request_id}/status";
};

export type GetFalAiPixverseV35EffectsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV35EffectsRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV35EffectsRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV35EffectsRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV35EffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/effects/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV35EffectsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV35EffectsRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV35EffectsRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV35EffectsRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV35EffectsData = {
  body: PixverseV35EffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v3.5/effects";
};

export type PostFalAiPixverseV35EffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV35EffectsResponse =
  PostFalAiPixverseV35EffectsResponses[keyof PostFalAiPixverseV35EffectsResponses];

export type GetFalAiPixverseV35EffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/effects/requests/{request_id}";
};

export type GetFalAiPixverseV35EffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV35EffectsOutput;
};

export type GetFalAiPixverseV35EffectsRequestsByRequestIdResponse =
  GetFalAiPixverseV35EffectsRequestsByRequestIdResponses[keyof GetFalAiPixverseV35EffectsRequestsByRequestIdResponses];

export type GetFalAiPixverseV35TransitionRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v3.5/transition/requests/{request_id}/status";
};

export type GetFalAiPixverseV35TransitionRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV35TransitionRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV35TransitionRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV35TransitionRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV35TransitionRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/transition/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV35TransitionRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV35TransitionRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV35TransitionRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV35TransitionRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV35TransitionData = {
  body: PixverseV35TransitionInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v3.5/transition";
};

export type PostFalAiPixverseV35TransitionResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV35TransitionResponse =
  PostFalAiPixverseV35TransitionResponses[keyof PostFalAiPixverseV35TransitionResponses];

export type GetFalAiPixverseV35TransitionRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/transition/requests/{request_id}";
};

export type GetFalAiPixverseV35TransitionRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV35TransitionOutput;
};

export type GetFalAiPixverseV35TransitionRequestsByRequestIdResponse =
  GetFalAiPixverseV35TransitionRequestsByRequestIdResponses[keyof GetFalAiPixverseV35TransitionRequestsByRequestIdResponses];

export type GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/luma-dream-machine/ray-2-flash/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/luma-dream-machine/ray-2-flash/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLumaDreamMachineRay2FlashImageToVideoData = {
  body: LumaDreamMachineRay2FlashImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2-flash/image-to-video";
};

export type PostFalAiLumaDreamMachineRay2FlashImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLumaDreamMachineRay2FlashImageToVideoResponse =
  PostFalAiLumaDreamMachineRay2FlashImageToVideoResponses[keyof PostFalAiLumaDreamMachineRay2FlashImageToVideoResponses];

export type GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/luma-dream-machine/ray-2-flash/image-to-video/requests/{request_id}";
  };

export type GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LumaDreamMachineRay2FlashImageToVideoOutput;
  };

export type GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2FlashImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPikaV15PikaffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v1.5/pikaffects/requests/{request_id}/status";
};

export type GetFalAiPikaV15PikaffectsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPikaV15PikaffectsRequestsByRequestIdStatusResponse =
  GetFalAiPikaV15PikaffectsRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV15PikaffectsRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV15PikaffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v1.5/pikaffects/requests/{request_id}/cancel";
};

export type PutFalAiPikaV15PikaffectsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPikaV15PikaffectsRequestsByRequestIdCancelResponse =
  PutFalAiPikaV15PikaffectsRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV15PikaffectsRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV15PikaffectsData = {
  body: PikaV15PikaffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v1.5/pikaffects";
};

export type PostFalAiPikaV15PikaffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV15PikaffectsResponse =
  PostFalAiPikaV15PikaffectsResponses[keyof PostFalAiPikaV15PikaffectsResponses];

export type GetFalAiPikaV15PikaffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v1.5/pikaffects/requests/{request_id}";
};

export type GetFalAiPikaV15PikaffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV15PikaffectsOutput;
};

export type GetFalAiPikaV15PikaffectsRequestsByRequestIdResponse =
  GetFalAiPikaV15PikaffectsRequestsByRequestIdResponses[keyof GetFalAiPikaV15PikaffectsRequestsByRequestIdResponses];

export type GetFalAiPikaV21ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v2.1/image-to-video/requests/{request_id}/status";
};

export type GetFalAiPikaV21ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPikaV21ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPikaV21ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV21ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV21ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.1/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPikaV21ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPikaV21ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPikaV21ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV21ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV21ImageToVideoData = {
  body: PikaV21ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v2.1/image-to-video";
};

export type PostFalAiPikaV21ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV21ImageToVideoResponse =
  PostFalAiPikaV21ImageToVideoResponses[keyof PostFalAiPikaV21ImageToVideoResponses];

export type GetFalAiPikaV21ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.1/image-to-video/requests/{request_id}";
};

export type GetFalAiPikaV21ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV21ImageToVideoOutput;
};

export type GetFalAiPikaV21ImageToVideoRequestsByRequestIdResponse =
  GetFalAiPikaV21ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiPikaV21ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPikaV22ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v2.2/image-to-video/requests/{request_id}/status";
};

export type GetFalAiPikaV22ImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPikaV22ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPikaV22ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV22ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV22ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.2/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPikaV22ImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPikaV22ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPikaV22ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV22ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV22ImageToVideoData = {
  body: PikaV22ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v2.2/image-to-video";
};

export type PostFalAiPikaV22ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV22ImageToVideoResponse =
  PostFalAiPikaV22ImageToVideoResponses[keyof PostFalAiPikaV22ImageToVideoResponses];

export type GetFalAiPikaV22ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.2/image-to-video/requests/{request_id}";
};

export type GetFalAiPikaV22ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV22ImageToVideoOutput;
};

export type GetFalAiPikaV22ImageToVideoRequestsByRequestIdResponse =
  GetFalAiPikaV22ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiPikaV22ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiPikaV22PikascenesRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v2.2/pikascenes/requests/{request_id}/status";
};

export type GetFalAiPikaV22PikascenesRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPikaV22PikascenesRequestsByRequestIdStatusResponse =
  GetFalAiPikaV22PikascenesRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV22PikascenesRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV22PikascenesRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.2/pikascenes/requests/{request_id}/cancel";
};

export type PutFalAiPikaV22PikascenesRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPikaV22PikascenesRequestsByRequestIdCancelResponse =
  PutFalAiPikaV22PikascenesRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV22PikascenesRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV22PikascenesData = {
  body: PikaV22PikascenesInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v2.2/pikascenes";
};

export type PostFalAiPikaV22PikascenesResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV22PikascenesResponse =
  PostFalAiPikaV22PikascenesResponses[keyof PostFalAiPikaV22PikascenesResponses];

export type GetFalAiPikaV22PikascenesRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.2/pikascenes/requests/{request_id}";
};

export type GetFalAiPikaV22PikascenesRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV22PikascenesOutput;
};

export type GetFalAiPikaV22PikascenesRequestsByRequestIdResponse =
  GetFalAiPikaV22PikascenesRequestsByRequestIdResponses[keyof GetFalAiPikaV22PikascenesRequestsByRequestIdResponses];

export type GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v2/turbo/image-to-video/requests/{request_id}/status";
};

export type GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV2TurboImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2/turbo/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPikaV2TurboImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPikaV2TurboImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPikaV2TurboImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV2TurboImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV2TurboImageToVideoData = {
  body: PikaV2TurboImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v2/turbo/image-to-video";
};

export type PostFalAiPikaV2TurboImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV2TurboImageToVideoResponse =
  PostFalAiPikaV2TurboImageToVideoResponses[keyof PostFalAiPikaV2TurboImageToVideoResponses];

export type GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2/turbo/image-to-video/requests/{request_id}";
};

export type GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV2TurboImageToVideoOutput;
};

export type GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdResponse =
  GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdResponses[keyof GetFalAiPikaV2TurboImageToVideoRequestsByRequestIdResponses];

export type GetFalAiViduImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/image-to-video/requests/{request_id}/status";
};

export type GetFalAiViduImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduImageToVideoData = {
  body: ViduImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/image-to-video";
};

export type PostFalAiViduImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduImageToVideoResponse =
  PostFalAiViduImageToVideoResponses[keyof PostFalAiViduImageToVideoResponses];

export type GetFalAiViduImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/image-to-video/requests/{request_id}";
};

export type GetFalAiViduImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduImageToVideoOutput;
};

export type GetFalAiViduImageToVideoRequestsByRequestIdResponse =
  GetFalAiViduImageToVideoRequestsByRequestIdResponses[keyof GetFalAiViduImageToVideoRequestsByRequestIdResponses];

export type GetFalAiViduReferenceToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/reference-to-video/requests/{request_id}/status";
};

export type GetFalAiViduReferenceToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduReferenceToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduReferenceToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduReferenceToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduReferenceToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/reference-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduReferenceToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduReferenceToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduReferenceToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduReferenceToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduReferenceToVideoData = {
  body: ViduReferenceToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/reference-to-video";
};

export type PostFalAiViduReferenceToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduReferenceToVideoResponse =
  PostFalAiViduReferenceToVideoResponses[keyof PostFalAiViduReferenceToVideoResponses];

export type GetFalAiViduReferenceToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/reference-to-video/requests/{request_id}";
};

export type GetFalAiViduReferenceToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduReferenceToVideoOutput;
};

export type GetFalAiViduReferenceToVideoRequestsByRequestIdResponse =
  GetFalAiViduReferenceToVideoRequestsByRequestIdResponses[keyof GetFalAiViduReferenceToVideoRequestsByRequestIdResponses];

export type GetFalAiViduStartEndToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/start-end-to-video/requests/{request_id}/status";
};

export type GetFalAiViduStartEndToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduStartEndToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduStartEndToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduStartEndToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduStartEndToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/start-end-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduStartEndToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduStartEndToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduStartEndToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduStartEndToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduStartEndToVideoData = {
  body: ViduStartEndToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/start-end-to-video";
};

export type PostFalAiViduStartEndToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduStartEndToVideoResponse =
  PostFalAiViduStartEndToVideoResponses[keyof PostFalAiViduStartEndToVideoResponses];

export type GetFalAiViduStartEndToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/start-end-to-video/requests/{request_id}";
};

export type GetFalAiViduStartEndToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduStartEndToVideoOutput;
};

export type GetFalAiViduStartEndToVideoRequestsByRequestIdResponse =
  GetFalAiViduStartEndToVideoRequestsByRequestIdResponses[keyof GetFalAiViduStartEndToVideoRequestsByRequestIdResponses];

export type GetFalAiViduTemplateToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/template-to-video/requests/{request_id}/status";
};

export type GetFalAiViduTemplateToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduTemplateToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduTemplateToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduTemplateToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduTemplateToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/template-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduTemplateToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduTemplateToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduTemplateToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduTemplateToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduTemplateToVideoData = {
  body: ViduTemplateToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/template-to-video";
};

export type PostFalAiViduTemplateToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduTemplateToVideoResponse =
  PostFalAiViduTemplateToVideoResponses[keyof PostFalAiViduTemplateToVideoResponses];

export type GetFalAiViduTemplateToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/template-to-video/requests/{request_id}";
};

export type GetFalAiViduTemplateToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduTemplateToVideoOutput;
};

export type GetFalAiViduTemplateToVideoRequestsByRequestIdResponse =
  GetFalAiViduTemplateToVideoRequestsByRequestIdResponses[keyof GetFalAiViduTemplateToVideoRequestsByRequestIdResponses];

export type GetFalAiWanI2vLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-i2v-lora/requests/{request_id}/status";
};

export type GetFalAiWanI2vLoraRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanI2vLoraRequestsByRequestIdStatusResponse =
  GetFalAiWanI2vLoraRequestsByRequestIdStatusResponses[keyof GetFalAiWanI2vLoraRequestsByRequestIdStatusResponses];

export type PutFalAiWanI2vLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-i2v-lora/requests/{request_id}/cancel";
};

export type PutFalAiWanI2vLoraRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanI2vLoraRequestsByRequestIdCancelResponse =
  PutFalAiWanI2vLoraRequestsByRequestIdCancelResponses[keyof PutFalAiWanI2vLoraRequestsByRequestIdCancelResponses];

export type PostFalAiWanI2vLoraData = {
  body: WanI2vLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-i2v-lora";
};

export type PostFalAiWanI2vLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanI2vLoraResponse =
  PostFalAiWanI2vLoraResponses[keyof PostFalAiWanI2vLoraResponses];

export type GetFalAiWanI2vLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-i2v-lora/requests/{request_id}";
};

export type GetFalAiWanI2vLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanI2vLoraOutput;
};

export type GetFalAiWanI2vLoraRequestsByRequestIdResponse =
  GetFalAiWanI2vLoraRequestsByRequestIdResponses[keyof GetFalAiWanI2vLoraRequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-video-image-to-video/requests/{request_id}/status";
};

export type GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanVideoImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiHunyuanVideoImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoImageToVideoData = {
  body: HunyuanVideoImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video-image-to-video";
};

export type PostFalAiHunyuanVideoImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoImageToVideoResponse =
  PostFalAiHunyuanVideoImageToVideoResponses[keyof PostFalAiHunyuanVideoImageToVideoResponses];

export type GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-image-to-video/requests/{request_id}";
};

export type GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoImageToVideoOutput;
};

export type GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoImageToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/video-01-director/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/video-01-director/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxVideo01DirectorImageToVideoData = {
  body: MinimaxVideo01DirectorImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/video-01-director/image-to-video";
};

export type PostFalAiMinimaxVideo01DirectorImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxVideo01DirectorImageToVideoResponse =
  PostFalAiMinimaxVideo01DirectorImageToVideoResponses[keyof PostFalAiMinimaxVideo01DirectorImageToVideoResponses];

export type GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/video-01-director/image-to-video/requests/{request_id}";
  };

export type GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxVideo01DirectorImageToVideoOutput;
  };

export type GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxVideo01DirectorImageToVideoRequestsByRequestIdResponses];

export type GetFalAiSkyreelsI2vRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/skyreels-i2v/requests/{request_id}/status";
};

export type GetFalAiSkyreelsI2vRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSkyreelsI2vRequestsByRequestIdStatusResponse =
  GetFalAiSkyreelsI2vRequestsByRequestIdStatusResponses[keyof GetFalAiSkyreelsI2vRequestsByRequestIdStatusResponses];

export type PutFalAiSkyreelsI2vRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/skyreels-i2v/requests/{request_id}/cancel";
};

export type PutFalAiSkyreelsI2vRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSkyreelsI2vRequestsByRequestIdCancelResponse =
  PutFalAiSkyreelsI2vRequestsByRequestIdCancelResponses[keyof PutFalAiSkyreelsI2vRequestsByRequestIdCancelResponses];

export type PostFalAiSkyreelsI2vData = {
  body: SkyreelsI2vInput;
  path?: never;
  query?: never;
  url: "/fal-ai/skyreels-i2v";
};

export type PostFalAiSkyreelsI2vResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSkyreelsI2vResponse =
  PostFalAiSkyreelsI2vResponses[keyof PostFalAiSkyreelsI2vResponses];

export type GetFalAiSkyreelsI2vRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/skyreels-i2v/requests/{request_id}";
};

export type GetFalAiSkyreelsI2vRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SkyreelsI2vOutput;
};

export type GetFalAiSkyreelsI2vRequestsByRequestIdResponse =
  GetFalAiSkyreelsI2vRequestsByRequestIdResponses[keyof GetFalAiSkyreelsI2vRequestsByRequestIdResponses];

export type GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/luma-dream-machine/ray-2/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/luma-dream-machine/ray-2/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLumaDreamMachineRay2ImageToVideoData = {
  body: LumaDreamMachineRay2ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/image-to-video";
};

export type PostFalAiLumaDreamMachineRay2ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLumaDreamMachineRay2ImageToVideoResponse =
  PostFalAiLumaDreamMachineRay2ImageToVideoResponses[keyof PostFalAiLumaDreamMachineRay2ImageToVideoResponses];

export type GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/image-to-video/requests/{request_id}";
};

export type GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LumaDreamMachineRay2ImageToVideoOutput;
  };

export type GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-video-img2vid-lora/requests/{request_id}/status";
};

export type GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-img2vid-lora/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoImg2VidLoraData = {
  body: HunyuanVideoImg2VidLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video-img2vid-lora";
};

export type PostFalAiHunyuanVideoImg2VidLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoImg2VidLoraResponse =
  PostFalAiHunyuanVideoImg2VidLoraResponses[keyof PostFalAiHunyuanVideoImg2VidLoraResponses];

export type GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-img2vid-lora/requests/{request_id}";
};

export type GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoImg2VidLoraOutput;
};

export type GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoImg2VidLoraRequestsByRequestIdResponses];

export type GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v3.5/image-to-video/fast/requests/{request_id}/status";
};

export type GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV35ImageToVideoFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/image-to-video/fast/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV35ImageToVideoFastRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV35ImageToVideoFastRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV35ImageToVideoFastRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV35ImageToVideoFastRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV35ImageToVideoFastData = {
  body: PixverseV35ImageToVideoFastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v3.5/image-to-video/fast";
};

export type PostFalAiPixverseV35ImageToVideoFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV35ImageToVideoFastResponse =
  PostFalAiPixverseV35ImageToVideoFastResponses[keyof PostFalAiPixverseV35ImageToVideoFastResponses];

export type GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/image-to-video/fast/requests/{request_id}";
};

export type GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV35ImageToVideoFastOutput;
};

export type GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdResponse =
  GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdResponses[keyof GetFalAiPixverseV35ImageToVideoFastRequestsByRequestIdResponses];

export type GetFalAiPixverseV35ImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v3.5/image-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV35ImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV35ImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV35ImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV35ImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV35ImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV35ImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV35ImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV35ImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV35ImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV35ImageToVideoData = {
  body: PixverseV35ImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v3.5/image-to-video";
};

export type PostFalAiPixverseV35ImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV35ImageToVideoResponse =
  PostFalAiPixverseV35ImageToVideoResponses[keyof PostFalAiPixverseV35ImageToVideoResponses];

export type GetFalAiPixverseV35ImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/image-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV35ImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV35ImageToVideoOutput;
};

export type GetFalAiPixverseV35ImageToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV35ImageToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV35ImageToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/video-01-subject-reference/requests/{request_id}/status";
  };

export type GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/video-01-subject-reference/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxVideo01SubjectReferenceData = {
  body: MinimaxVideo01SubjectReferenceInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/video-01-subject-reference";
};

export type PostFalAiMinimaxVideo01SubjectReferenceResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxVideo01SubjectReferenceResponse =
  PostFalAiMinimaxVideo01SubjectReferenceResponses[keyof PostFalAiMinimaxVideo01SubjectReferenceResponses];

export type GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01-subject-reference/requests/{request_id}";
};

export type GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxVideo01SubjectReferenceOutput;
  };

export type GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdResponse =
  GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdResponses[keyof GetFalAiMinimaxVideo01SubjectReferenceRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v1.6/standard/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v1.6/standard/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV16StandardImageToVideoData = {
  body: KlingVideoV16StandardImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.6/standard/image-to-video";
};

export type PostFalAiKlingVideoV16StandardImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV16StandardImageToVideoResponse =
  PostFalAiKlingVideoV16StandardImageToVideoResponses[keyof PostFalAiKlingVideoV16StandardImageToVideoResponses];

export type GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/standard/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV16StandardImageToVideoOutput;
  };

export type GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV16StandardImageToVideoRequestsByRequestIdResponses];

export type GetFalAiSadtalkerReferenceRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sadtalker/reference/requests/{request_id}/status";
};

export type GetFalAiSadtalkerReferenceRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSadtalkerReferenceRequestsByRequestIdStatusResponse =
  GetFalAiSadtalkerReferenceRequestsByRequestIdStatusResponses[keyof GetFalAiSadtalkerReferenceRequestsByRequestIdStatusResponses];

export type PutFalAiSadtalkerReferenceRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sadtalker/reference/requests/{request_id}/cancel";
};

export type PutFalAiSadtalkerReferenceRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSadtalkerReferenceRequestsByRequestIdCancelResponse =
  PutFalAiSadtalkerReferenceRequestsByRequestIdCancelResponses[keyof PutFalAiSadtalkerReferenceRequestsByRequestIdCancelResponses];

export type PostFalAiSadtalkerReferenceData = {
  body: SadtalkerReferenceInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sadtalker/reference";
};

export type PostFalAiSadtalkerReferenceResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSadtalkerReferenceResponse =
  PostFalAiSadtalkerReferenceResponses[keyof PostFalAiSadtalkerReferenceResponses];

export type GetFalAiSadtalkerReferenceRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sadtalker/reference/requests/{request_id}";
};

export type GetFalAiSadtalkerReferenceRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SadtalkerReferenceOutput;
};

export type GetFalAiSadtalkerReferenceRequestsByRequestIdResponse =
  GetFalAiSadtalkerReferenceRequestsByRequestIdResponses[keyof GetFalAiSadtalkerReferenceRequestsByRequestIdResponses];

export type GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/video-01-live/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/video-01-live/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxVideo01LiveImageToVideoData = {
  body: MinimaxVideo01LiveImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/video-01-live/image-to-video";
};

export type PostFalAiMinimaxVideo01LiveImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxVideo01LiveImageToVideoResponse =
  PostFalAiMinimaxVideo01LiveImageToVideoResponses[keyof PostFalAiMinimaxVideo01LiveImageToVideoResponses];

export type GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01-live/image-to-video/requests/{request_id}";
};

export type GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxVideo01LiveImageToVideoOutput;
  };

export type GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxVideo01LiveImageToVideoRequestsByRequestIdResponses];

export type GetFalAiLtxVideoImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video/image-to-video/requests/{request_id}/status";
};

export type GetFalAiLtxVideoImageToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtxVideoImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideoImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideoImageToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtxVideoImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideoImageToVideoData = {
  body: LtxVideoImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video/image-to-video";
};

export type PostFalAiLtxVideoImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideoImageToVideoResponse =
  PostFalAiLtxVideoImageToVideoResponses[keyof PostFalAiLtxVideoImageToVideoResponses];

export type GetFalAiLtxVideoImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video/image-to-video/requests/{request_id}";
};

export type GetFalAiLtxVideoImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideoImageToVideoOutput;
};

export type GetFalAiLtxVideoImageToVideoRequestsByRequestIdResponse =
  GetFalAiLtxVideoImageToVideoRequestsByRequestIdResponses[keyof GetFalAiLtxVideoImageToVideoRequestsByRequestIdResponses];

export type GetFalAiCogvideox5bImageToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/cogvideox-5b/image-to-video/requests/{request_id}/status";
};

export type GetFalAiCogvideox5bImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiCogvideox5bImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiCogvideox5bImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiCogvideox5bImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiCogvideox5bImageToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/cogvideox-5b/image-to-video/requests/{request_id}/cancel";
};

export type PutFalAiCogvideox5bImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiCogvideox5bImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiCogvideox5bImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiCogvideox5bImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiCogvideox5bImageToVideoData = {
  body: Cogvideox5bImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/cogvideox-5b/image-to-video";
};

export type PostFalAiCogvideox5bImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiCogvideox5bImageToVideoResponse =
  PostFalAiCogvideox5bImageToVideoResponses[keyof PostFalAiCogvideox5bImageToVideoResponses];

export type GetFalAiCogvideox5bImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/cogvideox-5b/image-to-video/requests/{request_id}";
};

export type GetFalAiCogvideox5bImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Cogvideox5bImageToVideoOutput;
};

export type GetFalAiCogvideox5bImageToVideoRequestsByRequestIdResponse =
  GetFalAiCogvideox5bImageToVideoRequestsByRequestIdResponses[keyof GetFalAiCogvideox5bImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v1.5/pro/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v1.5/pro/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV15ProImageToVideoData = {
  body: KlingVideoV15ProImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.5/pro/image-to-video";
};

export type PostFalAiKlingVideoV15ProImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV15ProImageToVideoResponse =
  PostFalAiKlingVideoV15ProImageToVideoResponses[keyof PostFalAiKlingVideoV15ProImageToVideoResponses];

export type GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.5/pro/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV15ProImageToVideoOutput;
};

export type GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV15ProImageToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v1/standard/image-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v1/standard/image-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV1StandardImageToVideoData = {
  body: KlingVideoV1StandardImageToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1/standard/image-to-video";
};

export type PostFalAiKlingVideoV1StandardImageToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV1StandardImageToVideoResponse =
  PostFalAiKlingVideoV1StandardImageToVideoResponses[keyof PostFalAiKlingVideoV1StandardImageToVideoResponses];

export type GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1/standard/image-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV1StandardImageToVideoOutput;
  };

export type GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV1StandardImageToVideoRequestsByRequestIdResponses];

export type GetFalAiStableVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/stable-video/requests/{request_id}/status";
};

export type GetFalAiStableVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiStableVideoRequestsByRequestIdStatusResponse =
  GetFalAiStableVideoRequestsByRequestIdStatusResponses[keyof GetFalAiStableVideoRequestsByRequestIdStatusResponses];

export type PutFalAiStableVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/stable-video/requests/{request_id}/cancel";
};

export type PutFalAiStableVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiStableVideoRequestsByRequestIdCancelResponse =
  PutFalAiStableVideoRequestsByRequestIdCancelResponses[keyof PutFalAiStableVideoRequestsByRequestIdCancelResponses];

export type PostFalAiStableVideoData = {
  body: StableVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/stable-video";
};

export type PostFalAiStableVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiStableVideoResponse =
  PostFalAiStableVideoResponses[keyof PostFalAiStableVideoResponses];

export type GetFalAiStableVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/stable-video/requests/{request_id}";
};

export type GetFalAiStableVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: StableVideoOutput;
};

export type GetFalAiStableVideoRequestsByRequestIdResponse =
  GetFalAiStableVideoRequestsByRequestIdResponses[keyof GetFalAiStableVideoRequestsByRequestIdResponses];

export type GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/amt-interpolation/frame-interpolation/requests/{request_id}/status";
  };

export type GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdStatusResponse =
  GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdStatusResponses[keyof GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdStatusResponses];

export type PutFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/amt-interpolation/frame-interpolation/requests/{request_id}/cancel";
  };

export type PutFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdCancelResponse =
  PutFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdCancelResponses[keyof PutFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdCancelResponses];

export type PostFalAiAmtInterpolationFrameInterpolationData = {
  body: AmtInterpolationFrameInterpolationInput;
  path?: never;
  query?: never;
  url: "/fal-ai/amt-interpolation/frame-interpolation";
};

export type PostFalAiAmtInterpolationFrameInterpolationResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiAmtInterpolationFrameInterpolationResponse =
  PostFalAiAmtInterpolationFrameInterpolationResponses[keyof PostFalAiAmtInterpolationFrameInterpolationResponses];

export type GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/amt-interpolation/frame-interpolation/requests/{request_id}";
  };

export type GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: AmtInterpolationFrameInterpolationOutput;
  };

export type GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdResponse =
  GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdResponses[keyof GetFalAiAmtInterpolationFrameInterpolationRequestsByRequestIdResponses];

export type GetFalAiLivePortraitRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/live-portrait/requests/{request_id}/status";
};

export type GetFalAiLivePortraitRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLivePortraitRequestsByRequestIdStatusResponse =
  GetFalAiLivePortraitRequestsByRequestIdStatusResponses[keyof GetFalAiLivePortraitRequestsByRequestIdStatusResponses];

export type PutFalAiLivePortraitRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/live-portrait/requests/{request_id}/cancel";
};

export type PutFalAiLivePortraitRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLivePortraitRequestsByRequestIdCancelResponse =
  PutFalAiLivePortraitRequestsByRequestIdCancelResponses[keyof PutFalAiLivePortraitRequestsByRequestIdCancelResponses];

export type PostFalAiLivePortraitData = {
  body: LivePortraitInput;
  path?: never;
  query?: never;
  url: "/fal-ai/live-portrait";
};

export type PostFalAiLivePortraitResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLivePortraitResponse =
  PostFalAiLivePortraitResponses[keyof PostFalAiLivePortraitResponses];

export type GetFalAiLivePortraitRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/live-portrait/requests/{request_id}";
};

export type GetFalAiLivePortraitRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LivePortraitOutput;
};

export type GetFalAiLivePortraitRequestsByRequestIdResponse =
  GetFalAiLivePortraitRequestsByRequestIdResponses[keyof GetFalAiLivePortraitRequestsByRequestIdResponses];

export type GetFalAiMusetalkRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/musetalk/requests/{request_id}/status";
};

export type GetFalAiMusetalkRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMusetalkRequestsByRequestIdStatusResponse =
  GetFalAiMusetalkRequestsByRequestIdStatusResponses[keyof GetFalAiMusetalkRequestsByRequestIdStatusResponses];

export type PutFalAiMusetalkRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/musetalk/requests/{request_id}/cancel";
};

export type PutFalAiMusetalkRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMusetalkRequestsByRequestIdCancelResponse =
  PutFalAiMusetalkRequestsByRequestIdCancelResponses[keyof PutFalAiMusetalkRequestsByRequestIdCancelResponses];

export type PostFalAiMusetalkData = {
  body: MusetalkInput;
  path?: never;
  query?: never;
  url: "/fal-ai/musetalk";
};

export type PostFalAiMusetalkResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMusetalkResponse =
  PostFalAiMusetalkResponses[keyof PostFalAiMusetalkResponses];

export type GetFalAiMusetalkRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/musetalk/requests/{request_id}";
};

export type GetFalAiMusetalkRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MusetalkOutput;
};

export type GetFalAiMusetalkRequestsByRequestIdResponse =
  GetFalAiMusetalkRequestsByRequestIdResponses[keyof GetFalAiMusetalkRequestsByRequestIdResponses];

export type GetFalAiSadtalkerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sadtalker/requests/{request_id}/status";
};

export type GetFalAiSadtalkerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSadtalkerRequestsByRequestIdStatusResponse =
  GetFalAiSadtalkerRequestsByRequestIdStatusResponses[keyof GetFalAiSadtalkerRequestsByRequestIdStatusResponses];

export type PutFalAiSadtalkerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sadtalker/requests/{request_id}/cancel";
};

export type PutFalAiSadtalkerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSadtalkerRequestsByRequestIdCancelResponse =
  PutFalAiSadtalkerRequestsByRequestIdCancelResponses[keyof PutFalAiSadtalkerRequestsByRequestIdCancelResponses];

export type PostFalAiSadtalkerData = {
  body: SadtalkerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sadtalker";
};

export type PostFalAiSadtalkerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSadtalkerResponse =
  PostFalAiSadtalkerResponses[keyof PostFalAiSadtalkerResponses];

export type GetFalAiSadtalkerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sadtalker/requests/{request_id}";
};

export type GetFalAiSadtalkerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SadtalkerOutput;
};

export type GetFalAiSadtalkerRequestsByRequestIdResponse =
  GetFalAiSadtalkerRequestsByRequestIdResponses[keyof GetFalAiSadtalkerRequestsByRequestIdResponses];

export type GetFalAiFastSvdLcmRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/fast-svd-lcm/requests/{request_id}/status";
};

export type GetFalAiFastSvdLcmRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFastSvdLcmRequestsByRequestIdStatusResponse =
  GetFalAiFastSvdLcmRequestsByRequestIdStatusResponses[keyof GetFalAiFastSvdLcmRequestsByRequestIdStatusResponses];

export type PutFalAiFastSvdLcmRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-svd-lcm/requests/{request_id}/cancel";
};

export type PutFalAiFastSvdLcmRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFastSvdLcmRequestsByRequestIdCancelResponse =
  PutFalAiFastSvdLcmRequestsByRequestIdCancelResponses[keyof PutFalAiFastSvdLcmRequestsByRequestIdCancelResponses];

export type PostFalAiFastSvdLcmData = {
  body: FastSvdLcmInput;
  path?: never;
  query?: never;
  url: "/fal-ai/fast-svd-lcm";
};

export type PostFalAiFastSvdLcmResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFastSvdLcmResponse =
  PostFalAiFastSvdLcmResponses[keyof PostFalAiFastSvdLcmResponses];

export type GetFalAiFastSvdLcmRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-svd-lcm/requests/{request_id}";
};

export type GetFalAiFastSvdLcmRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FastSvdLcmOutput;
};

export type GetFalAiFastSvdLcmRequestsByRequestIdResponse =
  GetFalAiFastSvdLcmRequestsByRequestIdResponses[keyof GetFalAiFastSvdLcmRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.5-turbo/pro/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.5-turbo/pro/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV25TurboProTextToVideoData = {
  body: KlingVideoV25TurboProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.5-turbo/pro/text-to-video";
};

export type PostFalAiKlingVideoV25TurboProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV25TurboProTextToVideoResponse =
  PostFalAiKlingVideoV25TurboProTextToVideoResponses[keyof PostFalAiKlingVideoV25TurboProTextToVideoResponses];

export type GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.5-turbo/pro/text-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV25TurboProTextToVideoOutput;
  };

export type GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV25TurboProTextToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo3FastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3/fast/requests/{request_id}/status";
};

export type GetFalAiVeo3FastRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo3FastRequestsByRequestIdStatusResponse =
  GetFalAiVeo3FastRequestsByRequestIdStatusResponses[keyof GetFalAiVeo3FastRequestsByRequestIdStatusResponses];

export type PutFalAiVeo3FastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3/fast/requests/{request_id}/cancel";
};

export type PutFalAiVeo3FastRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo3FastRequestsByRequestIdCancelResponse =
  PutFalAiVeo3FastRequestsByRequestIdCancelResponses[keyof PutFalAiVeo3FastRequestsByRequestIdCancelResponses];

export type PostFalAiVeo3FastData = {
  body: Veo3FastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3/fast";
};

export type PostFalAiVeo3FastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo3FastResponse =
  PostFalAiVeo3FastResponses[keyof PostFalAiVeo3FastResponses];

export type GetFalAiVeo3FastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3/fast/requests/{request_id}";
};

export type GetFalAiVeo3FastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo3FastOutput;
};

export type GetFalAiVeo3FastRequestsByRequestIdResponse =
  GetFalAiVeo3FastRequestsByRequestIdResponses[keyof GetFalAiVeo3FastRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-02/standard/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-02/standard/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo02StandardTextToVideoData = {
  body: MinimaxHailuo02StandardTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-02/standard/text-to-video";
};

export type PostFalAiMinimaxHailuo02StandardTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo02StandardTextToVideoResponse =
  PostFalAiMinimaxHailuo02StandardTextToVideoResponses[keyof PostFalAiMinimaxHailuo02StandardTextToVideoResponses];

export type GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-02/standard/text-to-video/requests/{request_id}";
  };

export type GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo02StandardTextToVideoOutput;
  };

export type GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo02StandardTextToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo3RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3/requests/{request_id}/status";
};

export type GetFalAiVeo3RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo3RequestsByRequestIdStatusResponse =
  GetFalAiVeo3RequestsByRequestIdStatusResponses[keyof GetFalAiVeo3RequestsByRequestIdStatusResponses];

export type PutFalAiVeo3RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3/requests/{request_id}/cancel";
};

export type PutFalAiVeo3RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo3RequestsByRequestIdCancelResponse =
  PutFalAiVeo3RequestsByRequestIdCancelResponses[keyof PutFalAiVeo3RequestsByRequestIdCancelResponses];

export type PostFalAiVeo3Data = {
  body: Veo3Input;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3";
};

export type PostFalAiVeo3Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo3Response =
  PostFalAiVeo3Responses[keyof PostFalAiVeo3Responses];

export type GetFalAiVeo3RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3/requests/{request_id}";
};

export type GetFalAiVeo3RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo3Output;
};

export type GetFalAiVeo3RequestsByRequestIdResponse =
  GetFalAiVeo3RequestsByRequestIdResponses[keyof GetFalAiVeo3RequestsByRequestIdResponses];

export type GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2/master/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2/master/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV2MasterTextToVideoData = {
  body: KlingVideoV2MasterTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2/master/text-to-video";
};

export type PostFalAiKlingVideoV2MasterTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV2MasterTextToVideoResponse =
  PostFalAiKlingVideoV2MasterTextToVideoResponses[keyof PostFalAiKlingVideoV2MasterTextToVideoResponses];

export type GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2/master/text-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV2MasterTextToVideoOutput;
  };

export type GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV2MasterTextToVideoRequestsByRequestIdResponses];

export type GetFalAiViduQ3TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q3/text-to-video/requests/{request_id}/status";
};

export type GetFalAiViduQ3TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduQ3TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduQ3TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ3TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ3TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q3/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduQ3TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduQ3TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduQ3TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ3TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ3TextToVideoData = {
  body: ViduQ3TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q3/text-to-video";
};

export type PostFalAiViduQ3TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ3TextToVideoResponse =
  PostFalAiViduQ3TextToVideoResponses[keyof PostFalAiViduQ3TextToVideoResponses];

export type GetFalAiViduQ3TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q3/text-to-video/requests/{request_id}";
};

export type GetFalAiViduQ3TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ3TextToVideoOutput;
};

export type GetFalAiViduQ3TextToVideoRequestsByRequestIdResponse =
  GetFalAiViduQ3TextToVideoRequestsByRequestIdResponses[keyof GetFalAiViduQ3TextToVideoRequestsByRequestIdResponses];

export type GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/xai/grok-imagine-video/text-to-video/requests/{request_id}/status";
};

export type GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdStatusResponse =
  GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdStatusResponses[keyof GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdStatusResponses];

export type PutXaiGrokImagineVideoTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/xai/grok-imagine-video/text-to-video/requests/{request_id}/cancel";
};

export type PutXaiGrokImagineVideoTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutXaiGrokImagineVideoTextToVideoRequestsByRequestIdCancelResponse =
  PutXaiGrokImagineVideoTextToVideoRequestsByRequestIdCancelResponses[keyof PutXaiGrokImagineVideoTextToVideoRequestsByRequestIdCancelResponses];

export type PostXaiGrokImagineVideoTextToVideoData = {
  body: GrokImagineVideoTextToVideoInput;
  path?: never;
  query?: never;
  url: "/xai/grok-imagine-video/text-to-video";
};

export type PostXaiGrokImagineVideoTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostXaiGrokImagineVideoTextToVideoResponse =
  PostXaiGrokImagineVideoTextToVideoResponses[keyof PostXaiGrokImagineVideoTextToVideoResponses];

export type GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/xai/grok-imagine-video/text-to-video/requests/{request_id}";
};

export type GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: GrokImagineVideoTextToVideoOutput;
};

export type GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdResponse =
  GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdResponses[keyof GetXaiGrokImagineVideoTextToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV56TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5.6/text-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV56TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV56TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV56TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV56TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV56TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.6/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV56TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV56TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV56TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV56TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV56TextToVideoData = {
  body: PixverseV56TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5.6/text-to-video";
};

export type PostFalAiPixverseV56TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV56TextToVideoResponse =
  PostFalAiPixverseV56TextToVideoResponses[keyof PostFalAiPixverseV56TextToVideoResponses];

export type GetFalAiPixverseV56TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.6/text-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV56TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV56TextToVideoOutput;
};

export type GetFalAiPixverseV56TextToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV56TextToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV56TextToVideoRequestsByRequestIdResponses];

export type GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-2-19b/distilled/text-to-video/lora/requests/{request_id}/status";
  };

export type GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-2-19b/distilled/text-to-video/lora/requests/{request_id}/cancel";
  };

export type PutFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledTextToVideoLoraData = {
  body: Ltx219bDistilledTextToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/text-to-video/lora";
};

export type PostFalAiLtx219bDistilledTextToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledTextToVideoLoraResponse =
  PostFalAiLtx219bDistilledTextToVideoLoraResponses[keyof PostFalAiLtx219bDistilledTextToVideoLoraResponses];

export type GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/text-to-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: Ltx219bDistilledTextToVideoLoraOutput;
  };

export type GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledTextToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/distilled/text-to-video/requests/{request_id}/status";
};

export type GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bDistilledTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledTextToVideoData = {
  body: Ltx219bDistilledTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/text-to-video";
};

export type PostFalAiLtx219bDistilledTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledTextToVideoResponse =
  PostFalAiLtx219bDistilledTextToVideoResponses[keyof PostFalAiLtx219bDistilledTextToVideoResponses];

export type GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/text-to-video/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bDistilledTextToVideoOutput;
};

export type GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledTextToVideoRequestsByRequestIdResponses];

export type GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/text-to-video/lora/requests/{request_id}/status";
};

export type GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bTextToVideoLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/text-to-video/lora/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bTextToVideoLoraRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx219bTextToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bTextToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bTextToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bTextToVideoLoraData = {
  body: Ltx219bTextToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/text-to-video/lora";
};

export type PostFalAiLtx219bTextToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bTextToVideoLoraResponse =
  PostFalAiLtx219bTextToVideoLoraResponses[keyof PostFalAiLtx219bTextToVideoLoraResponses];

export type GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/text-to-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bTextToVideoLoraOutput;
};

export type GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bTextToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/text-to-video/requests/{request_id}/status";
};

export type GetFalAiLtx219bTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx219bTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx219bTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bTextToVideoData = {
  body: Ltx219bTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/text-to-video";
};

export type PostFalAiLtx219bTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bTextToVideoResponse =
  PostFalAiLtx219bTextToVideoResponses[keyof PostFalAiLtx219bTextToVideoResponses];

export type GetFalAiLtx219bTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/text-to-video/requests/{request_id}";
};

export type GetFalAiLtx219bTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bTextToVideoOutput;
};

export type GetFalAiLtx219bTextToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bTextToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bTextToVideoRequestsByRequestIdResponses];

export type GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kandinsky5-pro/text-to-video/requests/{request_id}/status";
};

export type GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKandinsky5ProTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kandinsky5-pro/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiKandinsky5ProTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKandinsky5ProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKandinsky5ProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKandinsky5ProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKandinsky5ProTextToVideoData = {
  body: Kandinsky5ProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kandinsky5-pro/text-to-video";
};

export type PostFalAiKandinsky5ProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKandinsky5ProTextToVideoResponse =
  PostFalAiKandinsky5ProTextToVideoResponses[keyof PostFalAiKandinsky5ProTextToVideoResponses];

export type GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kandinsky5-pro/text-to-video/requests/{request_id}";
};

export type GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Kandinsky5ProTextToVideoOutput;
};

export type GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdResponse =
  GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKandinsky5ProTextToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance/seedance/v1.5/pro/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1.5/pro/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceSeedanceV15ProTextToVideoData = {
  body: BytedanceSeedanceV15ProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1.5/pro/text-to-video";
};

export type PostFalAiBytedanceSeedanceV15ProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceSeedanceV15ProTextToVideoResponse =
  PostFalAiBytedanceSeedanceV15ProTextToVideoResponses[keyof PostFalAiBytedanceSeedanceV15ProTextToVideoResponses];

export type GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1.5/pro/text-to-video/requests/{request_id}";
  };

export type GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceSeedanceV15ProTextToVideoOutput;
  };

export type GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceSeedanceV15ProTextToVideoRequestsByRequestIdResponses];

export type GetWanV26TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/wan/v2.6/text-to-video/requests/{request_id}/status";
};

export type GetWanV26TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetWanV26TextToVideoRequestsByRequestIdStatusResponse =
  GetWanV26TextToVideoRequestsByRequestIdStatusResponses[keyof GetWanV26TextToVideoRequestsByRequestIdStatusResponses];

export type PutWanV26TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/wan/v2.6/text-to-video/requests/{request_id}/cancel";
};

export type PutWanV26TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutWanV26TextToVideoRequestsByRequestIdCancelResponse =
  PutWanV26TextToVideoRequestsByRequestIdCancelResponses[keyof PutWanV26TextToVideoRequestsByRequestIdCancelResponses];

export type PostWanV26TextToVideoData = {
  body: V26TextToVideoInput;
  path?: never;
  query?: never;
  url: "/wan/v2.6/text-to-video";
};

export type PostWanV26TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostWanV26TextToVideoResponse =
  PostWanV26TextToVideoResponses[keyof PostWanV26TextToVideoResponses];

export type GetWanV26TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/wan/v2.6/text-to-video/requests/{request_id}";
};

export type GetWanV26TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: V26TextToVideoOutput;
};

export type GetWanV26TextToVideoRequestsByRequestIdResponse =
  GetWanV26TextToVideoRequestsByRequestIdResponses[keyof GetWanV26TextToVideoRequestsByRequestIdResponses];

export type GetVeedFabric10TextRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/veed/fabric-1.0/text/requests/{request_id}/status";
};

export type GetVeedFabric10TextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetVeedFabric10TextRequestsByRequestIdStatusResponse =
  GetVeedFabric10TextRequestsByRequestIdStatusResponses[keyof GetVeedFabric10TextRequestsByRequestIdStatusResponses];

export type PutVeedFabric10TextRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/fabric-1.0/text/requests/{request_id}/cancel";
};

export type PutVeedFabric10TextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutVeedFabric10TextRequestsByRequestIdCancelResponse =
  PutVeedFabric10TextRequestsByRequestIdCancelResponses[keyof PutVeedFabric10TextRequestsByRequestIdCancelResponses];

export type PostVeedFabric10TextData = {
  body: Fabric10TextInput;
  path?: never;
  query?: never;
  url: "/veed/fabric-1.0/text";
};

export type PostVeedFabric10TextResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostVeedFabric10TextResponse =
  PostVeedFabric10TextResponses[keyof PostVeedFabric10TextResponses];

export type GetVeedFabric10TextRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/fabric-1.0/text/requests/{request_id}";
};

export type GetVeedFabric10TextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Fabric10TextOutput;
};

export type GetVeedFabric10TextRequestsByRequestIdResponse =
  GetVeedFabric10TextRequestsByRequestIdResponses[keyof GetVeedFabric10TextRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/v2.6/pro/text-to-video/requests/{request_id}/status";
};

export type GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.6/pro/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV26ProTextToVideoData = {
  body: KlingVideoV26ProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.6/pro/text-to-video";
};

export type PostFalAiKlingVideoV26ProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV26ProTextToVideoResponse =
  PostFalAiKlingVideoV26ProTextToVideoResponses[keyof PostFalAiKlingVideoV26ProTextToVideoResponses];

export type GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.6/pro/text-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV26ProTextToVideoOutput;
};

export type GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV26ProTextToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV55TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5.5/text-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV55TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV55TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV55TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV55TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV55TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.5/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV55TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV55TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV55TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV55TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV55TextToVideoData = {
  body: PixverseV55TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5.5/text-to-video";
};

export type PostFalAiPixverseV55TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV55TextToVideoResponse =
  PostFalAiPixverseV55TextToVideoResponses[keyof PostFalAiPixverseV55TextToVideoResponses];

export type GetFalAiPixverseV55TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5.5/text-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV55TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV55TextToVideoOutput;
};

export type GetFalAiPixverseV55TextToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV55TextToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV55TextToVideoRequestsByRequestIdResponses];

export type GetFalAiLtx2TextToVideoFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2/text-to-video/fast/requests/{request_id}/status";
};

export type GetFalAiLtx2TextToVideoFastRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx2TextToVideoFastRequestsByRequestIdStatusResponse =
  GetFalAiLtx2TextToVideoFastRequestsByRequestIdStatusResponses[keyof GetFalAiLtx2TextToVideoFastRequestsByRequestIdStatusResponses];

export type PutFalAiLtx2TextToVideoFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/text-to-video/fast/requests/{request_id}/cancel";
};

export type PutFalAiLtx2TextToVideoFastRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx2TextToVideoFastRequestsByRequestIdCancelResponse =
  PutFalAiLtx2TextToVideoFastRequestsByRequestIdCancelResponses[keyof PutFalAiLtx2TextToVideoFastRequestsByRequestIdCancelResponses];

export type PostFalAiLtx2TextToVideoFastData = {
  body: Ltx2TextToVideoFastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2/text-to-video/fast";
};

export type PostFalAiLtx2TextToVideoFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx2TextToVideoFastResponse =
  PostFalAiLtx2TextToVideoFastResponses[keyof PostFalAiLtx2TextToVideoFastResponses];

export type GetFalAiLtx2TextToVideoFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/text-to-video/fast/requests/{request_id}";
};

export type GetFalAiLtx2TextToVideoFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx2TextToVideoFastOutput;
};

export type GetFalAiLtx2TextToVideoFastRequestsByRequestIdResponse =
  GetFalAiLtx2TextToVideoFastRequestsByRequestIdResponses[keyof GetFalAiLtx2TextToVideoFastRequestsByRequestIdResponses];

export type GetFalAiLtx2TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2/text-to-video/requests/{request_id}/status";
};

export type GetFalAiLtx2TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx2TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx2TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx2TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx2TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx2TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx2TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx2TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx2TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx2TextToVideoData = {
  body: Ltx2TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2/text-to-video";
};

export type PostFalAiLtx2TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx2TextToVideoResponse =
  PostFalAiLtx2TextToVideoResponses[keyof PostFalAiLtx2TextToVideoResponses];

export type GetFalAiLtx2TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/text-to-video/requests/{request_id}";
};

export type GetFalAiLtx2TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx2TextToVideoOutput;
};

export type GetFalAiLtx2TextToVideoRequestsByRequestIdResponse =
  GetFalAiLtx2TextToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx2TextToVideoRequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-video-v1.5/text-to-video/requests/{request_id}/status";
};

export type GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-v1.5/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoV15TextToVideoData = {
  body: HunyuanVideoV15TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video-v1.5/text-to-video";
};

export type PostFalAiHunyuanVideoV15TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoV15TextToVideoResponse =
  PostFalAiHunyuanVideoV15TextToVideoResponses[keyof PostFalAiHunyuanVideoV15TextToVideoResponses];

export type GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-v1.5/text-to-video/requests/{request_id}";
};

export type GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoV15TextToVideoOutput;
};

export type GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoV15TextToVideoRequestsByRequestIdResponses];

export type GetFalAiInfinityStarTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/infinity-star/text-to-video/requests/{request_id}/status";
};

export type GetFalAiInfinityStarTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiInfinityStarTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiInfinityStarTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiInfinityStarTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiInfinityStarTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/infinity-star/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiInfinityStarTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiInfinityStarTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiInfinityStarTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiInfinityStarTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiInfinityStarTextToVideoData = {
  body: InfinityStarTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/infinity-star/text-to-video";
};

export type PostFalAiInfinityStarTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiInfinityStarTextToVideoResponse =
  PostFalAiInfinityStarTextToVideoResponses[keyof PostFalAiInfinityStarTextToVideoResponses];

export type GetFalAiInfinityStarTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/infinity-star/text-to-video/requests/{request_id}";
};

export type GetFalAiInfinityStarTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: InfinityStarTextToVideoOutput;
};

export type GetFalAiInfinityStarTextToVideoRequestsByRequestIdResponse =
  GetFalAiInfinityStarTextToVideoRequestsByRequestIdResponses[keyof GetFalAiInfinityStarTextToVideoRequestsByRequestIdResponses];

export type GetFalAiSanaVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sana-video/requests/{request_id}/status";
};

export type GetFalAiSanaVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSanaVideoRequestsByRequestIdStatusResponse =
  GetFalAiSanaVideoRequestsByRequestIdStatusResponses[keyof GetFalAiSanaVideoRequestsByRequestIdStatusResponses];

export type PutFalAiSanaVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sana-video/requests/{request_id}/cancel";
};

export type PutFalAiSanaVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSanaVideoRequestsByRequestIdCancelResponse =
  PutFalAiSanaVideoRequestsByRequestIdCancelResponses[keyof PutFalAiSanaVideoRequestsByRequestIdCancelResponses];

export type PostFalAiSanaVideoData = {
  body: SanaVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sana-video";
};

export type PostFalAiSanaVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSanaVideoResponse =
  PostFalAiSanaVideoResponses[keyof PostFalAiSanaVideoResponses];

export type GetFalAiSanaVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sana-video/requests/{request_id}";
};

export type GetFalAiSanaVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SanaVideoOutput;
};

export type GetFalAiSanaVideoRequestsByRequestIdResponse =
  GetFalAiSanaVideoRequestsByRequestIdResponses[keyof GetFalAiSanaVideoRequestsByRequestIdResponses];

export type GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/longcat-video/text-to-video/720p/requests/{request_id}/status";
};

export type GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdStatusResponse =
  GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatVideoTextToVideo720pRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/longcat-video/text-to-video/720p/requests/{request_id}/cancel";
};

export type PutFalAiLongcatVideoTextToVideo720pRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatVideoTextToVideo720pRequestsByRequestIdCancelResponse =
  PutFalAiLongcatVideoTextToVideo720pRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatVideoTextToVideo720pRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatVideoTextToVideo720pData = {
  body: LongcatVideoTextToVideo720pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-video/text-to-video/720p";
};

export type PostFalAiLongcatVideoTextToVideo720pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatVideoTextToVideo720pResponse =
  PostFalAiLongcatVideoTextToVideo720pResponses[keyof PostFalAiLongcatVideoTextToVideo720pResponses];

export type GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/longcat-video/text-to-video/720p/requests/{request_id}";
};

export type GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LongcatVideoTextToVideo720pOutput;
};

export type GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdResponse =
  GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdResponses[keyof GetFalAiLongcatVideoTextToVideo720pRequestsByRequestIdResponses];

export type GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/longcat-video/text-to-video/480p/requests/{request_id}/status";
};

export type GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdStatusResponse =
  GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatVideoTextToVideo480pRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/longcat-video/text-to-video/480p/requests/{request_id}/cancel";
};

export type PutFalAiLongcatVideoTextToVideo480pRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatVideoTextToVideo480pRequestsByRequestIdCancelResponse =
  PutFalAiLongcatVideoTextToVideo480pRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatVideoTextToVideo480pRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatVideoTextToVideo480pData = {
  body: LongcatVideoTextToVideo480pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-video/text-to-video/480p";
};

export type PostFalAiLongcatVideoTextToVideo480pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatVideoTextToVideo480pResponse =
  PostFalAiLongcatVideoTextToVideo480pResponses[keyof PostFalAiLongcatVideoTextToVideo480pResponses];

export type GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/longcat-video/text-to-video/480p/requests/{request_id}";
};

export type GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LongcatVideoTextToVideo480pOutput;
};

export type GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdResponse =
  GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdResponses[keyof GetFalAiLongcatVideoTextToVideo480pRequestsByRequestIdResponses];

export type GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/longcat-video/distilled/text-to-video/720p/requests/{request_id}/status";
  };

export type GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdStatusResponse =
  GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/distilled/text-to-video/720p/requests/{request_id}/cancel";
  };

export type PutFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdCancelResponse =
  PutFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatVideoDistilledTextToVideo720pData = {
  body: LongcatVideoDistilledTextToVideo720pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-video/distilled/text-to-video/720p";
};

export type PostFalAiLongcatVideoDistilledTextToVideo720pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatVideoDistilledTextToVideo720pResponse =
  PostFalAiLongcatVideoDistilledTextToVideo720pResponses[keyof PostFalAiLongcatVideoDistilledTextToVideo720pResponses];

export type GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/distilled/text-to-video/720p/requests/{request_id}";
  };

export type GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LongcatVideoDistilledTextToVideo720pOutput;
  };

export type GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdResponse =
  GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdResponses[keyof GetFalAiLongcatVideoDistilledTextToVideo720pRequestsByRequestIdResponses];

export type GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/longcat-video/distilled/text-to-video/480p/requests/{request_id}/status";
  };

export type GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdStatusResponse =
  GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdStatusResponses[keyof GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdStatusResponses];

export type PutFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/distilled/text-to-video/480p/requests/{request_id}/cancel";
  };

export type PutFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdCancelResponse =
  PutFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdCancelResponses[keyof PutFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdCancelResponses];

export type PostFalAiLongcatVideoDistilledTextToVideo480pData = {
  body: LongcatVideoDistilledTextToVideo480pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/longcat-video/distilled/text-to-video/480p";
};

export type PostFalAiLongcatVideoDistilledTextToVideo480pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLongcatVideoDistilledTextToVideo480pResponse =
  PostFalAiLongcatVideoDistilledTextToVideo480pResponses[keyof PostFalAiLongcatVideoDistilledTextToVideo480pResponses];

export type GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/longcat-video/distilled/text-to-video/480p/requests/{request_id}";
  };

export type GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LongcatVideoDistilledTextToVideo480pOutput;
  };

export type GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdResponse =
  GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdResponses[keyof GetFalAiLongcatVideoDistilledTextToVideo480pRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-2.3/standard/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3/standard/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo23StandardTextToVideoData = {
  body: MinimaxHailuo23StandardTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-2.3/standard/text-to-video";
};

export type PostFalAiMinimaxHailuo23StandardTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo23StandardTextToVideoResponse =
  PostFalAiMinimaxHailuo23StandardTextToVideoResponses[keyof PostFalAiMinimaxHailuo23StandardTextToVideoResponses];

export type GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3/standard/text-to-video/requests/{request_id}";
  };

export type GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo23StandardTextToVideoOutput;
  };

export type GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo23StandardTextToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-2.3/pro/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-2.3/pro/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo23ProTextToVideoData = {
  body: MinimaxHailuo23ProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-2.3/pro/text-to-video";
};

export type PostFalAiMinimaxHailuo23ProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo23ProTextToVideoResponse =
  PostFalAiMinimaxHailuo23ProTextToVideoResponses[keyof PostFalAiMinimaxHailuo23ProTextToVideoResponses];

export type GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/hailuo-2.3/pro/text-to-video/requests/{request_id}";
};

export type GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo23ProTextToVideoOutput;
  };

export type GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo23ProTextToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance/seedance/v1/pro/fast/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/pro/fast/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceSeedanceV1ProFastTextToVideoData = {
  body: BytedanceSeedanceV1ProFastTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1/pro/fast/text-to-video";
};

export type PostFalAiBytedanceSeedanceV1ProFastTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceSeedanceV1ProFastTextToVideoResponse =
  PostFalAiBytedanceSeedanceV1ProFastTextToVideoResponses[keyof PostFalAiBytedanceSeedanceV1ProFastTextToVideoResponses];

export type GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/pro/fast/text-to-video/requests/{request_id}";
  };

export type GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceSeedanceV1ProFastTextToVideoOutput;
  };

export type GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceSeedanceV1ProFastTextToVideoRequestsByRequestIdResponses];

export type GetFalAiViduQ2TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q2/text-to-video/requests/{request_id}/status";
};

export type GetFalAiViduQ2TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduQ2TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduQ2TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ2TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ2TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduQ2TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduQ2TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduQ2TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ2TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ2TextToVideoData = {
  body: ViduQ2TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q2/text-to-video";
};

export type PostFalAiViduQ2TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ2TextToVideoResponse =
  PostFalAiViduQ2TextToVideoResponses[keyof PostFalAiViduQ2TextToVideoResponses];

export type GetFalAiViduQ2TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/text-to-video/requests/{request_id}";
};

export type GetFalAiViduQ2TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ2TextToVideoOutput;
};

export type GetFalAiViduQ2TextToVideoRequestsByRequestIdResponse =
  GetFalAiViduQ2TextToVideoRequestsByRequestIdResponses[keyof GetFalAiViduQ2TextToVideoRequestsByRequestIdResponses];

export type GetFalAiKreaWan14bTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/krea-wan-14b/text-to-video/requests/{request_id}/status";
};

export type GetFalAiKreaWan14bTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiKreaWan14bTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKreaWan14bTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKreaWan14bTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKreaWan14bTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/krea-wan-14b/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiKreaWan14bTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiKreaWan14bTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKreaWan14bTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKreaWan14bTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKreaWan14bTextToVideoData = {
  body: KreaWan14bTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/krea-wan-14b/text-to-video";
};

export type PostFalAiKreaWan14bTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKreaWan14bTextToVideoResponse =
  PostFalAiKreaWan14bTextToVideoResponses[keyof PostFalAiKreaWan14bTextToVideoResponses];

export type GetFalAiKreaWan14bTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/krea-wan-14b/text-to-video/requests/{request_id}";
};

export type GetFalAiKreaWan14bTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KreaWan14bTextToVideoOutput;
};

export type GetFalAiKreaWan14bTextToVideoRequestsByRequestIdResponse =
  GetFalAiKreaWan14bTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKreaWan14bTextToVideoRequestsByRequestIdResponses];

export type GetFalAiWanAlphaRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-alpha/requests/{request_id}/status";
};

export type GetFalAiWanAlphaRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanAlphaRequestsByRequestIdStatusResponse =
  GetFalAiWanAlphaRequestsByRequestIdStatusResponses[keyof GetFalAiWanAlphaRequestsByRequestIdStatusResponses];

export type PutFalAiWanAlphaRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-alpha/requests/{request_id}/cancel";
};

export type PutFalAiWanAlphaRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanAlphaRequestsByRequestIdCancelResponse =
  PutFalAiWanAlphaRequestsByRequestIdCancelResponses[keyof PutFalAiWanAlphaRequestsByRequestIdCancelResponses];

export type PostFalAiWanAlphaData = {
  body: WanAlphaInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-alpha";
};

export type PostFalAiWanAlphaResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanAlphaResponse =
  PostFalAiWanAlphaResponses[keyof PostFalAiWanAlphaResponses];

export type GetFalAiWanAlphaRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-alpha/requests/{request_id}";
};

export type GetFalAiWanAlphaRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanAlphaOutput;
};

export type GetFalAiWanAlphaRequestsByRequestIdResponse =
  GetFalAiWanAlphaRequestsByRequestIdResponses[keyof GetFalAiWanAlphaRequestsByRequestIdResponses];

export type GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kandinsky5/text-to-video/distill/requests/{request_id}/status";
  };

export type GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdStatusResponse =
  GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdStatusResponses[keyof GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdStatusResponses];

export type PutFalAiKandinsky5TextToVideoDistillRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kandinsky5/text-to-video/distill/requests/{request_id}/cancel";
  };

export type PutFalAiKandinsky5TextToVideoDistillRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKandinsky5TextToVideoDistillRequestsByRequestIdCancelResponse =
  PutFalAiKandinsky5TextToVideoDistillRequestsByRequestIdCancelResponses[keyof PutFalAiKandinsky5TextToVideoDistillRequestsByRequestIdCancelResponses];

export type PostFalAiKandinsky5TextToVideoDistillData = {
  body: Kandinsky5TextToVideoDistillInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kandinsky5/text-to-video/distill";
};

export type PostFalAiKandinsky5TextToVideoDistillResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKandinsky5TextToVideoDistillResponse =
  PostFalAiKandinsky5TextToVideoDistillResponses[keyof PostFalAiKandinsky5TextToVideoDistillResponses];

export type GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kandinsky5/text-to-video/distill/requests/{request_id}";
};

export type GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Kandinsky5TextToVideoDistillOutput;
};

export type GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdResponse =
  GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdResponses[keyof GetFalAiKandinsky5TextToVideoDistillRequestsByRequestIdResponses];

export type GetFalAiKandinsky5TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kandinsky5/text-to-video/requests/{request_id}/status";
};

export type GetFalAiKandinsky5TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiKandinsky5TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKandinsky5TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKandinsky5TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKandinsky5TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kandinsky5/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiKandinsky5TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiKandinsky5TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKandinsky5TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKandinsky5TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKandinsky5TextToVideoData = {
  body: Kandinsky5TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kandinsky5/text-to-video";
};

export type PostFalAiKandinsky5TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKandinsky5TextToVideoResponse =
  PostFalAiKandinsky5TextToVideoResponses[keyof PostFalAiKandinsky5TextToVideoResponses];

export type GetFalAiKandinsky5TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kandinsky5/text-to-video/requests/{request_id}";
};

export type GetFalAiKandinsky5TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Kandinsky5TextToVideoOutput;
};

export type GetFalAiKandinsky5TextToVideoRequestsByRequestIdResponse =
  GetFalAiKandinsky5TextToVideoRequestsByRequestIdResponses[keyof GetFalAiKandinsky5TextToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo31FastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3.1/fast/requests/{request_id}/status";
};

export type GetFalAiVeo31FastRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo31FastRequestsByRequestIdStatusResponse =
  GetFalAiVeo31FastRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31FastRequestsByRequestIdStatusResponses];

export type PutFalAiVeo31FastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/fast/requests/{request_id}/cancel";
};

export type PutFalAiVeo31FastRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo31FastRequestsByRequestIdCancelResponse =
  PutFalAiVeo31FastRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31FastRequestsByRequestIdCancelResponses];

export type PostFalAiVeo31FastData = {
  body: Veo31FastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3.1/fast";
};

export type PostFalAiVeo31FastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo31FastResponse =
  PostFalAiVeo31FastResponses[keyof PostFalAiVeo31FastResponses];

export type GetFalAiVeo31FastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/fast/requests/{request_id}";
};

export type GetFalAiVeo31FastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo31FastOutput;
};

export type GetFalAiVeo31FastRequestsByRequestIdResponse =
  GetFalAiVeo31FastRequestsByRequestIdResponses[keyof GetFalAiVeo31FastRequestsByRequestIdResponses];

export type GetFalAiVeo31RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3.1/requests/{request_id}/status";
};

export type GetFalAiVeo31RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo31RequestsByRequestIdStatusResponse =
  GetFalAiVeo31RequestsByRequestIdStatusResponses[keyof GetFalAiVeo31RequestsByRequestIdStatusResponses];

export type PutFalAiVeo31RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/requests/{request_id}/cancel";
};

export type PutFalAiVeo31RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo31RequestsByRequestIdCancelResponse =
  PutFalAiVeo31RequestsByRequestIdCancelResponses[keyof PutFalAiVeo31RequestsByRequestIdCancelResponses];

export type PostFalAiVeo31Data = {
  body: Veo31Input;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3.1";
};

export type PostFalAiVeo31Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo31Response =
  PostFalAiVeo31Responses[keyof PostFalAiVeo31Responses];

export type GetFalAiVeo31RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/requests/{request_id}";
};

export type GetFalAiVeo31RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo31Output;
};

export type GetFalAiVeo31RequestsByRequestIdResponse =
  GetFalAiVeo31RequestsByRequestIdResponses[keyof GetFalAiVeo31RequestsByRequestIdResponses];

export type GetFalAiSora2TextToVideoProRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sora-2/text-to-video/pro/requests/{request_id}/status";
};

export type GetFalAiSora2TextToVideoProRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSora2TextToVideoProRequestsByRequestIdStatusResponse =
  GetFalAiSora2TextToVideoProRequestsByRequestIdStatusResponses[keyof GetFalAiSora2TextToVideoProRequestsByRequestIdStatusResponses];

export type PutFalAiSora2TextToVideoProRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/text-to-video/pro/requests/{request_id}/cancel";
};

export type PutFalAiSora2TextToVideoProRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSora2TextToVideoProRequestsByRequestIdCancelResponse =
  PutFalAiSora2TextToVideoProRequestsByRequestIdCancelResponses[keyof PutFalAiSora2TextToVideoProRequestsByRequestIdCancelResponses];

export type PostFalAiSora2TextToVideoProData = {
  body: Sora2TextToVideoProInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sora-2/text-to-video/pro";
};

export type PostFalAiSora2TextToVideoProResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSora2TextToVideoProResponse =
  PostFalAiSora2TextToVideoProResponses[keyof PostFalAiSora2TextToVideoProResponses];

export type GetFalAiSora2TextToVideoProRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/text-to-video/pro/requests/{request_id}";
};

export type GetFalAiSora2TextToVideoProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Sora2TextToVideoProOutput;
};

export type GetFalAiSora2TextToVideoProRequestsByRequestIdResponse =
  GetFalAiSora2TextToVideoProRequestsByRequestIdResponses[keyof GetFalAiSora2TextToVideoProRequestsByRequestIdResponses];

export type GetFalAiSora2TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sora-2/text-to-video/requests/{request_id}/status";
};

export type GetFalAiSora2TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSora2TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiSora2TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiSora2TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiSora2TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiSora2TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSora2TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiSora2TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiSora2TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiSora2TextToVideoData = {
  body: Sora2TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sora-2/text-to-video";
};

export type PostFalAiSora2TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSora2TextToVideoResponse =
  PostFalAiSora2TextToVideoResponses[keyof PostFalAiSora2TextToVideoResponses];

export type GetFalAiSora2TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/text-to-video/requests/{request_id}";
};

export type GetFalAiSora2TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Sora2TextToVideoOutput;
};

export type GetFalAiSora2TextToVideoRequestsByRequestIdResponse =
  GetFalAiSora2TextToVideoRequestsByRequestIdResponses[keyof GetFalAiSora2TextToVideoRequestsByRequestIdResponses];

export type GetFalAiOviRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ovi/requests/{request_id}/status";
};

export type GetFalAiOviRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiOviRequestsByRequestIdStatusResponse =
  GetFalAiOviRequestsByRequestIdStatusResponses[keyof GetFalAiOviRequestsByRequestIdStatusResponses];

export type PutFalAiOviRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ovi/requests/{request_id}/cancel";
};

export type PutFalAiOviRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiOviRequestsByRequestIdCancelResponse =
  PutFalAiOviRequestsByRequestIdCancelResponses[keyof PutFalAiOviRequestsByRequestIdCancelResponses];

export type PostFalAiOviData = {
  body: OviInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ovi";
};

export type PostFalAiOviResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiOviResponse =
  PostFalAiOviResponses[keyof PostFalAiOviResponses];

export type GetFalAiOviRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ovi/requests/{request_id}";
};

export type GetFalAiOviRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: OviOutput;
};

export type GetFalAiOviRequestsByRequestIdResponse =
  GetFalAiOviRequestsByRequestIdResponses[keyof GetFalAiOviRequestsByRequestIdResponses];

export type GetFalAiWan25PreviewTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-25-preview/text-to-video/requests/{request_id}/status";
};

export type GetFalAiWan25PreviewTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWan25PreviewTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWan25PreviewTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWan25PreviewTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWan25PreviewTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-25-preview/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWan25PreviewTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWan25PreviewTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWan25PreviewTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWan25PreviewTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWan25PreviewTextToVideoData = {
  body: Wan25PreviewTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-25-preview/text-to-video";
};

export type PostFalAiWan25PreviewTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWan25PreviewTextToVideoResponse =
  PostFalAiWan25PreviewTextToVideoResponses[keyof PostFalAiWan25PreviewTextToVideoResponses];

export type GetFalAiWan25PreviewTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-25-preview/text-to-video/requests/{request_id}";
};

export type GetFalAiWan25PreviewTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Wan25PreviewTextToVideoOutput;
};

export type GetFalAiWan25PreviewTextToVideoRequestsByRequestIdResponse =
  GetFalAiWan25PreviewTextToVideoRequestsByRequestIdResponses[keyof GetFalAiWan25PreviewTextToVideoRequestsByRequestIdResponses];

export type GetArgilAvatarsTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/argil/avatars/text-to-video/requests/{request_id}/status";
};

export type GetArgilAvatarsTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetArgilAvatarsTextToVideoRequestsByRequestIdStatusResponse =
  GetArgilAvatarsTextToVideoRequestsByRequestIdStatusResponses[keyof GetArgilAvatarsTextToVideoRequestsByRequestIdStatusResponses];

export type PutArgilAvatarsTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/argil/avatars/text-to-video/requests/{request_id}/cancel";
};

export type PutArgilAvatarsTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutArgilAvatarsTextToVideoRequestsByRequestIdCancelResponse =
  PutArgilAvatarsTextToVideoRequestsByRequestIdCancelResponses[keyof PutArgilAvatarsTextToVideoRequestsByRequestIdCancelResponses];

export type PostArgilAvatarsTextToVideoData = {
  body: AvatarsTextToVideoInput;
  path?: never;
  query?: never;
  url: "/argil/avatars/text-to-video";
};

export type PostArgilAvatarsTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostArgilAvatarsTextToVideoResponse =
  PostArgilAvatarsTextToVideoResponses[keyof PostArgilAvatarsTextToVideoResponses];

export type GetArgilAvatarsTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/argil/avatars/text-to-video/requests/{request_id}";
};

export type GetArgilAvatarsTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AvatarsTextToVideoOutput;
};

export type GetArgilAvatarsTextToVideoRequestsByRequestIdResponse =
  GetArgilAvatarsTextToVideoRequestsByRequestIdResponses[keyof GetArgilAvatarsTextToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV5TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v5/text-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV5TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV5TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV5TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV5TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV5TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV5TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV5TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV5TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV5TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV5TextToVideoData = {
  body: PixverseV5TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v5/text-to-video";
};

export type PostFalAiPixverseV5TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV5TextToVideoResponse =
  PostFalAiPixverseV5TextToVideoResponses[keyof PostFalAiPixverseV5TextToVideoResponses];

export type GetFalAiPixverseV5TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v5/text-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV5TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV5TextToVideoOutput;
};

export type GetFalAiPixverseV5TextToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV5TextToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV5TextToVideoRequestsByRequestIdResponses];

export type GetFalAiInfinitalkSingleTextRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/infinitalk/single-text/requests/{request_id}/status";
};

export type GetFalAiInfinitalkSingleTextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiInfinitalkSingleTextRequestsByRequestIdStatusResponse =
  GetFalAiInfinitalkSingleTextRequestsByRequestIdStatusResponses[keyof GetFalAiInfinitalkSingleTextRequestsByRequestIdStatusResponses];

export type PutFalAiInfinitalkSingleTextRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/infinitalk/single-text/requests/{request_id}/cancel";
};

export type PutFalAiInfinitalkSingleTextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiInfinitalkSingleTextRequestsByRequestIdCancelResponse =
  PutFalAiInfinitalkSingleTextRequestsByRequestIdCancelResponses[keyof PutFalAiInfinitalkSingleTextRequestsByRequestIdCancelResponses];

export type PostFalAiInfinitalkSingleTextData = {
  body: InfinitalkSingleTextInput;
  path?: never;
  query?: never;
  url: "/fal-ai/infinitalk/single-text";
};

export type PostFalAiInfinitalkSingleTextResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiInfinitalkSingleTextResponse =
  PostFalAiInfinitalkSingleTextResponses[keyof PostFalAiInfinitalkSingleTextResponses];

export type GetFalAiInfinitalkSingleTextRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/infinitalk/single-text/requests/{request_id}";
};

export type GetFalAiInfinitalkSingleTextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: InfinitalkSingleTextOutput;
};

export type GetFalAiInfinitalkSingleTextRequestsByRequestIdResponse =
  GetFalAiInfinitalkSingleTextRequestsByRequestIdResponses[keyof GetFalAiInfinitalkSingleTextRequestsByRequestIdResponses];

export type GetMoonvalleyMareyT2vRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/moonvalley/marey/t2v/requests/{request_id}/status";
};

export type GetMoonvalleyMareyT2vRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetMoonvalleyMareyT2vRequestsByRequestIdStatusResponse =
  GetMoonvalleyMareyT2vRequestsByRequestIdStatusResponses[keyof GetMoonvalleyMareyT2vRequestsByRequestIdStatusResponses];

export type PutMoonvalleyMareyT2vRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/moonvalley/marey/t2v/requests/{request_id}/cancel";
};

export type PutMoonvalleyMareyT2vRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutMoonvalleyMareyT2vRequestsByRequestIdCancelResponse =
  PutMoonvalleyMareyT2vRequestsByRequestIdCancelResponses[keyof PutMoonvalleyMareyT2vRequestsByRequestIdCancelResponses];

export type PostMoonvalleyMareyT2vData = {
  body: MareyT2vInput;
  path?: never;
  query?: never;
  url: "/moonvalley/marey/t2v";
};

export type PostMoonvalleyMareyT2vResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostMoonvalleyMareyT2vResponse =
  PostMoonvalleyMareyT2vResponses[keyof PostMoonvalleyMareyT2vResponses];

export type GetMoonvalleyMareyT2vRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/moonvalley/marey/t2v/requests/{request_id}";
};

export type GetMoonvalleyMareyT2vRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MareyT2vOutput;
};

export type GetMoonvalleyMareyT2vRequestsByRequestIdResponse =
  GetMoonvalleyMareyT2vRequestsByRequestIdResponses[keyof GetMoonvalleyMareyT2vRequestsByRequestIdResponses];

export type GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/lora/requests/{request_id}/status";
};

export type GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/lora/requests/{request_id}/cancel";
};

export type PutFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiWanV22A14bTextToVideoLoraData = {
  body: WanV22A14bTextToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/lora";
};

export type PostFalAiWanV22A14bTextToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV22A14bTextToVideoLoraResponse =
  PostFalAiWanV22A14bTextToVideoLoraResponses[keyof PostFalAiWanV22A14bTextToVideoLoraResponses];

export type GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/lora/requests/{request_id}";
};

export type GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV22A14bTextToVideoLoraOutput;
};

export type GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdResponse =
  GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiWanV22A14bTextToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-5b/text-to-video/distill/requests/{request_id}/status";
};

export type GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdStatusResponse =
  GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdStatusResponses[keyof GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdStatusResponses];

export type PutFalAiWanV225bTextToVideoDistillRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/text-to-video/distill/requests/{request_id}/cancel";
};

export type PutFalAiWanV225bTextToVideoDistillRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWanV225bTextToVideoDistillRequestsByRequestIdCancelResponse =
  PutFalAiWanV225bTextToVideoDistillRequestsByRequestIdCancelResponses[keyof PutFalAiWanV225bTextToVideoDistillRequestsByRequestIdCancelResponses];

export type PostFalAiWanV225bTextToVideoDistillData = {
  body: WanV225bTextToVideoDistillInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/text-to-video/distill";
};

export type PostFalAiWanV225bTextToVideoDistillResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV225bTextToVideoDistillResponse =
  PostFalAiWanV225bTextToVideoDistillResponses[keyof PostFalAiWanV225bTextToVideoDistillResponses];

export type GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/text-to-video/distill/requests/{request_id}";
};

export type GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV225bTextToVideoDistillOutput;
};

export type GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdResponse =
  GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdResponses[keyof GetFalAiWanV225bTextToVideoDistillRequestsByRequestIdResponses];

export type GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-5b/text-to-video/fast-wan/requests/{request_id}/status";
};

export type GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdStatusResponse =
  GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdStatusResponses[keyof GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdStatusResponses];

export type PutFalAiWanV225bTextToVideoFastWanRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/text-to-video/fast-wan/requests/{request_id}/cancel";
};

export type PutFalAiWanV225bTextToVideoFastWanRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWanV225bTextToVideoFastWanRequestsByRequestIdCancelResponse =
  PutFalAiWanV225bTextToVideoFastWanRequestsByRequestIdCancelResponses[keyof PutFalAiWanV225bTextToVideoFastWanRequestsByRequestIdCancelResponses];

export type PostFalAiWanV225bTextToVideoFastWanData = {
  body: WanV225bTextToVideoFastWanInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/text-to-video/fast-wan";
};

export type PostFalAiWanV225bTextToVideoFastWanResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV225bTextToVideoFastWanResponse =
  PostFalAiWanV225bTextToVideoFastWanResponses[keyof PostFalAiWanV225bTextToVideoFastWanResponses];

export type GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/text-to-video/fast-wan/requests/{request_id}";
};

export type GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV225bTextToVideoFastWanOutput;
};

export type GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdResponse =
  GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdResponses[keyof GetFalAiWanV225bTextToVideoFastWanRequestsByRequestIdResponses];

export type GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/turbo/requests/{request_id}/status";
};

export type GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdStatusResponse =
  GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdStatusResponses[keyof GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdStatusResponses];

export type PutFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/turbo/requests/{request_id}/cancel";
};

export type PutFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdCancelResponse =
  PutFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdCancelResponses[keyof PutFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdCancelResponses];

export type PostFalAiWanV22A14bTextToVideoTurboData = {
  body: WanV22A14bTextToVideoTurboInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/turbo";
};

export type PostFalAiWanV22A14bTextToVideoTurboResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV22A14bTextToVideoTurboResponse =
  PostFalAiWanV22A14bTextToVideoTurboResponses[keyof PostFalAiWanV22A14bTextToVideoTurboResponses];

export type GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/turbo/requests/{request_id}";
};

export type GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV22A14bTextToVideoTurboOutput;
};

export type GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdResponse =
  GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdResponses[keyof GetFalAiWanV22A14bTextToVideoTurboRequestsByRequestIdResponses];

export type GetFalAiWanV225bTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-5b/text-to-video/requests/{request_id}/status";
};

export type GetFalAiWanV225bTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanV225bTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanV225bTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanV225bTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWanV225bTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWanV225bTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanV225bTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanV225bTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanV225bTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWanV225bTextToVideoData = {
  body: WanV225bTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/text-to-video";
};

export type PostFalAiWanV225bTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV225bTextToVideoResponse =
  PostFalAiWanV225bTextToVideoResponses[keyof PostFalAiWanV225bTextToVideoResponses];

export type GetFalAiWanV225bTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-5b/text-to-video/requests/{request_id}";
};

export type GetFalAiWanV225bTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV225bTextToVideoOutput;
};

export type GetFalAiWanV225bTextToVideoRequestsByRequestIdResponse =
  GetFalAiWanV225bTextToVideoRequestsByRequestIdResponses[keyof GetFalAiWanV225bTextToVideoRequestsByRequestIdResponses];

export type GetFalAiWanV22A14bTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/requests/{request_id}/status";
};

export type GetFalAiWanV22A14bTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanV22A14bTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanV22A14bTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanV22A14bTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWanV22A14bTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWanV22A14bTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanV22A14bTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanV22A14bTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanV22A14bTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWanV22A14bTextToVideoData = {
  body: WanV22A14bTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/text-to-video";
};

export type PostFalAiWanV22A14bTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV22A14bTextToVideoResponse =
  PostFalAiWanV22A14bTextToVideoResponses[keyof PostFalAiWanV22A14bTextToVideoResponses];

export type GetFalAiWanV22A14bTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/text-to-video/requests/{request_id}";
};

export type GetFalAiWanV22A14bTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV22A14bTextToVideoOutput;
};

export type GetFalAiWanV22A14bTextToVideoRequestsByRequestIdResponse =
  GetFalAiWanV22A14bTextToVideoRequestsByRequestIdResponses[keyof GetFalAiWanV22A14bTextToVideoRequestsByRequestIdResponses];

export type GetFalAiLtxv13B098DistilledRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltxv-13b-098-distilled/requests/{request_id}/status";
};

export type GetFalAiLtxv13B098DistilledRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtxv13B098DistilledRequestsByRequestIdStatusResponse =
  GetFalAiLtxv13B098DistilledRequestsByRequestIdStatusResponses[keyof GetFalAiLtxv13B098DistilledRequestsByRequestIdStatusResponses];

export type PutFalAiLtxv13B098DistilledRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltxv-13b-098-distilled/requests/{request_id}/cancel";
};

export type PutFalAiLtxv13B098DistilledRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtxv13B098DistilledRequestsByRequestIdCancelResponse =
  PutFalAiLtxv13B098DistilledRequestsByRequestIdCancelResponses[keyof PutFalAiLtxv13B098DistilledRequestsByRequestIdCancelResponses];

export type PostFalAiLtxv13B098DistilledData = {
  body: Ltxv13B098DistilledInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltxv-13b-098-distilled";
};

export type PostFalAiLtxv13B098DistilledResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxv13B098DistilledResponse =
  PostFalAiLtxv13B098DistilledResponses[keyof PostFalAiLtxv13B098DistilledResponses];

export type GetFalAiLtxv13B098DistilledRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltxv-13b-098-distilled/requests/{request_id}";
};

export type GetFalAiLtxv13B098DistilledRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltxv13B098DistilledOutput;
};

export type GetFalAiLtxv13B098DistilledRequestsByRequestIdResponse =
  GetFalAiLtxv13B098DistilledRequestsByRequestIdResponses[keyof GetFalAiLtxv13B098DistilledRequestsByRequestIdResponses];

export type GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/minimax/hailuo-02/pro/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/minimax/hailuo-02/pro/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxHailuo02ProTextToVideoData = {
  body: MinimaxHailuo02ProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/hailuo-02/pro/text-to-video";
};

export type PostFalAiMinimaxHailuo02ProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxHailuo02ProTextToVideoResponse =
  PostFalAiMinimaxHailuo02ProTextToVideoResponses[keyof PostFalAiMinimaxHailuo02ProTextToVideoResponses];

export type GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/hailuo-02/pro/text-to-video/requests/{request_id}";
};

export type GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: MinimaxHailuo02ProTextToVideoOutput;
  };

export type GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdResponse =
  GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiMinimaxHailuo02ProTextToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance/seedance/v1/pro/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/pro/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceSeedanceV1ProTextToVideoData = {
  body: BytedanceSeedanceV1ProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1/pro/text-to-video";
};

export type PostFalAiBytedanceSeedanceV1ProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceSeedanceV1ProTextToVideoResponse =
  PostFalAiBytedanceSeedanceV1ProTextToVideoResponses[keyof PostFalAiBytedanceSeedanceV1ProTextToVideoResponses];

export type GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1/pro/text-to-video/requests/{request_id}";
};

export type GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceSeedanceV1ProTextToVideoOutput;
  };

export type GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceSeedanceV1ProTextToVideoRequestsByRequestIdResponses];

export type GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance/seedance/v1/lite/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/lite/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceSeedanceV1LiteTextToVideoData = {
  body: BytedanceSeedanceV1LiteTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance/seedance/v1/lite/text-to-video";
};

export type PostFalAiBytedanceSeedanceV1LiteTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceSeedanceV1LiteTextToVideoResponse =
  PostFalAiBytedanceSeedanceV1LiteTextToVideoResponses[keyof PostFalAiBytedanceSeedanceV1LiteTextToVideoResponses];

export type GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance/seedance/v1/lite/text-to-video/requests/{request_id}";
  };

export type GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceSeedanceV1LiteTextToVideoOutput;
  };

export type GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceSeedanceV1LiteTextToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.1/master/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.1/master/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV21MasterTextToVideoData = {
  body: KlingVideoV21MasterTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.1/master/text-to-video";
};

export type PostFalAiKlingVideoV21MasterTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV21MasterTextToVideoResponse =
  PostFalAiKlingVideoV21MasterTextToVideoResponses[keyof PostFalAiKlingVideoV21MasterTextToVideoResponses];

export type GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.1/master/text-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV21MasterTextToVideoOutput;
  };

export type GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV21MasterTextToVideoRequestsByRequestIdResponses];

export type GetVeedAvatarsTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/veed/avatars/text-to-video/requests/{request_id}/status";
};

export type GetVeedAvatarsTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetVeedAvatarsTextToVideoRequestsByRequestIdStatusResponse =
  GetVeedAvatarsTextToVideoRequestsByRequestIdStatusResponses[keyof GetVeedAvatarsTextToVideoRequestsByRequestIdStatusResponses];

export type PutVeedAvatarsTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/avatars/text-to-video/requests/{request_id}/cancel";
};

export type PutVeedAvatarsTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutVeedAvatarsTextToVideoRequestsByRequestIdCancelResponse =
  PutVeedAvatarsTextToVideoRequestsByRequestIdCancelResponses[keyof PutVeedAvatarsTextToVideoRequestsByRequestIdCancelResponses];

export type PostVeedAvatarsTextToVideoData = {
  body: AvatarsTextToVideoInputType2;
  path?: never;
  query?: never;
  url: "/veed/avatars/text-to-video";
};

export type PostVeedAvatarsTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostVeedAvatarsTextToVideoResponse =
  PostVeedAvatarsTextToVideoResponses[keyof PostVeedAvatarsTextToVideoResponses];

export type GetVeedAvatarsTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/avatars/text-to-video/requests/{request_id}";
};

export type GetVeedAvatarsTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AvatarsTextToVideoOutputType2;
};

export type GetVeedAvatarsTextToVideoRequestsByRequestIdResponse =
  GetVeedAvatarsTextToVideoRequestsByRequestIdResponses[keyof GetVeedAvatarsTextToVideoRequestsByRequestIdResponses];

export type GetFalAiLtxVideo13bDevRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video-13b-dev/requests/{request_id}/status";
};

export type GetFalAiLtxVideo13bDevRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtxVideo13bDevRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDevRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDevRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideo13bDevRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideo13bDevRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtxVideo13bDevRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDevRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDevRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideo13bDevData = {
  body: LtxVideo13bDevInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev";
};

export type PostFalAiLtxVideo13bDevResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideo13bDevResponse =
  PostFalAiLtxVideo13bDevResponses[keyof PostFalAiLtxVideo13bDevResponses];

export type GetFalAiLtxVideo13bDevRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/requests/{request_id}";
};

export type GetFalAiLtxVideo13bDevRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideo13bDevOutput;
};

export type GetFalAiLtxVideo13bDevRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDevRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDevRequestsByRequestIdResponses];

export type GetFalAiLtxVideo13bDistilledRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video-13b-distilled/requests/{request_id}/status";
};

export type GetFalAiLtxVideo13bDistilledRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtxVideo13bDistilledRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDistilledRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDistilledRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideo13bDistilledRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-distilled/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideo13bDistilledRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtxVideo13bDistilledRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDistilledRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDistilledRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideo13bDistilledData = {
  body: LtxVideo13bDistilledInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-13b-distilled";
};

export type PostFalAiLtxVideo13bDistilledResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideo13bDistilledResponse =
  PostFalAiLtxVideo13bDistilledResponses[keyof PostFalAiLtxVideo13bDistilledResponses];

export type GetFalAiLtxVideo13bDistilledRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-distilled/requests/{request_id}";
};

export type GetFalAiLtxVideo13bDistilledRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideo13bDistilledOutput;
};

export type GetFalAiLtxVideo13bDistilledRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDistilledRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDistilledRequestsByRequestIdResponses];

export type GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4.5/text-to-video/fast/requests/{request_id}/status";
};

export type GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV45TextToVideoFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/text-to-video/fast/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV45TextToVideoFastRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV45TextToVideoFastRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV45TextToVideoFastRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV45TextToVideoFastRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV45TextToVideoFastData = {
  body: PixverseV45TextToVideoFastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4.5/text-to-video/fast";
};

export type PostFalAiPixverseV45TextToVideoFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV45TextToVideoFastResponse =
  PostFalAiPixverseV45TextToVideoFastResponses[keyof PostFalAiPixverseV45TextToVideoFastResponses];

export type GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/text-to-video/fast/requests/{request_id}";
};

export type GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV45TextToVideoFastOutput;
};

export type GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdResponse =
  GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdResponses[keyof GetFalAiPixverseV45TextToVideoFastRequestsByRequestIdResponses];

export type GetFalAiPixverseV45TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4.5/text-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV45TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV45TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV45TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV45TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV45TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV45TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV45TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV45TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV45TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV45TextToVideoData = {
  body: PixverseV45TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4.5/text-to-video";
};

export type PostFalAiPixverseV45TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV45TextToVideoResponse =
  PostFalAiPixverseV45TextToVideoResponses[keyof PostFalAiPixverseV45TextToVideoResponses];

export type GetFalAiPixverseV45TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4.5/text-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV45TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV45TextToVideoOutput;
};

export type GetFalAiPixverseV45TextToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV45TextToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV45TextToVideoRequestsByRequestIdResponses];

export type GetFalAiViduQ1TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q1/text-to-video/requests/{request_id}/status";
};

export type GetFalAiViduQ1TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiViduQ1TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiViduQ1TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ1TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ1TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q1/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiViduQ1TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiViduQ1TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiViduQ1TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ1TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ1TextToVideoData = {
  body: ViduQ1TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q1/text-to-video";
};

export type PostFalAiViduQ1TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ1TextToVideoResponse =
  PostFalAiViduQ1TextToVideoResponses[keyof PostFalAiViduQ1TextToVideoResponses];

export type GetFalAiViduQ1TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q1/text-to-video/requests/{request_id}";
};

export type GetFalAiViduQ1TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ1TextToVideoOutput;
};

export type GetFalAiViduQ1TextToVideoRequestsByRequestIdResponse =
  GetFalAiViduQ1TextToVideoRequestsByRequestIdResponses[keyof GetFalAiViduQ1TextToVideoRequestsByRequestIdResponses];

export type GetFalAiMagiRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/magi/requests/{request_id}/status";
};

export type GetFalAiMagiRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMagiRequestsByRequestIdStatusResponse =
  GetFalAiMagiRequestsByRequestIdStatusResponses[keyof GetFalAiMagiRequestsByRequestIdStatusResponses];

export type PutFalAiMagiRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi/requests/{request_id}/cancel";
};

export type PutFalAiMagiRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMagiRequestsByRequestIdCancelResponse =
  PutFalAiMagiRequestsByRequestIdCancelResponses[keyof PutFalAiMagiRequestsByRequestIdCancelResponses];

export type PostFalAiMagiData = {
  body: MagiInput;
  path?: never;
  query?: never;
  url: "/fal-ai/magi";
};

export type PostFalAiMagiResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMagiResponse =
  PostFalAiMagiResponses[keyof PostFalAiMagiResponses];

export type GetFalAiMagiRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi/requests/{request_id}";
};

export type GetFalAiMagiRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MagiOutput;
};

export type GetFalAiMagiRequestsByRequestIdResponse =
  GetFalAiMagiRequestsByRequestIdResponses[keyof GetFalAiMagiRequestsByRequestIdResponses];

export type GetFalAiMagiDistilledRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/magi-distilled/requests/{request_id}/status";
};

export type GetFalAiMagiDistilledRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMagiDistilledRequestsByRequestIdStatusResponse =
  GetFalAiMagiDistilledRequestsByRequestIdStatusResponses[keyof GetFalAiMagiDistilledRequestsByRequestIdStatusResponses];

export type PutFalAiMagiDistilledRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi-distilled/requests/{request_id}/cancel";
};

export type PutFalAiMagiDistilledRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMagiDistilledRequestsByRequestIdCancelResponse =
  PutFalAiMagiDistilledRequestsByRequestIdCancelResponses[keyof PutFalAiMagiDistilledRequestsByRequestIdCancelResponses];

export type PostFalAiMagiDistilledData = {
  body: MagiDistilledInput;
  path?: never;
  query?: never;
  url: "/fal-ai/magi-distilled";
};

export type PostFalAiMagiDistilledResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMagiDistilledResponse =
  PostFalAiMagiDistilledResponses[keyof PostFalAiMagiDistilledResponses];

export type GetFalAiMagiDistilledRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi-distilled/requests/{request_id}";
};

export type GetFalAiMagiDistilledRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MagiDistilledOutput;
};

export type GetFalAiMagiDistilledRequestsByRequestIdResponse =
  GetFalAiMagiDistilledRequestsByRequestIdResponses[keyof GetFalAiMagiDistilledRequestsByRequestIdResponses];

export type GetFalAiPixverseV4TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4/text-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV4TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV4TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV4TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV4TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV4TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV4TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV4TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV4TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV4TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV4TextToVideoData = {
  body: PixverseV4TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4/text-to-video";
};

export type PostFalAiPixverseV4TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV4TextToVideoResponse =
  PostFalAiPixverseV4TextToVideoResponses[keyof PostFalAiPixverseV4TextToVideoResponses];

export type GetFalAiPixverseV4TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/text-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV4TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV4TextToVideoOutput;
};

export type GetFalAiPixverseV4TextToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV4TextToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV4TextToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v4/text-to-video/fast/requests/{request_id}/status";
};

export type GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV4TextToVideoFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/text-to-video/fast/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV4TextToVideoFastRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV4TextToVideoFastRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV4TextToVideoFastRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV4TextToVideoFastRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV4TextToVideoFastData = {
  body: PixverseV4TextToVideoFastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v4/text-to-video/fast";
};

export type PostFalAiPixverseV4TextToVideoFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV4TextToVideoFastResponse =
  PostFalAiPixverseV4TextToVideoFastResponses[keyof PostFalAiPixverseV4TextToVideoFastResponses];

export type GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v4/text-to-video/fast/requests/{request_id}";
};

export type GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV4TextToVideoFastOutput;
};

export type GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdResponse =
  GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdResponses[keyof GetFalAiPixverseV4TextToVideoFastRequestsByRequestIdResponses];

export type GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/lipsync/audio-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/lipsync/audio-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoLipsyncAudioToVideoData = {
  body: KlingVideoLipsyncAudioToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/lipsync/audio-to-video";
};

export type PostFalAiKlingVideoLipsyncAudioToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoLipsyncAudioToVideoResponse =
  PostFalAiKlingVideoLipsyncAudioToVideoResponses[keyof PostFalAiKlingVideoLipsyncAudioToVideoResponses];

export type GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/lipsync/audio-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoLipsyncAudioToVideoOutput;
  };

export type GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoLipsyncAudioToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/lipsync/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/lipsync/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoLipsyncTextToVideoData = {
  body: KlingVideoLipsyncTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/lipsync/text-to-video";
};

export type PostFalAiKlingVideoLipsyncTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoLipsyncTextToVideoResponse =
  PostFalAiKlingVideoLipsyncTextToVideoResponses[keyof PostFalAiKlingVideoLipsyncTextToVideoResponses];

export type GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/lipsync/text-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoLipsyncTextToVideoOutput;
};

export type GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoLipsyncTextToVideoRequestsByRequestIdResponses];

export type GetFalAiWanT2vLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-t2v-lora/requests/{request_id}/status";
};

export type GetFalAiWanT2vLoraRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanT2vLoraRequestsByRequestIdStatusResponse =
  GetFalAiWanT2vLoraRequestsByRequestIdStatusResponses[keyof GetFalAiWanT2vLoraRequestsByRequestIdStatusResponses];

export type PutFalAiWanT2vLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-t2v-lora/requests/{request_id}/cancel";
};

export type PutFalAiWanT2vLoraRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanT2vLoraRequestsByRequestIdCancelResponse =
  PutFalAiWanT2vLoraRequestsByRequestIdCancelResponses[keyof PutFalAiWanT2vLoraRequestsByRequestIdCancelResponses];

export type PostFalAiWanT2vLoraData = {
  body: WanT2vLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-t2v-lora";
};

export type PostFalAiWanT2vLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanT2vLoraResponse =
  PostFalAiWanT2vLoraResponses[keyof PostFalAiWanT2vLoraResponses];

export type GetFalAiWanT2vLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-t2v-lora/requests/{request_id}";
};

export type GetFalAiWanT2vLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanT2vLoraOutput;
};

export type GetFalAiWanT2vLoraRequestsByRequestIdResponse =
  GetFalAiWanT2vLoraRequestsByRequestIdResponses[keyof GetFalAiWanT2vLoraRequestsByRequestIdResponses];

export type GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/luma-dream-machine/ray-2-flash/requests/{request_id}/status";
};

export type GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdStatusResponses];

export type PutFalAiLumaDreamMachineRay2FlashRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2-flash/requests/{request_id}/cancel";
};

export type PutFalAiLumaDreamMachineRay2FlashRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLumaDreamMachineRay2FlashRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2FlashRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2FlashRequestsByRequestIdCancelResponses];

export type PostFalAiLumaDreamMachineRay2FlashData = {
  body: LumaDreamMachineRay2FlashInput;
  path?: never;
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2-flash";
};

export type PostFalAiLumaDreamMachineRay2FlashResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLumaDreamMachineRay2FlashResponse =
  PostFalAiLumaDreamMachineRay2FlashResponses[keyof PostFalAiLumaDreamMachineRay2FlashResponses];

export type GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2-flash/requests/{request_id}";
};

export type GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LumaDreamMachineRay2FlashOutput;
};

export type GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2FlashRequestsByRequestIdResponses];

export type GetFalAiPikaV21TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v2.1/text-to-video/requests/{request_id}/status";
};

export type GetFalAiPikaV21TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPikaV21TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPikaV21TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV21TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV21TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.1/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPikaV21TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPikaV21TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPikaV21TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV21TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV21TextToVideoData = {
  body: PikaV21TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v2.1/text-to-video";
};

export type PostFalAiPikaV21TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV21TextToVideoResponse =
  PostFalAiPikaV21TextToVideoResponses[keyof PostFalAiPikaV21TextToVideoResponses];

export type GetFalAiPikaV21TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.1/text-to-video/requests/{request_id}";
};

export type GetFalAiPikaV21TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV21TextToVideoOutput;
};

export type GetFalAiPikaV21TextToVideoRequestsByRequestIdResponse =
  GetFalAiPikaV21TextToVideoRequestsByRequestIdResponses[keyof GetFalAiPikaV21TextToVideoRequestsByRequestIdResponses];

export type GetFalAiPikaV22TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v2.2/text-to-video/requests/{request_id}/status";
};

export type GetFalAiPikaV22TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPikaV22TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPikaV22TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV22TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV22TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.2/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPikaV22TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPikaV22TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPikaV22TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV22TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV22TextToVideoData = {
  body: PikaV22TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v2.2/text-to-video";
};

export type PostFalAiPikaV22TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV22TextToVideoResponse =
  PostFalAiPikaV22TextToVideoResponses[keyof PostFalAiPikaV22TextToVideoResponses];

export type GetFalAiPikaV22TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2.2/text-to-video/requests/{request_id}";
};

export type GetFalAiPikaV22TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV22TextToVideoOutput;
};

export type GetFalAiPikaV22TextToVideoRequestsByRequestIdResponse =
  GetFalAiPikaV22TextToVideoRequestsByRequestIdResponses[keyof GetFalAiPikaV22TextToVideoRequestsByRequestIdResponses];

export type GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v2/turbo/text-to-video/requests/{request_id}/status";
};

export type GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV2TurboTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2/turbo/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPikaV2TurboTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPikaV2TurboTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPikaV2TurboTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV2TurboTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV2TurboTextToVideoData = {
  body: PikaV2TurboTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v2/turbo/text-to-video";
};

export type PostFalAiPikaV2TurboTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV2TurboTextToVideoResponse =
  PostFalAiPikaV2TurboTextToVideoResponses[keyof PostFalAiPikaV2TurboTextToVideoResponses];

export type GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2/turbo/text-to-video/requests/{request_id}";
};

export type GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV2TurboTextToVideoOutput;
};

export type GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdResponse =
  GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdResponses[keyof GetFalAiPikaV2TurboTextToVideoRequestsByRequestIdResponses];

export type GetFalAiWanProTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-pro/text-to-video/requests/{request_id}/status";
};

export type GetFalAiWanProTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWanProTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-pro/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWanProTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWanProTextToVideoData = {
  body: WanProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-pro/text-to-video";
};

export type PostFalAiWanProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanProTextToVideoResponse =
  PostFalAiWanProTextToVideoResponses[keyof PostFalAiWanProTextToVideoResponses];

export type GetFalAiWanProTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-pro/text-to-video/requests/{request_id}";
};

export type GetFalAiWanProTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanProTextToVideoOutput;
};

export type GetFalAiWanProTextToVideoRequestsByRequestIdResponse =
  GetFalAiWanProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiWanProTextToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/v1.6/pro/effects/requests/{request_id}/status";
};

export type GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV16ProEffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/effects/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoV16ProEffectsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV16ProEffectsRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV16ProEffectsRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV16ProEffectsRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV16ProEffectsData = {
  body: KlingVideoV16ProEffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/effects";
};

export type PostFalAiKlingVideoV16ProEffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV16ProEffectsResponse =
  PostFalAiKlingVideoV16ProEffectsResponses[keyof PostFalAiKlingVideoV16ProEffectsResponses];

export type GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/effects/requests/{request_id}";
};

export type GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV16ProEffectsOutput;
};

export type GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdResponse =
  GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV16ProEffectsRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v1.6/standard/effects/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV16StandardEffectsRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v1.6/standard/effects/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV16StandardEffectsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV16StandardEffectsRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV16StandardEffectsRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV16StandardEffectsRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV16StandardEffectsData = {
  body: KlingVideoV16StandardEffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.6/standard/effects";
};

export type PostFalAiKlingVideoV16StandardEffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV16StandardEffectsResponse =
  PostFalAiKlingVideoV16StandardEffectsResponses[keyof PostFalAiKlingVideoV16StandardEffectsResponses];

export type GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/standard/effects/requests/{request_id}";
};

export type GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV16StandardEffectsOutput;
};

export type GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdResponse =
  GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV16StandardEffectsRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/v1.5/pro/effects/requests/{request_id}/status";
};

export type GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV15ProEffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.5/pro/effects/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoV15ProEffectsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV15ProEffectsRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV15ProEffectsRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV15ProEffectsRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV15ProEffectsData = {
  body: KlingVideoV15ProEffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.5/pro/effects";
};

export type PostFalAiKlingVideoV15ProEffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV15ProEffectsResponse =
  PostFalAiKlingVideoV15ProEffectsResponses[keyof PostFalAiKlingVideoV15ProEffectsResponses];

export type GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.5/pro/effects/requests/{request_id}";
};

export type GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV15ProEffectsOutput;
};

export type GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdResponse =
  GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV15ProEffectsRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/v1/standard/effects/requests/{request_id}/status";
};

export type GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV1StandardEffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1/standard/effects/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoV1StandardEffectsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV1StandardEffectsRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV1StandardEffectsRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV1StandardEffectsRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV1StandardEffectsData = {
  body: KlingVideoV1StandardEffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1/standard/effects";
};

export type PostFalAiKlingVideoV1StandardEffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV1StandardEffectsResponse =
  PostFalAiKlingVideoV1StandardEffectsResponses[keyof PostFalAiKlingVideoV1StandardEffectsResponses];

export type GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1/standard/effects/requests/{request_id}";
};

export type GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV1StandardEffectsOutput;
};

export type GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdResponse =
  GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV1StandardEffectsRequestsByRequestIdResponses];

export type GetFalAiLtxVideoV095RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video-v095/requests/{request_id}/status";
};

export type GetFalAiLtxVideoV095RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtxVideoV095RequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoV095RequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoV095RequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideoV095RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-v095/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideoV095RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtxVideoV095RequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoV095RequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoV095RequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideoV095Data = {
  body: LtxVideoV095Input;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-v095";
};

export type PostFalAiLtxVideoV095Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideoV095Response =
  PostFalAiLtxVideoV095Responses[keyof PostFalAiLtxVideoV095Responses];

export type GetFalAiLtxVideoV095RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-v095/requests/{request_id}";
};

export type GetFalAiLtxVideoV095RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideoV095Output;
};

export type GetFalAiLtxVideoV095RequestsByRequestIdResponse =
  GetFalAiLtxVideoV095RequestsByRequestIdResponses[keyof GetFalAiLtxVideoV095RequestsByRequestIdResponses];

export type GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/v1.6/pro/text-to-video/requests/{request_id}/status";
};

export type GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV16ProTextToVideoData = {
  body: KlingVideoV16ProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/text-to-video";
};

export type PostFalAiKlingVideoV16ProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV16ProTextToVideoResponse =
  PostFalAiKlingVideoV16ProTextToVideoResponses[keyof PostFalAiKlingVideoV16ProTextToVideoResponses];

export type GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/pro/text-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV16ProTextToVideoOutput;
};

export type GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV16ProTextToVideoRequestsByRequestIdResponses];

export type GetFalAiWanT2vRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-t2v/requests/{request_id}/status";
};

export type GetFalAiWanT2vRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanT2vRequestsByRequestIdStatusResponse =
  GetFalAiWanT2vRequestsByRequestIdStatusResponses[keyof GetFalAiWanT2vRequestsByRequestIdStatusResponses];

export type PutFalAiWanT2vRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-t2v/requests/{request_id}/cancel";
};

export type PutFalAiWanT2vRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanT2vRequestsByRequestIdCancelResponse =
  PutFalAiWanT2vRequestsByRequestIdCancelResponses[keyof PutFalAiWanT2vRequestsByRequestIdCancelResponses];

export type PostFalAiWanT2vData = {
  body: WanT2vInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-t2v";
};

export type PostFalAiWanT2vResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanT2vResponse =
  PostFalAiWanT2vResponses[keyof PostFalAiWanT2vResponses];

export type GetFalAiWanT2vRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-t2v/requests/{request_id}";
};

export type GetFalAiWanT2vRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanT2vOutput;
};

export type GetFalAiWanT2vRequestsByRequestIdResponse =
  GetFalAiWanT2vRequestsByRequestIdResponses[keyof GetFalAiWanT2vRequestsByRequestIdResponses];

export type GetFalAiVeo2RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo2/requests/{request_id}/status";
};

export type GetFalAiVeo2RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo2RequestsByRequestIdStatusResponse =
  GetFalAiVeo2RequestsByRequestIdStatusResponses[keyof GetFalAiVeo2RequestsByRequestIdStatusResponses];

export type PutFalAiVeo2RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo2/requests/{request_id}/cancel";
};

export type PutFalAiVeo2RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo2RequestsByRequestIdCancelResponse =
  PutFalAiVeo2RequestsByRequestIdCancelResponses[keyof PutFalAiVeo2RequestsByRequestIdCancelResponses];

export type PostFalAiVeo2Data = {
  body: Veo2Input;
  path?: never;
  query?: never;
  url: "/fal-ai/veo2";
};

export type PostFalAiVeo2Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo2Response =
  PostFalAiVeo2Responses[keyof PostFalAiVeo2Responses];

export type GetFalAiVeo2RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo2/requests/{request_id}";
};

export type GetFalAiVeo2RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo2Output;
};

export type GetFalAiVeo2RequestsByRequestIdResponse =
  GetFalAiVeo2RequestsByRequestIdResponses[keyof GetFalAiVeo2RequestsByRequestIdResponses];

export type GetFalAiMinimaxVideo01DirectorRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/minimax/video-01-director/requests/{request_id}/status";
};

export type GetFalAiMinimaxVideo01DirectorRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMinimaxVideo01DirectorRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxVideo01DirectorRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxVideo01DirectorRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxVideo01DirectorRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01-director/requests/{request_id}/cancel";
};

export type PutFalAiMinimaxVideo01DirectorRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMinimaxVideo01DirectorRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxVideo01DirectorRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxVideo01DirectorRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxVideo01DirectorData = {
  body: MinimaxVideo01DirectorInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/video-01-director";
};

export type PostFalAiMinimaxVideo01DirectorResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxVideo01DirectorResponse =
  PostFalAiMinimaxVideo01DirectorResponses[keyof PostFalAiMinimaxVideo01DirectorResponses];

export type GetFalAiMinimaxVideo01DirectorRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01-director/requests/{request_id}";
};

export type GetFalAiMinimaxVideo01DirectorRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MinimaxVideo01DirectorOutput;
};

export type GetFalAiMinimaxVideo01DirectorRequestsByRequestIdResponse =
  GetFalAiMinimaxVideo01DirectorRequestsByRequestIdResponses[keyof GetFalAiMinimaxVideo01DirectorRequestsByRequestIdResponses];

export type GetFalAiPixverseV35TextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v3.5/text-to-video/requests/{request_id}/status";
};

export type GetFalAiPixverseV35TextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseV35TextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV35TextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV35TextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV35TextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV35TextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseV35TextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV35TextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV35TextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV35TextToVideoData = {
  body: PixverseV35TextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v3.5/text-to-video";
};

export type PostFalAiPixverseV35TextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV35TextToVideoResponse =
  PostFalAiPixverseV35TextToVideoResponses[keyof PostFalAiPixverseV35TextToVideoResponses];

export type GetFalAiPixverseV35TextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/text-to-video/requests/{request_id}";
};

export type GetFalAiPixverseV35TextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV35TextToVideoOutput;
};

export type GetFalAiPixverseV35TextToVideoRequestsByRequestIdResponse =
  GetFalAiPixverseV35TextToVideoRequestsByRequestIdResponses[keyof GetFalAiPixverseV35TextToVideoRequestsByRequestIdResponses];

export type GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/v3.5/text-to-video/fast/requests/{request_id}/status";
};

export type GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdStatusResponse =
  GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseV35TextToVideoFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/text-to-video/fast/requests/{request_id}/cancel";
};

export type PutFalAiPixverseV35TextToVideoFastRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiPixverseV35TextToVideoFastRequestsByRequestIdCancelResponse =
  PutFalAiPixverseV35TextToVideoFastRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseV35TextToVideoFastRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseV35TextToVideoFastData = {
  body: PixverseV35TextToVideoFastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/v3.5/text-to-video/fast";
};

export type PostFalAiPixverseV35TextToVideoFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseV35TextToVideoFastResponse =
  PostFalAiPixverseV35TextToVideoFastResponses[keyof PostFalAiPixverseV35TextToVideoFastResponses];

export type GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/v3.5/text-to-video/fast/requests/{request_id}";
};

export type GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseV35TextToVideoFastOutput;
};

export type GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdResponse =
  GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdResponses[keyof GetFalAiPixverseV35TextToVideoFastRequestsByRequestIdResponses];

export type GetFalAiLumaDreamMachineRay2RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/luma-dream-machine/ray-2/requests/{request_id}/status";
};

export type GetFalAiLumaDreamMachineRay2RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLumaDreamMachineRay2RequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2RequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2RequestsByRequestIdStatusResponses];

export type PutFalAiLumaDreamMachineRay2RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/requests/{request_id}/cancel";
};

export type PutFalAiLumaDreamMachineRay2RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLumaDreamMachineRay2RequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2RequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2RequestsByRequestIdCancelResponses];

export type PostFalAiLumaDreamMachineRay2Data = {
  body: LumaDreamMachineRay2Input;
  path?: never;
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2";
};

export type PostFalAiLumaDreamMachineRay2Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLumaDreamMachineRay2Response =
  PostFalAiLumaDreamMachineRay2Responses[keyof PostFalAiLumaDreamMachineRay2Responses];

export type GetFalAiLumaDreamMachineRay2RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/requests/{request_id}";
};

export type GetFalAiLumaDreamMachineRay2RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LumaDreamMachineRay2Output;
};

export type GetFalAiLumaDreamMachineRay2RequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2RequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2RequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-video-lora/requests/{request_id}/status";
};

export type GetFalAiHunyuanVideoLoraRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiHunyuanVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-lora/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanVideoLoraRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiHunyuanVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoLoraData = {
  body: HunyuanVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video-lora";
};

export type PostFalAiHunyuanVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoLoraResponse =
  PostFalAiHunyuanVideoLoraResponses[keyof PostFalAiHunyuanVideoLoraResponses];

export type GetFalAiHunyuanVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-lora/requests/{request_id}";
};

export type GetFalAiHunyuanVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoLoraOutput;
};

export type GetFalAiHunyuanVideoLoraRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoLoraRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoLoraRequestsByRequestIdResponses];

export type GetFalAiTranspixarRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/transpixar/requests/{request_id}/status";
};

export type GetFalAiTranspixarRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiTranspixarRequestsByRequestIdStatusResponse =
  GetFalAiTranspixarRequestsByRequestIdStatusResponses[keyof GetFalAiTranspixarRequestsByRequestIdStatusResponses];

export type PutFalAiTranspixarRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/transpixar/requests/{request_id}/cancel";
};

export type PutFalAiTranspixarRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiTranspixarRequestsByRequestIdCancelResponse =
  PutFalAiTranspixarRequestsByRequestIdCancelResponses[keyof PutFalAiTranspixarRequestsByRequestIdCancelResponses];

export type PostFalAiTranspixarData = {
  body: TranspixarInput;
  path?: never;
  query?: never;
  url: "/fal-ai/transpixar";
};

export type PostFalAiTranspixarResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiTranspixarResponse =
  PostFalAiTranspixarResponses[keyof PostFalAiTranspixarResponses];

export type GetFalAiTranspixarRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/transpixar/requests/{request_id}";
};

export type GetFalAiTranspixarRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: TranspixarOutput;
};

export type GetFalAiTranspixarRequestsByRequestIdResponse =
  GetFalAiTranspixarRequestsByRequestIdResponses[keyof GetFalAiTranspixarRequestsByRequestIdResponses];

export type GetFalAiCogvideox5bRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/cogvideox-5b/requests/{request_id}/status";
};

export type GetFalAiCogvideox5bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiCogvideox5bRequestsByRequestIdStatusResponse =
  GetFalAiCogvideox5bRequestsByRequestIdStatusResponses[keyof GetFalAiCogvideox5bRequestsByRequestIdStatusResponses];

export type PutFalAiCogvideox5bRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/cogvideox-5b/requests/{request_id}/cancel";
};

export type PutFalAiCogvideox5bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiCogvideox5bRequestsByRequestIdCancelResponse =
  PutFalAiCogvideox5bRequestsByRequestIdCancelResponses[keyof PutFalAiCogvideox5bRequestsByRequestIdCancelResponses];

export type PostFalAiCogvideox5bData = {
  body: Cogvideox5bInput;
  path?: never;
  query?: never;
  url: "/fal-ai/cogvideox-5b";
};

export type PostFalAiCogvideox5bResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiCogvideox5bResponse =
  PostFalAiCogvideox5bResponses[keyof PostFalAiCogvideox5bResponses];

export type GetFalAiCogvideox5bRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/cogvideox-5b/requests/{request_id}";
};

export type GetFalAiCogvideox5bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Cogvideox5bOutput;
};

export type GetFalAiCogvideox5bRequestsByRequestIdResponse =
  GetFalAiCogvideox5bRequestsByRequestIdResponses[keyof GetFalAiCogvideox5bRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v1.6/standard/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v1.6/standard/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV16StandardTextToVideoData = {
  body: KlingVideoV16StandardTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.6/standard/text-to-video";
};

export type PostFalAiKlingVideoV16StandardTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV16StandardTextToVideoResponse =
  PostFalAiKlingVideoV16StandardTextToVideoResponses[keyof PostFalAiKlingVideoV16StandardTextToVideoResponses];

export type GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.6/standard/text-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV16StandardTextToVideoOutput;
  };

export type GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV16StandardTextToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxVideo01LiveRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/minimax/video-01-live/requests/{request_id}/status";
};

export type GetFalAiMinimaxVideo01LiveRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMinimaxVideo01LiveRequestsByRequestIdStatusResponse =
  GetFalAiMinimaxVideo01LiveRequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxVideo01LiveRequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxVideo01LiveRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01-live/requests/{request_id}/cancel";
};

export type PutFalAiMinimaxVideo01LiveRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMinimaxVideo01LiveRequestsByRequestIdCancelResponse =
  PutFalAiMinimaxVideo01LiveRequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxVideo01LiveRequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxVideo01LiveData = {
  body: MinimaxVideo01LiveInput;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/video-01-live";
};

export type PostFalAiMinimaxVideo01LiveResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxVideo01LiveResponse =
  PostFalAiMinimaxVideo01LiveResponses[keyof PostFalAiMinimaxVideo01LiveResponses];

export type GetFalAiMinimaxVideo01LiveRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01-live/requests/{request_id}";
};

export type GetFalAiMinimaxVideo01LiveRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MinimaxVideo01LiveOutput;
};

export type GetFalAiMinimaxVideo01LiveRequestsByRequestIdResponse =
  GetFalAiMinimaxVideo01LiveRequestsByRequestIdResponses[keyof GetFalAiMinimaxVideo01LiveRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v1/standard/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v1/standard/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV1StandardTextToVideoData = {
  body: KlingVideoV1StandardTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1/standard/text-to-video";
};

export type PostFalAiKlingVideoV1StandardTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV1StandardTextToVideoResponse =
  PostFalAiKlingVideoV1StandardTextToVideoResponses[keyof PostFalAiKlingVideoV1StandardTextToVideoResponses];

export type GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1/standard/text-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV1StandardTextToVideoOutput;
  };

export type GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV1StandardTextToVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/kling-video/v1.5/pro/text-to-video/requests/{request_id}/status";
};

export type GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.5/pro/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV15ProTextToVideoData = {
  body: KlingVideoV15ProTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v1.5/pro/text-to-video";
};

export type PostFalAiKlingVideoV15ProTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV15ProTextToVideoResponse =
  PostFalAiKlingVideoV15ProTextToVideoResponses[keyof PostFalAiKlingVideoV15ProTextToVideoResponses];

export type GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v1.5/pro/text-to-video/requests/{request_id}";
};

export type GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoV15ProTextToVideoOutput;
};

export type GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdResponse =
  GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV15ProTextToVideoRequestsByRequestIdResponses];

export type GetFalAiMochiV1RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/mochi-v1/requests/{request_id}/status";
};

export type GetFalAiMochiV1RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMochiV1RequestsByRequestIdStatusResponse =
  GetFalAiMochiV1RequestsByRequestIdStatusResponses[keyof GetFalAiMochiV1RequestsByRequestIdStatusResponses];

export type PutFalAiMochiV1RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/mochi-v1/requests/{request_id}/cancel";
};

export type PutFalAiMochiV1RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMochiV1RequestsByRequestIdCancelResponse =
  PutFalAiMochiV1RequestsByRequestIdCancelResponses[keyof PutFalAiMochiV1RequestsByRequestIdCancelResponses];

export type PostFalAiMochiV1Data = {
  body: MochiV1Input;
  path?: never;
  query?: never;
  url: "/fal-ai/mochi-v1";
};

export type PostFalAiMochiV1Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMochiV1Response =
  PostFalAiMochiV1Responses[keyof PostFalAiMochiV1Responses];

export type GetFalAiMochiV1RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/mochi-v1/requests/{request_id}";
};

export type GetFalAiMochiV1RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MochiV1Output;
};

export type GetFalAiMochiV1RequestsByRequestIdResponse =
  GetFalAiMochiV1RequestsByRequestIdResponses[keyof GetFalAiMochiV1RequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-video/requests/{request_id}/status";
};

export type GetFalAiHunyuanVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiHunyuanVideoRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiHunyuanVideoRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoData = {
  body: HunyuanVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video";
};

export type PostFalAiHunyuanVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoResponse =
  PostFalAiHunyuanVideoResponses[keyof PostFalAiHunyuanVideoResponses];

export type GetFalAiHunyuanVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video/requests/{request_id}";
};

export type GetFalAiHunyuanVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoOutput;
};

export type GetFalAiHunyuanVideoRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoRequestsByRequestIdResponses];

export type GetFalAiLtxVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video/requests/{request_id}/status";
};

export type GetFalAiLtxVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtxVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtxVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideoData = {
  body: LtxVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video";
};

export type PostFalAiLtxVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideoResponse =
  PostFalAiLtxVideoResponses[keyof PostFalAiLtxVideoResponses];

export type GetFalAiLtxVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video/requests/{request_id}";
};

export type GetFalAiLtxVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideoOutput;
};

export type GetFalAiLtxVideoRequestsByRequestIdResponse =
  GetFalAiLtxVideoRequestsByRequestIdResponses[keyof GetFalAiLtxVideoRequestsByRequestIdResponses];

export type GetFalAiFastSvdTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/fast-svd/text-to-video/requests/{request_id}/status";
};

export type GetFalAiFastSvdTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFastSvdTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiFastSvdTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFastSvdTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiFastSvdTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-svd/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiFastSvdTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFastSvdTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiFastSvdTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFastSvdTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiFastSvdTextToVideoData = {
  body: FastSvdTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/fast-svd/text-to-video";
};

export type PostFalAiFastSvdTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFastSvdTextToVideoResponse =
  PostFalAiFastSvdTextToVideoResponses[keyof PostFalAiFastSvdTextToVideoResponses];

export type GetFalAiFastSvdTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-svd/text-to-video/requests/{request_id}";
};

export type GetFalAiFastSvdTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FastSvdTextToVideoOutput;
};

export type GetFalAiFastSvdTextToVideoRequestsByRequestIdResponse =
  GetFalAiFastSvdTextToVideoRequestsByRequestIdResponses[keyof GetFalAiFastSvdTextToVideoRequestsByRequestIdResponses];

export type GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/fast-svd-lcm/text-to-video/requests/{request_id}/status";
};

export type GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiFastSvdLcmTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-svd-lcm/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiFastSvdLcmTextToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFastSvdLcmTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiFastSvdLcmTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFastSvdLcmTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiFastSvdLcmTextToVideoData = {
  body: FastSvdLcmTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/fast-svd-lcm/text-to-video";
};

export type PostFalAiFastSvdLcmTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFastSvdLcmTextToVideoResponse =
  PostFalAiFastSvdLcmTextToVideoResponses[keyof PostFalAiFastSvdLcmTextToVideoResponses];

export type GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-svd-lcm/text-to-video/requests/{request_id}";
};

export type GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FastSvdLcmTextToVideoOutput;
};

export type GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdResponse =
  GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdResponses[keyof GetFalAiFastSvdLcmTextToVideoRequestsByRequestIdResponses];

export type GetFalAiT2vTurboRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/t2v-turbo/requests/{request_id}/status";
};

export type GetFalAiT2vTurboRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiT2vTurboRequestsByRequestIdStatusResponse =
  GetFalAiT2vTurboRequestsByRequestIdStatusResponses[keyof GetFalAiT2vTurboRequestsByRequestIdStatusResponses];

export type PutFalAiT2vTurboRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/t2v-turbo/requests/{request_id}/cancel";
};

export type PutFalAiT2vTurboRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiT2vTurboRequestsByRequestIdCancelResponse =
  PutFalAiT2vTurboRequestsByRequestIdCancelResponses[keyof PutFalAiT2vTurboRequestsByRequestIdCancelResponses];

export type PostFalAiT2vTurboData = {
  body: T2vTurboInput;
  path?: never;
  query?: never;
  url: "/fal-ai/t2v-turbo";
};

export type PostFalAiT2vTurboResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiT2vTurboResponse =
  PostFalAiT2vTurboResponses[keyof PostFalAiT2vTurboResponses];

export type GetFalAiT2vTurboRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/t2v-turbo/requests/{request_id}";
};

export type GetFalAiT2vTurboRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: T2vTurboOutput;
};

export type GetFalAiT2vTurboRequestsByRequestIdResponse =
  GetFalAiT2vTurboRequestsByRequestIdResponses[keyof GetFalAiT2vTurboRequestsByRequestIdResponses];

export type GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/fast-animatediff/text-to-video/requests/{request_id}/status";
};

export type GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiFastAnimatediffTextToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-animatediff/text-to-video/requests/{request_id}/cancel";
};

export type PutFalAiFastAnimatediffTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFastAnimatediffTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiFastAnimatediffTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFastAnimatediffTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiFastAnimatediffTextToVideoData = {
  body: FastAnimatediffTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/fast-animatediff/text-to-video";
};

export type PostFalAiFastAnimatediffTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFastAnimatediffTextToVideoResponse =
  PostFalAiFastAnimatediffTextToVideoResponses[keyof PostFalAiFastAnimatediffTextToVideoResponses];

export type GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-animatediff/text-to-video/requests/{request_id}";
};

export type GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FastAnimatediffTextToVideoOutput;
};

export type GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdResponse =
  GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdResponses[keyof GetFalAiFastAnimatediffTextToVideoRequestsByRequestIdResponses];

export type GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/fast-animatediff/turbo/text-to-video/requests/{request_id}/status";
  };

export type GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdStatusResponse =
  GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/fast-animatediff/turbo/text-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdCancelResponse =
  PutFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiFastAnimatediffTurboTextToVideoData = {
  body: FastAnimatediffTurboTextToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/fast-animatediff/turbo/text-to-video";
};

export type PostFalAiFastAnimatediffTurboTextToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFastAnimatediffTurboTextToVideoResponse =
  PostFalAiFastAnimatediffTurboTextToVideoResponses[keyof PostFalAiFastAnimatediffTurboTextToVideoResponses];

export type GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-animatediff/turbo/text-to-video/requests/{request_id}";
};

export type GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: FastAnimatediffTurboTextToVideoOutput;
  };

export type GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdResponse =
  GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdResponses[keyof GetFalAiFastAnimatediffTurboTextToVideoRequestsByRequestIdResponses];

export type GetFalAiMinimaxVideo01RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/minimax/video-01/requests/{request_id}/status";
};

export type GetFalAiMinimaxVideo01RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMinimaxVideo01RequestsByRequestIdStatusResponse =
  GetFalAiMinimaxVideo01RequestsByRequestIdStatusResponses[keyof GetFalAiMinimaxVideo01RequestsByRequestIdStatusResponses];

export type PutFalAiMinimaxVideo01RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01/requests/{request_id}/cancel";
};

export type PutFalAiMinimaxVideo01RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMinimaxVideo01RequestsByRequestIdCancelResponse =
  PutFalAiMinimaxVideo01RequestsByRequestIdCancelResponses[keyof PutFalAiMinimaxVideo01RequestsByRequestIdCancelResponses];

export type PostFalAiMinimaxVideo01Data = {
  body: MinimaxVideo01Input;
  path?: never;
  query?: never;
  url: "/fal-ai/minimax/video-01";
};

export type PostFalAiMinimaxVideo01Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMinimaxVideo01Response =
  PostFalAiMinimaxVideo01Responses[keyof PostFalAiMinimaxVideo01Responses];

export type GetFalAiMinimaxVideo01RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/minimax/video-01/requests/{request_id}";
};

export type GetFalAiMinimaxVideo01RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MinimaxVideo01Output;
};

export type GetFalAiMinimaxVideo01RequestsByRequestIdResponse =
  GetFalAiMinimaxVideo01RequestsByRequestIdResponses[keyof GetFalAiMinimaxVideo01RequestsByRequestIdResponses];

export type GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/animatediff-sparsectrl-lcm/requests/{request_id}/status";
};

export type GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdStatusResponse =
  GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdStatusResponses[keyof GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdStatusResponses];

export type PutFalAiAnimatediffSparsectrlLcmRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/animatediff-sparsectrl-lcm/requests/{request_id}/cancel";
};

export type PutFalAiAnimatediffSparsectrlLcmRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiAnimatediffSparsectrlLcmRequestsByRequestIdCancelResponse =
  PutFalAiAnimatediffSparsectrlLcmRequestsByRequestIdCancelResponses[keyof PutFalAiAnimatediffSparsectrlLcmRequestsByRequestIdCancelResponses];

export type PostFalAiAnimatediffSparsectrlLcmData = {
  body: AnimatediffSparsectrlLcmInput;
  path?: never;
  query?: never;
  url: "/fal-ai/animatediff-sparsectrl-lcm";
};

export type PostFalAiAnimatediffSparsectrlLcmResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiAnimatediffSparsectrlLcmResponse =
  PostFalAiAnimatediffSparsectrlLcmResponses[keyof PostFalAiAnimatediffSparsectrlLcmResponses];

export type GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/animatediff-sparsectrl-lcm/requests/{request_id}";
};

export type GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AnimatediffSparsectrlLcmOutput;
};

export type GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdResponse =
  GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdResponses[keyof GetFalAiAnimatediffSparsectrlLcmRequestsByRequestIdResponses];

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/bria/video/background-removal/requests/{request_id}/status";
};

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusResponse =
  GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusResponses[keyof GetBriaVideoBackgroundRemovalRequestsByRequestIdStatusResponses];

export type PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/background-removal/requests/{request_id}/cancel";
};

export type PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelResponse =
  PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelResponses[keyof PutBriaVideoBackgroundRemovalRequestsByRequestIdCancelResponses];

export type PostBriaVideoBackgroundRemovalData = {
  body: VideoBackgroundRemovalInput;
  path?: never;
  query?: never;
  url: "/bria/video/background-removal";
};

export type PostBriaVideoBackgroundRemovalResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostBriaVideoBackgroundRemovalResponse =
  PostBriaVideoBackgroundRemovalResponses[keyof PostBriaVideoBackgroundRemovalResponses];

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/background-removal/requests/{request_id}";
};

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoBackgroundRemovalOutput;
};

export type GetBriaVideoBackgroundRemovalRequestsByRequestIdResponse =
  GetBriaVideoBackgroundRemovalRequestsByRequestIdResponses[keyof GetBriaVideoBackgroundRemovalRequestsByRequestIdResponses];

export type GetFalAiMmaudioV2RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/mmaudio-v2/requests/{request_id}/status";
};

export type GetFalAiMmaudioV2RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMmaudioV2RequestsByRequestIdStatusResponse =
  GetFalAiMmaudioV2RequestsByRequestIdStatusResponses[keyof GetFalAiMmaudioV2RequestsByRequestIdStatusResponses];

export type PutFalAiMmaudioV2RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/mmaudio-v2/requests/{request_id}/cancel";
};

export type PutFalAiMmaudioV2RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMmaudioV2RequestsByRequestIdCancelResponse =
  PutFalAiMmaudioV2RequestsByRequestIdCancelResponses[keyof PutFalAiMmaudioV2RequestsByRequestIdCancelResponses];

export type PostFalAiMmaudioV2Data = {
  body: MmaudioV2Input;
  path?: never;
  query?: never;
  url: "/fal-ai/mmaudio-v2";
};

export type PostFalAiMmaudioV2Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMmaudioV2Response =
  PostFalAiMmaudioV2Responses[keyof PostFalAiMmaudioV2Responses];

export type GetFalAiMmaudioV2RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/mmaudio-v2/requests/{request_id}";
};

export type GetFalAiMmaudioV2RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MmaudioV2Output;
};

export type GetFalAiMmaudioV2RequestsByRequestIdResponse =
  GetFalAiMmaudioV2RequestsByRequestIdResponses[keyof GetFalAiMmaudioV2RequestsByRequestIdResponses];

export type GetXaiGrokImagineVideoEditVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/xai/grok-imagine-video/edit-video/requests/{request_id}/status";
};

export type GetXaiGrokImagineVideoEditVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetXaiGrokImagineVideoEditVideoRequestsByRequestIdStatusResponse =
  GetXaiGrokImagineVideoEditVideoRequestsByRequestIdStatusResponses[keyof GetXaiGrokImagineVideoEditVideoRequestsByRequestIdStatusResponses];

export type PutXaiGrokImagineVideoEditVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/xai/grok-imagine-video/edit-video/requests/{request_id}/cancel";
};

export type PutXaiGrokImagineVideoEditVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutXaiGrokImagineVideoEditVideoRequestsByRequestIdCancelResponse =
  PutXaiGrokImagineVideoEditVideoRequestsByRequestIdCancelResponses[keyof PutXaiGrokImagineVideoEditVideoRequestsByRequestIdCancelResponses];

export type PostXaiGrokImagineVideoEditVideoData = {
  body: GrokImagineVideoEditVideoInput;
  path?: never;
  query?: never;
  url: "/xai/grok-imagine-video/edit-video";
};

export type PostXaiGrokImagineVideoEditVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostXaiGrokImagineVideoEditVideoResponse =
  PostXaiGrokImagineVideoEditVideoResponses[keyof PostXaiGrokImagineVideoEditVideoResponses];

export type GetXaiGrokImagineVideoEditVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/xai/grok-imagine-video/edit-video/requests/{request_id}";
};

export type GetXaiGrokImagineVideoEditVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: GrokImagineVideoEditVideoOutput;
};

export type GetXaiGrokImagineVideoEditVideoRequestsByRequestIdResponse =
  GetXaiGrokImagineVideoEditVideoRequestsByRequestIdResponses[keyof GetXaiGrokImagineVideoEditVideoRequestsByRequestIdResponses];

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/half-moon-ai/ai-face-swap/faceswapvideo/requests/{request_id}/status";
  };

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusResponse =
  GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusResponses[keyof GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdStatusResponses];

export type PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/half-moon-ai/ai-face-swap/faceswapvideo/requests/{request_id}/cancel";
  };

export type PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelResponse =
  PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelResponses[keyof PutHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdCancelResponses];

export type PostHalfMoonAiAiFaceSwapFaceswapvideoData = {
  body: AiFaceSwapFaceswapvideoInput;
  path?: never;
  query?: never;
  url: "/half-moon-ai/ai-face-swap/faceswapvideo";
};

export type PostHalfMoonAiAiFaceSwapFaceswapvideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostHalfMoonAiAiFaceSwapFaceswapvideoResponse =
  PostHalfMoonAiAiFaceSwapFaceswapvideoResponses[keyof PostHalfMoonAiAiFaceSwapFaceswapvideoResponses];

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/half-moon-ai/ai-face-swap/faceswapvideo/requests/{request_id}";
};

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AiFaceSwapFaceswapvideoOutput;
};

export type GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdResponse =
  GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdResponses[keyof GetHalfMoonAiAiFaceSwapFaceswapvideoRequestsByRequestIdResponses];

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-2-19b/distilled/video-to-video/lora/requests/{request_id}/status";
  };

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-2-19b/distilled/video-to-video/lora/requests/{request_id}/cancel";
  };

export type PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledVideoToVideoLoraData = {
  body: Ltx219bDistilledVideoToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/video-to-video/lora";
};

export type PostFalAiLtx219bDistilledVideoToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledVideoToVideoLoraResponse =
  PostFalAiLtx219bDistilledVideoToVideoLoraResponses[keyof PostFalAiLtx219bDistilledVideoToVideoLoraResponses];

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/video-to-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: Ltx219bDistilledVideoToVideoLoraOutput;
  };

export type GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledVideoToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-2-19b/distilled/video-to-video/requests/{request_id}/status";
  };

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-2-19b/distilled/video-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledVideoToVideoData = {
  body: Ltx219bDistilledVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/video-to-video";
};

export type PostFalAiLtx219bDistilledVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledVideoToVideoResponse =
  PostFalAiLtx219bDistilledVideoToVideoResponses[keyof PostFalAiLtx219bDistilledVideoToVideoResponses];

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/video-to-video/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bDistilledVideoToVideoOutput;
};

export type GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledVideoToVideoRequestsByRequestIdResponses];

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/video-to-video/lora/requests/{request_id}/status";
};

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/video-to-video/lora/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bVideoToVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bVideoToVideoLoraData = {
  body: Ltx219bVideoToVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/video-to-video/lora";
};

export type PostFalAiLtx219bVideoToVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bVideoToVideoLoraResponse =
  PostFalAiLtx219bVideoToVideoLoraResponses[keyof PostFalAiLtx219bVideoToVideoLoraResponses];

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/video-to-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bVideoToVideoLoraOutput;
};

export type GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bVideoToVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/video-to-video/requests/{request_id}/status";
};

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/video-to-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bVideoToVideoData = {
  body: Ltx219bVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/video-to-video";
};

export type PostFalAiLtx219bVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bVideoToVideoResponse =
  PostFalAiLtx219bVideoToVideoResponses[keyof PostFalAiLtx219bVideoToVideoResponses];

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/video-to-video/requests/{request_id}";
};

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bVideoToVideoOutput;
};

export type GetFalAiLtx219bVideoToVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bVideoToVideoRequestsByRequestIdResponses];

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-2-19b/distilled/extend-video/lora/requests/{request_id}/status";
  };

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-2-19b/distilled/extend-video/lora/requests/{request_id}/cancel";
  };

export type PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledExtendVideoLoraData = {
  body: Ltx219bDistilledExtendVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/extend-video/lora";
};

export type PostFalAiLtx219bDistilledExtendVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledExtendVideoLoraResponse =
  PostFalAiLtx219bDistilledExtendVideoLoraResponses[keyof PostFalAiLtx219bDistilledExtendVideoLoraResponses];

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/extend-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: Ltx219bDistilledExtendVideoLoraOutput;
  };

export type GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledExtendVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/distilled/extend-video/requests/{request_id}/status";
};

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/extend-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bDistilledExtendVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bDistilledExtendVideoData = {
  body: Ltx219bDistilledExtendVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/extend-video";
};

export type PostFalAiLtx219bDistilledExtendVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bDistilledExtendVideoResponse =
  PostFalAiLtx219bDistilledExtendVideoResponses[keyof PostFalAiLtx219bDistilledExtendVideoResponses];

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/distilled/extend-video/requests/{request_id}";
};

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bDistilledExtendVideoOutput;
};

export type GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bDistilledExtendVideoRequestsByRequestIdResponses];

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/extend-video/lora/requests/{request_id}/status";
};

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/extend-video/lora/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bExtendVideoLoraRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bExtendVideoLoraData = {
  body: Ltx219bExtendVideoLoraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/extend-video/lora";
};

export type PostFalAiLtx219bExtendVideoLoraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bExtendVideoLoraResponse =
  PostFalAiLtx219bExtendVideoLoraResponses[keyof PostFalAiLtx219bExtendVideoLoraResponses];

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/extend-video/lora/requests/{request_id}";
};

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bExtendVideoLoraOutput;
};

export type GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdResponse =
  GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdResponses[keyof GetFalAiLtx219bExtendVideoLoraRequestsByRequestIdResponses];

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2-19b/extend-video/requests/{request_id}/status";
};

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx219bExtendVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/extend-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx219bExtendVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx219bExtendVideoData = {
  body: Ltx219bExtendVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2-19b/extend-video";
};

export type PostFalAiLtx219bExtendVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx219bExtendVideoResponse =
  PostFalAiLtx219bExtendVideoResponses[keyof PostFalAiLtx219bExtendVideoResponses];

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2-19b/extend-video/requests/{request_id}";
};

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx219bExtendVideoOutput;
};

export type GetFalAiLtx219bExtendVideoRequestsByRequestIdResponse =
  GetFalAiLtx219bExtendVideoRequestsByRequestIdResponses[keyof GetFalAiLtx219bExtendVideoRequestsByRequestIdResponses];

export type GetBriaVideoEraseKeypointsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/bria/video/erase/keypoints/requests/{request_id}/status";
};

export type GetBriaVideoEraseKeypointsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetBriaVideoEraseKeypointsRequestsByRequestIdStatusResponse =
  GetBriaVideoEraseKeypointsRequestsByRequestIdStatusResponses[keyof GetBriaVideoEraseKeypointsRequestsByRequestIdStatusResponses];

export type PutBriaVideoEraseKeypointsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/erase/keypoints/requests/{request_id}/cancel";
};

export type PutBriaVideoEraseKeypointsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutBriaVideoEraseKeypointsRequestsByRequestIdCancelResponse =
  PutBriaVideoEraseKeypointsRequestsByRequestIdCancelResponses[keyof PutBriaVideoEraseKeypointsRequestsByRequestIdCancelResponses];

export type PostBriaVideoEraseKeypointsData = {
  body: VideoEraseKeypointsInput;
  path?: never;
  query?: never;
  url: "/bria/video/erase/keypoints";
};

export type PostBriaVideoEraseKeypointsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostBriaVideoEraseKeypointsResponse =
  PostBriaVideoEraseKeypointsResponses[keyof PostBriaVideoEraseKeypointsResponses];

export type GetBriaVideoEraseKeypointsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/erase/keypoints/requests/{request_id}";
};

export type GetBriaVideoEraseKeypointsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoEraseKeypointsOutput;
};

export type GetBriaVideoEraseKeypointsRequestsByRequestIdResponse =
  GetBriaVideoEraseKeypointsRequestsByRequestIdResponses[keyof GetBriaVideoEraseKeypointsRequestsByRequestIdResponses];

export type GetBriaVideoErasePromptRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/bria/video/erase/prompt/requests/{request_id}/status";
};

export type GetBriaVideoErasePromptRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetBriaVideoErasePromptRequestsByRequestIdStatusResponse =
  GetBriaVideoErasePromptRequestsByRequestIdStatusResponses[keyof GetBriaVideoErasePromptRequestsByRequestIdStatusResponses];

export type PutBriaVideoErasePromptRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/erase/prompt/requests/{request_id}/cancel";
};

export type PutBriaVideoErasePromptRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutBriaVideoErasePromptRequestsByRequestIdCancelResponse =
  PutBriaVideoErasePromptRequestsByRequestIdCancelResponses[keyof PutBriaVideoErasePromptRequestsByRequestIdCancelResponses];

export type PostBriaVideoErasePromptData = {
  body: VideoErasePromptInput;
  path?: never;
  query?: never;
  url: "/bria/video/erase/prompt";
};

export type PostBriaVideoErasePromptResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostBriaVideoErasePromptResponse =
  PostBriaVideoErasePromptResponses[keyof PostBriaVideoErasePromptResponses];

export type GetBriaVideoErasePromptRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/erase/prompt/requests/{request_id}";
};

export type GetBriaVideoErasePromptRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoErasePromptOutput;
};

export type GetBriaVideoErasePromptRequestsByRequestIdResponse =
  GetBriaVideoErasePromptRequestsByRequestIdResponses[keyof GetBriaVideoErasePromptRequestsByRequestIdResponses];

export type GetBriaVideoEraseMaskRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/bria/video/erase/mask/requests/{request_id}/status";
};

export type GetBriaVideoEraseMaskRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetBriaVideoEraseMaskRequestsByRequestIdStatusResponse =
  GetBriaVideoEraseMaskRequestsByRequestIdStatusResponses[keyof GetBriaVideoEraseMaskRequestsByRequestIdStatusResponses];

export type PutBriaVideoEraseMaskRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/erase/mask/requests/{request_id}/cancel";
};

export type PutBriaVideoEraseMaskRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutBriaVideoEraseMaskRequestsByRequestIdCancelResponse =
  PutBriaVideoEraseMaskRequestsByRequestIdCancelResponses[keyof PutBriaVideoEraseMaskRequestsByRequestIdCancelResponses];

export type PostBriaVideoEraseMaskData = {
  body: VideoEraseMaskInput;
  path?: never;
  query?: never;
  url: "/bria/video/erase/mask";
};

export type PostBriaVideoEraseMaskResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostBriaVideoEraseMaskResponse =
  PostBriaVideoEraseMaskResponses[keyof PostBriaVideoEraseMaskResponses];

export type GetBriaVideoEraseMaskRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/erase/mask/requests/{request_id}";
};

export type GetBriaVideoEraseMaskRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoEraseMaskOutput;
};

export type GetBriaVideoEraseMaskRequestsByRequestIdResponse =
  GetBriaVideoEraseMaskRequestsByRequestIdResponses[keyof GetBriaVideoEraseMaskRequestsByRequestIdResponses];

export type GetFalAiLightxRelightRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/lightx/relight/requests/{request_id}/status";
};

export type GetFalAiLightxRelightRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLightxRelightRequestsByRequestIdStatusResponse =
  GetFalAiLightxRelightRequestsByRequestIdStatusResponses[keyof GetFalAiLightxRelightRequestsByRequestIdStatusResponses];

export type PutFalAiLightxRelightRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/lightx/relight/requests/{request_id}/cancel";
};

export type PutFalAiLightxRelightRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLightxRelightRequestsByRequestIdCancelResponse =
  PutFalAiLightxRelightRequestsByRequestIdCancelResponses[keyof PutFalAiLightxRelightRequestsByRequestIdCancelResponses];

export type PostFalAiLightxRelightData = {
  body: LightxRelightInput;
  path?: never;
  query?: never;
  url: "/fal-ai/lightx/relight";
};

export type PostFalAiLightxRelightResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLightxRelightResponse =
  PostFalAiLightxRelightResponses[keyof PostFalAiLightxRelightResponses];

export type GetFalAiLightxRelightRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/lightx/relight/requests/{request_id}";
};

export type GetFalAiLightxRelightRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LightxRelightOutput;
};

export type GetFalAiLightxRelightRequestsByRequestIdResponse =
  GetFalAiLightxRelightRequestsByRequestIdResponses[keyof GetFalAiLightxRelightRequestsByRequestIdResponses];

export type GetFalAiLightxRecameraRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/lightx/recamera/requests/{request_id}/status";
};

export type GetFalAiLightxRecameraRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLightxRecameraRequestsByRequestIdStatusResponse =
  GetFalAiLightxRecameraRequestsByRequestIdStatusResponses[keyof GetFalAiLightxRecameraRequestsByRequestIdStatusResponses];

export type PutFalAiLightxRecameraRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/lightx/recamera/requests/{request_id}/cancel";
};

export type PutFalAiLightxRecameraRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLightxRecameraRequestsByRequestIdCancelResponse =
  PutFalAiLightxRecameraRequestsByRequestIdCancelResponses[keyof PutFalAiLightxRecameraRequestsByRequestIdCancelResponses];

export type PostFalAiLightxRecameraData = {
  body: LightxRecameraInput;
  path?: never;
  query?: never;
  url: "/fal-ai/lightx/recamera";
};

export type PostFalAiLightxRecameraResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLightxRecameraResponse =
  PostFalAiLightxRecameraResponses[keyof PostFalAiLightxRecameraResponses];

export type GetFalAiLightxRecameraRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/lightx/recamera/requests/{request_id}";
};

export type GetFalAiLightxRecameraRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LightxRecameraOutput;
};

export type GetFalAiLightxRecameraRequestsByRequestIdResponse =
  GetFalAiLightxRecameraRequestsByRequestIdResponses[keyof GetFalAiLightxRecameraRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.6/standard/motion-control/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.6/standard/motion-control/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV26StandardMotionControlData = {
  body: KlingVideoV26StandardMotionControlInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.6/standard/motion-control";
};

export type PostFalAiKlingVideoV26StandardMotionControlResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV26StandardMotionControlResponse =
  PostFalAiKlingVideoV26StandardMotionControlResponses[keyof PostFalAiKlingVideoV26StandardMotionControlResponses];

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.6/standard/motion-control/requests/{request_id}";
  };

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV26StandardMotionControlOutput;
  };

export type GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdResponse =
  GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV26StandardMotionControlRequestsByRequestIdResponses];

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/v2.6/pro/motion-control/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/v2.6/pro/motion-control/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoV26ProMotionControlRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoV26ProMotionControlData = {
  body: KlingVideoV26ProMotionControlInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/v2.6/pro/motion-control";
};

export type PostFalAiKlingVideoV26ProMotionControlResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoV26ProMotionControlResponse =
  PostFalAiKlingVideoV26ProMotionControlResponses[keyof PostFalAiKlingVideoV26ProMotionControlResponses];

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/v2.6/pro/motion-control/requests/{request_id}";
};

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoV26ProMotionControlOutput;
  };

export type GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdResponse =
  GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdResponses[keyof GetFalAiKlingVideoV26ProMotionControlRequestsByRequestIdResponses];

export type GetDecartLucyRestyleRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/decart/lucy-restyle/requests/{request_id}/status";
};

export type GetDecartLucyRestyleRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetDecartLucyRestyleRequestsByRequestIdStatusResponse =
  GetDecartLucyRestyleRequestsByRequestIdStatusResponses[keyof GetDecartLucyRestyleRequestsByRequestIdStatusResponses];

export type PutDecartLucyRestyleRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-restyle/requests/{request_id}/cancel";
};

export type PutDecartLucyRestyleRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutDecartLucyRestyleRequestsByRequestIdCancelResponse =
  PutDecartLucyRestyleRequestsByRequestIdCancelResponses[keyof PutDecartLucyRestyleRequestsByRequestIdCancelResponses];

export type PostDecartLucyRestyleData = {
  body: LucyRestyleInput;
  path?: never;
  query?: never;
  url: "/decart/lucy-restyle";
};

export type PostDecartLucyRestyleResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostDecartLucyRestyleResponse =
  PostDecartLucyRestyleResponses[keyof PostDecartLucyRestyleResponses];

export type GetDecartLucyRestyleRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-restyle/requests/{request_id}";
};

export type GetDecartLucyRestyleRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LucyRestyleOutput;
};

export type GetDecartLucyRestyleRequestsByRequestIdResponse =
  GetDecartLucyRestyleRequestsByRequestIdResponses[keyof GetDecartLucyRestyleRequestsByRequestIdResponses];

export type GetFalAiScailRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/scail/requests/{request_id}/status";
};

export type GetFalAiScailRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiScailRequestsByRequestIdStatusResponse =
  GetFalAiScailRequestsByRequestIdStatusResponses[keyof GetFalAiScailRequestsByRequestIdStatusResponses];

export type PutFalAiScailRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/scail/requests/{request_id}/cancel";
};

export type PutFalAiScailRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiScailRequestsByRequestIdCancelResponse =
  PutFalAiScailRequestsByRequestIdCancelResponses[keyof PutFalAiScailRequestsByRequestIdCancelResponses];

export type PostFalAiScailData = {
  body: ScailInput;
  path?: never;
  query?: never;
  url: "/fal-ai/scail";
};

export type PostFalAiScailResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiScailResponse =
  PostFalAiScailResponses[keyof PostFalAiScailResponses];

export type GetFalAiScailRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/scail/requests/{request_id}";
};

export type GetFalAiScailRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ScailOutput;
};

export type GetFalAiScailRequestsByRequestIdResponse =
  GetFalAiScailRequestsByRequestIdResponses[keyof GetFalAiScailRequestsByRequestIdResponses];

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/clarityai/crystal-video-upscaler/requests/{request_id}/status";
};

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusResponse =
  GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusResponses[keyof GetClarityaiCrystalVideoUpscalerRequestsByRequestIdStatusResponses];

export type PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/clarityai/crystal-video-upscaler/requests/{request_id}/cancel";
};

export type PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelResponse =
  PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelResponses[keyof PutClarityaiCrystalVideoUpscalerRequestsByRequestIdCancelResponses];

export type PostClarityaiCrystalVideoUpscalerData = {
  body: CrystalVideoUpscalerInput;
  path?: never;
  query?: never;
  url: "/clarityai/crystal-video-upscaler";
};

export type PostClarityaiCrystalVideoUpscalerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostClarityaiCrystalVideoUpscalerResponse =
  PostClarityaiCrystalVideoUpscalerResponses[keyof PostClarityaiCrystalVideoUpscalerResponses];

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/clarityai/crystal-video-upscaler/requests/{request_id}";
};

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: CrystalVideoUpscalerOutput;
};

export type GetClarityaiCrystalVideoUpscalerRequestsByRequestIdResponse =
  GetClarityaiCrystalVideoUpscalerRequestsByRequestIdResponses[keyof GetClarityaiCrystalVideoUpscalerRequestsByRequestIdResponses];

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/bria/bria_video_eraser/erase/mask/requests/{request_id}/status";
};

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusResponse =
  GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusResponses[keyof GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdStatusResponses];

export type PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/bria_video_eraser/erase/mask/requests/{request_id}/cancel";
};

export type PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelResponse =
  PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelResponses[keyof PutBriaBriaVideoEraserEraseMaskRequestsByRequestIdCancelResponses];

export type PostBriaBriaVideoEraserEraseMaskData = {
  body: BriaVideoEraserEraseMaskInput;
  path?: never;
  query?: never;
  url: "/bria/bria_video_eraser/erase/mask";
};

export type PostBriaBriaVideoEraserEraseMaskResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostBriaBriaVideoEraserEraseMaskResponse =
  PostBriaBriaVideoEraserEraseMaskResponses[keyof PostBriaBriaVideoEraserEraseMaskResponses];

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/bria_video_eraser/erase/mask/requests/{request_id}";
};

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: BriaVideoEraserEraseMaskOutput;
};

export type GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdResponse =
  GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdResponses[keyof GetBriaBriaVideoEraserEraseMaskRequestsByRequestIdResponses];

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/bria/bria_video_eraser/erase/keypoints/requests/{request_id}/status";
  };

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusResponse =
  GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusResponses[keyof GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdStatusResponses];

export type PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/bria/bria_video_eraser/erase/keypoints/requests/{request_id}/cancel";
  };

export type PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelResponse =
  PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelResponses[keyof PutBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdCancelResponses];

export type PostBriaBriaVideoEraserEraseKeypointsData = {
  body: BriaVideoEraserEraseKeypointsInput;
  path?: never;
  query?: never;
  url: "/bria/bria_video_eraser/erase/keypoints";
};

export type PostBriaBriaVideoEraserEraseKeypointsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostBriaBriaVideoEraserEraseKeypointsResponse =
  PostBriaBriaVideoEraserEraseKeypointsResponses[keyof PostBriaBriaVideoEraserEraseKeypointsResponses];

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/bria_video_eraser/erase/keypoints/requests/{request_id}";
};

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: BriaVideoEraserEraseKeypointsOutput;
};

export type GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdResponse =
  GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdResponses[keyof GetBriaBriaVideoEraserEraseKeypointsRequestsByRequestIdResponses];

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/bria/bria_video_eraser/erase/prompt/requests/{request_id}/status";
};

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusResponse =
  GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusResponses[keyof GetBriaBriaVideoEraserErasePromptRequestsByRequestIdStatusResponses];

export type PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/bria_video_eraser/erase/prompt/requests/{request_id}/cancel";
};

export type PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelResponse =
  PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelResponses[keyof PutBriaBriaVideoEraserErasePromptRequestsByRequestIdCancelResponses];

export type PostBriaBriaVideoEraserErasePromptData = {
  body: BriaVideoEraserErasePromptInput;
  path?: never;
  query?: never;
  url: "/bria/bria_video_eraser/erase/prompt";
};

export type PostBriaBriaVideoEraserErasePromptResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostBriaBriaVideoEraserErasePromptResponse =
  PostBriaBriaVideoEraserErasePromptResponses[keyof PostBriaBriaVideoEraserErasePromptResponses];

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/bria_video_eraser/erase/prompt/requests/{request_id}";
};

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: BriaVideoEraserErasePromptOutput;
};

export type GetBriaBriaVideoEraserErasePromptRequestsByRequestIdResponse =
  GetBriaBriaVideoEraserErasePromptRequestsByRequestIdResponses[keyof GetBriaBriaVideoEraserErasePromptRequestsByRequestIdResponses];

export type GetWanV26ReferenceToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/wan/v2.6/reference-to-video/requests/{request_id}/status";
};

export type GetWanV26ReferenceToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetWanV26ReferenceToVideoRequestsByRequestIdStatusResponse =
  GetWanV26ReferenceToVideoRequestsByRequestIdStatusResponses[keyof GetWanV26ReferenceToVideoRequestsByRequestIdStatusResponses];

export type PutWanV26ReferenceToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/wan/v2.6/reference-to-video/requests/{request_id}/cancel";
};

export type PutWanV26ReferenceToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutWanV26ReferenceToVideoRequestsByRequestIdCancelResponse =
  PutWanV26ReferenceToVideoRequestsByRequestIdCancelResponses[keyof PutWanV26ReferenceToVideoRequestsByRequestIdCancelResponses];

export type PostWanV26ReferenceToVideoData = {
  body: V26ReferenceToVideoInput;
  path?: never;
  query?: never;
  url: "/wan/v2.6/reference-to-video";
};

export type PostWanV26ReferenceToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostWanV26ReferenceToVideoResponse =
  PostWanV26ReferenceToVideoResponses[keyof PostWanV26ReferenceToVideoResponses];

export type GetWanV26ReferenceToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/wan/v2.6/reference-to-video/requests/{request_id}";
};

export type GetWanV26ReferenceToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: V26ReferenceToVideoOutput;
};

export type GetWanV26ReferenceToVideoRequestsByRequestIdResponse =
  GetWanV26ReferenceToVideoRequestsByRequestIdResponses[keyof GetWanV26ReferenceToVideoRequestsByRequestIdResponses];

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3.1/fast/extend-video/requests/{request_id}/status";
};

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31FastExtendVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/fast/extend-video/requests/{request_id}/cancel";
};

export type PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31FastExtendVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo31FastExtendVideoData = {
  body: Veo31FastExtendVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3.1/fast/extend-video";
};

export type PostFalAiVeo31FastExtendVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo31FastExtendVideoResponse =
  PostFalAiVeo31FastExtendVideoResponses[keyof PostFalAiVeo31FastExtendVideoResponses];

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/fast/extend-video/requests/{request_id}";
};

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo31FastExtendVideoOutput;
};

export type GetFalAiVeo31FastExtendVideoRequestsByRequestIdResponse =
  GetFalAiVeo31FastExtendVideoRequestsByRequestIdResponses[keyof GetFalAiVeo31FastExtendVideoRequestsByRequestIdResponses];

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/veo3.1/extend-video/requests/{request_id}/status";
};

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiVeo31ExtendVideoRequestsByRequestIdStatusResponses];

export type PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/extend-video/requests/{request_id}/cancel";
};

export type PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiVeo31ExtendVideoRequestsByRequestIdCancelResponses];

export type PostFalAiVeo31ExtendVideoData = {
  body: Veo31ExtendVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/veo3.1/extend-video";
};

export type PostFalAiVeo31ExtendVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVeo31ExtendVideoResponse =
  PostFalAiVeo31ExtendVideoResponses[keyof PostFalAiVeo31ExtendVideoResponses];

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/veo3.1/extend-video/requests/{request_id}";
};

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Veo31ExtendVideoOutput;
};

export type GetFalAiVeo31ExtendVideoRequestsByRequestIdResponse =
  GetFalAiVeo31ExtendVideoRequestsByRequestIdResponses[keyof GetFalAiVeo31ExtendVideoRequestsByRequestIdResponses];

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/o1/standard/video-to-video/reference/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/standard/video-to-video/reference/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoO1StandardVideoToVideoReferenceData = {
  body: KlingVideoO1StandardVideoToVideoReferenceInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/o1/standard/video-to-video/reference";
};

export type PostFalAiKlingVideoO1StandardVideoToVideoReferenceResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoO1StandardVideoToVideoReferenceResponse =
  PostFalAiKlingVideoO1StandardVideoToVideoReferenceResponses[keyof PostFalAiKlingVideoO1StandardVideoToVideoReferenceResponses];

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/standard/video-to-video/reference/requests/{request_id}";
  };

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoO1StandardVideoToVideoReferenceOutput;
  };

export type GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1StandardVideoToVideoReferenceRequestsByRequestIdResponses];

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/o1/standard/video-to-video/edit/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/standard/video-to-video/edit/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoO1StandardVideoToVideoEditData = {
  body: KlingVideoO1StandardVideoToVideoEditInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/o1/standard/video-to-video/edit";
};

export type PostFalAiKlingVideoO1StandardVideoToVideoEditResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoO1StandardVideoToVideoEditResponse =
  PostFalAiKlingVideoO1StandardVideoToVideoEditResponses[keyof PostFalAiKlingVideoO1StandardVideoToVideoEditResponses];

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/standard/video-to-video/edit/requests/{request_id}";
  };

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoO1StandardVideoToVideoEditOutput;
  };

export type GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1StandardVideoToVideoEditRequestsByRequestIdResponses];

export type GetFalAiSteadyDancerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/steady-dancer/requests/{request_id}/status";
};

export type GetFalAiSteadyDancerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSteadyDancerRequestsByRequestIdStatusResponse =
  GetFalAiSteadyDancerRequestsByRequestIdStatusResponses[keyof GetFalAiSteadyDancerRequestsByRequestIdStatusResponses];

export type PutFalAiSteadyDancerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/steady-dancer/requests/{request_id}/cancel";
};

export type PutFalAiSteadyDancerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSteadyDancerRequestsByRequestIdCancelResponse =
  PutFalAiSteadyDancerRequestsByRequestIdCancelResponses[keyof PutFalAiSteadyDancerRequestsByRequestIdCancelResponses];

export type PostFalAiSteadyDancerData = {
  body: SteadyDancerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/steady-dancer";
};

export type PostFalAiSteadyDancerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSteadyDancerResponse =
  PostFalAiSteadyDancerResponses[keyof PostFalAiSteadyDancerResponses];

export type GetFalAiSteadyDancerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/steady-dancer/requests/{request_id}";
};

export type GetFalAiSteadyDancerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SteadyDancerOutput;
};

export type GetFalAiSteadyDancerRequestsByRequestIdResponse =
  GetFalAiSteadyDancerRequestsByRequestIdResponses[keyof GetFalAiSteadyDancerRequestsByRequestIdResponses];

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/one-to-all-animation/1.3b/requests/{request_id}/status";
};

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusResponse =
  GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusResponses[keyof GetFalAiOneToAllAnimation13bRequestsByRequestIdStatusResponses];

export type PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/one-to-all-animation/1.3b/requests/{request_id}/cancel";
};

export type PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelResponse =
  PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelResponses[keyof PutFalAiOneToAllAnimation13bRequestsByRequestIdCancelResponses];

export type PostFalAiOneToAllAnimation13bData = {
  body: OneToAllAnimation13bInput;
  path?: never;
  query?: never;
  url: "/fal-ai/one-to-all-animation/1.3b";
};

export type PostFalAiOneToAllAnimation13bResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiOneToAllAnimation13bResponse =
  PostFalAiOneToAllAnimation13bResponses[keyof PostFalAiOneToAllAnimation13bResponses];

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/one-to-all-animation/1.3b/requests/{request_id}";
};

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: OneToAllAnimation13bOutput;
};

export type GetFalAiOneToAllAnimation13bRequestsByRequestIdResponse =
  GetFalAiOneToAllAnimation13bRequestsByRequestIdResponses[keyof GetFalAiOneToAllAnimation13bRequestsByRequestIdResponses];

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/one-to-all-animation/14b/requests/{request_id}/status";
};

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusResponse =
  GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusResponses[keyof GetFalAiOneToAllAnimation14bRequestsByRequestIdStatusResponses];

export type PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/one-to-all-animation/14b/requests/{request_id}/cancel";
};

export type PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelResponse =
  PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelResponses[keyof PutFalAiOneToAllAnimation14bRequestsByRequestIdCancelResponses];

export type PostFalAiOneToAllAnimation14bData = {
  body: OneToAllAnimation14bInput;
  path?: never;
  query?: never;
  url: "/fal-ai/one-to-all-animation/14b";
};

export type PostFalAiOneToAllAnimation14bResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiOneToAllAnimation14bResponse =
  PostFalAiOneToAllAnimation14bResponses[keyof PostFalAiOneToAllAnimation14bResponses];

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/one-to-all-animation/14b/requests/{request_id}";
};

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: OneToAllAnimation14bOutput;
};

export type GetFalAiOneToAllAnimation14bRequestsByRequestIdResponse =
  GetFalAiOneToAllAnimation14bRequestsByRequestIdResponses[keyof GetFalAiOneToAllAnimation14bRequestsByRequestIdResponses];

export type GetFalAiWanVisionEnhancerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vision-enhancer/requests/{request_id}/status";
};

export type GetFalAiWanVisionEnhancerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVisionEnhancerRequestsByRequestIdStatusResponse =
  GetFalAiWanVisionEnhancerRequestsByRequestIdStatusResponses[keyof GetFalAiWanVisionEnhancerRequestsByRequestIdStatusResponses];

export type PutFalAiWanVisionEnhancerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vision-enhancer/requests/{request_id}/cancel";
};

export type PutFalAiWanVisionEnhancerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVisionEnhancerRequestsByRequestIdCancelResponse =
  PutFalAiWanVisionEnhancerRequestsByRequestIdCancelResponses[keyof PutFalAiWanVisionEnhancerRequestsByRequestIdCancelResponses];

export type PostFalAiWanVisionEnhancerData = {
  body: WanVisionEnhancerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vision-enhancer";
};

export type PostFalAiWanVisionEnhancerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVisionEnhancerResponse =
  PostFalAiWanVisionEnhancerResponses[keyof PostFalAiWanVisionEnhancerResponses];

export type GetFalAiWanVisionEnhancerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vision-enhancer/requests/{request_id}";
};

export type GetFalAiWanVisionEnhancerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVisionEnhancerOutput;
};

export type GetFalAiWanVisionEnhancerRequestsByRequestIdResponse =
  GetFalAiWanVisionEnhancerRequestsByRequestIdResponses[keyof GetFalAiWanVisionEnhancerRequestsByRequestIdResponses];

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sync-lipsync/react-1/requests/{request_id}/status";
};

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusResponse =
  GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusResponses[keyof GetFalAiSyncLipsyncReact1RequestsByRequestIdStatusResponses];

export type PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sync-lipsync/react-1/requests/{request_id}/cancel";
};

export type PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelResponse =
  PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelResponses[keyof PutFalAiSyncLipsyncReact1RequestsByRequestIdCancelResponses];

export type PostFalAiSyncLipsyncReact1Data = {
  body: SyncLipsyncReact1Input;
  path?: never;
  query?: never;
  url: "/fal-ai/sync-lipsync/react-1";
};

export type PostFalAiSyncLipsyncReact1Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSyncLipsyncReact1Response =
  PostFalAiSyncLipsyncReact1Responses[keyof PostFalAiSyncLipsyncReact1Responses];

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sync-lipsync/react-1/requests/{request_id}";
};

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SyncLipsyncReact1Output;
};

export type GetFalAiSyncLipsyncReact1RequestsByRequestIdResponse =
  GetFalAiSyncLipsyncReact1RequestsByRequestIdResponses[keyof GetFalAiSyncLipsyncReact1RequestsByRequestIdResponses];

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/veed/video-background-removal/fast/requests/{request_id}/status";
};

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusResponse =
  GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusResponses[keyof GetVeedVideoBackgroundRemovalFastRequestsByRequestIdStatusResponses];

export type PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/video-background-removal/fast/requests/{request_id}/cancel";
};

export type PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelResponse =
  PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelResponses[keyof PutVeedVideoBackgroundRemovalFastRequestsByRequestIdCancelResponses];

export type PostVeedVideoBackgroundRemovalFastData = {
  body: VideoBackgroundRemovalFastInput;
  path?: never;
  query?: never;
  url: "/veed/video-background-removal/fast";
};

export type PostVeedVideoBackgroundRemovalFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostVeedVideoBackgroundRemovalFastResponse =
  PostVeedVideoBackgroundRemovalFastResponses[keyof PostVeedVideoBackgroundRemovalFastResponses];

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/video-background-removal/fast/requests/{request_id}";
};

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoBackgroundRemovalFastOutput;
};

export type GetVeedVideoBackgroundRemovalFastRequestsByRequestIdResponse =
  GetVeedVideoBackgroundRemovalFastRequestsByRequestIdResponses[keyof GetVeedVideoBackgroundRemovalFastRequestsByRequestIdResponses];

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/o1/video-to-video/edit/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/video-to-video/edit/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoO1VideoToVideoEditData = {
  body: KlingVideoO1VideoToVideoEditInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/o1/video-to-video/edit";
};

export type PostFalAiKlingVideoO1VideoToVideoEditResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoO1VideoToVideoEditResponse =
  PostFalAiKlingVideoO1VideoToVideoEditResponses[keyof PostFalAiKlingVideoO1VideoToVideoEditResponses];

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/o1/video-to-video/edit/requests/{request_id}";
};

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KlingVideoO1VideoToVideoEditOutput;
};

export type GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1VideoToVideoEditRequestsByRequestIdResponses];

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/kling-video/o1/video-to-video/reference/requests/{request_id}/status";
  };

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusResponse =
  GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusResponses[keyof GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdStatusResponses];

export type PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/kling-video/o1/video-to-video/reference/requests/{request_id}/cancel";
  };

export type PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelResponse =
  PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelResponses[keyof PutFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdCancelResponses];

export type PostFalAiKlingVideoO1VideoToVideoReferenceData = {
  body: KlingVideoO1VideoToVideoReferenceInput;
  path?: never;
  query?: never;
  url: "/fal-ai/kling-video/o1/video-to-video/reference";
};

export type PostFalAiKlingVideoO1VideoToVideoReferenceResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKlingVideoO1VideoToVideoReferenceResponse =
  PostFalAiKlingVideoO1VideoToVideoReferenceResponses[keyof PostFalAiKlingVideoO1VideoToVideoReferenceResponses];

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/kling-video/o1/video-to-video/reference/requests/{request_id}";
};

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: KlingVideoO1VideoToVideoReferenceOutput;
  };

export type GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdResponse =
  GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdResponses[keyof GetFalAiKlingVideoO1VideoToVideoReferenceRequestsByRequestIdResponses];

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/veed/video-background-removal/requests/{request_id}/status";
};

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusResponse =
  GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusResponses[keyof GetVeedVideoBackgroundRemovalRequestsByRequestIdStatusResponses];

export type PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/video-background-removal/requests/{request_id}/cancel";
};

export type PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelResponse =
  PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelResponses[keyof PutVeedVideoBackgroundRemovalRequestsByRequestIdCancelResponses];

export type PostVeedVideoBackgroundRemovalData = {
  body: VideoBackgroundRemovalInputType2;
  path?: never;
  query?: never;
  url: "/veed/video-background-removal";
};

export type PostVeedVideoBackgroundRemovalResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostVeedVideoBackgroundRemovalResponse =
  PostVeedVideoBackgroundRemovalResponses[keyof PostVeedVideoBackgroundRemovalResponses];

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/video-background-removal/requests/{request_id}";
};

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoBackgroundRemovalOutputType2;
};

export type GetVeedVideoBackgroundRemovalRequestsByRequestIdResponse =
  GetVeedVideoBackgroundRemovalRequestsByRequestIdResponses[keyof GetVeedVideoBackgroundRemovalRequestsByRequestIdResponses];

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/veed/video-background-removal/green-screen/requests/{request_id}/status";
  };

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusResponse =
  GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusResponses[keyof GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdStatusResponses];

export type PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/veed/video-background-removal/green-screen/requests/{request_id}/cancel";
  };

export type PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelResponse =
  PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelResponses[keyof PutVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdCancelResponses];

export type PostVeedVideoBackgroundRemovalGreenScreenData = {
  body: VideoBackgroundRemovalGreenScreenInput;
  path?: never;
  query?: never;
  url: "/veed/video-background-removal/green-screen";
};

export type PostVeedVideoBackgroundRemovalGreenScreenResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostVeedVideoBackgroundRemovalGreenScreenResponse =
  PostVeedVideoBackgroundRemovalGreenScreenResponses[keyof PostVeedVideoBackgroundRemovalGreenScreenResponses];

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/video-background-removal/green-screen/requests/{request_id}";
};

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: VideoBackgroundRemovalGreenScreenOutput;
  };

export type GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdResponse =
  GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdResponses[keyof GetVeedVideoBackgroundRemovalGreenScreenRequestsByRequestIdResponses];

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-2/retake-video/requests/{request_id}/status";
};

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusResponse =
  GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusResponses[keyof GetFalAiLtx2RetakeVideoRequestsByRequestIdStatusResponses];

export type PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/retake-video/requests/{request_id}/cancel";
};

export type PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelResponse =
  PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelResponses[keyof PutFalAiLtx2RetakeVideoRequestsByRequestIdCancelResponses];

export type PostFalAiLtx2RetakeVideoData = {
  body: Ltx2RetakeVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-2/retake-video";
};

export type PostFalAiLtx2RetakeVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx2RetakeVideoResponse =
  PostFalAiLtx2RetakeVideoResponses[keyof PostFalAiLtx2RetakeVideoResponses];

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-2/retake-video/requests/{request_id}";
};

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx2RetakeVideoOutput;
};

export type GetFalAiLtx2RetakeVideoRequestsByRequestIdResponse =
  GetFalAiLtx2RetakeVideoRequestsByRequestIdResponses[keyof GetFalAiLtx2RetakeVideoRequestsByRequestIdResponses];

export type GetDecartLucyEditFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/decart/lucy-edit/fast/requests/{request_id}/status";
};

export type GetDecartLucyEditFastRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetDecartLucyEditFastRequestsByRequestIdStatusResponse =
  GetDecartLucyEditFastRequestsByRequestIdStatusResponses[keyof GetDecartLucyEditFastRequestsByRequestIdStatusResponses];

export type PutDecartLucyEditFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-edit/fast/requests/{request_id}/cancel";
};

export type PutDecartLucyEditFastRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutDecartLucyEditFastRequestsByRequestIdCancelResponse =
  PutDecartLucyEditFastRequestsByRequestIdCancelResponses[keyof PutDecartLucyEditFastRequestsByRequestIdCancelResponses];

export type PostDecartLucyEditFastData = {
  body: LucyEditFastInput;
  path?: never;
  query?: never;
  url: "/decart/lucy-edit/fast";
};

export type PostDecartLucyEditFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostDecartLucyEditFastResponse =
  PostDecartLucyEditFastResponses[keyof PostDecartLucyEditFastResponses];

export type GetDecartLucyEditFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-edit/fast/requests/{request_id}";
};

export type GetDecartLucyEditFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LucyEditFastOutput;
};

export type GetDecartLucyEditFastRequestsByRequestIdResponse =
  GetDecartLucyEditFastRequestsByRequestIdResponses[keyof GetDecartLucyEditFastRequestsByRequestIdResponses];

export type GetFalAiSam3VideoRleRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sam-3/video-rle/requests/{request_id}/status";
};

export type GetFalAiSam3VideoRleRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSam3VideoRleRequestsByRequestIdStatusResponse =
  GetFalAiSam3VideoRleRequestsByRequestIdStatusResponses[keyof GetFalAiSam3VideoRleRequestsByRequestIdStatusResponses];

export type PutFalAiSam3VideoRleRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sam-3/video-rle/requests/{request_id}/cancel";
};

export type PutFalAiSam3VideoRleRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSam3VideoRleRequestsByRequestIdCancelResponse =
  PutFalAiSam3VideoRleRequestsByRequestIdCancelResponses[keyof PutFalAiSam3VideoRleRequestsByRequestIdCancelResponses];

export type PostFalAiSam3VideoRleData = {
  body: Sam3VideoRleInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sam-3/video-rle";
};

export type PostFalAiSam3VideoRleResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSam3VideoRleResponse =
  PostFalAiSam3VideoRleResponses[keyof PostFalAiSam3VideoRleResponses];

export type GetFalAiSam3VideoRleRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sam-3/video-rle/requests/{request_id}";
};

export type GetFalAiSam3VideoRleRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Sam3VideoRleOutput;
};

export type GetFalAiSam3VideoRleRequestsByRequestIdResponse =
  GetFalAiSam3VideoRleRequestsByRequestIdResponses[keyof GetFalAiSam3VideoRleRequestsByRequestIdResponses];

export type GetFalAiSam3VideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sam-3/video/requests/{request_id}/status";
};

export type GetFalAiSam3VideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSam3VideoRequestsByRequestIdStatusResponse =
  GetFalAiSam3VideoRequestsByRequestIdStatusResponses[keyof GetFalAiSam3VideoRequestsByRequestIdStatusResponses];

export type PutFalAiSam3VideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sam-3/video/requests/{request_id}/cancel";
};

export type PutFalAiSam3VideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSam3VideoRequestsByRequestIdCancelResponse =
  PutFalAiSam3VideoRequestsByRequestIdCancelResponses[keyof PutFalAiSam3VideoRequestsByRequestIdCancelResponses];

export type PostFalAiSam3VideoData = {
  body: Sam3VideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sam-3/video";
};

export type PostFalAiSam3VideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSam3VideoResponse =
  PostFalAiSam3VideoResponses[keyof PostFalAiSam3VideoResponses];

export type GetFalAiSam3VideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sam-3/video/requests/{request_id}";
};

export type GetFalAiSam3VideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Sam3VideoOutput;
};

export type GetFalAiSam3VideoRequestsByRequestIdResponse =
  GetFalAiSam3VideoRequestsByRequestIdResponses[keyof GetFalAiSam3VideoRequestsByRequestIdResponses];

export type GetFalAiEdittoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/editto/requests/{request_id}/status";
};

export type GetFalAiEdittoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiEdittoRequestsByRequestIdStatusResponse =
  GetFalAiEdittoRequestsByRequestIdStatusResponses[keyof GetFalAiEdittoRequestsByRequestIdStatusResponses];

export type PutFalAiEdittoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/editto/requests/{request_id}/cancel";
};

export type PutFalAiEdittoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiEdittoRequestsByRequestIdCancelResponse =
  PutFalAiEdittoRequestsByRequestIdCancelResponses[keyof PutFalAiEdittoRequestsByRequestIdCancelResponses];

export type PostFalAiEdittoData = {
  body: EdittoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/editto";
};

export type PostFalAiEdittoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiEdittoResponse =
  PostFalAiEdittoResponses[keyof PostFalAiEdittoResponses];

export type GetFalAiEdittoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/editto/requests/{request_id}";
};

export type GetFalAiEdittoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: EdittoOutput;
};

export type GetFalAiEdittoRequestsByRequestIdResponse =
  GetFalAiEdittoRequestsByRequestIdResponses[keyof GetFalAiEdittoRequestsByRequestIdResponses];

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flashvsr/upscale/video/requests/{request_id}/status";
};

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusResponse =
  GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdStatusResponses];

export type PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flashvsr/upscale/video/requests/{request_id}/cancel";
};

export type PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelResponse =
  PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFlashvsrUpscaleVideoRequestsByRequestIdCancelResponses];

export type PostFalAiFlashvsrUpscaleVideoData = {
  body: FlashvsrUpscaleVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flashvsr/upscale/video";
};

export type PostFalAiFlashvsrUpscaleVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFlashvsrUpscaleVideoResponse =
  PostFalAiFlashvsrUpscaleVideoResponses[keyof PostFalAiFlashvsrUpscaleVideoResponses];

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flashvsr/upscale/video/requests/{request_id}";
};

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FlashvsrUpscaleVideoOutput;
};

export type GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdResponse =
  GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdResponses[keyof GetFalAiFlashvsrUpscaleVideoRequestsByRequestIdResponses];

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/workflow-utilities/auto-subtitle/requests/{request_id}/status";
  };

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusResponse =
  GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusResponses[keyof GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdStatusResponses];

export type PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/workflow-utilities/auto-subtitle/requests/{request_id}/cancel";
  };

export type PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelResponse =
  PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelResponses[keyof PutFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdCancelResponses];

export type PostFalAiWorkflowUtilitiesAutoSubtitleData = {
  body: WorkflowUtilitiesAutoSubtitleInput;
  path?: never;
  query?: never;
  url: "/fal-ai/workflow-utilities/auto-subtitle";
};

export type PostFalAiWorkflowUtilitiesAutoSubtitleResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWorkflowUtilitiesAutoSubtitleResponse =
  PostFalAiWorkflowUtilitiesAutoSubtitleResponses[keyof PostFalAiWorkflowUtilitiesAutoSubtitleResponses];

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/workflow-utilities/auto-subtitle/requests/{request_id}";
};

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: WorkflowUtilitiesAutoSubtitleOutput;
  };

export type GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdResponse =
  GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdResponses[keyof GetFalAiWorkflowUtilitiesAutoSubtitleRequestsByRequestIdResponses];

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/bytedance-upscaler/upscale/video/requests/{request_id}/status";
  };

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusResponse =
  GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusResponses[keyof GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdStatusResponses];

export type PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/bytedance-upscaler/upscale/video/requests/{request_id}/cancel";
  };

export type PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelResponse =
  PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelResponses[keyof PutFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdCancelResponses];

export type PostFalAiBytedanceUpscalerUpscaleVideoData = {
  body: BytedanceUpscalerUpscaleVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/bytedance-upscaler/upscale/video";
};

export type PostFalAiBytedanceUpscalerUpscaleVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBytedanceUpscalerUpscaleVideoResponse =
  PostFalAiBytedanceUpscalerUpscaleVideoResponses[keyof PostFalAiBytedanceUpscalerUpscaleVideoResponses];

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/bytedance-upscaler/upscale/video/requests/{request_id}";
};

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: BytedanceUpscalerUpscaleVideoOutput;
  };

export type GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdResponse =
  GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdResponses[keyof GetFalAiBytedanceUpscalerUpscaleVideoRequestsByRequestIdResponses];

export type GetFalAiVideoAsPromptRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/video-as-prompt/requests/{request_id}/status";
};

export type GetFalAiVideoAsPromptRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVideoAsPromptRequestsByRequestIdStatusResponse =
  GetFalAiVideoAsPromptRequestsByRequestIdStatusResponses[keyof GetFalAiVideoAsPromptRequestsByRequestIdStatusResponses];

export type PutFalAiVideoAsPromptRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/video-as-prompt/requests/{request_id}/cancel";
};

export type PutFalAiVideoAsPromptRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVideoAsPromptRequestsByRequestIdCancelResponse =
  PutFalAiVideoAsPromptRequestsByRequestIdCancelResponses[keyof PutFalAiVideoAsPromptRequestsByRequestIdCancelResponses];

export type PostFalAiVideoAsPromptData = {
  body: VideoAsPromptInput;
  path?: never;
  query?: never;
  url: "/fal-ai/video-as-prompt";
};

export type PostFalAiVideoAsPromptResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVideoAsPromptResponse =
  PostFalAiVideoAsPromptResponses[keyof PostFalAiVideoAsPromptResponses];

export type GetFalAiVideoAsPromptRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/video-as-prompt/requests/{request_id}";
};

export type GetFalAiVideoAsPromptRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoAsPromptOutput;
};

export type GetFalAiVideoAsPromptRequestsByRequestIdResponse =
  GetFalAiVideoAsPromptRequestsByRequestIdResponses[keyof GetFalAiVideoAsPromptRequestsByRequestIdResponses];

export type GetFalAiBirefnetV2VideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/birefnet/v2/video/requests/{request_id}/status";
};

export type GetFalAiBirefnetV2VideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiBirefnetV2VideoRequestsByRequestIdStatusResponse =
  GetFalAiBirefnetV2VideoRequestsByRequestIdStatusResponses[keyof GetFalAiBirefnetV2VideoRequestsByRequestIdStatusResponses];

export type PutFalAiBirefnetV2VideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/birefnet/v2/video/requests/{request_id}/cancel";
};

export type PutFalAiBirefnetV2VideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiBirefnetV2VideoRequestsByRequestIdCancelResponse =
  PutFalAiBirefnetV2VideoRequestsByRequestIdCancelResponses[keyof PutFalAiBirefnetV2VideoRequestsByRequestIdCancelResponses];

export type PostFalAiBirefnetV2VideoData = {
  body: BirefnetV2VideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/birefnet/v2/video";
};

export type PostFalAiBirefnetV2VideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBirefnetV2VideoResponse =
  PostFalAiBirefnetV2VideoResponses[keyof PostFalAiBirefnetV2VideoResponses];

export type GetFalAiBirefnetV2VideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/birefnet/v2/video/requests/{request_id}";
};

export type GetFalAiBirefnetV2VideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: BirefnetV2VideoOutput;
};

export type GetFalAiBirefnetV2VideoRequestsByRequestIdResponse =
  GetFalAiBirefnetV2VideoRequestsByRequestIdResponses[keyof GetFalAiBirefnetV2VideoRequestsByRequestIdResponses];

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/vidu/q2/video-extension/pro/requests/{request_id}/status";
};

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusResponse =
  GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusResponses[keyof GetFalAiViduQ2VideoExtensionProRequestsByRequestIdStatusResponses];

export type PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/video-extension/pro/requests/{request_id}/cancel";
};

export type PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelResponse =
  PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelResponses[keyof PutFalAiViduQ2VideoExtensionProRequestsByRequestIdCancelResponses];

export type PostFalAiViduQ2VideoExtensionProData = {
  body: ViduQ2VideoExtensionProInput;
  path?: never;
  query?: never;
  url: "/fal-ai/vidu/q2/video-extension/pro";
};

export type PostFalAiViduQ2VideoExtensionProResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiViduQ2VideoExtensionProResponse =
  PostFalAiViduQ2VideoExtensionProResponses[keyof PostFalAiViduQ2VideoExtensionProResponses];

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/vidu/q2/video-extension/pro/requests/{request_id}";
};

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ViduQ2VideoExtensionProOutput;
};

export type GetFalAiViduQ2VideoExtensionProRequestsByRequestIdResponse =
  GetFalAiViduQ2VideoExtensionProRequestsByRequestIdResponses[keyof GetFalAiViduQ2VideoExtensionProRequestsByRequestIdResponses];

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/mirelo-ai/sfx-v1.5/video-to-video/requests/{request_id}/status";
};

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusResponse =
  GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusResponses[keyof GetMireloAiSfxV15VideoToVideoRequestsByRequestIdStatusResponses];

export type PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/mirelo-ai/sfx-v1.5/video-to-video/requests/{request_id}/cancel";
};

export type PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelResponse =
  PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelResponses[keyof PutMireloAiSfxV15VideoToVideoRequestsByRequestIdCancelResponses];

export type PostMireloAiSfxV15VideoToVideoData = {
  body: SfxV15VideoToVideoInput;
  path?: never;
  query?: never;
  url: "/mirelo-ai/sfx-v1.5/video-to-video";
};

export type PostMireloAiSfxV15VideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostMireloAiSfxV15VideoToVideoResponse =
  PostMireloAiSfxV15VideoToVideoResponses[keyof PostMireloAiSfxV15VideoToVideoResponses];

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/mirelo-ai/sfx-v1.5/video-to-video/requests/{request_id}";
};

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SfxV15VideoToVideoOutput;
};

export type GetMireloAiSfxV15VideoToVideoRequestsByRequestIdResponse =
  GetMireloAiSfxV15VideoToVideoRequestsByRequestIdResponses[keyof GetMireloAiSfxV15VideoToVideoRequestsByRequestIdResponses];

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/krea-wan-14b/video-to-video/requests/{request_id}/status";
};

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/krea-wan-14b/video-to-video/requests/{request_id}/cancel";
};

export type PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiKreaWan14bVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiKreaWan14bVideoToVideoData = {
  body: KreaWan14bVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/krea-wan-14b/video-to-video";
};

export type PostFalAiKreaWan14bVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiKreaWan14bVideoToVideoResponse =
  PostFalAiKreaWan14bVideoToVideoResponses[keyof PostFalAiKreaWan14bVideoToVideoResponses];

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/krea-wan-14b/video-to-video/requests/{request_id}";
};

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: KreaWan14bVideoToVideoOutput;
};

export type GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdResponse =
  GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiKreaWan14bVideoToVideoRequestsByRequestIdResponses];

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sora-2/video-to-video/remix/requests/{request_id}/status";
};

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusResponse =
  GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusResponses[keyof GetFalAiSora2VideoToVideoRemixRequestsByRequestIdStatusResponses];

export type PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/video-to-video/remix/requests/{request_id}/cancel";
};

export type PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelResponse =
  PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelResponses[keyof PutFalAiSora2VideoToVideoRemixRequestsByRequestIdCancelResponses];

export type PostFalAiSora2VideoToVideoRemixData = {
  body: Sora2VideoToVideoRemixInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sora-2/video-to-video/remix";
};

export type PostFalAiSora2VideoToVideoRemixResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSora2VideoToVideoRemixResponse =
  PostFalAiSora2VideoToVideoRemixResponses[keyof PostFalAiSora2VideoToVideoRemixResponses];

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sora-2/video-to-video/remix/requests/{request_id}";
};

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Sora2VideoToVideoRemixOutput;
};

export type GetFalAiSora2VideoToVideoRemixRequestsByRequestIdResponse =
  GetFalAiSora2VideoToVideoRemixRequestsByRequestIdResponses[keyof GetFalAiSora2VideoToVideoRemixRequestsByRequestIdResponses];

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace-apps/long-reframe/requests/{request_id}/status";
};

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusResponse =
  GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusResponses[keyof GetFalAiWanVaceAppsLongReframeRequestsByRequestIdStatusResponses];

export type PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-apps/long-reframe/requests/{request_id}/cancel";
};

export type PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelResponse =
  PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelResponses[keyof PutFalAiWanVaceAppsLongReframeRequestsByRequestIdCancelResponses];

export type PostFalAiWanVaceAppsLongReframeData = {
  body: WanVaceAppsLongReframeInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace-apps/long-reframe";
};

export type PostFalAiWanVaceAppsLongReframeResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVaceAppsLongReframeResponse =
  PostFalAiWanVaceAppsLongReframeResponses[keyof PostFalAiWanVaceAppsLongReframeResponses];

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-apps/long-reframe/requests/{request_id}";
};

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVaceAppsLongReframeOutput;
};

export type GetFalAiWanVaceAppsLongReframeRequestsByRequestIdResponse =
  GetFalAiWanVaceAppsLongReframeRequestsByRequestIdResponses[keyof GetFalAiWanVaceAppsLongReframeRequestsByRequestIdResponses];

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/infinitalk/video-to-video/requests/{request_id}/status";
};

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiInfinitalkVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/infinitalk/video-to-video/requests/{request_id}/cancel";
};

export type PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiInfinitalkVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiInfinitalkVideoToVideoData = {
  body: InfinitalkVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/infinitalk/video-to-video";
};

export type PostFalAiInfinitalkVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiInfinitalkVideoToVideoResponse =
  PostFalAiInfinitalkVideoToVideoResponses[keyof PostFalAiInfinitalkVideoToVideoResponses];

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/infinitalk/video-to-video/requests/{request_id}";
};

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: InfinitalkVideoToVideoOutput;
};

export type GetFalAiInfinitalkVideoToVideoRequestsByRequestIdResponse =
  GetFalAiInfinitalkVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiInfinitalkVideoToVideoRequestsByRequestIdResponses];

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/seedvr/upscale/video/requests/{request_id}/status";
};

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusResponse =
  GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusResponses[keyof GetFalAiSeedvrUpscaleVideoRequestsByRequestIdStatusResponses];

export type PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/seedvr/upscale/video/requests/{request_id}/cancel";
};

export type PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelResponse =
  PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelResponses[keyof PutFalAiSeedvrUpscaleVideoRequestsByRequestIdCancelResponses];

export type PostFalAiSeedvrUpscaleVideoData = {
  body: SeedvrUpscaleVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/seedvr/upscale/video";
};

export type PostFalAiSeedvrUpscaleVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSeedvrUpscaleVideoResponse =
  PostFalAiSeedvrUpscaleVideoResponses[keyof PostFalAiSeedvrUpscaleVideoResponses];

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/seedvr/upscale/video/requests/{request_id}";
};

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SeedvrUpscaleVideoOutput;
};

export type GetFalAiSeedvrUpscaleVideoRequestsByRequestIdResponse =
  GetFalAiSeedvrUpscaleVideoRequestsByRequestIdResponses[keyof GetFalAiSeedvrUpscaleVideoRequestsByRequestIdResponses];

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace-apps/video-edit/requests/{request_id}/status";
};

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusResponse =
  GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusResponses[keyof GetFalAiWanVaceAppsVideoEditRequestsByRequestIdStatusResponses];

export type PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-apps/video-edit/requests/{request_id}/cancel";
};

export type PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelResponse =
  PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelResponses[keyof PutFalAiWanVaceAppsVideoEditRequestsByRequestIdCancelResponses];

export type PostFalAiWanVaceAppsVideoEditData = {
  body: WanVaceAppsVideoEditInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace-apps/video-edit";
};

export type PostFalAiWanVaceAppsVideoEditResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVaceAppsVideoEditResponse =
  PostFalAiWanVaceAppsVideoEditResponses[keyof PostFalAiWanVaceAppsVideoEditResponses];

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-apps/video-edit/requests/{request_id}";
};

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVaceAppsVideoEditOutput;
};

export type GetFalAiWanVaceAppsVideoEditRequestsByRequestIdResponse =
  GetFalAiWanVaceAppsVideoEditRequestsByRequestIdResponses[keyof GetFalAiWanVaceAppsVideoEditRequestsByRequestIdResponses];

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-14b/animate/replace/requests/{request_id}/status";
};

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusResponse =
  GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusResponses[keyof GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdStatusResponses];

export type PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-14b/animate/replace/requests/{request_id}/cancel";
};

export type PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelResponse =
  PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelResponses[keyof PutFalAiWanV2214bAnimateReplaceRequestsByRequestIdCancelResponses];

export type PostFalAiWanV2214bAnimateReplaceData = {
  body: WanV2214bAnimateReplaceInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-14b/animate/replace";
};

export type PostFalAiWanV2214bAnimateReplaceResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV2214bAnimateReplaceResponse =
  PostFalAiWanV2214bAnimateReplaceResponses[keyof PostFalAiWanV2214bAnimateReplaceResponses];

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-14b/animate/replace/requests/{request_id}";
};

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV2214bAnimateReplaceOutput;
};

export type GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdResponse =
  GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdResponses[keyof GetFalAiWanV2214bAnimateReplaceRequestsByRequestIdResponses];

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-14b/animate/move/requests/{request_id}/status";
};

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusResponse =
  GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusResponses[keyof GetFalAiWanV2214bAnimateMoveRequestsByRequestIdStatusResponses];

export type PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-14b/animate/move/requests/{request_id}/cancel";
};

export type PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelResponse =
  PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelResponses[keyof PutFalAiWanV2214bAnimateMoveRequestsByRequestIdCancelResponses];

export type PostFalAiWanV2214bAnimateMoveData = {
  body: WanV2214bAnimateMoveInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-14b/animate/move";
};

export type PostFalAiWanV2214bAnimateMoveResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV2214bAnimateMoveResponse =
  PostFalAiWanV2214bAnimateMoveResponses[keyof PostFalAiWanV2214bAnimateMoveResponses];

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-14b/animate/move/requests/{request_id}";
};

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV2214bAnimateMoveOutput;
};

export type GetFalAiWanV2214bAnimateMoveRequestsByRequestIdResponse =
  GetFalAiWanV2214bAnimateMoveRequestsByRequestIdResponses[keyof GetFalAiWanV2214bAnimateMoveRequestsByRequestIdResponses];

export type GetDecartLucyEditProRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/decart/lucy-edit/pro/requests/{request_id}/status";
};

export type GetDecartLucyEditProRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetDecartLucyEditProRequestsByRequestIdStatusResponse =
  GetDecartLucyEditProRequestsByRequestIdStatusResponses[keyof GetDecartLucyEditProRequestsByRequestIdStatusResponses];

export type PutDecartLucyEditProRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-edit/pro/requests/{request_id}/cancel";
};

export type PutDecartLucyEditProRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutDecartLucyEditProRequestsByRequestIdCancelResponse =
  PutDecartLucyEditProRequestsByRequestIdCancelResponses[keyof PutDecartLucyEditProRequestsByRequestIdCancelResponses];

export type PostDecartLucyEditProData = {
  body: LucyEditProInput;
  path?: never;
  query?: never;
  url: "/decart/lucy-edit/pro";
};

export type PostDecartLucyEditProResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostDecartLucyEditProResponse =
  PostDecartLucyEditProResponses[keyof PostDecartLucyEditProResponses];

export type GetDecartLucyEditProRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-edit/pro/requests/{request_id}";
};

export type GetDecartLucyEditProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LucyEditProOutput;
};

export type GetDecartLucyEditProRequestsByRequestIdResponse =
  GetDecartLucyEditProRequestsByRequestIdResponses[keyof GetDecartLucyEditProRequestsByRequestIdResponses];

export type GetDecartLucyEditDevRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/decart/lucy-edit/dev/requests/{request_id}/status";
};

export type GetDecartLucyEditDevRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetDecartLucyEditDevRequestsByRequestIdStatusResponse =
  GetDecartLucyEditDevRequestsByRequestIdStatusResponses[keyof GetDecartLucyEditDevRequestsByRequestIdStatusResponses];

export type PutDecartLucyEditDevRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-edit/dev/requests/{request_id}/cancel";
};

export type PutDecartLucyEditDevRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutDecartLucyEditDevRequestsByRequestIdCancelResponse =
  PutDecartLucyEditDevRequestsByRequestIdCancelResponses[keyof PutDecartLucyEditDevRequestsByRequestIdCancelResponses];

export type PostDecartLucyEditDevData = {
  body: LucyEditDevInput;
  path?: never;
  query?: never;
  url: "/decart/lucy-edit/dev";
};

export type PostDecartLucyEditDevResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostDecartLucyEditDevResponse =
  PostDecartLucyEditDevResponses[keyof PostDecartLucyEditDevResponses];

export type GetDecartLucyEditDevRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/decart/lucy-edit/dev/requests/{request_id}";
};

export type GetDecartLucyEditDevRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LucyEditDevOutput;
};

export type GetDecartLucyEditDevRequestsByRequestIdResponse =
  GetDecartLucyEditDevRequestsByRequestIdResponses[keyof GetDecartLucyEditDevRequestsByRequestIdResponses];

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-22-vace-fun-a14b/reframe/requests/{request_id}/status";
};

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdStatusResponses];

export type PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/reframe/requests/{request_id}/cancel";
};

export type PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bReframeRequestsByRequestIdCancelResponses];

export type PostFalAiWan22VaceFunA14bReframeData = {
  body: Wan22VaceFunA14bReframeInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/reframe";
};

export type PostFalAiWan22VaceFunA14bReframeResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWan22VaceFunA14bReframeResponse =
  PostFalAiWan22VaceFunA14bReframeResponses[keyof PostFalAiWan22VaceFunA14bReframeResponses];

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/reframe/requests/{request_id}";
};

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Wan22VaceFunA14bReframeOutput;
};

export type GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bReframeRequestsByRequestIdResponses];

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-22-vace-fun-a14b/outpainting/requests/{request_id}/status";
};

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdStatusResponses];

export type PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/outpainting/requests/{request_id}/cancel";
};

export type PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdCancelResponses];

export type PostFalAiWan22VaceFunA14bOutpaintingData = {
  body: Wan22VaceFunA14bOutpaintingInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/outpainting";
};

export type PostFalAiWan22VaceFunA14bOutpaintingResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWan22VaceFunA14bOutpaintingResponse =
  PostFalAiWan22VaceFunA14bOutpaintingResponses[keyof PostFalAiWan22VaceFunA14bOutpaintingResponses];

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/outpainting/requests/{request_id}";
};

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Wan22VaceFunA14bOutpaintingOutput;
};

export type GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bOutpaintingRequestsByRequestIdResponses];

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-22-vace-fun-a14b/inpainting/requests/{request_id}/status";
};

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdStatusResponses];

export type PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/inpainting/requests/{request_id}/cancel";
};

export type PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdCancelResponses];

export type PostFalAiWan22VaceFunA14bInpaintingData = {
  body: Wan22VaceFunA14bInpaintingInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/inpainting";
};

export type PostFalAiWan22VaceFunA14bInpaintingResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWan22VaceFunA14bInpaintingResponse =
  PostFalAiWan22VaceFunA14bInpaintingResponses[keyof PostFalAiWan22VaceFunA14bInpaintingResponses];

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/inpainting/requests/{request_id}";
};

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Wan22VaceFunA14bInpaintingOutput;
};

export type GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bInpaintingRequestsByRequestIdResponses];

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-22-vace-fun-a14b/depth/requests/{request_id}/status";
};

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdStatusResponses];

export type PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/depth/requests/{request_id}/cancel";
};

export type PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bDepthRequestsByRequestIdCancelResponses];

export type PostFalAiWan22VaceFunA14bDepthData = {
  body: Wan22VaceFunA14bDepthInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/depth";
};

export type PostFalAiWan22VaceFunA14bDepthResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWan22VaceFunA14bDepthResponse =
  PostFalAiWan22VaceFunA14bDepthResponses[keyof PostFalAiWan22VaceFunA14bDepthResponses];

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/depth/requests/{request_id}";
};

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Wan22VaceFunA14bDepthOutput;
};

export type GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bDepthRequestsByRequestIdResponses];

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-22-vace-fun-a14b/pose/requests/{request_id}/status";
};

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusResponse =
  GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusResponses[keyof GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdStatusResponses];

export type PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/pose/requests/{request_id}/cancel";
};

export type PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelResponse =
  PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelResponses[keyof PutFalAiWan22VaceFunA14bPoseRequestsByRequestIdCancelResponses];

export type PostFalAiWan22VaceFunA14bPoseData = {
  body: Wan22VaceFunA14bPoseInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/pose";
};

export type PostFalAiWan22VaceFunA14bPoseResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWan22VaceFunA14bPoseResponse =
  PostFalAiWan22VaceFunA14bPoseResponses[keyof PostFalAiWan22VaceFunA14bPoseResponses];

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-vace-fun-a14b/pose/requests/{request_id}";
};

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Wan22VaceFunA14bPoseOutput;
};

export type GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdResponse =
  GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdResponses[keyof GetFalAiWan22VaceFunA14bPoseRequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-video-foley/requests/{request_id}/status";
};

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoFoleyRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-foley/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoFoleyRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoFoleyData = {
  body: HunyuanVideoFoleyInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video-foley";
};

export type PostFalAiHunyuanVideoFoleyResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoFoleyResponse =
  PostFalAiHunyuanVideoFoleyResponses[keyof PostFalAiHunyuanVideoFoleyResponses];

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-foley/requests/{request_id}";
};

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoFoleyOutput;
};

export type GetFalAiHunyuanVideoFoleyRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoFoleyRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoFoleyRequestsByRequestIdResponses];

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sync-lipsync/v2/pro/requests/{request_id}/status";
};

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusResponse =
  GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusResponses[keyof GetFalAiSyncLipsyncV2ProRequestsByRequestIdStatusResponses];

export type PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sync-lipsync/v2/pro/requests/{request_id}/cancel";
};

export type PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelResponse =
  PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelResponses[keyof PutFalAiSyncLipsyncV2ProRequestsByRequestIdCancelResponses];

export type PostFalAiSyncLipsyncV2ProData = {
  body: SyncLipsyncV2ProInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sync-lipsync/v2/pro";
};

export type PostFalAiSyncLipsyncV2ProResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSyncLipsyncV2ProResponse =
  PostFalAiSyncLipsyncV2ProResponses[keyof PostFalAiSyncLipsyncV2ProResponses];

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sync-lipsync/v2/pro/requests/{request_id}";
};

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SyncLipsyncV2ProOutput;
};

export type GetFalAiSyncLipsyncV2ProRequestsByRequestIdResponse =
  GetFalAiSyncLipsyncV2ProRequestsByRequestIdResponses[keyof GetFalAiSyncLipsyncV2ProRequestsByRequestIdResponses];

export type GetFalAiWanFunControlRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-fun-control/requests/{request_id}/status";
};

export type GetFalAiWanFunControlRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanFunControlRequestsByRequestIdStatusResponse =
  GetFalAiWanFunControlRequestsByRequestIdStatusResponses[keyof GetFalAiWanFunControlRequestsByRequestIdStatusResponses];

export type PutFalAiWanFunControlRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-fun-control/requests/{request_id}/cancel";
};

export type PutFalAiWanFunControlRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanFunControlRequestsByRequestIdCancelResponse =
  PutFalAiWanFunControlRequestsByRequestIdCancelResponses[keyof PutFalAiWanFunControlRequestsByRequestIdCancelResponses];

export type PostFalAiWanFunControlData = {
  body: WanFunControlInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-fun-control";
};

export type PostFalAiWanFunControlResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanFunControlResponse =
  PostFalAiWanFunControlResponses[keyof PostFalAiWanFunControlResponses];

export type GetFalAiWanFunControlRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-fun-control/requests/{request_id}";
};

export type GetFalAiWanFunControlRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanFunControlOutput;
};

export type GetFalAiWanFunControlRequestsByRequestIdResponse =
  GetFalAiWanFunControlRequestsByRequestIdResponses[keyof GetFalAiWanFunControlRequestsByRequestIdResponses];

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/bria/video/increase-resolution/requests/{request_id}/status";
};

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusResponse =
  GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusResponses[keyof GetBriaVideoIncreaseResolutionRequestsByRequestIdStatusResponses];

export type PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/increase-resolution/requests/{request_id}/cancel";
};

export type PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelResponse =
  PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelResponses[keyof PutBriaVideoIncreaseResolutionRequestsByRequestIdCancelResponses];

export type PostBriaVideoIncreaseResolutionData = {
  body: VideoIncreaseResolutionInput;
  path?: never;
  query?: never;
  url: "/bria/video/increase-resolution";
};

export type PostBriaVideoIncreaseResolutionResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostBriaVideoIncreaseResolutionResponse =
  PostBriaVideoIncreaseResolutionResponses[keyof PostBriaVideoIncreaseResolutionResponses];

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/bria/video/increase-resolution/requests/{request_id}";
};

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoIncreaseResolutionOutput;
};

export type GetBriaVideoIncreaseResolutionRequestsByRequestIdResponse =
  GetBriaVideoIncreaseResolutionRequestsByRequestIdResponses[keyof GetBriaVideoIncreaseResolutionRequestsByRequestIdResponses];

export type GetFalAiInfinitalkRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/infinitalk/requests/{request_id}/status";
};

export type GetFalAiInfinitalkRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiInfinitalkRequestsByRequestIdStatusResponse =
  GetFalAiInfinitalkRequestsByRequestIdStatusResponses[keyof GetFalAiInfinitalkRequestsByRequestIdStatusResponses];

export type PutFalAiInfinitalkRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/infinitalk/requests/{request_id}/cancel";
};

export type PutFalAiInfinitalkRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiInfinitalkRequestsByRequestIdCancelResponse =
  PutFalAiInfinitalkRequestsByRequestIdCancelResponses[keyof PutFalAiInfinitalkRequestsByRequestIdCancelResponses];

export type PostFalAiInfinitalkData = {
  body: InfinitalkInput;
  path?: never;
  query?: never;
  url: "/fal-ai/infinitalk";
};

export type PostFalAiInfinitalkResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiInfinitalkResponse =
  PostFalAiInfinitalkResponses[keyof PostFalAiInfinitalkResponses];

export type GetFalAiInfinitalkRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/infinitalk/requests/{request_id}";
};

export type GetFalAiInfinitalkRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: InfinitalkOutput;
};

export type GetFalAiInfinitalkRequestsByRequestIdResponse =
  GetFalAiInfinitalkRequestsByRequestIdResponses[keyof GetFalAiInfinitalkRequestsByRequestIdResponses];

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/mirelo-ai/sfx-v1/video-to-video/requests/{request_id}/status";
};

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusResponse =
  GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusResponses[keyof GetMireloAiSfxV1VideoToVideoRequestsByRequestIdStatusResponses];

export type PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/mirelo-ai/sfx-v1/video-to-video/requests/{request_id}/cancel";
};

export type PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelResponse =
  PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelResponses[keyof PutMireloAiSfxV1VideoToVideoRequestsByRequestIdCancelResponses];

export type PostMireloAiSfxV1VideoToVideoData = {
  body: SfxV1VideoToVideoInput;
  path?: never;
  query?: never;
  url: "/mirelo-ai/sfx-v1/video-to-video";
};

export type PostMireloAiSfxV1VideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostMireloAiSfxV1VideoToVideoResponse =
  PostMireloAiSfxV1VideoToVideoResponses[keyof PostMireloAiSfxV1VideoToVideoResponses];

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/mirelo-ai/sfx-v1/video-to-video/requests/{request_id}";
};

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SfxV1VideoToVideoOutput;
};

export type GetMireloAiSfxV1VideoToVideoRequestsByRequestIdResponse =
  GetMireloAiSfxV1VideoToVideoRequestsByRequestIdResponses[keyof GetMireloAiSfxV1VideoToVideoRequestsByRequestIdResponses];

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/moonvalley/marey/pose-transfer/requests/{request_id}/status";
};

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusResponse =
  GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusResponses[keyof GetMoonvalleyMareyPoseTransferRequestsByRequestIdStatusResponses];

export type PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/moonvalley/marey/pose-transfer/requests/{request_id}/cancel";
};

export type PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelResponse =
  PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelResponses[keyof PutMoonvalleyMareyPoseTransferRequestsByRequestIdCancelResponses];

export type PostMoonvalleyMareyPoseTransferData = {
  body: MareyPoseTransferInput;
  path?: never;
  query?: never;
  url: "/moonvalley/marey/pose-transfer";
};

export type PostMoonvalleyMareyPoseTransferResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostMoonvalleyMareyPoseTransferResponse =
  PostMoonvalleyMareyPoseTransferResponses[keyof PostMoonvalleyMareyPoseTransferResponses];

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/moonvalley/marey/pose-transfer/requests/{request_id}";
};

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MareyPoseTransferOutput;
};

export type GetMoonvalleyMareyPoseTransferRequestsByRequestIdResponse =
  GetMoonvalleyMareyPoseTransferRequestsByRequestIdResponses[keyof GetMoonvalleyMareyPoseTransferRequestsByRequestIdResponses];

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/moonvalley/marey/motion-transfer/requests/{request_id}/status";
};

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusResponse =
  GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusResponses[keyof GetMoonvalleyMareyMotionTransferRequestsByRequestIdStatusResponses];

export type PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/moonvalley/marey/motion-transfer/requests/{request_id}/cancel";
};

export type PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelResponse =
  PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelResponses[keyof PutMoonvalleyMareyMotionTransferRequestsByRequestIdCancelResponses];

export type PostMoonvalleyMareyMotionTransferData = {
  body: MareyMotionTransferInput;
  path?: never;
  query?: never;
  url: "/moonvalley/marey/motion-transfer";
};

export type PostMoonvalleyMareyMotionTransferResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostMoonvalleyMareyMotionTransferResponse =
  PostMoonvalleyMareyMotionTransferResponses[keyof PostMoonvalleyMareyMotionTransferResponses];

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/moonvalley/marey/motion-transfer/requests/{request_id}";
};

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MareyMotionTransferOutput;
};

export type GetMoonvalleyMareyMotionTransferRequestsByRequestIdResponse =
  GetMoonvalleyMareyMotionTransferRequestsByRequestIdResponses[keyof GetMoonvalleyMareyMotionTransferRequestsByRequestIdResponses];

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ffmpeg-api/merge-videos/requests/{request_id}/status";
};

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusResponse =
  GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusResponses[keyof GetFalAiFfmpegApiMergeVideosRequestsByRequestIdStatusResponses];

export type PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ffmpeg-api/merge-videos/requests/{request_id}/cancel";
};

export type PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelResponse =
  PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelResponses[keyof PutFalAiFfmpegApiMergeVideosRequestsByRequestIdCancelResponses];

export type PostFalAiFfmpegApiMergeVideosData = {
  body: FfmpegApiMergeVideosInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ffmpeg-api/merge-videos";
};

export type PostFalAiFfmpegApiMergeVideosResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFfmpegApiMergeVideosResponse =
  PostFalAiFfmpegApiMergeVideosResponses[keyof PostFalAiFfmpegApiMergeVideosResponses];

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ffmpeg-api/merge-videos/requests/{request_id}";
};

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FfmpegApiMergeVideosOutput;
};

export type GetFalAiFfmpegApiMergeVideosRequestsByRequestIdResponse =
  GetFalAiFfmpegApiMergeVideosRequestsByRequestIdResponses[keyof GetFalAiFfmpegApiMergeVideosRequestsByRequestIdResponses];

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan/v2.2-a14b/video-to-video/requests/{request_id}/status";
};

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/video-to-video/requests/{request_id}/cancel";
};

export type PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiWanV22A14bVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiWanV22A14bVideoToVideoData = {
  body: WanV22A14bVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/video-to-video";
};

export type PostFalAiWanV22A14bVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanV22A14bVideoToVideoResponse =
  PostFalAiWanV22A14bVideoToVideoResponses[keyof PostFalAiWanV22A14bVideoToVideoResponses];

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan/v2.2-a14b/video-to-video/requests/{request_id}";
};

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanV22A14bVideoToVideoOutput;
};

export type GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdResponse =
  GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiWanV22A14bVideoToVideoRequestsByRequestIdResponses];

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltxv-13b-098-distilled/extend/requests/{request_id}/status";
};

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusResponse =
  GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusResponses[keyof GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdStatusResponses];

export type PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltxv-13b-098-distilled/extend/requests/{request_id}/cancel";
};

export type PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelResponse =
  PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelResponses[keyof PutFalAiLtxv13B098DistilledExtendRequestsByRequestIdCancelResponses];

export type PostFalAiLtxv13B098DistilledExtendData = {
  body: Ltxv13B098DistilledExtendInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltxv-13b-098-distilled/extend";
};

export type PostFalAiLtxv13B098DistilledExtendResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxv13B098DistilledExtendResponse =
  PostFalAiLtxv13B098DistilledExtendResponses[keyof PostFalAiLtxv13B098DistilledExtendResponses];

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltxv-13b-098-distilled/extend/requests/{request_id}";
};

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltxv13B098DistilledExtendOutput;
};

export type GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdResponse =
  GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdResponses[keyof GetFalAiLtxv13B098DistilledExtendRequestsByRequestIdResponses];

export type GetFalAiRifeVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/rife/video/requests/{request_id}/status";
};

export type GetFalAiRifeVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiRifeVideoRequestsByRequestIdStatusResponse =
  GetFalAiRifeVideoRequestsByRequestIdStatusResponses[keyof GetFalAiRifeVideoRequestsByRequestIdStatusResponses];

export type PutFalAiRifeVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/rife/video/requests/{request_id}/cancel";
};

export type PutFalAiRifeVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiRifeVideoRequestsByRequestIdCancelResponse =
  PutFalAiRifeVideoRequestsByRequestIdCancelResponses[keyof PutFalAiRifeVideoRequestsByRequestIdCancelResponses];

export type PostFalAiRifeVideoData = {
  body: RifeVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/rife/video";
};

export type PostFalAiRifeVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiRifeVideoResponse =
  PostFalAiRifeVideoResponses[keyof PostFalAiRifeVideoResponses];

export type GetFalAiRifeVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/rife/video/requests/{request_id}";
};

export type GetFalAiRifeVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: RifeVideoOutput;
};

export type GetFalAiRifeVideoRequestsByRequestIdResponse =
  GetFalAiRifeVideoRequestsByRequestIdResponses[keyof GetFalAiRifeVideoRequestsByRequestIdResponses];

export type GetFalAiFilmVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/film/video/requests/{request_id}/status";
};

export type GetFalAiFilmVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFilmVideoRequestsByRequestIdStatusResponse =
  GetFalAiFilmVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFilmVideoRequestsByRequestIdStatusResponses];

export type PutFalAiFilmVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/film/video/requests/{request_id}/cancel";
};

export type PutFalAiFilmVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFilmVideoRequestsByRequestIdCancelResponse =
  PutFalAiFilmVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFilmVideoRequestsByRequestIdCancelResponses];

export type PostFalAiFilmVideoData = {
  body: FilmVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/film/video";
};

export type PostFalAiFilmVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFilmVideoResponse =
  PostFalAiFilmVideoResponses[keyof PostFalAiFilmVideoResponses];

export type GetFalAiFilmVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/film/video/requests/{request_id}";
};

export type GetFalAiFilmVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FilmVideoOutput;
};

export type GetFalAiFilmVideoRequestsByRequestIdResponse =
  GetFalAiFilmVideoRequestsByRequestIdResponses[keyof GetFalAiFilmVideoRequestsByRequestIdResponses];

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/luma-dream-machine/ray-2-flash/modify/requests/{request_id}/status";
  };

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdStatusResponses];

export type PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/luma-dream-machine/ray-2-flash/modify/requests/{request_id}/cancel";
  };

export type PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdCancelResponses];

export type PostFalAiLumaDreamMachineRay2FlashModifyData = {
  body: LumaDreamMachineRay2FlashModifyInput;
  path?: never;
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2-flash/modify";
};

export type PostFalAiLumaDreamMachineRay2FlashModifyResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLumaDreamMachineRay2FlashModifyResponse =
  PostFalAiLumaDreamMachineRay2FlashModifyResponses[keyof PostFalAiLumaDreamMachineRay2FlashModifyResponses];

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2-flash/modify/requests/{request_id}";
};

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LumaDreamMachineRay2FlashModifyOutput;
  };

export type GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2FlashModifyRequestsByRequestIdResponses];

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltxv-13b-098-distilled/multiconditioning/requests/{request_id}/status";
  };

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdStatusResponses];

export type PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltxv-13b-098-distilled/multiconditioning/requests/{request_id}/cancel";
  };

export type PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdCancelResponses];

export type PostFalAiLtxv13B098DistilledMulticonditioningData = {
  body: Ltxv13B098DistilledMulticonditioningInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltxv-13b-098-distilled/multiconditioning";
};

export type PostFalAiLtxv13B098DistilledMulticonditioningResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxv13B098DistilledMulticonditioningResponse =
  PostFalAiLtxv13B098DistilledMulticonditioningResponses[keyof PostFalAiLtxv13B098DistilledMulticonditioningResponses];

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltxv-13b-098-distilled/multiconditioning/requests/{request_id}";
  };

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: Ltxv13B098DistilledMulticonditioningOutput;
  };

export type GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxv13B098DistilledMulticonditioningRequestsByRequestIdResponses];

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/sound-effects/requests/{request_id}/status";
};

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusResponse =
  GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseSoundEffectsRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/sound-effects/requests/{request_id}/cancel";
};

export type PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelResponse =
  PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseSoundEffectsRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseSoundEffectsData = {
  body: PixverseSoundEffectsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/sound-effects";
};

export type PostFalAiPixverseSoundEffectsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseSoundEffectsResponse =
  PostFalAiPixverseSoundEffectsResponses[keyof PostFalAiPixverseSoundEffectsResponses];

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/sound-effects/requests/{request_id}";
};

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseSoundEffectsOutput;
};

export type GetFalAiPixverseSoundEffectsRequestsByRequestIdResponse =
  GetFalAiPixverseSoundEffectsRequestsByRequestIdResponses[keyof GetFalAiPixverseSoundEffectsRequestsByRequestIdResponses];

export type GetFalAiThinksoundAudioRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/thinksound/audio/requests/{request_id}/status";
};

export type GetFalAiThinksoundAudioRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiThinksoundAudioRequestsByRequestIdStatusResponse =
  GetFalAiThinksoundAudioRequestsByRequestIdStatusResponses[keyof GetFalAiThinksoundAudioRequestsByRequestIdStatusResponses];

export type PutFalAiThinksoundAudioRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/thinksound/audio/requests/{request_id}/cancel";
};

export type PutFalAiThinksoundAudioRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiThinksoundAudioRequestsByRequestIdCancelResponse =
  PutFalAiThinksoundAudioRequestsByRequestIdCancelResponses[keyof PutFalAiThinksoundAudioRequestsByRequestIdCancelResponses];

export type PostFalAiThinksoundAudioData = {
  body: ThinksoundAudioInput;
  path?: never;
  query?: never;
  url: "/fal-ai/thinksound/audio";
};

export type PostFalAiThinksoundAudioResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiThinksoundAudioResponse =
  PostFalAiThinksoundAudioResponses[keyof PostFalAiThinksoundAudioResponses];

export type GetFalAiThinksoundAudioRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/thinksound/audio/requests/{request_id}";
};

export type GetFalAiThinksoundAudioRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ThinksoundAudioOutput;
};

export type GetFalAiThinksoundAudioRequestsByRequestIdResponse =
  GetFalAiThinksoundAudioRequestsByRequestIdResponses[keyof GetFalAiThinksoundAudioRequestsByRequestIdResponses];

export type GetFalAiThinksoundRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/thinksound/requests/{request_id}/status";
};

export type GetFalAiThinksoundRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiThinksoundRequestsByRequestIdStatusResponse =
  GetFalAiThinksoundRequestsByRequestIdStatusResponses[keyof GetFalAiThinksoundRequestsByRequestIdStatusResponses];

export type PutFalAiThinksoundRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/thinksound/requests/{request_id}/cancel";
};

export type PutFalAiThinksoundRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiThinksoundRequestsByRequestIdCancelResponse =
  PutFalAiThinksoundRequestsByRequestIdCancelResponses[keyof PutFalAiThinksoundRequestsByRequestIdCancelResponses];

export type PostFalAiThinksoundData = {
  body: ThinksoundInput;
  path?: never;
  query?: never;
  url: "/fal-ai/thinksound";
};

export type PostFalAiThinksoundResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiThinksoundResponse =
  PostFalAiThinksoundResponses[keyof PostFalAiThinksoundResponses];

export type GetFalAiThinksoundRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/thinksound/requests/{request_id}";
};

export type GetFalAiThinksoundRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ThinksoundOutput;
};

export type GetFalAiThinksoundRequestsByRequestIdResponse =
  GetFalAiThinksoundRequestsByRequestIdResponses[keyof GetFalAiThinksoundRequestsByRequestIdResponses];

export type GetFalAiPixverseExtendFastRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/extend/fast/requests/{request_id}/status";
};

export type GetFalAiPixverseExtendFastRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseExtendFastRequestsByRequestIdStatusResponse =
  GetFalAiPixverseExtendFastRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseExtendFastRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseExtendFastRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/extend/fast/requests/{request_id}/cancel";
};

export type PutFalAiPixverseExtendFastRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseExtendFastRequestsByRequestIdCancelResponse =
  PutFalAiPixverseExtendFastRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseExtendFastRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseExtendFastData = {
  body: PixverseExtendFastInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/extend/fast";
};

export type PostFalAiPixverseExtendFastResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseExtendFastResponse =
  PostFalAiPixverseExtendFastResponses[keyof PostFalAiPixverseExtendFastResponses];

export type GetFalAiPixverseExtendFastRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/extend/fast/requests/{request_id}";
};

export type GetFalAiPixverseExtendFastRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseExtendFastOutput;
};

export type GetFalAiPixverseExtendFastRequestsByRequestIdResponse =
  GetFalAiPixverseExtendFastRequestsByRequestIdResponses[keyof GetFalAiPixverseExtendFastRequestsByRequestIdResponses];

export type GetFalAiPixverseExtendRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/extend/requests/{request_id}/status";
};

export type GetFalAiPixverseExtendRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseExtendRequestsByRequestIdStatusResponse =
  GetFalAiPixverseExtendRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseExtendRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseExtendRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/extend/requests/{request_id}/cancel";
};

export type PutFalAiPixverseExtendRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseExtendRequestsByRequestIdCancelResponse =
  PutFalAiPixverseExtendRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseExtendRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseExtendData = {
  body: PixverseExtendInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/extend";
};

export type PostFalAiPixverseExtendResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseExtendResponse =
  PostFalAiPixverseExtendResponses[keyof PostFalAiPixverseExtendResponses];

export type GetFalAiPixverseExtendRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/extend/requests/{request_id}";
};

export type GetFalAiPixverseExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseExtendOutput;
};

export type GetFalAiPixverseExtendRequestsByRequestIdResponse =
  GetFalAiPixverseExtendRequestsByRequestIdResponses[keyof GetFalAiPixverseExtendRequestsByRequestIdResponses];

export type GetFalAiPixverseLipsyncRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pixverse/lipsync/requests/{request_id}/status";
};

export type GetFalAiPixverseLipsyncRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPixverseLipsyncRequestsByRequestIdStatusResponse =
  GetFalAiPixverseLipsyncRequestsByRequestIdStatusResponses[keyof GetFalAiPixverseLipsyncRequestsByRequestIdStatusResponses];

export type PutFalAiPixverseLipsyncRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/lipsync/requests/{request_id}/cancel";
};

export type PutFalAiPixverseLipsyncRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPixverseLipsyncRequestsByRequestIdCancelResponse =
  PutFalAiPixverseLipsyncRequestsByRequestIdCancelResponses[keyof PutFalAiPixverseLipsyncRequestsByRequestIdCancelResponses];

export type PostFalAiPixverseLipsyncData = {
  body: PixverseLipsyncInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pixverse/lipsync";
};

export type PostFalAiPixverseLipsyncResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPixverseLipsyncResponse =
  PostFalAiPixverseLipsyncResponses[keyof PostFalAiPixverseLipsyncResponses];

export type GetFalAiPixverseLipsyncRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pixverse/lipsync/requests/{request_id}";
};

export type GetFalAiPixverseLipsyncRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PixverseLipsyncOutput;
};

export type GetFalAiPixverseLipsyncRequestsByRequestIdResponse =
  GetFalAiPixverseLipsyncRequestsByRequestIdResponses[keyof GetFalAiPixverseLipsyncRequestsByRequestIdResponses];

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/luma-dream-machine/ray-2/modify/requests/{request_id}/status";
};

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdStatusResponses];

export type PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/modify/requests/{request_id}/cancel";
};

export type PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdCancelResponses];

export type PostFalAiLumaDreamMachineRay2ModifyData = {
  body: LumaDreamMachineRay2ModifyInput;
  path?: never;
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/modify";
};

export type PostFalAiLumaDreamMachineRay2ModifyResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLumaDreamMachineRay2ModifyResponse =
  PostFalAiLumaDreamMachineRay2ModifyResponses[keyof PostFalAiLumaDreamMachineRay2ModifyResponses];

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/modify/requests/{request_id}";
};

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LumaDreamMachineRay2ModifyOutput;
};

export type GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2ModifyRequestsByRequestIdResponses];

export type GetFalAiWanVace14bReframeRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace-14b/reframe/requests/{request_id}/status";
};

export type GetFalAiWanVace14bReframeRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVace14bReframeRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bReframeRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bReframeRequestsByRequestIdStatusResponses];

export type PutFalAiWanVace14bReframeRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/reframe/requests/{request_id}/cancel";
};

export type PutFalAiWanVace14bReframeRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVace14bReframeRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bReframeRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bReframeRequestsByRequestIdCancelResponses];

export type PostFalAiWanVace14bReframeData = {
  body: WanVace14bReframeInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace-14b/reframe";
};

export type PostFalAiWanVace14bReframeResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVace14bReframeResponse =
  PostFalAiWanVace14bReframeResponses[keyof PostFalAiWanVace14bReframeResponses];

export type GetFalAiWanVace14bReframeRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/reframe/requests/{request_id}";
};

export type GetFalAiWanVace14bReframeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVace14bReframeOutput;
};

export type GetFalAiWanVace14bReframeRequestsByRequestIdResponse =
  GetFalAiWanVace14bReframeRequestsByRequestIdResponses[keyof GetFalAiWanVace14bReframeRequestsByRequestIdResponses];

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace-14b/outpainting/requests/{request_id}/status";
};

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bOutpaintingRequestsByRequestIdStatusResponses];

export type PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/outpainting/requests/{request_id}/cancel";
};

export type PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bOutpaintingRequestsByRequestIdCancelResponses];

export type PostFalAiWanVace14bOutpaintingData = {
  body: WanVace14bOutpaintingInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace-14b/outpainting";
};

export type PostFalAiWanVace14bOutpaintingResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVace14bOutpaintingResponse =
  PostFalAiWanVace14bOutpaintingResponses[keyof PostFalAiWanVace14bOutpaintingResponses];

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/outpainting/requests/{request_id}";
};

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVace14bOutpaintingOutput;
};

export type GetFalAiWanVace14bOutpaintingRequestsByRequestIdResponse =
  GetFalAiWanVace14bOutpaintingRequestsByRequestIdResponses[keyof GetFalAiWanVace14bOutpaintingRequestsByRequestIdResponses];

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace-14b/inpainting/requests/{request_id}/status";
};

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bInpaintingRequestsByRequestIdStatusResponses];

export type PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/inpainting/requests/{request_id}/cancel";
};

export type PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bInpaintingRequestsByRequestIdCancelResponses];

export type PostFalAiWanVace14bInpaintingData = {
  body: WanVace14bInpaintingInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace-14b/inpainting";
};

export type PostFalAiWanVace14bInpaintingResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVace14bInpaintingResponse =
  PostFalAiWanVace14bInpaintingResponses[keyof PostFalAiWanVace14bInpaintingResponses];

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/inpainting/requests/{request_id}";
};

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVace14bInpaintingOutput;
};

export type GetFalAiWanVace14bInpaintingRequestsByRequestIdResponse =
  GetFalAiWanVace14bInpaintingRequestsByRequestIdResponses[keyof GetFalAiWanVace14bInpaintingRequestsByRequestIdResponses];

export type GetFalAiWanVace14bPoseRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace-14b/pose/requests/{request_id}/status";
};

export type GetFalAiWanVace14bPoseRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVace14bPoseRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bPoseRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bPoseRequestsByRequestIdStatusResponses];

export type PutFalAiWanVace14bPoseRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/pose/requests/{request_id}/cancel";
};

export type PutFalAiWanVace14bPoseRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVace14bPoseRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bPoseRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bPoseRequestsByRequestIdCancelResponses];

export type PostFalAiWanVace14bPoseData = {
  body: WanVace14bPoseInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace-14b/pose";
};

export type PostFalAiWanVace14bPoseResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVace14bPoseResponse =
  PostFalAiWanVace14bPoseResponses[keyof PostFalAiWanVace14bPoseResponses];

export type GetFalAiWanVace14bPoseRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/pose/requests/{request_id}";
};

export type GetFalAiWanVace14bPoseRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVace14bPoseOutput;
};

export type GetFalAiWanVace14bPoseRequestsByRequestIdResponse =
  GetFalAiWanVace14bPoseRequestsByRequestIdResponses[keyof GetFalAiWanVace14bPoseRequestsByRequestIdResponses];

export type GetFalAiWanVace14bDepthRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace-14b/depth/requests/{request_id}/status";
};

export type GetFalAiWanVace14bDepthRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVace14bDepthRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bDepthRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bDepthRequestsByRequestIdStatusResponses];

export type PutFalAiWanVace14bDepthRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/depth/requests/{request_id}/cancel";
};

export type PutFalAiWanVace14bDepthRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVace14bDepthRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bDepthRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bDepthRequestsByRequestIdCancelResponses];

export type PostFalAiWanVace14bDepthData = {
  body: WanVace14bDepthInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace-14b/depth";
};

export type PostFalAiWanVace14bDepthResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVace14bDepthResponse =
  PostFalAiWanVace14bDepthResponses[keyof PostFalAiWanVace14bDepthResponses];

export type GetFalAiWanVace14bDepthRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/depth/requests/{request_id}";
};

export type GetFalAiWanVace14bDepthRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVace14bDepthOutput;
};

export type GetFalAiWanVace14bDepthRequestsByRequestIdResponse =
  GetFalAiWanVace14bDepthRequestsByRequestIdResponses[keyof GetFalAiWanVace14bDepthRequestsByRequestIdResponses];

export type GetFalAiDwposeVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/dwpose/video/requests/{request_id}/status";
};

export type GetFalAiDwposeVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiDwposeVideoRequestsByRequestIdStatusResponse =
  GetFalAiDwposeVideoRequestsByRequestIdStatusResponses[keyof GetFalAiDwposeVideoRequestsByRequestIdStatusResponses];

export type PutFalAiDwposeVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/dwpose/video/requests/{request_id}/cancel";
};

export type PutFalAiDwposeVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiDwposeVideoRequestsByRequestIdCancelResponse =
  PutFalAiDwposeVideoRequestsByRequestIdCancelResponses[keyof PutFalAiDwposeVideoRequestsByRequestIdCancelResponses];

export type PostFalAiDwposeVideoData = {
  body: DwposeVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/dwpose/video";
};

export type PostFalAiDwposeVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiDwposeVideoResponse =
  PostFalAiDwposeVideoResponses[keyof PostFalAiDwposeVideoResponses];

export type GetFalAiDwposeVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/dwpose/video/requests/{request_id}";
};

export type GetFalAiDwposeVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: DwposeVideoOutput;
};

export type GetFalAiDwposeVideoRequestsByRequestIdResponse =
  GetFalAiDwposeVideoRequestsByRequestIdResponses[keyof GetFalAiDwposeVideoRequestsByRequestIdResponses];

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ffmpeg-api/merge-audio-video/requests/{request_id}/status";
};

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusResponse =
  GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdStatusResponses];

export type PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ffmpeg-api/merge-audio-video/requests/{request_id}/cancel";
};

export type PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelResponse =
  PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdCancelResponses];

export type PostFalAiFfmpegApiMergeAudioVideoData = {
  body: FfmpegApiMergeAudioVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ffmpeg-api/merge-audio-video";
};

export type PostFalAiFfmpegApiMergeAudioVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFfmpegApiMergeAudioVideoResponse =
  PostFalAiFfmpegApiMergeAudioVideoResponses[keyof PostFalAiFfmpegApiMergeAudioVideoResponses];

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ffmpeg-api/merge-audio-video/requests/{request_id}";
};

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FfmpegApiMergeAudioVideoOutput;
};

export type GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdResponse =
  GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdResponses[keyof GetFalAiFfmpegApiMergeAudioVideoRequestsByRequestIdResponses];

export type GetFalAiWanVace13bRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace-1-3b/requests/{request_id}/status";
};

export type GetFalAiWanVace13bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVace13bRequestsByRequestIdStatusResponse =
  GetFalAiWanVace13bRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace13bRequestsByRequestIdStatusResponses];

export type PutFalAiWanVace13bRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-1-3b/requests/{request_id}/cancel";
};

export type PutFalAiWanVace13bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVace13bRequestsByRequestIdCancelResponse =
  PutFalAiWanVace13bRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace13bRequestsByRequestIdCancelResponses];

export type PostFalAiWanVace13bData = {
  body: WanVace13bInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace-1-3b";
};

export type PostFalAiWanVace13bResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVace13bResponse =
  PostFalAiWanVace13bResponses[keyof PostFalAiWanVace13bResponses];

export type GetFalAiWanVace13bRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-1-3b/requests/{request_id}";
};

export type GetFalAiWanVace13bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVace13bOutput;
};

export type GetFalAiWanVace13bRequestsByRequestIdResponse =
  GetFalAiWanVace13bRequestsByRequestIdResponses[keyof GetFalAiWanVace13bRequestsByRequestIdResponses];

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/luma-dream-machine/ray-2-flash/reframe/requests/{request_id}/status";
  };

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdStatusResponses];

export type PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/luma-dream-machine/ray-2-flash/reframe/requests/{request_id}/cancel";
  };

export type PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdCancelResponses];

export type PostFalAiLumaDreamMachineRay2FlashReframeData = {
  body: LumaDreamMachineRay2FlashReframeInput;
  path?: never;
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2-flash/reframe";
};

export type PostFalAiLumaDreamMachineRay2FlashReframeResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLumaDreamMachineRay2FlashReframeResponse =
  PostFalAiLumaDreamMachineRay2FlashReframeResponses[keyof PostFalAiLumaDreamMachineRay2FlashReframeResponses];

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2-flash/reframe/requests/{request_id}";
};

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LumaDreamMachineRay2FlashReframeOutput;
  };

export type GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2FlashReframeRequestsByRequestIdResponses];

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/luma-dream-machine/ray-2/reframe/requests/{request_id}/status";
};

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusResponse =
  GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusResponses[keyof GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdStatusResponses];

export type PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/reframe/requests/{request_id}/cancel";
};

export type PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelResponse =
  PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelResponses[keyof PutFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdCancelResponses];

export type PostFalAiLumaDreamMachineRay2ReframeData = {
  body: LumaDreamMachineRay2ReframeInput;
  path?: never;
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/reframe";
};

export type PostFalAiLumaDreamMachineRay2ReframeResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLumaDreamMachineRay2ReframeResponse =
  PostFalAiLumaDreamMachineRay2ReframeResponses[keyof PostFalAiLumaDreamMachineRay2ReframeResponses];

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/luma-dream-machine/ray-2/reframe/requests/{request_id}";
};

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LumaDreamMachineRay2ReframeOutput;
};

export type GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdResponse =
  GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdResponses[keyof GetFalAiLumaDreamMachineRay2ReframeRequestsByRequestIdResponses];

export type GetVeedLipsyncRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/veed/lipsync/requests/{request_id}/status";
};

export type GetVeedLipsyncRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetVeedLipsyncRequestsByRequestIdStatusResponse =
  GetVeedLipsyncRequestsByRequestIdStatusResponses[keyof GetVeedLipsyncRequestsByRequestIdStatusResponses];

export type PutVeedLipsyncRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/lipsync/requests/{request_id}/cancel";
};

export type PutVeedLipsyncRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutVeedLipsyncRequestsByRequestIdCancelResponse =
  PutVeedLipsyncRequestsByRequestIdCancelResponses[keyof PutVeedLipsyncRequestsByRequestIdCancelResponses];

export type PostVeedLipsyncData = {
  body: LipsyncInput;
  path?: never;
  query?: never;
  url: "/veed/lipsync";
};

export type PostVeedLipsyncResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostVeedLipsyncResponse =
  PostVeedLipsyncResponses[keyof PostVeedLipsyncResponses];

export type GetVeedLipsyncRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/veed/lipsync/requests/{request_id}";
};

export type GetVeedLipsyncRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LipsyncOutput;
};

export type GetVeedLipsyncRequestsByRequestIdResponse =
  GetVeedLipsyncRequestsByRequestIdResponses[keyof GetVeedLipsyncRequestsByRequestIdResponses];

export type GetFalAiWanVace14bRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace-14b/requests/{request_id}/status";
};

export type GetFalAiWanVace14bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVace14bRequestsByRequestIdStatusResponse =
  GetFalAiWanVace14bRequestsByRequestIdStatusResponses[keyof GetFalAiWanVace14bRequestsByRequestIdStatusResponses];

export type PutFalAiWanVace14bRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/requests/{request_id}/cancel";
};

export type PutFalAiWanVace14bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVace14bRequestsByRequestIdCancelResponse =
  PutFalAiWanVace14bRequestsByRequestIdCancelResponses[keyof PutFalAiWanVace14bRequestsByRequestIdCancelResponses];

export type PostFalAiWanVace14bData = {
  body: WanVace14bInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace-14b";
};

export type PostFalAiWanVace14bResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVace14bResponse =
  PostFalAiWanVace14bResponses[keyof PostFalAiWanVace14bResponses];

export type GetFalAiWanVace14bRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace-14b/requests/{request_id}";
};

export type GetFalAiWanVace14bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVace14bOutput;
};

export type GetFalAiWanVace14bRequestsByRequestIdResponse =
  GetFalAiWanVace14bRequestsByRequestIdResponses[keyof GetFalAiWanVace14bRequestsByRequestIdResponses];

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video-13b-distilled/extend/requests/{request_id}/status";
};

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-distilled/extend/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDistilledExtendRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideo13bDistilledExtendData = {
  body: LtxVideo13bDistilledExtendInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-13b-distilled/extend";
};

export type PostFalAiLtxVideo13bDistilledExtendResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideo13bDistilledExtendResponse =
  PostFalAiLtxVideo13bDistilledExtendResponses[keyof PostFalAiLtxVideo13bDistilledExtendResponses];

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-distilled/extend/requests/{request_id}";
};

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideo13bDistilledExtendOutput;
};

export type GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDistilledExtendRequestsByRequestIdResponses];

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-video-13b-distilled/multiconditioning/requests/{request_id}/status";
  };

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-video-13b-distilled/multiconditioning/requests/{request_id}/cancel";
  };

export type PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideo13bDistilledMulticonditioningData = {
  body: LtxVideo13bDistilledMulticonditioningInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-13b-distilled/multiconditioning";
};

export type PostFalAiLtxVideo13bDistilledMulticonditioningResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideo13bDistilledMulticonditioningResponse =
  PostFalAiLtxVideo13bDistilledMulticonditioningResponses[keyof PostFalAiLtxVideo13bDistilledMulticonditioningResponses];

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-video-13b-distilled/multiconditioning/requests/{request_id}";
  };

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LtxVideo13bDistilledMulticonditioningOutput;
  };

export type GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDistilledMulticonditioningRequestsByRequestIdResponses];

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-video-13b-dev/multiconditioning/requests/{request_id}/status";
  };

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-video-13b-dev/multiconditioning/requests/{request_id}/cancel";
  };

export type PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideo13bDevMulticonditioningData = {
  body: LtxVideo13bDevMulticonditioningInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/multiconditioning";
};

export type PostFalAiLtxVideo13bDevMulticonditioningResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideo13bDevMulticonditioningResponse =
  PostFalAiLtxVideo13bDevMulticonditioningResponses[keyof PostFalAiLtxVideo13bDevMulticonditioningResponses];

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/multiconditioning/requests/{request_id}";
};

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LtxVideo13bDevMulticonditioningOutput;
  };

export type GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDevMulticonditioningRequestsByRequestIdResponses];

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video-13b-dev/extend/requests/{request_id}/status";
};

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideo13bDevExtendRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/extend/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideo13bDevExtendRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideo13bDevExtendData = {
  body: LtxVideo13bDevExtendInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/extend";
};

export type PostFalAiLtxVideo13bDevExtendResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideo13bDevExtendResponse =
  PostFalAiLtxVideo13bDevExtendResponses[keyof PostFalAiLtxVideo13bDevExtendResponses];

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-13b-dev/extend/requests/{request_id}";
};

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideo13bDevExtendOutput;
};

export type GetFalAiLtxVideo13bDevExtendRequestsByRequestIdResponse =
  GetFalAiLtxVideo13bDevExtendRequestsByRequestIdResponses[keyof GetFalAiLtxVideo13bDevExtendRequestsByRequestIdResponses];

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-video-lora/multiconditioning/requests/{request_id}/status";
  };

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-video-lora/multiconditioning/requests/{request_id}/cancel";
  };

export type PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideoLoraMulticonditioningData = {
  body: LtxVideoLoraMulticonditioningInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-lora/multiconditioning";
};

export type PostFalAiLtxVideoLoraMulticonditioningResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideoLoraMulticonditioningResponse =
  PostFalAiLtxVideoLoraMulticonditioningResponses[keyof PostFalAiLtxVideoLoraMulticonditioningResponses];

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-lora/multiconditioning/requests/{request_id}";
};

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LtxVideoLoraMulticonditioningOutput;
  };

export type GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxVideoLoraMulticonditioningRequestsByRequestIdResponses];

export type GetFalAiMagiExtendVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/magi/extend-video/requests/{request_id}/status";
};

export type GetFalAiMagiExtendVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiMagiExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiMagiExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMagiExtendVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMagiExtendVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi/extend-video/requests/{request_id}/cancel";
};

export type PutFalAiMagiExtendVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiMagiExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiMagiExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMagiExtendVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMagiExtendVideoData = {
  body: MagiExtendVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/magi/extend-video";
};

export type PostFalAiMagiExtendVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMagiExtendVideoResponse =
  PostFalAiMagiExtendVideoResponses[keyof PostFalAiMagiExtendVideoResponses];

export type GetFalAiMagiExtendVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi/extend-video/requests/{request_id}";
};

export type GetFalAiMagiExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MagiExtendVideoOutput;
};

export type GetFalAiMagiExtendVideoRequestsByRequestIdResponse =
  GetFalAiMagiExtendVideoRequestsByRequestIdResponses[keyof GetFalAiMagiExtendVideoRequestsByRequestIdResponses];

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/magi-distilled/extend-video/requests/{request_id}/status";
};

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusResponse =
  GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusResponses[keyof GetFalAiMagiDistilledExtendVideoRequestsByRequestIdStatusResponses];

export type PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi-distilled/extend-video/requests/{request_id}/cancel";
};

export type PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelResponse =
  PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelResponses[keyof PutFalAiMagiDistilledExtendVideoRequestsByRequestIdCancelResponses];

export type PostFalAiMagiDistilledExtendVideoData = {
  body: MagiDistilledExtendVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/magi-distilled/extend-video";
};

export type PostFalAiMagiDistilledExtendVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiMagiDistilledExtendVideoResponse =
  PostFalAiMagiDistilledExtendVideoResponses[keyof PostFalAiMagiDistilledExtendVideoResponses];

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/magi-distilled/extend-video/requests/{request_id}";
};

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: MagiDistilledExtendVideoOutput;
};

export type GetFalAiMagiDistilledExtendVideoRequestsByRequestIdResponse =
  GetFalAiMagiDistilledExtendVideoRequestsByRequestIdResponses[keyof GetFalAiMagiDistilledExtendVideoRequestsByRequestIdResponses];

export type GetFalAiWanVaceRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-vace/requests/{request_id}/status";
};

export type GetFalAiWanVaceRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanVaceRequestsByRequestIdStatusResponse =
  GetFalAiWanVaceRequestsByRequestIdStatusResponses[keyof GetFalAiWanVaceRequestsByRequestIdStatusResponses];

export type PutFalAiWanVaceRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace/requests/{request_id}/cancel";
};

export type PutFalAiWanVaceRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanVaceRequestsByRequestIdCancelResponse =
  PutFalAiWanVaceRequestsByRequestIdCancelResponses[keyof PutFalAiWanVaceRequestsByRequestIdCancelResponses];

export type PostFalAiWanVaceData = {
  body: WanVaceInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-vace";
};

export type PostFalAiWanVaceResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanVaceResponse =
  PostFalAiWanVaceResponses[keyof PostFalAiWanVaceResponses];

export type GetFalAiWanVaceRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-vace/requests/{request_id}";
};

export type GetFalAiWanVaceRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanVaceOutput;
};

export type GetFalAiWanVaceRequestsByRequestIdResponse =
  GetFalAiWanVaceRequestsByRequestIdResponses[keyof GetFalAiWanVaceRequestsByRequestIdResponses];

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/cassetteai/video-sound-effects-generator/requests/{request_id}/status";
  };

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusResponse =
  GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusResponses[keyof GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdStatusResponses];

export type PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/cassetteai/video-sound-effects-generator/requests/{request_id}/cancel";
  };

export type PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelResponse =
  PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelResponses[keyof PutCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdCancelResponses];

export type PostCassetteaiVideoSoundEffectsGeneratorData = {
  body: VideoSoundEffectsGeneratorInput;
  path?: never;
  query?: never;
  url: "/cassetteai/video-sound-effects-generator";
};

export type PostCassetteaiVideoSoundEffectsGeneratorResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostCassetteaiVideoSoundEffectsGeneratorResponse =
  PostCassetteaiVideoSoundEffectsGeneratorResponses[keyof PostCassetteaiVideoSoundEffectsGeneratorResponses];

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/cassetteai/video-sound-effects-generator/requests/{request_id}";
};

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: VideoSoundEffectsGeneratorOutput;
  };

export type GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdResponse =
  GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdResponses[keyof GetCassetteaiVideoSoundEffectsGeneratorRequestsByRequestIdResponses];

export type GetFalAiSyncLipsyncV2RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sync-lipsync/v2/requests/{request_id}/status";
};

export type GetFalAiSyncLipsyncV2RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSyncLipsyncV2RequestsByRequestIdStatusResponse =
  GetFalAiSyncLipsyncV2RequestsByRequestIdStatusResponses[keyof GetFalAiSyncLipsyncV2RequestsByRequestIdStatusResponses];

export type PutFalAiSyncLipsyncV2RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sync-lipsync/v2/requests/{request_id}/cancel";
};

export type PutFalAiSyncLipsyncV2RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSyncLipsyncV2RequestsByRequestIdCancelResponse =
  PutFalAiSyncLipsyncV2RequestsByRequestIdCancelResponses[keyof PutFalAiSyncLipsyncV2RequestsByRequestIdCancelResponses];

export type PostFalAiSyncLipsyncV2Data = {
  body: SyncLipsyncV2Input;
  path?: never;
  query?: never;
  url: "/fal-ai/sync-lipsync/v2";
};

export type PostFalAiSyncLipsyncV2Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSyncLipsyncV2Response =
  PostFalAiSyncLipsyncV2Responses[keyof PostFalAiSyncLipsyncV2Responses];

export type GetFalAiSyncLipsyncV2RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sync-lipsync/v2/requests/{request_id}";
};

export type GetFalAiSyncLipsyncV2RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SyncLipsyncV2Output;
};

export type GetFalAiSyncLipsyncV2RequestsByRequestIdResponse =
  GetFalAiSyncLipsyncV2RequestsByRequestIdResponses[keyof GetFalAiSyncLipsyncV2RequestsByRequestIdResponses];

export type GetFalAiLatentsyncRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/latentsync/requests/{request_id}/status";
};

export type GetFalAiLatentsyncRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLatentsyncRequestsByRequestIdStatusResponse =
  GetFalAiLatentsyncRequestsByRequestIdStatusResponses[keyof GetFalAiLatentsyncRequestsByRequestIdStatusResponses];

export type PutFalAiLatentsyncRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/latentsync/requests/{request_id}/cancel";
};

export type PutFalAiLatentsyncRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLatentsyncRequestsByRequestIdCancelResponse =
  PutFalAiLatentsyncRequestsByRequestIdCancelResponses[keyof PutFalAiLatentsyncRequestsByRequestIdCancelResponses];

export type PostFalAiLatentsyncData = {
  body: LatentsyncInput;
  path?: never;
  query?: never;
  url: "/fal-ai/latentsync";
};

export type PostFalAiLatentsyncResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLatentsyncResponse =
  PostFalAiLatentsyncResponses[keyof PostFalAiLatentsyncResponses];

export type GetFalAiLatentsyncRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/latentsync/requests/{request_id}";
};

export type GetFalAiLatentsyncRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LatentsyncOutput;
};

export type GetFalAiLatentsyncRequestsByRequestIdResponse =
  GetFalAiLatentsyncRequestsByRequestIdResponses[keyof GetFalAiLatentsyncRequestsByRequestIdResponses];

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/pika/v2/pikadditions/requests/{request_id}/status";
};

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusResponse =
  GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusResponses[keyof GetFalAiPikaV2PikadditionsRequestsByRequestIdStatusResponses];

export type PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2/pikadditions/requests/{request_id}/cancel";
};

export type PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelResponse =
  PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelResponses[keyof PutFalAiPikaV2PikadditionsRequestsByRequestIdCancelResponses];

export type PostFalAiPikaV2PikadditionsData = {
  body: PikaV2PikadditionsInput;
  path?: never;
  query?: never;
  url: "/fal-ai/pika/v2/pikadditions";
};

export type PostFalAiPikaV2PikadditionsResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiPikaV2PikadditionsResponse =
  PostFalAiPikaV2PikadditionsResponses[keyof PostFalAiPikaV2PikadditionsResponses];

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/pika/v2/pikadditions/requests/{request_id}";
};

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: PikaV2PikadditionsOutput;
};

export type GetFalAiPikaV2PikadditionsRequestsByRequestIdResponse =
  GetFalAiPikaV2PikadditionsRequestsByRequestIdResponses[keyof GetFalAiPikaV2PikadditionsRequestsByRequestIdResponses];

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video-v095/extend/requests/{request_id}/status";
};

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoV095ExtendRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-v095/extend/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoV095ExtendRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideoV095ExtendData = {
  body: LtxVideoV095ExtendInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-v095/extend";
};

export type PostFalAiLtxVideoV095ExtendResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideoV095ExtendResponse =
  PostFalAiLtxVideoV095ExtendResponses[keyof PostFalAiLtxVideoV095ExtendResponses];

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-v095/extend/requests/{request_id}";
};

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideoV095ExtendOutput;
};

export type GetFalAiLtxVideoV095ExtendRequestsByRequestIdResponse =
  GetFalAiLtxVideoV095ExtendRequestsByRequestIdResponses[keyof GetFalAiLtxVideoV095ExtendRequestsByRequestIdResponses];

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/ltx-video-v095/multiconditioning/requests/{request_id}/status";
  };

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/ltx-video-v095/multiconditioning/requests/{request_id}/cancel";
  };

export type PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoV095MulticonditioningRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideoV095MulticonditioningData = {
  body: LtxVideoV095MulticonditioningInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-v095/multiconditioning";
};

export type PostFalAiLtxVideoV095MulticonditioningResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideoV095MulticonditioningResponse =
  PostFalAiLtxVideoV095MulticonditioningResponses[keyof PostFalAiLtxVideoV095MulticonditioningResponses];

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-v095/multiconditioning/requests/{request_id}";
};

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: LtxVideoV095MulticonditioningOutput;
  };

export type GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdResponse =
  GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdResponses[keyof GetFalAiLtxVideoV095MulticonditioningRequestsByRequestIdResponses];

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/topaz/upscale/video/requests/{request_id}/status";
};

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusResponse =
  GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusResponses[keyof GetFalAiTopazUpscaleVideoRequestsByRequestIdStatusResponses];

export type PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/topaz/upscale/video/requests/{request_id}/cancel";
};

export type PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelResponse =
  PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelResponses[keyof PutFalAiTopazUpscaleVideoRequestsByRequestIdCancelResponses];

export type PostFalAiTopazUpscaleVideoData = {
  body: TopazUpscaleVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/topaz/upscale/video";
};

export type PostFalAiTopazUpscaleVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiTopazUpscaleVideoResponse =
  PostFalAiTopazUpscaleVideoResponses[keyof PostFalAiTopazUpscaleVideoResponses];

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/topaz/upscale/video/requests/{request_id}";
};

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: TopazUpscaleVideoOutput;
};

export type GetFalAiTopazUpscaleVideoRequestsByRequestIdResponse =
  GetFalAiTopazUpscaleVideoRequestsByRequestIdResponses[keyof GetFalAiTopazUpscaleVideoRequestsByRequestIdResponses];

export type GetFalAiBenV2VideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ben/v2/video/requests/{request_id}/status";
};

export type GetFalAiBenV2VideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiBenV2VideoRequestsByRequestIdStatusResponse =
  GetFalAiBenV2VideoRequestsByRequestIdStatusResponses[keyof GetFalAiBenV2VideoRequestsByRequestIdStatusResponses];

export type PutFalAiBenV2VideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ben/v2/video/requests/{request_id}/cancel";
};

export type PutFalAiBenV2VideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiBenV2VideoRequestsByRequestIdCancelResponse =
  PutFalAiBenV2VideoRequestsByRequestIdCancelResponses[keyof PutFalAiBenV2VideoRequestsByRequestIdCancelResponses];

export type PostFalAiBenV2VideoData = {
  body: BenV2VideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ben/v2/video";
};

export type PostFalAiBenV2VideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiBenV2VideoResponse =
  PostFalAiBenV2VideoResponses[keyof PostFalAiBenV2VideoResponses];

export type GetFalAiBenV2VideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ben/v2/video/requests/{request_id}";
};

export type GetFalAiBenV2VideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: BenV2VideoOutput;
};

export type GetFalAiBenV2VideoRequestsByRequestIdResponse =
  GetFalAiBenV2VideoRequestsByRequestIdResponses[keyof GetFalAiBenV2VideoRequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/hunyuan-video-lora/video-to-video/requests/{request_id}/status";
  };

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/hunyuan-video-lora/video-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoLoraVideoToVideoData = {
  body: HunyuanVideoLoraVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video-lora/video-to-video";
};

export type PostFalAiHunyuanVideoLoraVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoLoraVideoToVideoResponse =
  PostFalAiHunyuanVideoLoraVideoToVideoResponses[keyof PostFalAiHunyuanVideoLoraVideoToVideoResponses];

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-lora/video-to-video/requests/{request_id}";
};

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoLoraVideoToVideoOutput;
};

export type GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoLoraVideoToVideoRequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-video/video-to-video/requests/{request_id}/status";
};

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video/video-to-video/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoVideoToVideoData = {
  body: HunyuanVideoVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video/video-to-video";
};

export type PostFalAiHunyuanVideoVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoVideoToVideoResponse =
  PostFalAiHunyuanVideoVideoToVideoResponses[keyof PostFalAiHunyuanVideoVideoToVideoResponses];

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video/video-to-video/requests/{request_id}";
};

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoVideoToVideoOutput;
};

export type GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoVideoToVideoRequestsByRequestIdResponses];

export type GetFalAiFfmpegApiComposeRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ffmpeg-api/compose/requests/{request_id}/status";
};

export type GetFalAiFfmpegApiComposeRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFfmpegApiComposeRequestsByRequestIdStatusResponse =
  GetFalAiFfmpegApiComposeRequestsByRequestIdStatusResponses[keyof GetFalAiFfmpegApiComposeRequestsByRequestIdStatusResponses];

export type PutFalAiFfmpegApiComposeRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ffmpeg-api/compose/requests/{request_id}/cancel";
};

export type PutFalAiFfmpegApiComposeRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFfmpegApiComposeRequestsByRequestIdCancelResponse =
  PutFalAiFfmpegApiComposeRequestsByRequestIdCancelResponses[keyof PutFalAiFfmpegApiComposeRequestsByRequestIdCancelResponses];

export type PostFalAiFfmpegApiComposeData = {
  body: FfmpegApiComposeInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ffmpeg-api/compose";
};

export type PostFalAiFfmpegApiComposeResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFfmpegApiComposeResponse =
  PostFalAiFfmpegApiComposeResponses[keyof PostFalAiFfmpegApiComposeResponses];

export type GetFalAiFfmpegApiComposeRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ffmpeg-api/compose/requests/{request_id}";
};

export type GetFalAiFfmpegApiComposeRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FfmpegApiComposeOutput;
};

export type GetFalAiFfmpegApiComposeRequestsByRequestIdResponse =
  GetFalAiFfmpegApiComposeRequestsByRequestIdResponses[keyof GetFalAiFfmpegApiComposeRequestsByRequestIdResponses];

export type GetFalAiSyncLipsyncRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sync-lipsync/requests/{request_id}/status";
};

export type GetFalAiSyncLipsyncRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSyncLipsyncRequestsByRequestIdStatusResponse =
  GetFalAiSyncLipsyncRequestsByRequestIdStatusResponses[keyof GetFalAiSyncLipsyncRequestsByRequestIdStatusResponses];

export type PutFalAiSyncLipsyncRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sync-lipsync/requests/{request_id}/cancel";
};

export type PutFalAiSyncLipsyncRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSyncLipsyncRequestsByRequestIdCancelResponse =
  PutFalAiSyncLipsyncRequestsByRequestIdCancelResponses[keyof PutFalAiSyncLipsyncRequestsByRequestIdCancelResponses];

export type PostFalAiSyncLipsyncData = {
  body: SyncLipsyncInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sync-lipsync";
};

export type PostFalAiSyncLipsyncResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSyncLipsyncResponse =
  PostFalAiSyncLipsyncResponses[keyof PostFalAiSyncLipsyncResponses];

export type GetFalAiSyncLipsyncRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sync-lipsync/requests/{request_id}";
};

export type GetFalAiSyncLipsyncRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SyncLipsyncOutput;
};

export type GetFalAiSyncLipsyncRequestsByRequestIdResponse =
  GetFalAiSyncLipsyncRequestsByRequestIdResponses[keyof GetFalAiSyncLipsyncRequestsByRequestIdResponses];

export type GetFalAiAutoCaptionRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/auto-caption/requests/{request_id}/status";
};

export type GetFalAiAutoCaptionRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiAutoCaptionRequestsByRequestIdStatusResponse =
  GetFalAiAutoCaptionRequestsByRequestIdStatusResponses[keyof GetFalAiAutoCaptionRequestsByRequestIdStatusResponses];

export type PutFalAiAutoCaptionRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/auto-caption/requests/{request_id}/cancel";
};

export type PutFalAiAutoCaptionRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiAutoCaptionRequestsByRequestIdCancelResponse =
  PutFalAiAutoCaptionRequestsByRequestIdCancelResponses[keyof PutFalAiAutoCaptionRequestsByRequestIdCancelResponses];

export type PostFalAiAutoCaptionData = {
  body: AutoCaptionInput;
  path?: never;
  query?: never;
  url: "/fal-ai/auto-caption";
};

export type PostFalAiAutoCaptionResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiAutoCaptionResponse =
  PostFalAiAutoCaptionResponses[keyof PostFalAiAutoCaptionResponses];

export type GetFalAiAutoCaptionRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/auto-caption/requests/{request_id}";
};

export type GetFalAiAutoCaptionRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AutoCaptionOutput;
};

export type GetFalAiAutoCaptionRequestsByRequestIdResponse =
  GetFalAiAutoCaptionRequestsByRequestIdResponses[keyof GetFalAiAutoCaptionRequestsByRequestIdResponses];

export type GetFalAiDubbingRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/dubbing/requests/{request_id}/status";
};

export type GetFalAiDubbingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiDubbingRequestsByRequestIdStatusResponse =
  GetFalAiDubbingRequestsByRequestIdStatusResponses[keyof GetFalAiDubbingRequestsByRequestIdStatusResponses];

export type PutFalAiDubbingRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/dubbing/requests/{request_id}/cancel";
};

export type PutFalAiDubbingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiDubbingRequestsByRequestIdCancelResponse =
  PutFalAiDubbingRequestsByRequestIdCancelResponses[keyof PutFalAiDubbingRequestsByRequestIdCancelResponses];

export type PostFalAiDubbingData = {
  body: DubbingInput;
  path?: never;
  query?: never;
  url: "/fal-ai/dubbing";
};

export type PostFalAiDubbingResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiDubbingResponse =
  PostFalAiDubbingResponses[keyof PostFalAiDubbingResponses];

export type GetFalAiDubbingRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/dubbing/requests/{request_id}";
};

export type GetFalAiDubbingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: DubbingOutput;
};

export type GetFalAiDubbingRequestsByRequestIdResponse =
  GetFalAiDubbingRequestsByRequestIdResponses[keyof GetFalAiDubbingRequestsByRequestIdResponses];

export type GetFalAiVideoUpscalerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/video-upscaler/requests/{request_id}/status";
};

export type GetFalAiVideoUpscalerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiVideoUpscalerRequestsByRequestIdStatusResponse =
  GetFalAiVideoUpscalerRequestsByRequestIdStatusResponses[keyof GetFalAiVideoUpscalerRequestsByRequestIdStatusResponses];

export type PutFalAiVideoUpscalerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/video-upscaler/requests/{request_id}/cancel";
};

export type PutFalAiVideoUpscalerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiVideoUpscalerRequestsByRequestIdCancelResponse =
  PutFalAiVideoUpscalerRequestsByRequestIdCancelResponses[keyof PutFalAiVideoUpscalerRequestsByRequestIdCancelResponses];

export type PostFalAiVideoUpscalerData = {
  body: VideoUpscalerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/video-upscaler";
};

export type PostFalAiVideoUpscalerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiVideoUpscalerResponse =
  PostFalAiVideoUpscalerResponses[keyof PostFalAiVideoUpscalerResponses];

export type GetFalAiVideoUpscalerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/video-upscaler/requests/{request_id}";
};

export type GetFalAiVideoUpscalerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: VideoUpscalerOutput;
};

export type GetFalAiVideoUpscalerRequestsByRequestIdResponse =
  GetFalAiVideoUpscalerRequestsByRequestIdResponses[keyof GetFalAiVideoUpscalerRequestsByRequestIdResponses];

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/cogvideox-5b/video-to-video/requests/{request_id}/status";
};

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/cogvideox-5b/video-to-video/requests/{request_id}/cancel";
};

export type PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiCogvideox5bVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiCogvideox5bVideoToVideoData = {
  body: Cogvideox5bVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/cogvideox-5b/video-to-video";
};

export type PostFalAiCogvideox5bVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiCogvideox5bVideoToVideoResponse =
  PostFalAiCogvideox5bVideoToVideoResponses[keyof PostFalAiCogvideox5bVideoToVideoResponses];

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/cogvideox-5b/video-to-video/requests/{request_id}";
};

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Cogvideox5bVideoToVideoOutput;
};

export type GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdResponse =
  GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiCogvideox5bVideoToVideoRequestsByRequestIdResponses];

export type GetFalAiControlnextRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/controlnext/requests/{request_id}/status";
};

export type GetFalAiControlnextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiControlnextRequestsByRequestIdStatusResponse =
  GetFalAiControlnextRequestsByRequestIdStatusResponses[keyof GetFalAiControlnextRequestsByRequestIdStatusResponses];

export type PutFalAiControlnextRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/controlnext/requests/{request_id}/cancel";
};

export type PutFalAiControlnextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiControlnextRequestsByRequestIdCancelResponse =
  PutFalAiControlnextRequestsByRequestIdCancelResponses[keyof PutFalAiControlnextRequestsByRequestIdCancelResponses];

export type PostFalAiControlnextData = {
  body: ControlnextInput;
  path?: never;
  query?: never;
  url: "/fal-ai/controlnext";
};

export type PostFalAiControlnextResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiControlnextResponse =
  PostFalAiControlnextResponses[keyof PostFalAiControlnextResponses];

export type GetFalAiControlnextRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/controlnext/requests/{request_id}";
};

export type GetFalAiControlnextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ControlnextOutput;
};

export type GetFalAiControlnextRequestsByRequestIdResponse =
  GetFalAiControlnextRequestsByRequestIdResponses[keyof GetFalAiControlnextRequestsByRequestIdResponses];

export type GetFalAiSam2VideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/sam2/video/requests/{request_id}/status";
};

export type GetFalAiSam2VideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiSam2VideoRequestsByRequestIdStatusResponse =
  GetFalAiSam2VideoRequestsByRequestIdStatusResponses[keyof GetFalAiSam2VideoRequestsByRequestIdStatusResponses];

export type PutFalAiSam2VideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sam2/video/requests/{request_id}/cancel";
};

export type PutFalAiSam2VideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiSam2VideoRequestsByRequestIdCancelResponse =
  PutFalAiSam2VideoRequestsByRequestIdCancelResponses[keyof PutFalAiSam2VideoRequestsByRequestIdCancelResponses];

export type PostFalAiSam2VideoData = {
  body: Sam2VideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/sam2/video";
};

export type PostFalAiSam2VideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiSam2VideoResponse =
  PostFalAiSam2VideoResponses[keyof PostFalAiSam2VideoResponses];

export type GetFalAiSam2VideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/sam2/video/requests/{request_id}";
};

export type GetFalAiSam2VideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Sam2VideoOutput;
};

export type GetFalAiSam2VideoRequestsByRequestIdResponse =
  GetFalAiSam2VideoRequestsByRequestIdResponses[keyof GetFalAiSam2VideoRequestsByRequestIdResponses];

export type GetFalAiAmtInterpolationRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/amt-interpolation/requests/{request_id}/status";
};

export type GetFalAiAmtInterpolationRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiAmtInterpolationRequestsByRequestIdStatusResponse =
  GetFalAiAmtInterpolationRequestsByRequestIdStatusResponses[keyof GetFalAiAmtInterpolationRequestsByRequestIdStatusResponses];

export type PutFalAiAmtInterpolationRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/amt-interpolation/requests/{request_id}/cancel";
};

export type PutFalAiAmtInterpolationRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiAmtInterpolationRequestsByRequestIdCancelResponse =
  PutFalAiAmtInterpolationRequestsByRequestIdCancelResponses[keyof PutFalAiAmtInterpolationRequestsByRequestIdCancelResponses];

export type PostFalAiAmtInterpolationData = {
  body: AmtInterpolationInput;
  path?: never;
  query?: never;
  url: "/fal-ai/amt-interpolation";
};

export type PostFalAiAmtInterpolationResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiAmtInterpolationResponse =
  PostFalAiAmtInterpolationResponses[keyof PostFalAiAmtInterpolationResponses];

export type GetFalAiAmtInterpolationRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/amt-interpolation/requests/{request_id}";
};

export type GetFalAiAmtInterpolationRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: AmtInterpolationOutput;
};

export type GetFalAiAmtInterpolationRequestsByRequestIdResponse =
  GetFalAiAmtInterpolationRequestsByRequestIdResponses[keyof GetFalAiAmtInterpolationRequestsByRequestIdResponses];

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number;
    };
    url: "/fal-ai/fast-animatediff/turbo/video-to-video/requests/{request_id}/status";
  };

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelData =
  {
    body?: never;
    path: {
      /**
       * Request ID
       */
      request_id: string;
    };
    query?: never;
    url: "/fal-ai/fast-animatediff/turbo/video-to-video/requests/{request_id}/cancel";
  };

export type PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiFastAnimatediffTurboVideoToVideoData = {
  body: FastAnimatediffTurboVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/fast-animatediff/turbo/video-to-video";
};

export type PostFalAiFastAnimatediffTurboVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFastAnimatediffTurboVideoToVideoResponse =
  PostFalAiFastAnimatediffTurboVideoToVideoResponses[keyof PostFalAiFastAnimatediffTurboVideoToVideoResponses];

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-animatediff/turbo/video-to-video/requests/{request_id}";
};

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: FastAnimatediffTurboVideoToVideoOutput;
  };

export type GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdResponse =
  GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiFastAnimatediffTurboVideoToVideoRequestsByRequestIdResponses];

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/fast-animatediff/video-to-video/requests/{request_id}/status";
};

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusResponse =
  GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusResponses[keyof GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdStatusResponses];

export type PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-animatediff/video-to-video/requests/{request_id}/cancel";
};

export type PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelResponse =
  PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelResponses[keyof PutFalAiFastAnimatediffVideoToVideoRequestsByRequestIdCancelResponses];

export type PostFalAiFastAnimatediffVideoToVideoData = {
  body: FastAnimatediffVideoToVideoInput;
  path?: never;
  query?: never;
  url: "/fal-ai/fast-animatediff/video-to-video";
};

export type PostFalAiFastAnimatediffVideoToVideoResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFastAnimatediffVideoToVideoResponse =
  PostFalAiFastAnimatediffVideoToVideoResponses[keyof PostFalAiFastAnimatediffVideoToVideoResponses];

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/fast-animatediff/video-to-video/requests/{request_id}";
};

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FastAnimatediffVideoToVideoOutput;
};

export type GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdResponse =
  GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdResponses[keyof GetFalAiFastAnimatediffVideoToVideoRequestsByRequestIdResponses];
