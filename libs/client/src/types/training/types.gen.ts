// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: "https://queue.fal.run" | (string & {});
};

/**
 * Output
 */
export type HunyuanVideoLoraTrainingOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * File
 */
export type File = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number | unknown;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string | unknown;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string | unknown;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
};

/**
 * PublicInput
 */
export type HunyuanVideoLoraTrainingInput = {
  /**
   * Trigger Word
   *
   * The trigger word to use.
   */
  trigger_word?: string;
  /**
   * Images Data Url
   *
   *
   * URL to zip archive with images. Try to use at least 4 images in general the more the better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
   *
   */
  images_data_url: string;
  /**
   * Steps
   *
   * Number of steps to train the LoRA on.
   */
  steps: number;
  /**
   * Data Archive Format
   *
   * The format of the archive. If not specified, the format will be inferred from the URL.
   */
  data_archive_format?: string | unknown | null;
  /**
   * Learning Rate
   *
   * Learning rate to use for training.
   */
  learning_rate?: number;
  /**
   * Do Caption
   *
   * Whether to generate captions for the images.
   */
  do_caption?: boolean;
};

/**
 * Output
 */
export type WanTrainerOutput = {
  /**
   * Lora File
   *
   * URL to the trained LoRA weights.
   */
  lora_file: FileType2;
  /**
   * Config File
   *
   * Configuration used for setting up the inference endpoints.
   */
  config_file: FileType2;
};

/**
 * File
 */
export type FileType2 = {
  /**
   * File Size
   *
   * The size of the file in bytes.
   */
  file_size?: number;
  /**
   * File Name
   *
   * The name of the file. It will be auto-generated if not provided.
   */
  file_name?: string;
  /**
   * Content Type
   *
   * The mime type of the file.
   */
  content_type?: string;
  /**
   * Url
   *
   * The URL where the file can be downloaded from.
   */
  url: string;
  /**
   * File Data
   *
   * File data
   */
  file_data?: Blob | File;
};

/**
 * Input
 */
export type WanTrainerInput = {
  /**
   * Number Of Steps
   *
   * The number of steps to train for.
   */
  number_of_steps?: number;
  /**
   * Training Data URL
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string;
  /**
   * Trigger Phrase
   *
   * The phrase that will trigger the model to generate an image.
   */
  trigger_phrase?: string;
  /**
   * Learning Rate
   *
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
   */
  learning_rate?: number;
  /**
   * Auto-Scale Input
   *
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};

/**
 * Output
 */
export type TurboFluxTrainerOutput = {
  /**
   * Config File
   *
   * URL to the trained diffusers config file.
   */
  config_file: FileType2;
  /**
   * Diffusers Lora File
   *
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: FileType2;
};

/**
 * Input
 */
export type TurboFluxTrainerInput = {
  /**
   * Images Data Url
   *
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   */
  images_data_url: string;
  /**
   * Trigger Phrase
   *
   * Trigger phrase to be used in the captions. If None, a trigger word will not be used.
   * If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.
   *
   */
  trigger_phrase?: string;
  /**
   * Steps
   *
   * Number of steps to train the LoRA on.
   */
  steps?: number;
  /**
   * Learning Rate
   *
   * Learning rate for the training.
   */
  learning_rate?: number;
  /**
   * Training Style
   *
   * Training style to use.
   */
  training_style?: "subject" | "style";
  /**
   * Face Crop
   *
   * Whether to try to detect the face and crop the images to the face.
   */
  face_crop?: boolean;
};

/**
 * StyleReferenceOutput
 */
export type RecraftV3CreateStyleOutput = {
  /**
   * Style Id
   *
   * The ID of the created style, this ID can be used to reference the style in the future.
   */
  style_id: string;
};

/**
 * StyleReferenceInput
 */
export type RecraftV3CreateStyleInput = {
  /**
   * Images Data Url
   *
   * URL to zip archive with images, use PNG format. Maximum 5 images are allowed.
   */
  images_data_url: string;
  /**
   * Base Style
   *
   * The base style of the generated images, this topic is covered above.
   */
  base_style?:
    | "any"
    | "realistic_image"
    | "digital_illustration"
    | "vector_illustration"
    | "realistic_image/b_and_w"
    | "realistic_image/hard_flash"
    | "realistic_image/hdr"
    | "realistic_image/natural_light"
    | "realistic_image/studio_portrait"
    | "realistic_image/enterprise"
    | "realistic_image/motion_blur"
    | "realistic_image/evening_light"
    | "realistic_image/faded_nostalgia"
    | "realistic_image/forest_life"
    | "realistic_image/mystic_naturalism"
    | "realistic_image/natural_tones"
    | "realistic_image/organic_calm"
    | "realistic_image/real_life_glow"
    | "realistic_image/retro_realism"
    | "realistic_image/retro_snapshot"
    | "realistic_image/urban_drama"
    | "realistic_image/village_realism"
    | "realistic_image/warm_folk"
    | "digital_illustration/pixel_art"
    | "digital_illustration/hand_drawn"
    | "digital_illustration/grain"
    | "digital_illustration/infantile_sketch"
    | "digital_illustration/2d_art_poster"
    | "digital_illustration/handmade_3d"
    | "digital_illustration/hand_drawn_outline"
    | "digital_illustration/engraving_color"
    | "digital_illustration/2d_art_poster_2"
    | "digital_illustration/antiquarian"
    | "digital_illustration/bold_fantasy"
    | "digital_illustration/child_book"
    | "digital_illustration/child_books"
    | "digital_illustration/cover"
    | "digital_illustration/crosshatch"
    | "digital_illustration/digital_engraving"
    | "digital_illustration/expressionism"
    | "digital_illustration/freehand_details"
    | "digital_illustration/grain_20"
    | "digital_illustration/graphic_intensity"
    | "digital_illustration/hard_comics"
    | "digital_illustration/long_shadow"
    | "digital_illustration/modern_folk"
    | "digital_illustration/multicolor"
    | "digital_illustration/neon_calm"
    | "digital_illustration/noir"
    | "digital_illustration/nostalgic_pastel"
    | "digital_illustration/outline_details"
    | "digital_illustration/pastel_gradient"
    | "digital_illustration/pastel_sketch"
    | "digital_illustration/pop_art"
    | "digital_illustration/pop_renaissance"
    | "digital_illustration/street_art"
    | "digital_illustration/tablet_sketch"
    | "digital_illustration/urban_glow"
    | "digital_illustration/urban_sketching"
    | "digital_illustration/vanilla_dreams"
    | "digital_illustration/young_adult_book"
    | "digital_illustration/young_adult_book_2"
    | "vector_illustration/bold_stroke"
    | "vector_illustration/chemistry"
    | "vector_illustration/colored_stencil"
    | "vector_illustration/contour_pop_art"
    | "vector_illustration/cosmics"
    | "vector_illustration/cutout"
    | "vector_illustration/depressive"
    | "vector_illustration/editorial"
    | "vector_illustration/emotional_flat"
    | "vector_illustration/infographical"
    | "vector_illustration/marker_outline"
    | "vector_illustration/mosaic"
    | "vector_illustration/naivector"
    | "vector_illustration/roundish_flat"
    | "vector_illustration/segmented_colors"
    | "vector_illustration/sharp_contrast"
    | "vector_illustration/thin"
    | "vector_illustration/vector_photo"
    | "vector_illustration/vivid_shapes"
    | "vector_illustration/engraving"
    | "vector_illustration/line_art"
    | "vector_illustration/line_circuit"
    | "vector_illustration/linocut";
};

/**
 * TrainingOutput
 */
export type LtxVideoTrainerOutput = {
  lora_file: File;
  config_file: File;
  /**
   * The URL to the validations video.
   */
  video: File | unknown;
};

/**
 * Input
 */
export type LtxVideoTrainerInput = {
  /**
   * Number Of Steps
   *
   * The number of steps to train for.
   */
  number_of_steps?: number;
  /**
   * Frame Rate
   *
   * The target frames per second for the video.
   */
  frame_rate?: number;
  /**
   * Learning Rate
   *
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
   */
  learning_rate?: number;
  /**
   * Validation
   *
   * A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.
   */
  validation?: Array<Validation>;
  /**
   * Number Of Frames
   *
   * The number of frames to use for training. This is the number of frames per second multiplied by the number of seconds.
   */
  number_of_frames?: number;
  /**
   * Validation Reverse
   *
   * If true, the validation videos will be reversed. This is useful for effects that are learned in reverse and then applied in reverse.
   */
  validation_reverse?: boolean;
  /**
   * Training Data Url
   *
   * URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
   *
   * **Supported video formats:** .mp4, .mov, .avi, .mkv
   * **Supported image formats:** .png, .jpg, .jpeg
   *
   * Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
   *
   * The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
   */
  training_data_url: string;
  /**
   * Split Input Duration Threshold
   *
   * The duration threshold in seconds. If a video is longer than this, it will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned.
   */
  split_input_duration_threshold?: number;
  /**
   * Rank
   *
   * The rank of the LoRA.
   */
  rank?: 8 | 16 | 32 | 64 | 128;
  /**
   * Aspect Ratio
   *
   * The aspect ratio to use for training. This is the aspect ratio of the video.
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * Trigger Phrase
   *
   * The phrase that will trigger the model to generate an image.
   */
  trigger_phrase?: string;
  /**
   * Resolution
   *
   * The resolution to use for training. This is the resolution of the video.
   */
  resolution?: "low" | "medium" | "high";
  /**
   * Split Input Into Scenes
   *
   * If true, videos above a certain duration threshold will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned. This option has no effect on image datasets.
   */
  split_input_into_scenes?: boolean;
  /**
   * Validation Resolution
   *
   * The resolution to use for validation.
   */
  validation_resolution?: "low" | "medium" | "high";
  /**
   * Validation Number Of Frames
   *
   * The number of frames to use for validation.
   */
  validation_number_of_frames?: number;
  /**
   * Validation Aspect Ratio
   *
   * The aspect ratio to use for validation.
   */
  validation_aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * Validation Negative Prompt
   *
   * A negative prompt to use for validation.
   */
  validation_negative_prompt?: string;
  /**
   * Auto Scale Input
   *
   * If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
   */
  auto_scale_input?: boolean;
};

/**
 * Validation
 */
export type Validation = {
  /**
   * Prompt
   *
   * The prompt to use for validation.
   */
  prompt: string;
  /**
   * Image Url
   *
   * An image to use for image-to-video validation. If provided for one validation, _all_ validation inputs must have an image.
   */
  image_url?: string | unknown;
};

/**
 * Output
 */
export type WanTrainerFlf2V720pOutput = {
  /**
   * Lora File
   *
   * URL to the trained LoRA weights.
   */
  lora_file: FileType2;
  /**
   * Config File
   *
   * Configuration used for setting up the inference endpoints.
   */
  config_file: FileType2;
};

/**
 * Input
 */
export type WanTrainerFlf2V720pInput = {
  /**
   * Number Of Steps
   *
   * The number of steps to train for.
   */
  number_of_steps?: number;
  /**
   * Training Data URL
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string;
  /**
   * Trigger Phrase
   *
   * The phrase that will trigger the model to generate an image.
   */
  trigger_phrase?: string;
  /**
   * Learning Rate
   *
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
   */
  learning_rate?: number;
  /**
   * Auto-Scale Input
   *
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};

/**
 * Output
 */
export type WanTrainerI2V720pOutput = {
  /**
   * Lora File
   *
   * URL to the trained LoRA weights.
   */
  lora_file: FileType2;
  /**
   * Config File
   *
   * Configuration used for setting up the inference endpoints.
   */
  config_file: FileType2;
};

/**
 * Input
 */
export type WanTrainerI2V720pInput = {
  /**
   * Number Of Steps
   *
   * The number of steps to train for.
   */
  number_of_steps?: number;
  /**
   * Training Data URL
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string;
  /**
   * Trigger Phrase
   *
   * The phrase that will trigger the model to generate an image.
   */
  trigger_phrase?: string;
  /**
   * Learning Rate
   *
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
   */
  learning_rate?: number;
  /**
   * Auto-Scale Input
   *
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};

/**
 * Output
 */
export type WanTrainerT2V14bOutput = {
  /**
   * Lora File
   *
   * URL to the trained LoRA weights.
   */
  lora_file: FileType2;
  /**
   * Config File
   *
   * Configuration used for setting up the inference endpoints.
   */
  config_file: FileType2;
};

/**
 * Input
 */
export type WanTrainerT2V14bInput = {
  /**
   * Number Of Steps
   *
   * The number of steps to train for.
   */
  number_of_steps?: number;
  /**
   * Training Data URL
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string;
  /**
   * Trigger Phrase
   *
   * The phrase that will trigger the model to generate an image.
   */
  trigger_phrase?: string;
  /**
   * Learning Rate
   *
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
   */
  learning_rate?: number;
  /**
   * Auto-Scale Input
   *
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};

/**
 * Output
 */
export type WanTrainerT2vOutput = {
  /**
   * Lora File
   *
   * URL to the trained LoRA weights.
   */
  lora_file: FileType2;
  /**
   * Config File
   *
   * Configuration used for setting up the inference endpoints.
   */
  config_file: FileType2;
};

/**
 * Input
 */
export type WanTrainerT2vInput = {
  /**
   * Number Of Steps
   *
   * The number of steps to train for.
   */
  number_of_steps?: number;
  /**
   * Training Data URL
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.
   */
  training_data_url: string;
  /**
   * Trigger Phrase
   *
   * The phrase that will trigger the model to generate an image.
   */
  trigger_phrase?: string;
  /**
   * Learning Rate
   *
   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting.
   */
  learning_rate?: number;
  /**
   * Auto-Scale Input
   *
   * If true, the input will be automatically scale the video to 81 frames at 16fps.
   */
  auto_scale_input?: boolean;
};

/**
 * WanTrainerResponse
 */
export type Wan22ImageTrainerOutput = {
  /**
   * Config File
   *
   * Config file helping inference endpoints after training.
   */
  config_file: FileType2;
  /**
   * High Noise LoRA
   *
   * High noise LoRA file.
   */
  high_noise_lora: FileType2;
  /**
   * Low Noise LoRA
   *
   * Low noise LoRA file.
   */
  diffusers_lora_file: FileType2;
};

/**
 * BasicInput
 */
export type Wan22ImageTrainerInput = {
  /**
   * Trigger Phrase
   *
   * Trigger phrase for the model.
   */
  trigger_phrase: string;
  /**
   * Use Masks
   *
   * Whether to use masks for the training data.
   */
  use_masks?: boolean;
  /**
   * Learning Rate
   *
   * Learning rate for training.
   */
  learning_rate?: number;
  /**
   * Use Face Cropping
   *
   * Whether to use face cropping for the training data. When enabled, images will be cropped to the face before resizing.
   */
  use_face_cropping?: boolean;
  /**
   * Training Data URL
   *
   * URL to the training data.
   */
  training_data_url: string;
  /**
   * Number of Steps
   *
   * Number of training steps.
   */
  steps?: number;
  /**
   * Include Synthetic Captions
   *
   * Whether to include synthetic captions.
   */
  include_synthetic_captions?: boolean;
  /**
   * Is Style
   *
   * Whether the training data is style data. If true, face specific options like masking and face detection will be disabled.
   */
  is_style?: boolean;
  /**
   * Use Face Detection
   *
   * Whether to use face detection for the training data. When enabled, images will use the center of the face as the center of the image when resizing.
   */
  use_face_detection?: boolean;
};

/**
 * Output
 */
export type QwenImageTrainerOutput = {
  /**
   * Lora File
   *
   * URL to the trained LoRA weights file.
   */
  lora_file: FileType2;
  /**
   * Config File
   *
   * URL to the training configuration file.
   */
  config_file: FileType2;
};

/**
 * PublicInput
 */
export type QwenImageTrainerInput = {
  /**
   * Steps
   *
   * Total number of training steps to perform. Default is 4000.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to zip archive with images for training. The archive should contain images and corresponding text files with captions.
   * Each text file should have the same name as the image file it corresponds to (e.g., image1.jpg and image1.txt).
   * If text files are missing for some images, you can provide a trigger_phrase to automatically create them.
   * Supported image formats: PNG, JPG, JPEG, WEBP.
   * Try to use at least 10 images, although more is better.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate for training. Default is 5e-4
   */
  learning_rate?: number;
  /**
   * Trigger Phrase
   *
   * Default caption to use for images that don't have corresponding text files. If provided, missing .txt files will be created automatically.
   */
  trigger_phrase?: string;
};

/**
 * Output
 */
export type QwenImageEditTrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputEdit
 */
export type QwenImageEditTrainerInput = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate for LoRA parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImageEditPlusTrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputPlus
 */
export type QwenImageEditPlusTrainerInput = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain more than one reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The Reference Image Count field should be set to the number of reference images.
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate for LoRA parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * Output
 */
export type Flux2TrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputT2I
 */
export type Flux2TrainerInput = {
  /**
   * Steps
   *
   * Total number of training steps.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate applied to trainable parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
  /**
   * Output Lora Format
   *
   * Dictates the naming scheme for the output weights
   */
  output_lora_format?: "fal" | "comfy";
};

/**
 * Output
 */
export type Flux2TrainerEditOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputEdit
 */
export type Flux2TrainerEditInput = {
  /**
   * Steps
   *
   * Total number of training steps.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain up to four reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate applied to trainable parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
  /**
   * Output Lora Format
   *
   * Dictates the naming scheme for the output weights
   */
  output_lora_format?: "fal" | "comfy";
};

/**
 * Output
 */
export type ZImageTrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * Input
 */
export type ZImageTrainerInput = {
  /**
   * Steps
   *
   * Total number of training steps.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Training Type
   *
   * Type of training to perform. Use 'content' to focus on the content of the images, 'style' to focus on the style of the images, and 'balanced' to focus on a combination of both.
   */
  training_type?: "content" | "style" | "balanced";
  /**
   * Learning Rate
   *
   * Learning rate applied to trainable parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImageEdit2509TrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputPlus
 */
export type QwenImageEdit2509TrainerInput = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain more than one reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The Reference Image Count field should be set to the number of reference images.
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate for LoRA parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImageLayeredTrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * Input
 */
export type QwenImageLayeredTrainerInput = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain groups of images. The images should be named:
   *
   * ROOT_start.EXT, ROOT_end.EXT, ROOT_end2.EXT, ..., ROOT_endN.EXT
   * For example:
   * photo_start.png, photo_end.png, photo_end2.png, ..., photo_endN.png
   *
   * The start image is the base image that will be decomposed into layers.
   * The end images are the layers that will be added to the base image.  ROOT_end.EXT is the first layer, ROOT_end2.EXT is the second layer, and so on.
   * You can have up to 8 layers.
   * All image groups must have the same number of output layers.
   *
   * The end images can contain transparent regions. Only PNG and WebP images are supported since these are the only formats that support transparency.
   *
   * The zip can also contain a text file for each image group. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify a description of the base image.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate for LoRA parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImageEdit2511TrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * Input2511
 */
export type QwenImageEdit2511TrainerInput = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain more than one reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The Reference Image Count field should be set to the number of reference images.
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate for LoRA parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * Output
 */
export type QwenImage2512TrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputImage
 */
export type QwenImage2512TrainerInput = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive for text-to-image training.
   *
   * The zip should contain images with their corresponding text captions:
   *
   * image.EXT and image.txt
   * For example:
   * photo.jpg and photo.txt
   *
   * The text file contains the caption/prompt describing the target image.
   *
   * If no text file is provided for an image, the default_caption will be used.
   *
   * If no default_caption is provided and a text file is missing, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate for LoRA parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * LTX2Output
 *
 * Output from LTX-2 training.
 */
export type Ltx2VideoTrainerOutput = {
  lora_file: File;
  config_file: File;
  /**
   * URL to the debug dataset archive containing decoded videos and audio.
   */
  debug_dataset?: File | unknown;
  /**
   * The URL to the validation videos, if any.
   */
  video: File | unknown;
};

/**
 * LTX2Input
 *
 * Input configuration for LTX-2 text-to-video training.
 */
export type Ltx2VideoTrainerInput = {
  /**
   * Number Of Steps
   *
   * The number of training steps.
   */
  number_of_steps?: number;
  /**
   * Audio Preserve Pitch
   *
   * When audio duration doesn't match video duration, stretch/compress audio without changing pitch. If disabled, audio is trimmed or padded with silence.
   */
  audio_preserve_pitch?: boolean;
  /**
   * Frame Rate
   *
   * Target frames per second for the video.
   */
  frame_rate?: number;
  /**
   * Audio Normalize
   *
   * Normalize audio peak amplitude to a consistent level. Recommended for consistent audio levels across the dataset.
   */
  audio_normalize?: boolean;
  /**
   * Validation
   *
   * A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.
   */
  validation?: Array<Validation>;
  /**
   * Learning Rate
   *
   * Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.
   */
  learning_rate?: number;
  /**
   * Number Of Frames
   *
   * Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).
   */
  number_of_frames?: number;
  /**
   * Training Data Url
   *
   * URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
   *
   * **Supported video formats:** .mp4, .mov, .avi, .mkv
   * **Supported image formats:** .png, .jpg, .jpeg
   *
   * Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
   *
   * The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
   */
  training_data_url: string;
  /**
   * Split Input Duration Threshold
   *
   * The duration threshold in seconds. If a video is longer than this, it will be split into scenes.
   */
  split_input_duration_threshold?: number;
  /**
   * Rank
   *
   * The rank of the LoRA adaptation. Higher values increase capacity but use more memory.
   */
  rank?: 8 | 16 | 32 | 64 | 128;
  /**
   * First Frame Conditioning P
   *
   * Probability of conditioning on the first frame during training. Higher values improve image-to-video performance.
   */
  first_frame_conditioning_p?: number;
  /**
   * Stg Scale
   *
   * STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.
   */
  stg_scale?: number;
  /**
   * Aspect Ratio
   *
   * Aspect ratio to use for training.
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * With Audio
   *
   * Enable joint audio-video training. If None (default), automatically detects whether input videos have audio. Set to True to force audio training, or False to disable.
   */
  with_audio?: boolean | unknown;
  /**
   * Trigger Phrase
   *
   * A phrase that will trigger the LoRA style. Will be prepended to captions during training.
   */
  trigger_phrase?: string;
  /**
   * Validation Frame Rate
   *
   * Target frames per second for validation videos.
   */
  validation_frame_rate?: number;
  /**
   * Resolution
   *
   * Resolution to use for training. Higher resolutions require more memory.
   */
  resolution?: "low" | "medium" | "high";
  /**
   * Split Input Into Scenes
   *
   * If true, videos above a certain duration threshold will be split into scenes.
   */
  split_input_into_scenes?: boolean;
  /**
   * Generate Audio In Validation
   *
   * Whether to generate audio in validation samples.
   */
  generate_audio_in_validation?: boolean;
  /**
   * Validation Resolution
   *
   * The resolution to use for validation.
   */
  validation_resolution?: "low" | "medium" | "high";
  /**
   * Validation Number Of Frames
   *
   * The number of frames in validation videos.
   */
  validation_number_of_frames?: number;
  /**
   * Validation Aspect Ratio
   *
   * The aspect ratio to use for validation.
   */
  validation_aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * Validation Negative Prompt
   *
   * A negative prompt to use for validation.
   */
  validation_negative_prompt?: string;
  /**
   * Auto Scale Input
   *
   * If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
   */
  auto_scale_input?: boolean;
};

/**
 * V2VValidation
 *
 * Validation input for video-to-video training.
 */
export type V2vValidation = {
  /**
   * Prompt
   *
   * The prompt to use for validation.
   */
  prompt: string;
  /**
   * Reference Video Url
   *
   * URL to reference video for IC-LoRA validation. This is the input video that will be transformed.
   */
  reference_video_url: string;
};

/**
 * LTX2V2VOutput
 *
 * Output from LTX-2 video-to-video training.
 */
export type Ltx2V2vTrainerOutput = {
  lora_file: File;
  config_file: File;
  /**
   * URL to the debug dataset archive containing decoded videos.
   */
  debug_dataset?: File | unknown;
  /**
   * The URL to the validation videos (with reference videos side-by-side), if any.
   */
  video: File | unknown;
};

/**
 * LTX2V2VInput
 *
 * Input configuration for LTX-2 video-to-video (IC-LoRA) training.
 */
export type Ltx2V2vTrainerInput = {
  /**
   * Number Of Steps
   *
   * The number of training steps.
   */
  number_of_steps?: number;
  /**
   * Frame Rate
   *
   * Target frames per second for the video.
   */
  frame_rate?: number;
  /**
   * Learning Rate
   *
   * Learning rate for optimization. Higher values can lead to faster training but may cause overfitting.
   */
  learning_rate?: number;
  /**
   * Validation
   *
   * A list of validation inputs with prompts and reference videos.
   */
  validation?: Array<V2vValidation>;
  /**
   * Number Of Frames
   *
   * Number of frames per training sample. Must satisfy frames % 8 == 1 (e.g., 1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97).
   */
  number_of_frames?: number;
  /**
   * Training Data Url
   *
   * URL to zip archive with videos or images. Try to use at least 10 files, although more is better.
   *
   * **Supported video formats:** .mp4, .mov, .avi, .mkv
   * **Supported image formats:** .png, .jpg, .jpeg
   *
   * Note: The dataset must contain ONLY videos OR ONLY images - mixed datasets are not supported.
   *
   * The archive can also contain text files with captions. Each text file should have the same name as the media file it corresponds to.
   */
  training_data_url: string;
  /**
   * Split Input Duration Threshold
   *
   * The duration threshold in seconds. If a video is longer than this, it will be split into scenes.
   */
  split_input_duration_threshold?: number;
  /**
   * Rank
   *
   * The rank of the LoRA adaptation. Higher values increase capacity but use more memory.
   */
  rank?: 8 | 16 | 32 | 64 | 128;
  /**
   * Stg Scale
   *
   * STG (Spatio-Temporal Guidance) scale. 0.0 disables STG. Recommended value is 1.0.
   */
  stg_scale?: number;
  /**
   * First Frame Conditioning P
   *
   * Probability of conditioning on the first frame during training. Lower values work better for video-to-video transformation.
   */
  first_frame_conditioning_p?: number;
  /**
   * Aspect Ratio
   *
   * Aspect ratio to use for training.
   */
  aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * Trigger Phrase
   *
   * A phrase that will trigger the LoRA style. Will be prepended to captions during training.
   */
  trigger_phrase?: string;
  /**
   * Resolution
   *
   * Resolution to use for training. Higher resolutions require more memory.
   */
  resolution?: "low" | "medium" | "high";
  /**
   * Validation Frame Rate
   *
   * Target frames per second for validation videos.
   */
  validation_frame_rate?: number;
  /**
   * Split Input Into Scenes
   *
   * If true, videos above a certain duration threshold will be split into scenes.
   */
  split_input_into_scenes?: boolean;
  /**
   * Validation Resolution
   *
   * The resolution to use for validation.
   */
  validation_resolution?: "low" | "medium" | "high";
  /**
   * Validation Number Of Frames
   *
   * The number of frames in validation videos.
   */
  validation_number_of_frames?: number;
  /**
   * Validation Aspect Ratio
   *
   * The aspect ratio to use for validation.
   */
  validation_aspect_ratio?: "16:9" | "1:1" | "9:16";
  /**
   * Validation Negative Prompt
   *
   * A negative prompt to use for validation.
   */
  validation_negative_prompt?: string;
  /**
   * Auto Scale Input
   *
   * If true, videos will be automatically scaled to the target frame count and fps. This option has no effect on image datasets.
   */
  auto_scale_input?: boolean;
};

/**
 * Output
 */
export type Flux2TrainerV2Output = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputT2IV2
 *
 * V2 input with multi-resolution bucketing.
 */
export type Flux2TrainerV2Input = {
  /**
   * Steps
   *
   * Total number of training steps.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate applied to trainable parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
  /**
   * Output Lora Format
   *
   * Dictates the naming scheme for the output weights
   */
  output_lora_format?: "fal" | "comfy";
};

/**
 * Output
 */
export type Flux2TrainerV2EditOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputEditV2
 */
export type Flux2TrainerV2EditInput = {
  /**
   * Steps
   *
   * Total number of training steps.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain up to four reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate applied to trainable parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
  /**
   * Output Lora Format
   *
   * Dictates the naming scheme for the output weights
   */
  output_lora_format?: "fal" | "comfy";
};

/**
 * Output
 */
export type QwenImage2512TrainerV2Output = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * Input
 */
export type QwenImage2512TrainerV2Input = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images and corresponding captions.
   *
   * The images should be named: ROOT.EXT. For example: 001.jpg
   *
   * The corresponding captions should be named: ROOT.txt. For example: 001.txt
   *
   * If no text file is provided for an image, the default_caption will be used.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * Output
 */
export type Flux2Klein4bBaseTrainerEditOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputEditV2
 */
export type Flux2Klein4bBaseTrainerEditInput = {
  /**
   * Steps
   *
   * Total number of training steps.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain up to four reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate applied to trainable parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
  /**
   * Output Lora Format
   *
   * Dictates the naming scheme for the output weights
   */
  output_lora_format?: "fal" | "comfy";
};

/**
 * Output
 */
export type Flux2Klein4bBaseTrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputT2IV2
 *
 * V2 input with multi-resolution bucketing.
 */
export type Flux2Klein4bBaseTrainerInput = {
  /**
   * Steps
   *
   * Total number of training steps.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate applied to trainable parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
  /**
   * Output Lora Format
   *
   * Dictates the naming scheme for the output weights
   */
  output_lora_format?: "fal" | "comfy";
};

/**
 * Output
 */
export type Flux2Klein9bBaseTrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputT2IV2
 *
 * V2 input with multi-resolution bucketing.
 */
export type Flux2Klein9bBaseTrainerInput = {
  /**
   * Steps
   *
   * Total number of training steps.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * The zip can also contain a text file for each image. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate applied to trainable parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
  /**
   * Output Lora Format
   *
   * Dictates the naming scheme for the output weights
   */
  output_lora_format?: "fal" | "comfy";
};

/**
 * Output
 */
export type Flux2Klein9bBaseTrainerEditOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * InputEditV2
 */
export type Flux2Klein9bBaseTrainerEditInput = {
  /**
   * Steps
   *
   * Total number of training steps.
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain up to four reference image for each image pair. The reference images should be named:
   * ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT
   * For example:
   * photo_start.jpg, photo_start2.jpg, photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate applied to trainable parameters.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
  /**
   * Output Lora Format
   *
   * Dictates the naming scheme for the output weights
   */
  output_lora_format?: "fal" | "comfy";
};

/**
 * Output
 */
export type ZImageTurboTrainerV2Output = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * Input
 */
export type ZImageTurboTrainerV2Input = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images and corresponding captions.
   *
   * The images should be named: ROOT.EXT. For example: 001.jpg
   *
   * The corresponding captions should be named: ROOT.txt. For example: 001.txt
   *
   * If no text file is provided for an image, the default_caption will be used.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * Output
 */
export type ZImageBaseTrainerOutput = {
  config_file: File;
  diffusers_lora_file: File;
};

/**
 * Input
 */
export type ZImageBaseTrainerInput = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images and corresponding captions.
   *
   * The images should be named: ROOT.EXT. For example: 001.jpg
   *
   * The corresponding captions should be named: ROOT.txt. For example: 001.txt
   *
   * If no text file is provided for an image, the default_caption will be used.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   *
   * Learning rate.
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string | unknown;
};

/**
 * Output
 */
export type FluxLoraPortraitTrainerOutput = {
  /**
   * Config File
   *
   * URL to the training configuration file.
   */
  config_file: FileType2;
  /**
   * Diffusers Lora File
   *
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: FileType2;
};

/**
 * PublicInput
 */
export type FluxLoraPortraitTrainerInput = {
  /**
   * Images Data Url
   *
   *
   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
   *
   * The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.
   *
   */
  images_data_url: string;
  /**
   * Trigger Phrase
   *
   * Trigger phrase to be used in the captions. If None, a trigger word will not be used.
   * If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.
   *
   */
  trigger_phrase?: string | null;
  /**
   * Resume From Checkpoint
   *
   * URL to a checkpoint to resume training from.
   */
  resume_from_checkpoint?: string;
  /**
   * Subject Crop
   *
   * If True, the subject will be cropped from the image.
   */
  subject_crop?: boolean;
  /**
   * Learning Rate
   *
   * Learning rate to use for training.
   */
  learning_rate?: number;
  /**
   * Multiresolution Training
   *
   * If True, multiresolution training will be used.
   */
  multiresolution_training?: boolean;
  /**
   * Steps
   *
   * Number of steps to train the LoRA on.
   */
  steps?: number;
  /**
   * Data Archive Format
   *
   * The format of the archive. If not specified, the format will be inferred from the URL.
   */
  data_archive_format?: string | null;
  /**
   * Create Masks
   *
   * If True, masks will be created for the subject.
   */
  create_masks?: boolean;
};

/**
 * Output
 */
export type FluxLoraFastTrainingOutput = {
  /**
   * Config File
   *
   * URL to the training configuration file.
   */
  config_file: FileType2;
  /**
   * Debug Preprocessed Output
   *
   * URL to the preprocessed images.
   */
  debug_preprocessed_output?: FileType2;
  /**
   * Diffusers Lora File
   *
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: FileType2;
};

/**
 * PublicInput
 */
export type FluxLoraFastTrainingInput = {
  /**
   * Images Data Url
   *
   *
   * URL to zip archive with images. Try to use at least 4 images in general the more the better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
   *
   */
  images_data_url: string;
  /**
   * Is Input Format Already Preprocessed
   *
   * Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.
   */
  is_input_format_already_preprocessed?: boolean;
  /**
   * Trigger Word
   *
   * Trigger word to be used in the captions. If None, a trigger word will not be used.
   * If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.
   *
   */
  trigger_word?: string | null;
  /**
   * Steps
   *
   * Number of steps to train the LoRA on.
   */
  steps?: number;
  /**
   * Data Archive Format
   *
   * The format of the archive. If not specified, the format will be inferred from the URL.
   */
  data_archive_format?: string | null;
  /**
   * Is Style
   *
   * If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.
   */
  is_style?: boolean;
  /**
   * Create Masks
   *
   * If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.
   */
  create_masks?: boolean;
};

/**
 * Output
 */
export type FluxKontextTrainerOutput = {
  /**
   * Config File
   *
   * URL to the configuration file for the trained model.
   */
  config_file: FileType2;
  /**
   * Diffusers Lora File
   *
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: FileType2;
};

/**
 * Input
 */
export type FluxKontextTrainerInput = {
  /**
   * Steps
   *
   * Number of steps to train for
   */
  steps?: number;
  /**
   * Image Data Url
   *
   *
   * URL to the input data zip archive.
   *
   * The zip should contain pairs of images. The images should be named:
   *
   * ROOT_start.EXT and ROOT_end.EXT
   * For example:
   * photo_start.jpg and photo_end.jpg
   *
   * The zip can also contain a text file for each image pair. The text file should be named:
   * ROOT.txt
   * For example:
   * photo.txt
   *
   * This text file can be used to specify the edit instructions for the image pair.
   *
   * If no text file is provided, the default_caption will be used.
   *
   * If no default_caption is provided, the training will fail.
   *
   */
  image_data_url: string;
  /**
   * Learning Rate
   */
  learning_rate?: number;
  /**
   * Default Caption
   *
   * Default caption to use when caption files are missing. If None, missing captions will cause an error.
   */
  default_caption?: string;
  /**
   * Output Lora Format
   *
   * Dictates the naming scheme for the output weights
   */
  output_lora_format?: "fal" | "comfy";
};

/**
 * Output
 */
export type FluxKreaTrainerOutput = {
  /**
   * Config File
   *
   * URL to the training configuration file.
   */
  config_file: FileType2;
  /**
   * Debug Preprocessed Output
   *
   * URL to the preprocessed images.
   */
  debug_preprocessed_output?: FileType2;
  /**
   * Diffusers Lora File
   *
   * URL to the trained diffusers lora weights.
   */
  diffusers_lora_file: FileType2;
};

/**
 * PublicInput
 */
export type FluxKreaTrainerInput = {
  /**
   * Images Data Url
   *
   *
   * URL to zip archive with images. Try to use at least 4 images in general the more the better.
   *
   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.
   *
   */
  images_data_url: string;
  /**
   * Is Input Format Already Preprocessed
   *
   * Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.
   */
  is_input_format_already_preprocessed?: boolean;
  /**
   * Trigger Word
   *
   * Trigger word to be used in the captions. If None, a trigger word will not be used.
   * If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.
   *
   */
  trigger_word?: string | null;
  /**
   * Steps
   *
   * Number of steps to train the LoRA on.
   */
  steps?: number;
  /**
   * Data Archive Format
   *
   * The format of the archive. If not specified, the format will be inferred from the URL.
   */
  data_archive_format?: string | null;
  /**
   * Is Style
   *
   * If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.
   */
  is_style?: boolean;
  /**
   * Create Masks
   *
   * If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible.
   */
  create_masks?: boolean;
};

export type QueueStatus = {
  status: "IN_QUEUE" | "IN_PROGRESS" | "COMPLETED";
  /**
   * The request id.
   */
  request_id: string;
  /**
   * The response url.
   */
  response_url?: string;
  /**
   * The status url.
   */
  status_url?: string;
  /**
   * The cancel url.
   */
  cancel_url?: string;
  /**
   * The logs.
   */
  logs?: {
    [key: string]: unknown;
  };
  /**
   * The metrics.
   */
  metrics?: {
    [key: string]: unknown;
  };
  /**
   * The queue position.
   */
  queue_position?: number;
};

export type GetFalAiFluxKreaTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-krea-trainer/requests/{request_id}/status";
};

export type GetFalAiFluxKreaTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFluxKreaTrainerRequestsByRequestIdStatusResponse =
  GetFalAiFluxKreaTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiFluxKreaTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiFluxKreaTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-krea-trainer/requests/{request_id}/cancel";
};

export type PutFalAiFluxKreaTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFluxKreaTrainerRequestsByRequestIdCancelResponse =
  PutFalAiFluxKreaTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiFluxKreaTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiFluxKreaTrainerData = {
  body: FluxKreaTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-krea-trainer";
};

export type PostFalAiFluxKreaTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFluxKreaTrainerResponse =
  PostFalAiFluxKreaTrainerResponses[keyof PostFalAiFluxKreaTrainerResponses];

export type GetFalAiFluxKreaTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-krea-trainer/requests/{request_id}";
};

export type GetFalAiFluxKreaTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FluxKreaTrainerOutput;
};

export type GetFalAiFluxKreaTrainerRequestsByRequestIdResponse =
  GetFalAiFluxKreaTrainerRequestsByRequestIdResponses[keyof GetFalAiFluxKreaTrainerRequestsByRequestIdResponses];

export type GetFalAiFluxKontextTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-kontext-trainer/requests/{request_id}/status";
};

export type GetFalAiFluxKontextTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFluxKontextTrainerRequestsByRequestIdStatusResponse =
  GetFalAiFluxKontextTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiFluxKontextTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiFluxKontextTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-kontext-trainer/requests/{request_id}/cancel";
};

export type PutFalAiFluxKontextTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFluxKontextTrainerRequestsByRequestIdCancelResponse =
  PutFalAiFluxKontextTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiFluxKontextTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiFluxKontextTrainerData = {
  body: FluxKontextTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-kontext-trainer";
};

export type PostFalAiFluxKontextTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFluxKontextTrainerResponse =
  PostFalAiFluxKontextTrainerResponses[keyof PostFalAiFluxKontextTrainerResponses];

export type GetFalAiFluxKontextTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-kontext-trainer/requests/{request_id}";
};

export type GetFalAiFluxKontextTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FluxKontextTrainerOutput;
};

export type GetFalAiFluxKontextTrainerRequestsByRequestIdResponse =
  GetFalAiFluxKontextTrainerRequestsByRequestIdResponses[keyof GetFalAiFluxKontextTrainerRequestsByRequestIdResponses];

export type GetFalAiFluxLoraFastTrainingRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-lora-fast-training/requests/{request_id}/status";
};

export type GetFalAiFluxLoraFastTrainingRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFluxLoraFastTrainingRequestsByRequestIdStatusResponse =
  GetFalAiFluxLoraFastTrainingRequestsByRequestIdStatusResponses[keyof GetFalAiFluxLoraFastTrainingRequestsByRequestIdStatusResponses];

export type PutFalAiFluxLoraFastTrainingRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-lora-fast-training/requests/{request_id}/cancel";
};

export type PutFalAiFluxLoraFastTrainingRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFluxLoraFastTrainingRequestsByRequestIdCancelResponse =
  PutFalAiFluxLoraFastTrainingRequestsByRequestIdCancelResponses[keyof PutFalAiFluxLoraFastTrainingRequestsByRequestIdCancelResponses];

export type PostFalAiFluxLoraFastTrainingData = {
  body: FluxLoraFastTrainingInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-lora-fast-training";
};

export type PostFalAiFluxLoraFastTrainingResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFluxLoraFastTrainingResponse =
  PostFalAiFluxLoraFastTrainingResponses[keyof PostFalAiFluxLoraFastTrainingResponses];

export type GetFalAiFluxLoraFastTrainingRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-lora-fast-training/requests/{request_id}";
};

export type GetFalAiFluxLoraFastTrainingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FluxLoraFastTrainingOutput;
};

export type GetFalAiFluxLoraFastTrainingRequestsByRequestIdResponse =
  GetFalAiFluxLoraFastTrainingRequestsByRequestIdResponses[keyof GetFalAiFluxLoraFastTrainingRequestsByRequestIdResponses];

export type GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-lora-portrait-trainer/requests/{request_id}/status";
};

export type GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdStatusResponse =
  GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiFluxLoraPortraitTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-lora-portrait-trainer/requests/{request_id}/cancel";
};

export type PutFalAiFluxLoraPortraitTrainerRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFluxLoraPortraitTrainerRequestsByRequestIdCancelResponse =
  PutFalAiFluxLoraPortraitTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiFluxLoraPortraitTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiFluxLoraPortraitTrainerData = {
  body: FluxLoraPortraitTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-lora-portrait-trainer";
};

export type PostFalAiFluxLoraPortraitTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFluxLoraPortraitTrainerResponse =
  PostFalAiFluxLoraPortraitTrainerResponses[keyof PostFalAiFluxLoraPortraitTrainerResponses];

export type GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-lora-portrait-trainer/requests/{request_id}";
};

export type GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: FluxLoraPortraitTrainerOutput;
};

export type GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdResponse =
  GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdResponses[keyof GetFalAiFluxLoraPortraitTrainerRequestsByRequestIdResponses];

export type GetFalAiZImageBaseTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/z-image-base-trainer/requests/{request_id}/status";
};

export type GetFalAiZImageBaseTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiZImageBaseTrainerRequestsByRequestIdStatusResponse =
  GetFalAiZImageBaseTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiZImageBaseTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiZImageBaseTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/z-image-base-trainer/requests/{request_id}/cancel";
};

export type PutFalAiZImageBaseTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiZImageBaseTrainerRequestsByRequestIdCancelResponse =
  PutFalAiZImageBaseTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiZImageBaseTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiZImageBaseTrainerData = {
  body: ZImageBaseTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/z-image-base-trainer";
};

export type PostFalAiZImageBaseTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiZImageBaseTrainerResponse =
  PostFalAiZImageBaseTrainerResponses[keyof PostFalAiZImageBaseTrainerResponses];

export type GetFalAiZImageBaseTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/z-image-base-trainer/requests/{request_id}";
};

export type GetFalAiZImageBaseTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ZImageBaseTrainerOutput;
};

export type GetFalAiZImageBaseTrainerRequestsByRequestIdResponse =
  GetFalAiZImageBaseTrainerRequestsByRequestIdResponses[keyof GetFalAiZImageBaseTrainerRequestsByRequestIdResponses];

export type GetFalAiZImageTurboTrainerV2RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/z-image-turbo-trainer-v2/requests/{request_id}/status";
};

export type GetFalAiZImageTurboTrainerV2RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiZImageTurboTrainerV2RequestsByRequestIdStatusResponse =
  GetFalAiZImageTurboTrainerV2RequestsByRequestIdStatusResponses[keyof GetFalAiZImageTurboTrainerV2RequestsByRequestIdStatusResponses];

export type PutFalAiZImageTurboTrainerV2RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/z-image-turbo-trainer-v2/requests/{request_id}/cancel";
};

export type PutFalAiZImageTurboTrainerV2RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiZImageTurboTrainerV2RequestsByRequestIdCancelResponse =
  PutFalAiZImageTurboTrainerV2RequestsByRequestIdCancelResponses[keyof PutFalAiZImageTurboTrainerV2RequestsByRequestIdCancelResponses];

export type PostFalAiZImageTurboTrainerV2Data = {
  body: ZImageTurboTrainerV2Input;
  path?: never;
  query?: never;
  url: "/fal-ai/z-image-turbo-trainer-v2";
};

export type PostFalAiZImageTurboTrainerV2Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiZImageTurboTrainerV2Response =
  PostFalAiZImageTurboTrainerV2Responses[keyof PostFalAiZImageTurboTrainerV2Responses];

export type GetFalAiZImageTurboTrainerV2RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/z-image-turbo-trainer-v2/requests/{request_id}";
};

export type GetFalAiZImageTurboTrainerV2RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ZImageTurboTrainerV2Output;
};

export type GetFalAiZImageTurboTrainerV2RequestsByRequestIdResponse =
  GetFalAiZImageTurboTrainerV2RequestsByRequestIdResponses[keyof GetFalAiZImageTurboTrainerV2RequestsByRequestIdResponses];

export type GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-2-klein-9b-base-trainer/edit/requests/{request_id}/status";
};

export type GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdStatusResponse =
  GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdStatusResponses[keyof GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdStatusResponses];

export type PutFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-klein-9b-base-trainer/edit/requests/{request_id}/cancel";
};

export type PutFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdCancelResponse =
  PutFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdCancelResponses[keyof PutFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdCancelResponses];

export type PostFalAiFlux2Klein9bBaseTrainerEditData = {
  body: Flux2Klein9bBaseTrainerEditInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-2-klein-9b-base-trainer/edit";
};

export type PostFalAiFlux2Klein9bBaseTrainerEditResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFlux2Klein9bBaseTrainerEditResponse =
  PostFalAiFlux2Klein9bBaseTrainerEditResponses[keyof PostFalAiFlux2Klein9bBaseTrainerEditResponses];

export type GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-klein-9b-base-trainer/edit/requests/{request_id}";
};

export type GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Flux2Klein9bBaseTrainerEditOutput;
};

export type GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdResponse =
  GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdResponses[keyof GetFalAiFlux2Klein9bBaseTrainerEditRequestsByRequestIdResponses];

export type GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-2-klein-9b-base-trainer/requests/{request_id}/status";
};

export type GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdStatusResponse =
  GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-klein-9b-base-trainer/requests/{request_id}/cancel";
};

export type PutFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdCancelResponse =
  PutFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiFlux2Klein9bBaseTrainerData = {
  body: Flux2Klein9bBaseTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-2-klein-9b-base-trainer";
};

export type PostFalAiFlux2Klein9bBaseTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFlux2Klein9bBaseTrainerResponse =
  PostFalAiFlux2Klein9bBaseTrainerResponses[keyof PostFalAiFlux2Klein9bBaseTrainerResponses];

export type GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-klein-9b-base-trainer/requests/{request_id}";
};

export type GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Flux2Klein9bBaseTrainerOutput;
};

export type GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdResponse =
  GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdResponses[keyof GetFalAiFlux2Klein9bBaseTrainerRequestsByRequestIdResponses];

export type GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-2-klein-4b-base-trainer/requests/{request_id}/status";
};

export type GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdStatusResponse =
  GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-klein-4b-base-trainer/requests/{request_id}/cancel";
};

export type PutFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdCancelResponse =
  PutFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiFlux2Klein4bBaseTrainerData = {
  body: Flux2Klein4bBaseTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-2-klein-4b-base-trainer";
};

export type PostFalAiFlux2Klein4bBaseTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFlux2Klein4bBaseTrainerResponse =
  PostFalAiFlux2Klein4bBaseTrainerResponses[keyof PostFalAiFlux2Klein4bBaseTrainerResponses];

export type GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-klein-4b-base-trainer/requests/{request_id}";
};

export type GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Flux2Klein4bBaseTrainerOutput;
};

export type GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdResponse =
  GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdResponses[keyof GetFalAiFlux2Klein4bBaseTrainerRequestsByRequestIdResponses];

export type GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-2-klein-4b-base-trainer/edit/requests/{request_id}/status";
};

export type GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdStatusResponse =
  GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdStatusResponses[keyof GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdStatusResponses];

export type PutFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-klein-4b-base-trainer/edit/requests/{request_id}/cancel";
};

export type PutFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdCancelResponse =
  PutFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdCancelResponses[keyof PutFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdCancelResponses];

export type PostFalAiFlux2Klein4bBaseTrainerEditData = {
  body: Flux2Klein4bBaseTrainerEditInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-2-klein-4b-base-trainer/edit";
};

export type PostFalAiFlux2Klein4bBaseTrainerEditResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFlux2Klein4bBaseTrainerEditResponse =
  PostFalAiFlux2Klein4bBaseTrainerEditResponses[keyof PostFalAiFlux2Klein4bBaseTrainerEditResponses];

export type GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-klein-4b-base-trainer/edit/requests/{request_id}";
};

export type GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Flux2Klein4bBaseTrainerEditOutput;
};

export type GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdResponse =
  GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdResponses[keyof GetFalAiFlux2Klein4bBaseTrainerEditRequestsByRequestIdResponses];

export type GetFalAiQwenImage2512TrainerV2RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/qwen-image-2512-trainer-v2/requests/{request_id}/status";
};

export type GetFalAiQwenImage2512TrainerV2RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiQwenImage2512TrainerV2RequestsByRequestIdStatusResponse =
  GetFalAiQwenImage2512TrainerV2RequestsByRequestIdStatusResponses[keyof GetFalAiQwenImage2512TrainerV2RequestsByRequestIdStatusResponses];

export type PutFalAiQwenImage2512TrainerV2RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-2512-trainer-v2/requests/{request_id}/cancel";
};

export type PutFalAiQwenImage2512TrainerV2RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiQwenImage2512TrainerV2RequestsByRequestIdCancelResponse =
  PutFalAiQwenImage2512TrainerV2RequestsByRequestIdCancelResponses[keyof PutFalAiQwenImage2512TrainerV2RequestsByRequestIdCancelResponses];

export type PostFalAiQwenImage2512TrainerV2Data = {
  body: QwenImage2512TrainerV2Input;
  path?: never;
  query?: never;
  url: "/fal-ai/qwen-image-2512-trainer-v2";
};

export type PostFalAiQwenImage2512TrainerV2Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiQwenImage2512TrainerV2Response =
  PostFalAiQwenImage2512TrainerV2Responses[keyof PostFalAiQwenImage2512TrainerV2Responses];

export type GetFalAiQwenImage2512TrainerV2RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-2512-trainer-v2/requests/{request_id}";
};

export type GetFalAiQwenImage2512TrainerV2RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: QwenImage2512TrainerV2Output;
};

export type GetFalAiQwenImage2512TrainerV2RequestsByRequestIdResponse =
  GetFalAiQwenImage2512TrainerV2RequestsByRequestIdResponses[keyof GetFalAiQwenImage2512TrainerV2RequestsByRequestIdResponses];

export type GetFalAiFlux2TrainerV2EditRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-2-trainer-v2/edit/requests/{request_id}/status";
};

export type GetFalAiFlux2TrainerV2EditRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFlux2TrainerV2EditRequestsByRequestIdStatusResponse =
  GetFalAiFlux2TrainerV2EditRequestsByRequestIdStatusResponses[keyof GetFalAiFlux2TrainerV2EditRequestsByRequestIdStatusResponses];

export type PutFalAiFlux2TrainerV2EditRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-trainer-v2/edit/requests/{request_id}/cancel";
};

export type PutFalAiFlux2TrainerV2EditRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFlux2TrainerV2EditRequestsByRequestIdCancelResponse =
  PutFalAiFlux2TrainerV2EditRequestsByRequestIdCancelResponses[keyof PutFalAiFlux2TrainerV2EditRequestsByRequestIdCancelResponses];

export type PostFalAiFlux2TrainerV2EditData = {
  body: Flux2TrainerV2EditInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-2-trainer-v2/edit";
};

export type PostFalAiFlux2TrainerV2EditResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFlux2TrainerV2EditResponse =
  PostFalAiFlux2TrainerV2EditResponses[keyof PostFalAiFlux2TrainerV2EditResponses];

export type GetFalAiFlux2TrainerV2EditRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-trainer-v2/edit/requests/{request_id}";
};

export type GetFalAiFlux2TrainerV2EditRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Flux2TrainerV2EditOutput;
};

export type GetFalAiFlux2TrainerV2EditRequestsByRequestIdResponse =
  GetFalAiFlux2TrainerV2EditRequestsByRequestIdResponses[keyof GetFalAiFlux2TrainerV2EditRequestsByRequestIdResponses];

export type GetFalAiFlux2TrainerV2RequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-2-trainer-v2/requests/{request_id}/status";
};

export type GetFalAiFlux2TrainerV2RequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFlux2TrainerV2RequestsByRequestIdStatusResponse =
  GetFalAiFlux2TrainerV2RequestsByRequestIdStatusResponses[keyof GetFalAiFlux2TrainerV2RequestsByRequestIdStatusResponses];

export type PutFalAiFlux2TrainerV2RequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-trainer-v2/requests/{request_id}/cancel";
};

export type PutFalAiFlux2TrainerV2RequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFlux2TrainerV2RequestsByRequestIdCancelResponse =
  PutFalAiFlux2TrainerV2RequestsByRequestIdCancelResponses[keyof PutFalAiFlux2TrainerV2RequestsByRequestIdCancelResponses];

export type PostFalAiFlux2TrainerV2Data = {
  body: Flux2TrainerV2Input;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-2-trainer-v2";
};

export type PostFalAiFlux2TrainerV2Responses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFlux2TrainerV2Response =
  PostFalAiFlux2TrainerV2Responses[keyof PostFalAiFlux2TrainerV2Responses];

export type GetFalAiFlux2TrainerV2RequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-trainer-v2/requests/{request_id}";
};

export type GetFalAiFlux2TrainerV2RequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Flux2TrainerV2Output;
};

export type GetFalAiFlux2TrainerV2RequestsByRequestIdResponse =
  GetFalAiFlux2TrainerV2RequestsByRequestIdResponses[keyof GetFalAiFlux2TrainerV2RequestsByRequestIdResponses];

export type GetFalAiLtx2V2vTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx2-v2v-trainer/requests/{request_id}/status";
};

export type GetFalAiLtx2V2vTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx2V2vTrainerRequestsByRequestIdStatusResponse =
  GetFalAiLtx2V2vTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiLtx2V2vTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiLtx2V2vTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx2-v2v-trainer/requests/{request_id}/cancel";
};

export type PutFalAiLtx2V2vTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx2V2vTrainerRequestsByRequestIdCancelResponse =
  PutFalAiLtx2V2vTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiLtx2V2vTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiLtx2V2vTrainerData = {
  body: Ltx2V2vTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx2-v2v-trainer";
};

export type PostFalAiLtx2V2vTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx2V2vTrainerResponse =
  PostFalAiLtx2V2vTrainerResponses[keyof PostFalAiLtx2V2vTrainerResponses];

export type GetFalAiLtx2V2vTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx2-v2v-trainer/requests/{request_id}";
};

export type GetFalAiLtx2V2vTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx2V2vTrainerOutput;
};

export type GetFalAiLtx2V2vTrainerRequestsByRequestIdResponse =
  GetFalAiLtx2V2vTrainerRequestsByRequestIdResponses[keyof GetFalAiLtx2V2vTrainerRequestsByRequestIdResponses];

export type GetFalAiLtx2VideoTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx2-video-trainer/requests/{request_id}/status";
};

export type GetFalAiLtx2VideoTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtx2VideoTrainerRequestsByRequestIdStatusResponse =
  GetFalAiLtx2VideoTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiLtx2VideoTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiLtx2VideoTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx2-video-trainer/requests/{request_id}/cancel";
};

export type PutFalAiLtx2VideoTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtx2VideoTrainerRequestsByRequestIdCancelResponse =
  PutFalAiLtx2VideoTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiLtx2VideoTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiLtx2VideoTrainerData = {
  body: Ltx2VideoTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx2-video-trainer";
};

export type PostFalAiLtx2VideoTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtx2VideoTrainerResponse =
  PostFalAiLtx2VideoTrainerResponses[keyof PostFalAiLtx2VideoTrainerResponses];

export type GetFalAiLtx2VideoTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx2-video-trainer/requests/{request_id}";
};

export type GetFalAiLtx2VideoTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Ltx2VideoTrainerOutput;
};

export type GetFalAiLtx2VideoTrainerRequestsByRequestIdResponse =
  GetFalAiLtx2VideoTrainerRequestsByRequestIdResponses[keyof GetFalAiLtx2VideoTrainerRequestsByRequestIdResponses];

export type GetFalAiQwenImage2512TrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/qwen-image-2512-trainer/requests/{request_id}/status";
};

export type GetFalAiQwenImage2512TrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiQwenImage2512TrainerRequestsByRequestIdStatusResponse =
  GetFalAiQwenImage2512TrainerRequestsByRequestIdStatusResponses[keyof GetFalAiQwenImage2512TrainerRequestsByRequestIdStatusResponses];

export type PutFalAiQwenImage2512TrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-2512-trainer/requests/{request_id}/cancel";
};

export type PutFalAiQwenImage2512TrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiQwenImage2512TrainerRequestsByRequestIdCancelResponse =
  PutFalAiQwenImage2512TrainerRequestsByRequestIdCancelResponses[keyof PutFalAiQwenImage2512TrainerRequestsByRequestIdCancelResponses];

export type PostFalAiQwenImage2512TrainerData = {
  body: QwenImage2512TrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/qwen-image-2512-trainer";
};

export type PostFalAiQwenImage2512TrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiQwenImage2512TrainerResponse =
  PostFalAiQwenImage2512TrainerResponses[keyof PostFalAiQwenImage2512TrainerResponses];

export type GetFalAiQwenImage2512TrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-2512-trainer/requests/{request_id}";
};

export type GetFalAiQwenImage2512TrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: QwenImage2512TrainerOutput;
};

export type GetFalAiQwenImage2512TrainerRequestsByRequestIdResponse =
  GetFalAiQwenImage2512TrainerRequestsByRequestIdResponses[keyof GetFalAiQwenImage2512TrainerRequestsByRequestIdResponses];

export type GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/qwen-image-edit-2511-trainer/requests/{request_id}/status";
};

export type GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdStatusResponse =
  GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdStatusResponses[keyof GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdStatusResponses];

export type PutFalAiQwenImageEdit2511TrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-edit-2511-trainer/requests/{request_id}/cancel";
};

export type PutFalAiQwenImageEdit2511TrainerRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiQwenImageEdit2511TrainerRequestsByRequestIdCancelResponse =
  PutFalAiQwenImageEdit2511TrainerRequestsByRequestIdCancelResponses[keyof PutFalAiQwenImageEdit2511TrainerRequestsByRequestIdCancelResponses];

export type PostFalAiQwenImageEdit2511TrainerData = {
  body: QwenImageEdit2511TrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/qwen-image-edit-2511-trainer";
};

export type PostFalAiQwenImageEdit2511TrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiQwenImageEdit2511TrainerResponse =
  PostFalAiQwenImageEdit2511TrainerResponses[keyof PostFalAiQwenImageEdit2511TrainerResponses];

export type GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-edit-2511-trainer/requests/{request_id}";
};

export type GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: QwenImageEdit2511TrainerOutput;
};

export type GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdResponse =
  GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdResponses[keyof GetFalAiQwenImageEdit2511TrainerRequestsByRequestIdResponses];

export type GetFalAiQwenImageLayeredTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/qwen-image-layered-trainer/requests/{request_id}/status";
};

export type GetFalAiQwenImageLayeredTrainerRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiQwenImageLayeredTrainerRequestsByRequestIdStatusResponse =
  GetFalAiQwenImageLayeredTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiQwenImageLayeredTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiQwenImageLayeredTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-layered-trainer/requests/{request_id}/cancel";
};

export type PutFalAiQwenImageLayeredTrainerRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiQwenImageLayeredTrainerRequestsByRequestIdCancelResponse =
  PutFalAiQwenImageLayeredTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiQwenImageLayeredTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiQwenImageLayeredTrainerData = {
  body: QwenImageLayeredTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/qwen-image-layered-trainer";
};

export type PostFalAiQwenImageLayeredTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiQwenImageLayeredTrainerResponse =
  PostFalAiQwenImageLayeredTrainerResponses[keyof PostFalAiQwenImageLayeredTrainerResponses];

export type GetFalAiQwenImageLayeredTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-layered-trainer/requests/{request_id}";
};

export type GetFalAiQwenImageLayeredTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: QwenImageLayeredTrainerOutput;
};

export type GetFalAiQwenImageLayeredTrainerRequestsByRequestIdResponse =
  GetFalAiQwenImageLayeredTrainerRequestsByRequestIdResponses[keyof GetFalAiQwenImageLayeredTrainerRequestsByRequestIdResponses];

export type GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/qwen-image-edit-2509-trainer/requests/{request_id}/status";
};

export type GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdStatusResponse =
  GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdStatusResponses[keyof GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdStatusResponses];

export type PutFalAiQwenImageEdit2509TrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-edit-2509-trainer/requests/{request_id}/cancel";
};

export type PutFalAiQwenImageEdit2509TrainerRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiQwenImageEdit2509TrainerRequestsByRequestIdCancelResponse =
  PutFalAiQwenImageEdit2509TrainerRequestsByRequestIdCancelResponses[keyof PutFalAiQwenImageEdit2509TrainerRequestsByRequestIdCancelResponses];

export type PostFalAiQwenImageEdit2509TrainerData = {
  body: QwenImageEdit2509TrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/qwen-image-edit-2509-trainer";
};

export type PostFalAiQwenImageEdit2509TrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiQwenImageEdit2509TrainerResponse =
  PostFalAiQwenImageEdit2509TrainerResponses[keyof PostFalAiQwenImageEdit2509TrainerResponses];

export type GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-edit-2509-trainer/requests/{request_id}";
};

export type GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: QwenImageEdit2509TrainerOutput;
};

export type GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdResponse =
  GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdResponses[keyof GetFalAiQwenImageEdit2509TrainerRequestsByRequestIdResponses];

export type GetFalAiZImageTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/z-image-trainer/requests/{request_id}/status";
};

export type GetFalAiZImageTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiZImageTrainerRequestsByRequestIdStatusResponse =
  GetFalAiZImageTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiZImageTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiZImageTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/z-image-trainer/requests/{request_id}/cancel";
};

export type PutFalAiZImageTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiZImageTrainerRequestsByRequestIdCancelResponse =
  PutFalAiZImageTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiZImageTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiZImageTrainerData = {
  body: ZImageTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/z-image-trainer";
};

export type PostFalAiZImageTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiZImageTrainerResponse =
  PostFalAiZImageTrainerResponses[keyof PostFalAiZImageTrainerResponses];

export type GetFalAiZImageTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/z-image-trainer/requests/{request_id}";
};

export type GetFalAiZImageTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: ZImageTrainerOutput;
};

export type GetFalAiZImageTrainerRequestsByRequestIdResponse =
  GetFalAiZImageTrainerRequestsByRequestIdResponses[keyof GetFalAiZImageTrainerRequestsByRequestIdResponses];

export type GetFalAiFlux2TrainerEditRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-2-trainer/edit/requests/{request_id}/status";
};

export type GetFalAiFlux2TrainerEditRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFlux2TrainerEditRequestsByRequestIdStatusResponse =
  GetFalAiFlux2TrainerEditRequestsByRequestIdStatusResponses[keyof GetFalAiFlux2TrainerEditRequestsByRequestIdStatusResponses];

export type PutFalAiFlux2TrainerEditRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-trainer/edit/requests/{request_id}/cancel";
};

export type PutFalAiFlux2TrainerEditRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFlux2TrainerEditRequestsByRequestIdCancelResponse =
  PutFalAiFlux2TrainerEditRequestsByRequestIdCancelResponses[keyof PutFalAiFlux2TrainerEditRequestsByRequestIdCancelResponses];

export type PostFalAiFlux2TrainerEditData = {
  body: Flux2TrainerEditInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-2-trainer/edit";
};

export type PostFalAiFlux2TrainerEditResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFlux2TrainerEditResponse =
  PostFalAiFlux2TrainerEditResponses[keyof PostFalAiFlux2TrainerEditResponses];

export type GetFalAiFlux2TrainerEditRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-trainer/edit/requests/{request_id}";
};

export type GetFalAiFlux2TrainerEditRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Flux2TrainerEditOutput;
};

export type GetFalAiFlux2TrainerEditRequestsByRequestIdResponse =
  GetFalAiFlux2TrainerEditRequestsByRequestIdResponses[keyof GetFalAiFlux2TrainerEditRequestsByRequestIdResponses];

export type GetFalAiFlux2TrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/flux-2-trainer/requests/{request_id}/status";
};

export type GetFalAiFlux2TrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiFlux2TrainerRequestsByRequestIdStatusResponse =
  GetFalAiFlux2TrainerRequestsByRequestIdStatusResponses[keyof GetFalAiFlux2TrainerRequestsByRequestIdStatusResponses];

export type PutFalAiFlux2TrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-trainer/requests/{request_id}/cancel";
};

export type PutFalAiFlux2TrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiFlux2TrainerRequestsByRequestIdCancelResponse =
  PutFalAiFlux2TrainerRequestsByRequestIdCancelResponses[keyof PutFalAiFlux2TrainerRequestsByRequestIdCancelResponses];

export type PostFalAiFlux2TrainerData = {
  body: Flux2TrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/flux-2-trainer";
};

export type PostFalAiFlux2TrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiFlux2TrainerResponse =
  PostFalAiFlux2TrainerResponses[keyof PostFalAiFlux2TrainerResponses];

export type GetFalAiFlux2TrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/flux-2-trainer/requests/{request_id}";
};

export type GetFalAiFlux2TrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Flux2TrainerOutput;
};

export type GetFalAiFlux2TrainerRequestsByRequestIdResponse =
  GetFalAiFlux2TrainerRequestsByRequestIdResponses[keyof GetFalAiFlux2TrainerRequestsByRequestIdResponses];

export type GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/qwen-image-edit-plus-trainer/requests/{request_id}/status";
};

export type GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdStatusResponse =
  GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiQwenImageEditPlusTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-edit-plus-trainer/requests/{request_id}/cancel";
};

export type PutFalAiQwenImageEditPlusTrainerRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiQwenImageEditPlusTrainerRequestsByRequestIdCancelResponse =
  PutFalAiQwenImageEditPlusTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiQwenImageEditPlusTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiQwenImageEditPlusTrainerData = {
  body: QwenImageEditPlusTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/qwen-image-edit-plus-trainer";
};

export type PostFalAiQwenImageEditPlusTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiQwenImageEditPlusTrainerResponse =
  PostFalAiQwenImageEditPlusTrainerResponses[keyof PostFalAiQwenImageEditPlusTrainerResponses];

export type GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-edit-plus-trainer/requests/{request_id}";
};

export type GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: QwenImageEditPlusTrainerOutput;
};

export type GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdResponse =
  GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdResponses[keyof GetFalAiQwenImageEditPlusTrainerRequestsByRequestIdResponses];

export type GetFalAiQwenImageEditTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/qwen-image-edit-trainer/requests/{request_id}/status";
};

export type GetFalAiQwenImageEditTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiQwenImageEditTrainerRequestsByRequestIdStatusResponse =
  GetFalAiQwenImageEditTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiQwenImageEditTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiQwenImageEditTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-edit-trainer/requests/{request_id}/cancel";
};

export type PutFalAiQwenImageEditTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiQwenImageEditTrainerRequestsByRequestIdCancelResponse =
  PutFalAiQwenImageEditTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiQwenImageEditTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiQwenImageEditTrainerData = {
  body: QwenImageEditTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/qwen-image-edit-trainer";
};

export type PostFalAiQwenImageEditTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiQwenImageEditTrainerResponse =
  PostFalAiQwenImageEditTrainerResponses[keyof PostFalAiQwenImageEditTrainerResponses];

export type GetFalAiQwenImageEditTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-edit-trainer/requests/{request_id}";
};

export type GetFalAiQwenImageEditTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: QwenImageEditTrainerOutput;
};

export type GetFalAiQwenImageEditTrainerRequestsByRequestIdResponse =
  GetFalAiQwenImageEditTrainerRequestsByRequestIdResponses[keyof GetFalAiQwenImageEditTrainerRequestsByRequestIdResponses];

export type GetFalAiQwenImageTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/qwen-image-trainer/requests/{request_id}/status";
};

export type GetFalAiQwenImageTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiQwenImageTrainerRequestsByRequestIdStatusResponse =
  GetFalAiQwenImageTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiQwenImageTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiQwenImageTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-trainer/requests/{request_id}/cancel";
};

export type PutFalAiQwenImageTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiQwenImageTrainerRequestsByRequestIdCancelResponse =
  PutFalAiQwenImageTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiQwenImageTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiQwenImageTrainerData = {
  body: QwenImageTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/qwen-image-trainer";
};

export type PostFalAiQwenImageTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiQwenImageTrainerResponse =
  PostFalAiQwenImageTrainerResponses[keyof PostFalAiQwenImageTrainerResponses];

export type GetFalAiQwenImageTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/qwen-image-trainer/requests/{request_id}";
};

export type GetFalAiQwenImageTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: QwenImageTrainerOutput;
};

export type GetFalAiQwenImageTrainerRequestsByRequestIdResponse =
  GetFalAiQwenImageTrainerRequestsByRequestIdResponses[keyof GetFalAiQwenImageTrainerRequestsByRequestIdResponses];

export type GetFalAiWan22ImageTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-22-image-trainer/requests/{request_id}/status";
};

export type GetFalAiWan22ImageTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWan22ImageTrainerRequestsByRequestIdStatusResponse =
  GetFalAiWan22ImageTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiWan22ImageTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiWan22ImageTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-image-trainer/requests/{request_id}/cancel";
};

export type PutFalAiWan22ImageTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWan22ImageTrainerRequestsByRequestIdCancelResponse =
  PutFalAiWan22ImageTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiWan22ImageTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiWan22ImageTrainerData = {
  body: Wan22ImageTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-22-image-trainer";
};

export type PostFalAiWan22ImageTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWan22ImageTrainerResponse =
  PostFalAiWan22ImageTrainerResponses[keyof PostFalAiWan22ImageTrainerResponses];

export type GetFalAiWan22ImageTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-22-image-trainer/requests/{request_id}";
};

export type GetFalAiWan22ImageTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: Wan22ImageTrainerOutput;
};

export type GetFalAiWan22ImageTrainerRequestsByRequestIdResponse =
  GetFalAiWan22ImageTrainerRequestsByRequestIdResponses[keyof GetFalAiWan22ImageTrainerRequestsByRequestIdResponses];

export type GetFalAiWanTrainerT2vRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-trainer/t2v/requests/{request_id}/status";
};

export type GetFalAiWanTrainerT2vRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanTrainerT2vRequestsByRequestIdStatusResponse =
  GetFalAiWanTrainerT2vRequestsByRequestIdStatusResponses[keyof GetFalAiWanTrainerT2vRequestsByRequestIdStatusResponses];

export type PutFalAiWanTrainerT2vRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/t2v/requests/{request_id}/cancel";
};

export type PutFalAiWanTrainerT2vRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanTrainerT2vRequestsByRequestIdCancelResponse =
  PutFalAiWanTrainerT2vRequestsByRequestIdCancelResponses[keyof PutFalAiWanTrainerT2vRequestsByRequestIdCancelResponses];

export type PostFalAiWanTrainerT2vData = {
  body: WanTrainerT2vInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-trainer/t2v";
};

export type PostFalAiWanTrainerT2vResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanTrainerT2vResponse =
  PostFalAiWanTrainerT2vResponses[keyof PostFalAiWanTrainerT2vResponses];

export type GetFalAiWanTrainerT2vRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/t2v/requests/{request_id}";
};

export type GetFalAiWanTrainerT2vRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanTrainerT2vOutput;
};

export type GetFalAiWanTrainerT2vRequestsByRequestIdResponse =
  GetFalAiWanTrainerT2vRequestsByRequestIdResponses[keyof GetFalAiWanTrainerT2vRequestsByRequestIdResponses];

export type GetFalAiWanTrainerT2V14bRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-trainer/t2v-14b/requests/{request_id}/status";
};

export type GetFalAiWanTrainerT2V14bRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanTrainerT2V14bRequestsByRequestIdStatusResponse =
  GetFalAiWanTrainerT2V14bRequestsByRequestIdStatusResponses[keyof GetFalAiWanTrainerT2V14bRequestsByRequestIdStatusResponses];

export type PutFalAiWanTrainerT2V14bRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/t2v-14b/requests/{request_id}/cancel";
};

export type PutFalAiWanTrainerT2V14bRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanTrainerT2V14bRequestsByRequestIdCancelResponse =
  PutFalAiWanTrainerT2V14bRequestsByRequestIdCancelResponses[keyof PutFalAiWanTrainerT2V14bRequestsByRequestIdCancelResponses];

export type PostFalAiWanTrainerT2V14bData = {
  body: WanTrainerT2V14bInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-trainer/t2v-14b";
};

export type PostFalAiWanTrainerT2V14bResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanTrainerT2V14bResponse =
  PostFalAiWanTrainerT2V14bResponses[keyof PostFalAiWanTrainerT2V14bResponses];

export type GetFalAiWanTrainerT2V14bRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/t2v-14b/requests/{request_id}";
};

export type GetFalAiWanTrainerT2V14bRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanTrainerT2V14bOutput;
};

export type GetFalAiWanTrainerT2V14bRequestsByRequestIdResponse =
  GetFalAiWanTrainerT2V14bRequestsByRequestIdResponses[keyof GetFalAiWanTrainerT2V14bRequestsByRequestIdResponses];

export type GetFalAiWanTrainerI2V720pRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-trainer/i2v-720p/requests/{request_id}/status";
};

export type GetFalAiWanTrainerI2V720pRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanTrainerI2V720pRequestsByRequestIdStatusResponse =
  GetFalAiWanTrainerI2V720pRequestsByRequestIdStatusResponses[keyof GetFalAiWanTrainerI2V720pRequestsByRequestIdStatusResponses];

export type PutFalAiWanTrainerI2V720pRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/i2v-720p/requests/{request_id}/cancel";
};

export type PutFalAiWanTrainerI2V720pRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanTrainerI2V720pRequestsByRequestIdCancelResponse =
  PutFalAiWanTrainerI2V720pRequestsByRequestIdCancelResponses[keyof PutFalAiWanTrainerI2V720pRequestsByRequestIdCancelResponses];

export type PostFalAiWanTrainerI2V720pData = {
  body: WanTrainerI2V720pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-trainer/i2v-720p";
};

export type PostFalAiWanTrainerI2V720pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanTrainerI2V720pResponse =
  PostFalAiWanTrainerI2V720pResponses[keyof PostFalAiWanTrainerI2V720pResponses];

export type GetFalAiWanTrainerI2V720pRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/i2v-720p/requests/{request_id}";
};

export type GetFalAiWanTrainerI2V720pRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanTrainerI2V720pOutput;
};

export type GetFalAiWanTrainerI2V720pRequestsByRequestIdResponse =
  GetFalAiWanTrainerI2V720pRequestsByRequestIdResponses[keyof GetFalAiWanTrainerI2V720pRequestsByRequestIdResponses];

export type GetFalAiWanTrainerFlf2V720pRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-trainer/flf2v-720p/requests/{request_id}/status";
};

export type GetFalAiWanTrainerFlf2V720pRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanTrainerFlf2V720pRequestsByRequestIdStatusResponse =
  GetFalAiWanTrainerFlf2V720pRequestsByRequestIdStatusResponses[keyof GetFalAiWanTrainerFlf2V720pRequestsByRequestIdStatusResponses];

export type PutFalAiWanTrainerFlf2V720pRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/flf2v-720p/requests/{request_id}/cancel";
};

export type PutFalAiWanTrainerFlf2V720pRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanTrainerFlf2V720pRequestsByRequestIdCancelResponse =
  PutFalAiWanTrainerFlf2V720pRequestsByRequestIdCancelResponses[keyof PutFalAiWanTrainerFlf2V720pRequestsByRequestIdCancelResponses];

export type PostFalAiWanTrainerFlf2V720pData = {
  body: WanTrainerFlf2V720pInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-trainer/flf2v-720p";
};

export type PostFalAiWanTrainerFlf2V720pResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanTrainerFlf2V720pResponse =
  PostFalAiWanTrainerFlf2V720pResponses[keyof PostFalAiWanTrainerFlf2V720pResponses];

export type GetFalAiWanTrainerFlf2V720pRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/flf2v-720p/requests/{request_id}";
};

export type GetFalAiWanTrainerFlf2V720pRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanTrainerFlf2V720pOutput;
};

export type GetFalAiWanTrainerFlf2V720pRequestsByRequestIdResponse =
  GetFalAiWanTrainerFlf2V720pRequestsByRequestIdResponses[keyof GetFalAiWanTrainerFlf2V720pRequestsByRequestIdResponses];

export type GetFalAiLtxVideoTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/ltx-video-trainer/requests/{request_id}/status";
};

export type GetFalAiLtxVideoTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiLtxVideoTrainerRequestsByRequestIdStatusResponse =
  GetFalAiLtxVideoTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiLtxVideoTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiLtxVideoTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-trainer/requests/{request_id}/cancel";
};

export type PutFalAiLtxVideoTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiLtxVideoTrainerRequestsByRequestIdCancelResponse =
  PutFalAiLtxVideoTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiLtxVideoTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiLtxVideoTrainerData = {
  body: LtxVideoTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/ltx-video-trainer";
};

export type PostFalAiLtxVideoTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiLtxVideoTrainerResponse =
  PostFalAiLtxVideoTrainerResponses[keyof PostFalAiLtxVideoTrainerResponses];

export type GetFalAiLtxVideoTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/ltx-video-trainer/requests/{request_id}";
};

export type GetFalAiLtxVideoTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: LtxVideoTrainerOutput;
};

export type GetFalAiLtxVideoTrainerRequestsByRequestIdResponse =
  GetFalAiLtxVideoTrainerRequestsByRequestIdResponses[keyof GetFalAiLtxVideoTrainerRequestsByRequestIdResponses];

export type GetFalAiRecraftV3CreateStyleRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/recraft/v3/create-style/requests/{request_id}/status";
};

export type GetFalAiRecraftV3CreateStyleRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiRecraftV3CreateStyleRequestsByRequestIdStatusResponse =
  GetFalAiRecraftV3CreateStyleRequestsByRequestIdStatusResponses[keyof GetFalAiRecraftV3CreateStyleRequestsByRequestIdStatusResponses];

export type PutFalAiRecraftV3CreateStyleRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/recraft/v3/create-style/requests/{request_id}/cancel";
};

export type PutFalAiRecraftV3CreateStyleRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiRecraftV3CreateStyleRequestsByRequestIdCancelResponse =
  PutFalAiRecraftV3CreateStyleRequestsByRequestIdCancelResponses[keyof PutFalAiRecraftV3CreateStyleRequestsByRequestIdCancelResponses];

export type PostFalAiRecraftV3CreateStyleData = {
  body: RecraftV3CreateStyleInput;
  path?: never;
  query?: never;
  url: "/fal-ai/recraft/v3/create-style";
};

export type PostFalAiRecraftV3CreateStyleResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiRecraftV3CreateStyleResponse =
  PostFalAiRecraftV3CreateStyleResponses[keyof PostFalAiRecraftV3CreateStyleResponses];

export type GetFalAiRecraftV3CreateStyleRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/recraft/v3/create-style/requests/{request_id}";
};

export type GetFalAiRecraftV3CreateStyleRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: RecraftV3CreateStyleOutput;
};

export type GetFalAiRecraftV3CreateStyleRequestsByRequestIdResponse =
  GetFalAiRecraftV3CreateStyleRequestsByRequestIdResponses[keyof GetFalAiRecraftV3CreateStyleRequestsByRequestIdResponses];

export type GetFalAiTurboFluxTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/turbo-flux-trainer/requests/{request_id}/status";
};

export type GetFalAiTurboFluxTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiTurboFluxTrainerRequestsByRequestIdStatusResponse =
  GetFalAiTurboFluxTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiTurboFluxTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiTurboFluxTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/turbo-flux-trainer/requests/{request_id}/cancel";
};

export type PutFalAiTurboFluxTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiTurboFluxTrainerRequestsByRequestIdCancelResponse =
  PutFalAiTurboFluxTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiTurboFluxTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiTurboFluxTrainerData = {
  body: TurboFluxTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/turbo-flux-trainer";
};

export type PostFalAiTurboFluxTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiTurboFluxTrainerResponse =
  PostFalAiTurboFluxTrainerResponses[keyof PostFalAiTurboFluxTrainerResponses];

export type GetFalAiTurboFluxTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/turbo-flux-trainer/requests/{request_id}";
};

export type GetFalAiTurboFluxTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: TurboFluxTrainerOutput;
};

export type GetFalAiTurboFluxTrainerRequestsByRequestIdResponse =
  GetFalAiTurboFluxTrainerRequestsByRequestIdResponses[keyof GetFalAiTurboFluxTrainerRequestsByRequestIdResponses];

export type GetFalAiWanTrainerRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/wan-trainer/requests/{request_id}/status";
};

export type GetFalAiWanTrainerRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type GetFalAiWanTrainerRequestsByRequestIdStatusResponse =
  GetFalAiWanTrainerRequestsByRequestIdStatusResponses[keyof GetFalAiWanTrainerRequestsByRequestIdStatusResponses];

export type PutFalAiWanTrainerRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/requests/{request_id}/cancel";
};

export type PutFalAiWanTrainerRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean;
  };
};

export type PutFalAiWanTrainerRequestsByRequestIdCancelResponse =
  PutFalAiWanTrainerRequestsByRequestIdCancelResponses[keyof PutFalAiWanTrainerRequestsByRequestIdCancelResponses];

export type PostFalAiWanTrainerData = {
  body: WanTrainerInput;
  path?: never;
  query?: never;
  url: "/fal-ai/wan-trainer";
};

export type PostFalAiWanTrainerResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiWanTrainerResponse =
  PostFalAiWanTrainerResponses[keyof PostFalAiWanTrainerResponses];

export type GetFalAiWanTrainerRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/wan-trainer/requests/{request_id}";
};

export type GetFalAiWanTrainerRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: WanTrainerOutput;
};

export type GetFalAiWanTrainerRequestsByRequestIdResponse =
  GetFalAiWanTrainerRequestsByRequestIdResponses[keyof GetFalAiWanTrainerRequestsByRequestIdResponses];

export type GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdStatusData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number;
  };
  url: "/fal-ai/hunyuan-video-lora-training/requests/{request_id}/status";
};

export type GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: QueueStatus;
  };

export type GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdStatusResponse =
  GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdStatusResponses[keyof GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdStatusResponses];

export type PutFalAiHunyuanVideoLoraTrainingRequestsByRequestIdCancelData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-lora-training/requests/{request_id}/cancel";
};

export type PutFalAiHunyuanVideoLoraTrainingRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean;
    };
  };

export type PutFalAiHunyuanVideoLoraTrainingRequestsByRequestIdCancelResponse =
  PutFalAiHunyuanVideoLoraTrainingRequestsByRequestIdCancelResponses[keyof PutFalAiHunyuanVideoLoraTrainingRequestsByRequestIdCancelResponses];

export type PostFalAiHunyuanVideoLoraTrainingData = {
  body: HunyuanVideoLoraTrainingInput;
  path?: never;
  query?: never;
  url: "/fal-ai/hunyuan-video-lora-training";
};

export type PostFalAiHunyuanVideoLoraTrainingResponses = {
  /**
   * The request status.
   */
  200: QueueStatus;
};

export type PostFalAiHunyuanVideoLoraTrainingResponse =
  PostFalAiHunyuanVideoLoraTrainingResponses[keyof PostFalAiHunyuanVideoLoraTrainingResponses];

export type GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdData = {
  body?: never;
  path: {
    /**
     * Request ID
     */
    request_id: string;
  };
  query?: never;
  url: "/fal-ai/hunyuan-video-lora-training/requests/{request_id}";
};

export type GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: HunyuanVideoLoraTrainingOutput;
};

export type GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdResponse =
  GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdResponses[keyof GetFalAiHunyuanVideoLoraTrainingRequestsByRequestIdResponses];
